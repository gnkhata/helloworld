{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnkhata/helloworld/blob/main/Copy_of_RAGatouille_Dataset2_AllBatch_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYVG51WIHYf0"
      },
      "source": [
        "# Load Collection to Index\n",
        "\n",
        "Must be in 'tsv' file format where first column is docID/pid and second column is text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-_ITtgmKqWl",
        "outputId": "e1ff4c28-fc0e-4b98-d6e7-1f89646bc502",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ragatouille\n",
            "  Downloading RAGatouille-0.0.9-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting llama-index (from ragatouille)\n",
            "  Downloading llama_index-0.12.17-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting faiss-cpu (from ragatouille)\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.3.33)\n",
            "Collecting colbert-ai>=0.2.19 (from ragatouille)\n",
            "  Downloading colbert_ai-0.2.21-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.3.17)\n",
            "Collecting onnx (from ragatouille)\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.11/dist-packages (from ragatouille) (2.5.1)\n",
            "Collecting voyager (from ragatouille)\n",
            "  Downloading voyager-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (2.5.1+cu124)\n",
            "Collecting fast-pytorch-kmeans (from ragatouille)\n",
            "  Downloading fast_pytorch_kmeans-0.2.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from ragatouille) (3.4.1)\n",
            "Collecting bitarray (from colbert-ai>=0.2.19->ragatouille)\n",
            "  Downloading bitarray-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Collecting datasets (from colbert-ai>=0.2.19->ragatouille)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (3.1.0)\n",
            "Collecting git-python (from colbert-ai>=0.2.19->ragatouille)\n",
            "  Downloading git_python-1.0.3-py2.py3-none-any.whl.metadata (331 bytes)\n",
            "Collecting python-dotenv (from colbert-ai>=0.2.19->ragatouille)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ninja (from colbert-ai>=0.2.19->ragatouille)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (4.48.2)\n",
            "Collecting ujson (from colbert-ai>=0.2.19->ragatouille)\n",
            "  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13->ragatouille)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13->ragatouille)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13->ragatouille)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13->ragatouille)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13->ragatouille)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13->ragatouille)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13->ragatouille)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13->ragatouille)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13->ragatouille)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13->ragatouille)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->ragatouille) (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille) (24.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (3.11.12)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core->ragatouille) (1.33)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index->ragatouille)\n",
            "  Downloading llama_index_agent_openai-0.4.5-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index->ragatouille)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.17 (from llama-index->ragatouille)\n",
            "  Downloading llama_index_core-0.12.17-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index->ragatouille)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index->ragatouille)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index->ragatouille)\n",
            "  Downloading llama_index_llms_openai-0.3.19-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index->ragatouille)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index->ragatouille)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index->ragatouille)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index->ragatouille)\n",
            "  Downloading llama_index_readers_file-0.4.5-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index->ragatouille)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (3.9.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx->ragatouille) (4.25.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille) (11.1.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from srsly->ragatouille) (2.0.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille) (1.18.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core->ragatouille) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragatouille) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragatouille) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragatouille) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragatouille) (0.23.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->ragatouille) (1.61.1)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.17->llama-index->ragatouille)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index->ragatouille) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.17->llama-index->ragatouille)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.17->llama-index->ragatouille)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index->ragatouille) (1.6.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.17->llama-index->ragatouille)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.17->llama-index->ragatouille)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index->ragatouille) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.8 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille)\n",
            "  Downloading llama_cloud-0.1.12-py3-none-any.whl.metadata (851 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ragatouille) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ragatouille) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ragatouille)\n",
            "  Downloading pypdf-5.3.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ragatouille)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille)\n",
            "  Downloading llama_parse-0.6.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragatouille) (3.1.1)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille) (0.5.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->colbert-ai>=0.2.19->ragatouille)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->colbert-ai>=0.2.19->ragatouille)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->colbert-ai>=0.2.19->ragatouille)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch>=1.13->ragatouille)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->ragatouille) (3.0.2)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from git-python->colbert-ai>=0.2.19->ragatouille) (3.1.44)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->ragatouille) (3.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ragatouille) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->ragatouille) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->ragatouille) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->ragatouille) (0.14.0)\n",
            "Collecting llama-cloud-services>=0.6.1 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille)\n",
            "  Downloading llama_cloud_services-0.6.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->ragatouille) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->ragatouille) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->ragatouille) (1.3.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index->ragatouille)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.17->llama-index->ragatouille)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->git-python->colbert-ai>=0.2.19->ragatouille) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ragatouille) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ragatouille) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ragatouille) (2025.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai>=0.2.19->ragatouille) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ragatouille) (1.17.0)\n",
            "Downloading RAGatouille-0.0.9-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colbert_ai-0.2.21-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.1/116.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fast_pytorch_kmeans-0.2.2-py3-none-any.whl (9.8 kB)\n",
            "Downloading llama_index-0.12.17-py3-none-any.whl (7.0 kB)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading voyager-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.4.5-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.12.17-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_llms_openai-0.3.19-py3-none-any.whl (15 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.5-py3-none-any.whl (39 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading bitarray-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.12-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.1-py3-none-any.whl (4.8 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.3.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_cloud_services-0.6.1-py3-none-any.whl (22 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, bitarray, xxhash, voyager, ujson, python-dotenv, pypdf, onnx, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, mypy-extensions, marshmallow, fsspec, faiss-cpu, dill, typing-inspect, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, llama-cloud, git-python, dataclasses-json, llama-index-core, datasets, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, fast-pytorch-kmeans, colbert-ai, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index, ragatouille\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitarray-3.0.0 colbert-ai-0.2.21 dataclasses-json-0.6.7 datasets-3.2.0 dill-0.3.8 dirtyjson-1.0.8 faiss-cpu-1.10.0 fast-pytorch-kmeans-0.2.2 filetype-1.2.0 fsspec-2024.9.0 git-python-1.0.3 llama-cloud-0.1.12 llama-cloud-services-0.6.1 llama-index-0.12.17 llama-index-agent-openai-0.4.5 llama-index-cli-0.4.0 llama-index-core-0.12.17 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.4 llama-index-llms-openai-0.3.19 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.5 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.1 marshmallow-3.26.1 multiprocess-0.70.16 mypy-extensions-1.0.0 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnx-1.17.0 pypdf-5.3.0 python-dotenv-1.0.1 ragatouille-0.0.9 striprtf-0.0.26 tiktoken-0.8.0 typing-inspect-0.9.0 ujson-5.10.0 voyager-2.1.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ragatouille"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOTDYy4jMksw",
        "outputId": "3247c8c7-8a98-4358-9322-c4d4c4079de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        docID                                              title  \\\n",
            "0  1966542038  Uplink sum-rate analysis of multi-cell multi-u...   \n",
            "1  1966543040  Cardiac electromechanical models: from cell to...   \n",
            "2  1966556005                                                NaN   \n",
            "3  1966558473  VeSense: High-performance and energy-efficient...   \n",
            "4   832120473  Automating Hint Generation with Solution Space...   \n",
            "\n",
            "                                                text  \\\n",
            "0  In this paper, the uplink sum-rate of the mult...   \n",
            "1  The heart is a multiphysics and multiscale sys...   \n",
            "2  : The purpose of this study is to evaluate whe...   \n",
            "3  a b s t r a c t Although vehicular sensing whe...   \n",
            "4  Developing intelligent tutoring systems from s...   \n",
            "\n",
            "                                            keywords  \n",
            "0  correlated rayleigh fading channels, the multi...  \n",
            "1  organ models, myofilament models, em models, s...  \n",
            "2  prostate cancer patients, pelvic lymph nodes, ...  \n",
            "3  vehicular sensing, various vehicular sensing s...  \n",
            "4  solution states, intermediate solution states,...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "collection = pd.read_csv('bm25_docs_all.tsv', sep='\\t')\n",
        "\n",
        "print(collection.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVD4bFsCIIJv"
      },
      "source": [
        "**Divide tsv file into separate lists for text and docIDs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kWjgJvhcF4GC"
      },
      "outputs": [],
      "source": [
        "collection_list = collection['text'].tolist()\n",
        "collection_ids = collection['docID'].astype(str).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8NovG9tLtwE",
        "outputId": "a0eed871-f32c-47cb-9b8b-9bacc67ed17a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3903\n",
            "3903\n"
          ]
        }
      ],
      "source": [
        "print(len(collection_list))\n",
        "print(len(collection_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if1ljomKIEY0"
      },
      "source": [
        "# Create Index\n",
        "\n",
        "(if not already created)\n",
        "\n",
        "Pass text content and docIDs as parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3e501fa4e575439398fc060c912649b0",
            "b1916431cac94c2c9159e5db76d76cb8",
            "98ad3f183267462aa28a2b0d5bfa2b9e",
            "e65e402ca7a844898950944fa93a03b8",
            "4bd2c4cf05a841ba9d67548250d0176b",
            "c6abb15809ee49a0bdebe6c521c87e58",
            "a03c6b9f60d64bb392b15cd161392bf2",
            "3f3e6b6e7c844fa38532b53f0675b5d2",
            "90bd898753a947ecafaff8ca9300781e",
            "2c225aea8f194c1fb81128e7e98a4530",
            "2cb6e6c0db874823b6013ba8bd07ce66",
            "b324cead5d914b6e836e1ce1bed5bfff",
            "36c8a996c9e14ae197776548a5fd31df",
            "d3af08638a454ab6829746a901494578",
            "6a1ebfc60ef248f59612bf9631baee4d",
            "3afd6b15fb4c4970b445ee2a7e6130b7",
            "dfc43905d92f4b878f1524906295231b",
            "9bf0e035ba184a5fafcd57aacab2a851",
            "4ca68e0e2dce48c3900a6770a4e524f0",
            "9eb5e71513e4400ebb7b071dba8bd880",
            "9bab35a255de4638b95bd373ddef2d34",
            "df413cb0a2104891847c41360173e057",
            "afd20f988f454e598941f54f1c55c1dc",
            "92a5afd444a94513b6b95d21bf79a8e1",
            "04c95094b6c14bc6811b0a52e3d0ee0d",
            "4a2e5004a3094c5d8db6afbcc35e2cfc",
            "ae470a3f348f4342a9df26a3e29167ab",
            "1f86b6e2fb924472b1ed88380df96b18",
            "a4cb158700f5462e8d16a92d4aa73a4f",
            "a44b8e82126c47a998385cc1a083e433",
            "b34f7a856380418fad6b31b2ef296f60",
            "ac28c379700047b78efa4bf7aa1b7abe",
            "4704667bd5d54e8aa4234c7e456d9a99",
            "1d6ca87f55c2483c8f0e6c96905fb227",
            "840d25354d1549e58bcf9f0d94078ac8",
            "6d01e4dfc63644dab3b8d9ff316e0afa",
            "5638f9c221d54983aed87b6f158d4e6e",
            "5c67c00d6bc14f269e72e8840d2779e6",
            "d7b2578dcd794d248626cfbd880c50dc",
            "d02b28af06ff4008ab2b5e7929bd4137",
            "6aa18fb275f44b569a4e96743b15a5cb",
            "e16cae277e5743b18bf215ae8dfb6314",
            "80513a6c1777440ab331d3b5a6f5834f",
            "05f1916e2c1a472fbca9a907f7a2cfe2",
            "ba9e773f3a864f438dcfe3bf5296e521",
            "604582f442554eef861a8ca95f22cf29",
            "78a0a05e57a34ed3b7d71369307e9617",
            "e1998397075c4da193b4f1293c1231f2",
            "a2f4229d5c3c43c29992994602b402f2",
            "49b62e2727684d7baa930e9d24e43052",
            "60929da74ba04fbf909d80db44e43714",
            "3099ff8ac3bc496188b5d97eadcaba67",
            "4ae694d7601949a787313f7157e664a2",
            "d29f22283eb748a5ac24d08af3ebb68c",
            "152bd15c232249f2800e016f264b9fc6",
            "948c68f8e17449fea1bcfaf56cfbac78",
            "7852479fa56f41b0a337e2753c0ef167",
            "8adbfb374b6e41b599a81188e5dede67",
            "d3bd4b6b399e47da97931ca624e66de8",
            "510959e4f2f9478d8f1455107f7bf790",
            "639cbb2c753b472db88536c19a6d2beb",
            "d11a4d12be1949b38af153535e891b6e",
            "ef7ad9a97f9e4f909a5a2888c9b6ef93",
            "367918d1de85452389541f507c73b42c",
            "e71e43d786f743058b34e7515b072dbe",
            "d0266b33255e47e29a1c8bb1306d9a03",
            "44b1bf96d5334d0ba17d59633c1145bc",
            "2c3cf946e3dd461fa472adf5a90b482d",
            "13d782ef3d3b4d0d9521af0124d81a58",
            "98b7f292364742f2907b3ecabdfb8b1d",
            "600388b80d2c4090ac5d8ee573aa8715",
            "b8e3aac63feb4a49b47b6b064c95948b",
            "58bc23b25ed145a28c8469fa2324ecc5",
            "ed42a47db7384a168814c878ebf21eab",
            "29718f8b5b93491ba4dcbc6d942a183c",
            "921dffef62f34e64949a3caee34f1254",
            "a616e2943d7c4db9bfc02bdc8e98e2b7"
          ]
        },
        "collapsed": true,
        "id": "jEhu2jZYLDOQ",
        "outputId": "249e3849-4d92-443e-f2bd-33f5f689ebc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e501fa4e575439398fc060c912649b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b324cead5d914b6e836e1ce1bed5bfff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afd20f988f454e598941f54f1c55c1dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d6ca87f55c2483c8f0e6c96905fb227"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba9e773f3a864f438dcfe3bf5296e521"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "948c68f8e17449fea1bcfaf56cfbac78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44b1bf96d5334d0ba17d59633c1145bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
            "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
            "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
            "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
            "--------------------\n",
            "\n",
            "\n",
            "[Feb 13, 17:03:33] #> Creating directory .ragatouille/colbert/indexes/index-2 \n",
            "\n",
            "\n",
            "[Feb 13, 17:03:34] [0] \t\t #> Encoding 4914 passages..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 13, 17:03:54] [0] \t\t avg_doclen_est = 164.08934020996094 \t len(local_sample) = 4,914\n",
            "[Feb 13, 17:03:54] [0] \t\t Creating 8,192 partitions.\n",
            "[Feb 13, 17:03:54] [0] \t\t *Estimated* 806,335 embeddings.\n",
            "[Feb 13, 17:03:54] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/index-2/plan.json ..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/colbert/indexing/collection_indexer.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  sub_sample = torch.load(sub_sample_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch-based indexing did not succeed with error: CUDA out of memory. Tried to allocate 23.38 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.46 GiB is free. Process 2502 has 13.27 GiB memory in use. Of the allocated memory 7.05 GiB is allocated by PyTorch, and 6.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ! Reverting to using FAISS and attempting again...\n",
            "________________________________________________________________________________\n",
            "WARNING! You have a GPU available, but only `faiss-cpu` is currently installed.\n",
            " This means that indexing will be slow. To make use of your GPU.\n",
            "Please install `faiss-gpu` by running:\n",
            "pip uninstall --y faiss-cpu & pip install faiss-gpu\n",
            " ________________________________________________________________________________\n",
            "Will continue with CPU indexing in 5 seconds...\n",
            "\n",
            "\n",
            "[Feb 13, 17:04:01] #> Note: Output directory .ragatouille/colbert/indexes/index-2 already exists\n",
            "\n",
            "\n",
            "[Feb 13, 17:04:01] #> Will delete 1 files already at .ragatouille/colbert/indexes/index-2 in 20 seconds...\n",
            "[Feb 13, 17:04:22] [0] \t\t #> Encoding 4914 passages..\n",
            "[Feb 13, 17:04:41] [0] \t\t avg_doclen_est = 164.08934020996094 \t len(local_sample) = 4,914\n",
            "[Feb 13, 17:04:41] [0] \t\t Creating 8,192 partitions.\n",
            "[Feb 13, 17:04:41] [0] \t\t *Estimated* 806,335 embeddings.\n",
            "[Feb 13, 17:04:41] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/index-2/plan.json ..\n",
            "[Feb 13, 17:12:10] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 13, 17:13:30] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.034, 0.036, 0.035, 0.031, 0.032, 0.037, 0.035, 0.032, 0.033, 0.033, 0.033, 0.033, 0.034, 0.036, 0.032, 0.036, 0.03, 0.032, 0.031, 0.032, 0.034, 0.035, 0.032, 0.034, 0.032, 0.033, 0.035, 0.032, 0.034, 0.034, 0.033, 0.037, 0.035, 0.033, 0.033, 0.03, 0.036, 0.033, 0.033, 0.039, 0.034, 0.034, 0.033, 0.036, 0.033, 0.032, 0.033, 0.037, 0.035, 0.033, 0.032, 0.034, 0.037, 0.035, 0.033, 0.033, 0.041, 0.034, 0.04, 0.034, 0.031, 0.037, 0.035, 0.034, 0.035, 0.036, 0.035, 0.034, 0.033, 0.032, 0.037, 0.031, 0.034, 0.037, 0.034, 0.034, 0.038, 0.035, 0.035, 0.038, 0.036, 0.033, 0.035, 0.036, 0.032, 0.034, 0.033, 0.034, 0.032, 0.036, 0.033, 0.037, 0.033, 0.037, 0.034, 0.034, 0.036, 0.032, 0.034, 0.035, 0.034, 0.037, 0.034, 0.033, 0.035, 0.033, 0.033, 0.033, 0.034, 0.033, 0.037, 0.035, 0.034, 0.033, 0.036, 0.032, 0.036, 0.034, 0.032, 0.035, 0.032, 0.034, 0.034, 0.036, 0.031, 0.038, 0.035, 0.032]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/colbert/indexing/codecs/residual.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  centroids = torch.load(centroids_path, map_location='cpu')\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/indexing/codecs/residual.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  avg_residual = torch.load(avgresidual_path, map_location='cpu')\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/indexing/codecs/residual.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bucket_cutoffs, bucket_weights = torch.load(buckets_path, map_location='cpu')\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 13, 17:14:46] [0] \t\t #> Encoding 4914 passages..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "1it [00:19, 19.13s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/indexing/codecs/residual_embeddings.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(codes_path, map_location='cpu')\n",
            "100%|██████████| 1/1 [00:00<00:00, 219.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 13, 17:15:05] #> Optimizing IVF to store map from centroids to list of pids..\n",
            "[Feb 13, 17:15:05] #> Building the emb2pid mapping..\n",
            "[Feb 13, 17:15:05] len(emb2pid) = 806335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 8192/8192 [00:00<00:00, 54847.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 13, 17:15:05] #> Saved optimized IVF to .ragatouille/colbert/indexes/index-2/ivf.pid.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done indexing!\n"
          ]
        }
      ],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from ragatouille.utils import get_wikipedia_page\n",
        "\n",
        "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "index_path = RAG.index(index_name=\"index-2\", collection=collection_list, document_ids=collection_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POmcqI6MxJdp"
      },
      "source": [
        "## Saving Index Locally\n",
        "\n",
        "To allow re-use later without regeneration of index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "gTI8-awUxL5X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "collapsed": true,
        "outputId": "46ef300d-c896-45e6-909f-fb5fa8415601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: .ragatouille/colbert/indexes/index-2/ (stored 0%)\n",
            "updating: .ragatouille/colbert/indexes/index-2/plan.json (deflated 59%)\n",
            "updating: .ragatouille/colbert/indexes/index-2/centroids.pt (deflated 8%)\n",
            "updating: .ragatouille/colbert/indexes/index-2/0.codes.pt (deflated 60%)\n",
            "updating: .ragatouille/colbert/indexes/index-2/ivf.pid.pt (deflated 53%)\n",
            "updating: .ragatouille/colbert/indexes/index-2/buckets.pt (deflated 54%)\n",
            "updating: .ragatouille/colbert/indexes/index-2/collection.json (deflated 68%)\n",
            "updating: .ragatouille/colbert/indexes/index-2/pid_docid_map.json (deflated 72%)\n",
            "updating: .ragatouille/colbert/indexes/index-2/0.metadata.json (deflated 38%)\n",
            "updating: .ragatouille/colbert/indexes/index-2/0.residuals.pt (deflated 0%)\n",
            "updating: .ragatouille/colbert/indexes/index-2/metadata.json (deflated 57%)\n",
            "updating: .ragatouille/colbert/indexes/index-2/avg_residual.pt (deflated 60%)\n",
            "updating: .ragatouille/colbert/indexes/index-2/doclens.0.json (deflated 64%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_62d9a8ec-e2a9-4d17-8fcc-a9a658ad66bf\", \"index-2.zip\", 57225554)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "index_directory = '.ragatouille/colbert/indexes/index-2'\n",
        "\n",
        "zip_filename = 'index-2.zip'\n",
        "\n",
        "# Compress the directory into a zip file\n",
        "!zip -r {zip_filename} {index_directory}\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQVXtL2qIclu"
      },
      "source": [
        "## Create a RAG object for retrieval by providing path of the index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "W0ULSo0IMHC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e948b0-528e-461f-d54b-44917c664c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "RAG = RAGPretrainedModel.from_index(\".ragatouille/colbert/indexes/index-2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwDwtEtUxdTH"
      },
      "source": [
        "# Loading Existing Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc9WJ6MPRIWl"
      },
      "source": [
        "### Load Index through File Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "VbMsr3pXEpoD"
      },
      "outputs": [],
      "source": [
        "#Load and unzip saved index\n",
        "\n",
        "import zipfile\n",
        "\n",
        "zip_path = '/content/index-2.zip'\n",
        "\n",
        "# Directory to extract the files\n",
        "extract_dir = '/content/index-2'\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3CNHoOqRd7r"
      },
      "source": [
        "### Create RAG Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "peen4gALFMjv"
      },
      "outputs": [],
      "source": [
        "index_path = '/content/index-2/.ragatouille/colbert/indexes/index-2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "collapsed": true,
        "id": "TLpKdpV0yJe9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "RAG = RAGPretrainedModel.from_index(index_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHj5Os_3-HOQ"
      },
      "source": [
        "# Generating User Profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "crHVduNM-OQn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "# Load user_doc_dict.json\n",
        "with open(\"user_doc_dict_all.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    user_doc_dict = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjUH1G5dV4co"
      },
      "source": [
        "### Doc Text as Profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EVqTPcF9-Pbc"
      },
      "outputs": [],
      "source": [
        "# Load document content from user_docs.tsv\n",
        "doc_texts = {}\n",
        "with open(\"user_docs_all.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "    next(reader)  # Skip header\n",
        "\n",
        "    for row in reader:\n",
        "        doc_id, title, text, keywords = row  # Extract document details\n",
        "        doc_texts[doc_id] = text  # Store text content indexed by doc_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsGmRDyvV8gZ"
      },
      "source": [
        "### Doc Keywords as Profile\n",
        "\n",
        "(Optional - gave poor results so exclude from testing!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-qwrOXDcV_mc"
      },
      "outputs": [],
      "source": [
        "# Load document content from user_docs.tsv\n",
        "doc_texts = {}\n",
        "with open(\"user_docs_all.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "    next(reader)  # Skip header\n",
        "\n",
        "    for row in reader:\n",
        "        doc_id, title, text, keywords = row  # Extract document details\n",
        "        doc_texts[doc_id] = keywords  # Store text content indexed by doc_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hdKUYkA5XL0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0940286d-30aa-46f7-c1ad-45c39e1a3dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User profiles created and saved to user_profiles.json\n"
          ]
        }
      ],
      "source": [
        "# Create user profile dictionary\n",
        "user_profiles = {}\n",
        "for user_id, doc_ids in user_doc_dict.items():\n",
        "    user_profiles[user_id] = \" \".join([doc_texts[doc_id] for doc_id in doc_ids if doc_id in doc_texts])\n",
        "\n",
        "# Save user profiles to a JSON file\n",
        "with open(\"user_profiles.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(user_profiles, f, indent=4)\n",
        "\n",
        "print(\"User profiles created and saved to user_profiles.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mbrk2OHs-12Q",
        "outputId": "714ff1e0-b481-4394-b001-075552ba307d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"1767269677\": \"Trajectory data have been used in a variety of studies, including human behavior analysis, transportation management, and wildlife tracking. While each study area introduces a different perspective, they share the need to integrate positioning data with domain-specific information. Semantic annotations are necessary to improve discovery, reuse, and integration of trajectory data from different sources. Consequently, it would be beneficial if the common structure encountered in trajectory data could be annotated based on a shared vocabulary, abstracting from domain-specific aspects. Ontology design patterns are an increasingly popular approach to define such flexible and self-contained building blocks of annotations. They appear more suitable for the annotation of interdisciplinary, multi-thematic, and multi-perspective data than the use of foundational and domain ontologies alone. In this paper, we introduce such an ontology design pattern for semantic trajectories. It was developed as a community effort across multiple disciplines and in a data-driven fashion. We discuss the formalization of the pattern using the Web Ontology Language (OWL) and apply the pattern to two different scenarios, personal travel and wildlife monitoring. Metadata for scientific publications contain various explicit and implicit spatio-temporal references. Data on conference locations as well as author and editor affiliations \\u2013 both changing over time \\u2013 enable insights into the geographic distribution of scientific fields and particular specializations. At the same time, these byproducts of scientific bibliographies offer a great opportunity to integrate data across different bibliographies to get a more complete picture of a domain. In this paper, we demonstrate how the Linked Data paradigm can assist in enriching and integrating such collections. Starting from the bibliographies of the GIScience, COSIT, ACM GIS, and AGILE conference series, we show how to convert the data to Linked Data and integrate the previously separate datasets. We focus on the spatio-temporal aspects and discuss how they help in matching and disambiguating entities such as authors or universities. We introduce a novel user interface to explore the integrated dataset, demonstrating the potential of Linked Data for innovative applications using spatio-temporal information, and discuss how more complex queries can be addressed. While we focus on bibliographies, the presented work is part of the broader vision of a Linked Science infrastructure for e-Science. In this document, we give a high-level overview of selected Semantic (Web) technologies, methods, and other important considerations, that are relevant for the success of EarthCube. The goal of this initial document is to provide entry points and references for discussions between the Semantic Technologies experts and the domain experts within EarthCube. The selected topics are intended to ground the EarthCube roadmap in the state of the art in semantics research and ontology engineering. The Semantic Web journal by IOS Press follows a unique open and transparent process during which each submitted manuscript is available online together with the full history of its successive decision statuses, assigned editors, solicited and voluntary reviewers, their full text reviews, and in many cases also the authors' response letters. Combined with a highly-customized, Drupal-based journal management system, this provides the journal with semantically rich manuscript time lines and networked data about authors, reviewers, and editors. These data are now exposed using a SPARQL endpoint, an extended Bibo ontology, and a modular Linked Data portal that provides interactive scientometrics based on established and new analysis methods. The portal can be customized for other journals as well. Computing user similarity is key for personalized location-based recommender systems and geographic information retrieval. So far, most existing work has focused on structured or semi-structured data to establish such measures. In this work, we propose topic modeling to exploit sparse, unstructured data, e.g., tips and reviews, as an additional feature to compute user similarity. Our model employs diagnosticity weighting based on the entropy of topics in order to assess the role of commonalities and variabilities between similar users. Finally, we offer a validation technique and results using data from the location-based social network Foursquare. Semantic technologies and ontologies play an increasing role in scienti\\ufb01c work\\ufb02ow systems and knowledge infrastructures. While ontologies are mostly used for the semantic annotation of metadata, semantic technologies enable searching metadata catalogs beyond simple keywords, with some early evidence of semantics used for data translation. However, the next generation of distributed and interdisciplinary knowledge infrastructures will require capabilities beyond simple subsumption reasoning over subclass relations. In this work, we report from the EarthCube Semantics Community by highlighting which role semantics and ontologies should play in the EarthCube knowledge infrastructure. We target the interested domain scientist and, thus, introduce the value proposition of semantic technologies in a non-technical language. Finally, we commit ourselves to some guiding principles for the successful implementation and application of semantic technologies and ontologies within EarthCube. The Linked Data paradigm has made significant inroads into research and practice around spatial information and it is time to reflect on what this means for GIScience. Technically, Linked Data is just data in the simplest possible data model (that of triples), allowing for linking records or data sets anywhere across the web using controlled semantics. Conceptually, Linked Data offers radically new ways of thinking about, structuring, publishing, discovering, accessing, and integrating data. It is of particular novelty and value to the producers and users of geographic data, as these are commonly thought to require more complex data models. The paper explains the main innovations brought about by Linked Data and demonstrates them with examples. It concludes that many longstanding problems in GIScience have become approachable in novel ways, while new and more specific research challenges emerge. The Sensor Web provides access to observations and measurements through standardized interfaces defined by the Open Geospa-tial Consortium's Sensor Web Enablement (SWE) initiative. While clients compliant to these standards have access to the generated sensor data, it remains partially hidden from other knowledge infrastructures building on higher-level W3C standards. To overcome this problem, it has been proposed to make sensor data accessible using Linked Data principles and RESTful services. This position paper discusses the embedding of such data into the Linked Data cloud with a focus on the outgoing links that hook them up with other data sources. We outline how such links can be generated in a semi-automatic way, and argue why curation of the links is required. Finally, we point to the query potential of such an additional interface to observation data, and outline the requirements for SPARQL endpoints. Abstract While geocoding returns coordinates for a full or partial address, the converse process of reverse geocoding maps coordinates to a set of candidate place identifiers such as addresses or toponyms. For example, numerous Web APIs map geographic point coordinates, e.g., from a user\\u2019s smartphone, to an ordered set of nearby Places Of Interest (POI). Typically, these services return the k nearest POI within a certain radius and measure distance to order the results. Reverse geocoding is a crucial task for many applications and research questions as it translates between spatial and platial views on geographic location. What makes this process difficult is the uncertainty of the queried location and of the point features used to represent places. Even if both could be determined with a high level of accuracy, it would still be unclear how to map a smartphone\\u2019s GPS fix to one of many possible places in a multi-story building or a shopping mall. In this work, we break up the dependency on space alone by introducing time as a second variable for reverse geocoding. We mine the geosocial behavior of users of online location-based social networks to extract temporal semantic signatures. In analogy to the notion of scale distortion in cartography, we present a model that uses these signatures to distort the location of POI relative to the query location and time, thereby reordering the set of potentially matching places. We demonstrate the strengths of our method by evaluating it against a purely spatial baseline by determining the Mean Reciprocal Rank and the normalized Discounted Cumulative Gain. Our method performs substantially better than said baseline. Sensor observations are usually offered in relation to a specific purpose, e.g., for reporting fine dust emissions, following strict procedures, and spatio-temporal scales. Consequently, the huge amount of data gathered by today's public and private sensor networks is most often not reused outside of its initial creation context. Fostering the reusability of observations and derived applications calls for (i) spatial, temporal, and thematic aggregation of measured values, and (ii) easy integration mechanisms with external data sources. In this paper, we investigate how work on sensor observation aggregation can be incorporated into a Linked Data framework focusing on external linkage as well as provenance information. We show that Linked Data adds new aspects to the aggregation problem, e.g., whether external links from one of the original observations can be preserved for the aggregate. The Stimulus-Sensor-Observation (SSO) ontology design pattern is extended by classes and relations necessary to model the aggregation of sensor observations. ArcGIS Online is a unified Web portal designed by Environment System Research Institute (ESRI). It contains a rich collection of Web maps, layers, and services contributed by GIS users throughout the world. The metadata about these GIS resources reside in data silos that can be accessed via a Web API. While this is sufficient for simple syntax-based searches, it does not support more advanced queries, e.g., finding maps based on the semantics of the search terms, or performing customized queries that are not pre-designed in the API. In metadata, titles and descriptions are commonly available attributes which provide important information about the content of the GIS resources. However, such data cannot be easily used since they are in the form of unstructured natural language. To address these difficulties, we combine data-driven techniques with theory-driven approaches to enable semantic search and knowledge discovery for ArcGIS Online. We develop an ontology for ArcGIS Online data, convert the metadata into Linked Data, and enrich the metadata by extracting thematic concepts and geographic entities from titles and descriptions. Based on a human participant experiment, we calibrate a linear regression model for semantic search, and demonstrate the flexible queries for knowledge discovery that are not possible in the existing Web API. While this research is based on the ArcGIS Online data, the presented methods can also be applied to other GIS cloud services and data infrastructures. EarthCube is a major effort of the National Science Foundation to establish a next-generation knowledge architecture for the broader geosciences. Data storage, retrieval, access, and reuse are central parts of this new effort. Currently, EarthCube is organized around several building blocks and research coordination networks. OceanLink is a semanticsenabled building block that aims at improving data retrieval and reuse via ontologies, Semantic Web technologies, and Linked Data for the ocean sciences. Cruises, in the sense of research expeditions, are central events for ocean scientists. Consequently, information about these cruises and the involved vessels has to be shared and made retrievable. For example, the ability to find cruises in the vicinity of physiographic features of interest, e.g., a hydrothermal vent field or a fracture zone, is of primary interest for oceanographers. In this paper, we use a design pattern-centric strategy to engineer ontologies for OceanLink. We provide a formal axiomatization of the introduced patterns and ontologies using the Web Ontology Language, explain design choices, discuss the re-usability of our models, and provide lessons learned for the future geo-ontologies. The Semantic Web journal relies on a non-standard review process which in particular features open and transparent reviews and encourages reviewers to disclose their identities [1,2]. While the continued success of the Semantic Web journal shows that this setup works in practice and is suitable for a high-quality journal, the overhead caused by the non-standard process has been significant, in particular for the editors and the editors-in-chief. The main reason for this overhead was the unavailability of suitable manuscript review systems, as existing systems support only standard processes and are merely customizable within rigid and conventional bounds. For more than two years we thus had a rather cumbersome setup using, on the one hand, a commercial journal review management system, and on the other hand a public Website [2]. As a result, we had to constantly copy information from the review management system to the Website, and in fact we have done so manually. It was clear from the beginning that this was not going to be a solution which is sustainable over the long run, and so it was necessary to set up our own manuscript review system, tailored to the specific needs of the journal. In particular, we needed a tight integration of the review system with the journal\\u2019s website, so that it would no longer be necessary to handle two independent systems. The new review system went online in October 2012, and it has since served us very well. It is realized in Drupal, the popular Web Content Management System which underlies the Semantic Web journal website. Visitors of the website may in fact hardly notice a difference. However, authors who want to submit papers do so directly via the journal\\u2019s website, rather than through a separate submission site. Likewise, reviewers and editors interact directly with the website. While we are very pleased with the improvements resulting from the custom system, we will continue to develop it, and are happy about any feedback or comments we may receive. For 2013 we plan to embed Semantic Web technologies and thus support new types of queries over the journal\\u2019s dataset. In this issue of the journal, we present three papers. The first two are very substantial surveys of current research topics, and they continue our series of highly selective and well-received high-quality surveys published in the journal. The third article is a vision paper which argues for a systematic investigation of order (in the mathematical sense) for reasoning in the Big Data age. While we usually do not publish vision papers, we are happy to include this article of exceptional quality. It was fully reviewed to the standards of the journal. Shared reference is an essential aspect of meaning. It is also indispensable for the semantic web, since it enables to weave the global graph, i.e., it allows different users to contribute to an identical referent. For example, an essential kind of referent is a geographic place, to which users may contribute observations. We argue for a human-centric, operational approach towards reference, based on respective human competences. These competences encompass perceptual, cognitive as well as technical ones, and together they allow humans to inter-subjectively refer to a phenomenon in their environment. The technology stack of the semantic web should be extended by such operations. This would allow establishing new kinds of observation-based reference systems that help constrain and integrate the semantic web bottom-up. Ontologies are a common approach to improve semantic interoperability by explicitly specifying the vocabulary used by a particular information community. Complex expressions are defined in terms of primitive ones. This shifts the problem of semantic interoperability to the problem of how to ground primitive symbols. One approach are semantic datums, which determine reproducible mappings (measurement scales) from observable structures to symbols. Measurement theory offers a formal basis for such mappings. From an ontological point of view, this leaves two important questions unanswered. Which qualities provide semantic datums? How are these qualities related to the primitive entities in our ontology? Based on a scenario from hydrology, we first argue that human or technical sensors implement semantic datums, and secondly that primitive symbols are definable from the meaningful environment, a formalized quality space established through such sensors. The Geosciences and Geography are not just yet another application area for semantic technologies. The vast heterogeneity of the involved disciplines ranging from the natural sciences to the social sciences introduces new challenges in terms of interoperability. Moreover, the inherent spatial and temporal information components also require distinct semantic approaches. For these reasons, geospatial semantics, geo-ontologies, and semantic interoperability have been active research areas over the last 20 years. The geospatial semantics community has been among the early adopters of the Semantic Web, contributing methods, ontologies, use cases, and datasets. Today, geographic information is a crucial part of many central hubs on the Linked Data Web. In this editorial, we outline the research field of geospatial semantics, highlight major research directions and trends, and glance at future challenges. We hope that this text will be valuable for geoscientists interested in semantics research as well as knowledge engineers interested in spatiotemporal data. In the last years, several methodologies for ontology engineering have been proposed. Most of these methodologies guide the engineer from a first paper draft to an implemented --mostly description logics-based --ontology. A quality assessment of how accurately the resulting ontology fits the initial conceptualization and intended application has not been proposed so far. In this paper, we investigate the role of semantic similarity as a quality indicator. Based on similarity rankings, our approach allows for a qualitative estimation whether the domain experts' initial conceptualization is reflected by the developed ontology and whether it fits the users' application area. Our approach does not propose yet another ontology engineering methodology but can be integrated into existing ones. A plug-in to the Protege ontology editor implementing our approach is introduced and applied to a scenario from hydrology. The benefits and restrictions of similarity as a quality indicator are pointed out. The SIM-DL theory has been developed to enable similarity measurement between concept specifications using description logics. It thus closes the gap between similarity theories from psychology and formal representation languages from the AI community, such as the Web Ontology Language (OWL). In this paper, we present the results of a human participants test which investigates the cognitive plausibility of SIM-DL, that is, how well the rankings computed by the similarity theory match human similarity judgments. For this purpose, a questionnaire on the similarity between geographic feature types from the hydrographic domain was handed out to a group of participants. We discuss the set up and the results of this test, as well as the development of the according hydrographic feature type ontology and user interface. Finally, we give an outlook on the future development of SIM-DL and further potential application areas. Similarity measurement is currently being established as a method to explore content on the Semantic Web. Semantically annotated content requires formal concept specifications. Such concepts are dynamic and their semantics can change depending on the current context. The influence of context on similarity measurement is beyond dispute and reflected in recent similarity theories. However, the systematics of this influence has not been investigated so far. Intuitively, the results of similarity measurements should change depending on the impact of the current context. Particularly, such change should converge to 0 with a decreasing impact of the respective contexts. To hold up to this assertion, a quantification of the impact of context on similarity measurements is required. In this paper, we use a combination of the SIM-DL theory, which measures similarity between concepts represented using description logic, and a context model distinguishing between internal and external context to quantify this impact. The behavior of similarity measurements within an ontology specifying geospatial feature types is observed under varying contexts. The results are discussed with respect to the corresponding impact values. Space and time have not received much attention on the Semantic Web so far. While their importance has been recognized recently, existing work reduces them to simple latitude-longitude pairs and time stamps. In contrast, we argue that space and time are fundamental ordering relations for knowledge organization, representation, and reasoning. While most research on Semantic Web reasoning has focused on thematic aspects, this paper argues for a unified view combining a spatial, temporal, and thematic component. Besides their impact on the representation of and reasoning about individuals and classes, we outline the role of space and time for ontology modularization, evolution, and the handling of vague and contradictory knowledge. Instead of proposing yet another specific methodology, the presented work illustrates the relevance of space and time using various examples from the geo-sciences. When interacting with the environment subjects tend to classify entities with respect to the functionalities they offer for solving specific tasks. The theory of affordances accounts for this agent-environment interaction, while similarity allows for measuring resemblances among entities and entity types. Most similarity measures separate the similarity estimations from the context--the agents, their tasks and environment--and focus on structural and static descriptions of the compared entities and types. This paper argues that an affordance-based representation of the context in which similarity is measured, makes the estimations situation-aware and therefore improves their quality. It also leads to a better understanding of how unfamiliar entities are grouped together to ad-hoc categories, which has not been explained in terms of similarity yet. We propose that types of entities are the more similar the more common functionalities their instances afford an agent. This paper presents a framework for representing affordances, which allows determining similarity between them. The approach is demonstrated through a planning task. The understanding of complex environmental phenomena, such as deforestation and epidemics, requires observations at multiple scales. This scale dependency is not handled well by today's rather technical sensor definitions. Geosensor networks are normally defined as distributed ad-hoc wireless networks of computing platforms serving to monitor phenomena in geographic space. Such definitions also do not admit animals as sensors. Consequently, they exclude human sensors, which are the key to volunteered geographic information, and they fail to support connections between phenomena observed at multiple scales. We propose definitions of sensors as information sources at multiple aggregation levels, relating physical stimuli to observations. An algebraic formalization shows their behavior as well as their aggregations and generalizations. It is intended as a basis for defining consistent application programming interfaces to sense the environment at multiple scales of observations and with different types of sensors. Semantic similarity measurement is a key methodology in various domains ranging from cognitive science to geographic information retrieval on the Web. Meaningful notions of similarity, however, cannot be determined without taking additional contextual information into account. One way to make similarity measures context-aware is by introducing weights for specific characteristics. Existing approaches to automatically determine such weights are rather limited or require application specific adjustments. In the past, the possibility to tweak similarity theories until they fit a specific use case has been one of the major criticisms for their evaluation. In this work, we propose a novel approach to semi-automatically adapt similarity theories to the user's needs and hence make them context-aware. Our methodology is inspired by the process of georeferencing images in which known control points between the image and geographic space are used to compute a suitable transformation. We propose to semi-automatically calibrate weights to compute inter-instance and inter-concept similarities by allowing the user to adjust pre-computed similarity rankings. These known control similarities are then used to reference other similarity values. Acquisition and semantic annotation of data are fundamental tasks within the domain of cultural heritage. With the increasing amount of available data and ad hoc cross linking between their providers and users (e.g. through web services), data integration and knowledge refinement becomes even more important. To integrate information from several sources it has to be guaranteed that objects of discourse (which may be artifacts, events, persons, places or periods) refer to the same real world phenomena within all involved data sources. Local (database) identifiers however only disambiguate internal data, but fail in establishing connections to/between external data, while global identifiers can only partially solve this problem. Software assistants should support users in establishing such connections by delivering identity assumptions, i.e. by estimating whether examined data actually concerns the same real word phenomenon. This paper points out how similarity measures can act as groundwork for such assistants by introducing a similarity-based identity assumption assistant for historical places to support scholars in establishing links between distributed historical knowledge. While semantic similarity plays a crucial role for human categorization and reasoning, computational similarity measures have also been applied to fields such as semantics-based information retrieval or ontology engineering. Several measures have been developed to compare concepts specified in various description logics. In most cases, these measures are either structural or require a populated ontology. Structural measures fail with an increasing expressivity of the used description logic, while several ontologies, e.g., geographic feature type ontologies, are not populated at all. In this paper, we present an approach to reduce inter-concept to inter-instance similarity and thereby avoid the canonization problem of structural measures. The novel approach, called SIM-DLA , reuses existing similarity functions such as co-occurrence or network measures from our previous SIM-DL measure. The required instances for comparison are derived from the completion tree of a slightly modified DL-tableau algorithm as used for satisfiability checking. Instead of trying to find one (clash-free) model, the new algorithm generates a set of proxy individuals used for comparison. The paper presents the algorithm, alignment matrix, and similarity functions as well as a detailed example. Around 2006, the inception of Linked Data [2] has led to a realignment of the Semantic Web vision and the realization that data is not merely a way to evaluate our theoretical considerations, but a key research enabler in its own right that inspires novel theoretical and foundational research questions. Since then, Linked Data is growing rapidly and is altering research, governments, and industry. Simply put, Linked Data takes the World Wide Web\\u2019s ideas of global identifiers and links and applies them to (raw) data, not just documents. Moreover, and regularly highlighted by Tim Berners-Lee, Anybody can say Anything about Any topic (AAA)1 [1], which leads to a multi-thematic, multi-perspective, and multi-medial global data graph. More recently, Big Data has made its appearance in the shared mindset of researchers, practitioners, and funding agencies, driven by the awareness that concerted efforts are needed to address 21st century data collection, analysis, management, ownership, and privacy issues. While there is no generally agreed understanding of what exactly is (or more importantly, what is not) Big Data, an increasing number of V\\u2019s has been used to characterize different dimensions and challenges of Big Data: volume, velocity, variety, value, and veracity. Interestingly, different (scientific) disciplines highlight certain dimensions and neglect others. For instance, super computing seems to be mostly interested in the volume dimension while researchers working on sensor webs and the internet of things seem to push on the velocity front. The social sciences and humanities, in contrast, are more interested in value and veracity. As argued before [13,17], the variety dimensions seems to be the most intriguing one for the Semantic Web and the one where we can contribute- Numerous systems and tools have been developed for spatial decision support (SDS), but they generally suffer from a lack of re-usability, inconsistent terminology, and weak conceptualization. We introduce a collaborative effort by the SDS Consortium to build a SDS knowledge portal. We present the formal representation of knowledge about SDS, the various ontologies captured and made accessible by the portal, and the processes used to create them. We describe the portal in action, and the ways in which users can search, browse, and make use of its content. Finally, we discuss the lessons learned from this effort, and future development directions. Our work demonstrates how ontologies and semantic technologies can support the documentation and retrieval of dynamic knowledge in GIScience by offering flexible schemata instead of fixed data structures. The volume, velocity, and variety of data that are now becoming available allow us to study urban environments based on human behaviour with a spatial, temporal, and thematic granularity that was not achievable until now. Such data-driven approaches open up additional, complementary perspectives on how urban systems function, especially if they are based on user-generated content (UGC). While the data sources, such as social media, introduce specific biases, they also open up new possibilities for scientists and the broader public. For instance, they provide answers to questions that previously could only be addressed by complex simulations or extensive human-participant surveys. Unfortunately, many of the required data sets are locked in data silos that are accessible only via restricted APIs. Even if these data could be fully accessed, their na\\u03cave processing and visualization would surpass the abilities of modern computer architectures. Finally, the established place schemata used to study urban spaces differ substantially from UGC-based point-of-interest (POI) schemata. In this work, we present a multi-granular, data-driven, and theory-informed approach that addresses the key issues outlined above by introducing a theoretical and technical framework to interactively explore the pulse of a city based on social media. RESUME: Le volume, la rapidite et la variete des donnees maintenant disponibles permettent d\\u2019etudier les milieux urbains en fonction du comportement humain a un niveau spatial, temporel et granulaire thematique sans precedent. De telles methodes axees sur les donnees, et surtout celles fondees sur le contenu genere par l\\u2019utilisateur (CGU), proposent d\\u2019autres perspectives complementaires sur le fonctionnement des systemes urbains. Les sources de donnees (p. ex., medias sociaux) introduisent des biais precis, mais elles offrent de nouvelles possibilites aux scientifiques et au grand public (p. ex., en repondant a des questions qui, auparavant, necessitaient des simulations complexes ou de vastes enquetes aupres de participants humains). Malheureusement, beaucoup d\\u2019ensembles de donnees necessaires sont enfermes dans des reserves de donnees accessibles seulement par l\\u2019entremise d\\u2019interfaces de programmation d\\u2019applications (API) restreintes. Meme si l\\u2019on pouvait avoir pleinement accesa ces donnees, leur traitement et leur visualisation na\\u03cafs depasseraient les capacites des architectures informatiques modernes. Enfin, le schema des lieux etabli que l\\u2019on utilise pour etudier les espaces urbains est considerablement different du schema des points d\\u2019interet (PI) fondes sur le CGU. L\\u2019article presente une methode multigranulaire fondee sur la theorie et les donnees qui abordent les principales difficultes soulevees precedemment en introduisant un cadre theorique et technique pour explorer de facon interactive le pouls d\\u2019une ville en fonction des medias sociaux. Geospatial semantics as a research field studies how to publish, retrieve, reuse, and integrate geo-data, how to describe geo-data by conceptual models, and how to develop formal specifications on top of data structures to reduce the risk of incompatibilities. Geo-data is highly heterogeneous and ranges from qualitative interviews and thematic maps to satellite imagery and complex simulations. This makes ontologies, semantic annotations, and reasoning support essential ingredients towards a Geospatial Semantic Web. In this paper, we present an overview of major research questions, recent findings, and core literature. Abstract Urban areas of interest (AOI) refer to the regions within an urban environment that attract people's attention. Such areas often have high exposure to the general public, and receive a large number of visits. As a result, urban AOI can reveal useful information for city planners, transportation analysts, and location-based service providers to plan new business, extend existing infrastructure, and so forth. Urban AOI exist in people's perception and are defined by behaviors. However, such perception was rarely captured until the Social Web information technology revolution. Social media data record the interactions between users and their surrounding environment, and thus have the potential to uncover interesting urban areas and their underlying spatiotemporal dynamics. This paper presents a coherent framework for extracting and understanding urban AOI based on geotagged photos. Six different cities from six different countries have been selected for this study, and Flickr photo data covering these cities in the past ten years (2004\\u20132014) have been retrieved. We identify AOI using DBSCAN clustering algorithm, understand AOI by extracting distinctive textual tags and preferable photos, and discuss the spatiotemporal dynamics as well as some insights derived from the AOI. An interactive prototype has also been implemented as a proof-of-concept. While Flickr data have been used in this study, the presented framework can also be applied to other geotagged photos. Semantic similarity measurement gained attention as a methodology for ontology-based information retrieval within GIScience over the last years. Several theories explain how to determine the similarity between entities, concepts or spatial scenes, while concrete implementations and applications are still missing. In addition, most existing similarity theories use their own representation language while the majority of geo-ontologies is annotated using the Web Ontology Language (OWL). This paper presents a context and blocking aware semantic similarity theory for the description logic ALCHQ as well as its prototypical implementation within the open source SIM-DL similarity server. An application scenario is introduced showing how the Alexandria Digital Library Gazetteer can benefit from similarity in terms of improved search and annotation capabilities. Directions for further work are discussed. While similarity has gained in importance in research about information retrieval on the (geospatial) semantic Web, information retrieval paradigms and their integration into existing spatial data infrastructures have not been examined in detail so far. In this paper, intensional and extensional paradigms for similarity-based information retrieval are introduced. The differences between these paradigms with respect to the query and results are pointed out. Web user interfaces implementing two of these paradigms are presented, and steps towards the integration of the SIM-DL similarity theory into a spatial data infrastructure are discussed. Remaining difficulties are highlighted and directions of further work are given. In 2010 Tim Berners-Lee introduced a 5 star rating to his Linked Data design issues page to encourage data publishers along the road to good Linked Data. What makes the star rating so effective is its simplicity, clarity, and a pinch of psychology --is your data 5 star? While there is an abundance of 5 star Linked Data available today, finding, querying, and integrating/interlinking these data is, to say the least, difficult. While the literature has largely focused on describing datasets, e.g., by adding provenance information, or interlinking them, e.g., by co-reference resolution tools, we would like to take Berners-Lee's original proposal to the next level by introducing a 5 star rating for Linked Data vocabulary use. Life Cycle Assessment (LCA) is the study of the environmental impact of products taking into account their entire life-span and production chain. This requires gathering data from a variety of heterogeneous sources into a Life Cycle Inventory (LCI). LCI preparation involves the integration of observations and engineering models with reference data and literature results from around the world, from different domains, and at varying levels of granularity. Existing LCA data formats only address syntactic interoperability, thereby often ignoring semantics. This leads to inefficiencies in information collection and management and thus a variety of challenges, e.g., difficulties in reproducing assessments published in the literature. In this work, we present an ontology pattern that specifies key aspects of LCA/LCI data models, i.e., the notions of flows, activities, agents, and products, as well as their properties. Abstract The temporal characteristics of human behavior with respect to points of interest (POI) differ significantly with place type. Intuitively, we are more likely to visit a restaurant during typical lunch and dinner times than at midnight. Aggregating geosocial check-ins of millions of users to the place type level leads to powerful temporal bands and signatures. In previous work these signatures have been used to estimate the place being visited based purely on the check-in time, to label uncategorized places based on their individual signature's similarity to a type signature, and to mine POI categories and their hierarchical structure from the bottom up. However, not all hours of the day and days of the week are equally indicative of the place type, i.e., the information gain between temporal bands that jointly form a place type signature differs. To give a concrete example, places can be more easily categorized into weekend and weekday places than into Monday and Tuesday places. Nonetheless, research on the regional variability of temporal signatures is lacking. Intuitively, one would assume that certain types of places are more prone to regional differences with respect to the temporal check-in behavior than others. This variability will impact the predictive power of the signatures and reduce the number of POI types that can be distinguished. In this work, we address the regional variability hypothesis by trying to prove that all place types are created equal with respect to their temporal signatures, i.e., temporal check-in behavior does not change across space. We reject this hypothesis by comparing the inter-signature similarity of 321 place types in three major cities in the USA (Los Angeles, New York, and Chicago). Next, we identify a common core of least varying place types and compare it against signatures extracted from the city of Shanghai, China for cross-culture comparison. Finally, we discuss the impact of our findings on POI categorization and the reliability of temporal signatures for check-in behavior in general. The categorization of our environment into feature types is an essential prerequisite for cartography, geographic information retrieval, routing applications, spatial decision support systems, and data sharing in general. However, there is no a priori conceptualization of the world and the creation of features and types is an act of cognition. Humans conceptualize their environment based on multiple criteria such as their cultural background, knowledge, motivation, and particularly by space and time. Sharing and making these conceptualizations explicit in a formal, unambiguous way is at the core of semantic interoperability. One way to cope with semantic heterogeneities is by standardization, i.e., by agreeing on a shared conceptualization. This bears the danger of losing local diversity. In contrast, this work proposes the use of microtheories for Spatial Data Infrastructures, such as INSPIRE, to account for the diversity of local conceptualizations while maintaining their semantic interoperability at a global level. We introduce a novel methodology to structure ontologies by spatial and temporal aspects, in our case administrative boundaries, which reflect variations in feature conceptualization. A local, bottom-up approach, based on non-standard inference, is used to compute global feature definitions which are neither too broad nor too specific. Using different conceptualizations of rivers and other geographic feature types, we demonstrate how the present approach can improve the INSPIRE data model and ease its adoption by European member states. With the increasing success and commercial integration of Volunteered Geographic Information (VGI), the focus shifts away from coverage to data quality and homogeneity. Within the last years, several studies have been published analyzing the positional accuracy of features, completeness of specific attributes, or the topological consistency of line and polygon features. However, most of these studies do not take geographic feature types into account. This is for two reasons. First, and in contrast to street networks, choosing a reference set is difficult. Second, we lack the measures to quantify the degree of feature type miscategorization. In this work, we present a methodology to analyze the spatial-semantic interaction of point features in Volunteered Geographic Information. Feature types in VGI can be considered special in both, the way they are formed and the way they are applied. Given that they reflect community agreement more accurately than top-down approaches, we argue that they should be used as the primary basis for assessing spatial-semantic interaction. We present a case study on a spatial and semantic subset of OpenStreetMap, and introduce a novel semantic similarity measure based on the change history of OpenStreetMap elements. Our results set the stage for systems that assist VGI contributors in suggesting the types of new features, cleaning up existing data, and integrating data from different sources. The Digital Earth [13] aims at developing a digital representation of the planet. It is motivated by the need for integrating and interlinking vast geo-referenced, multi-thematic, and multi-perspective knowledge archives that cut through domain boundaries. Complex scientific questions cannot be answered from within one domain alone but span over multiple scientific disciplines. For instance, studying disease dynamics for prediction and policy making requires data and models from a diverse body of science ranging from medical science and epidemiology over geography and economics to mining the social Web. The naive assumption that such problems can simply be addressed by more data with a higher spatial, temporal, and thematic resolution fails as long as this more on data is not supported by more knowledge on how to combine and interpret the data. This makes semantic interoperability a core research topic of data-intensive science. While the Digital Earth vision includes processing services, it is, at its very core, a data archive and infrastructure. We propose to redefine the Digital Earth as a knowledge engine and discuss what the Semantic Web has to offer in this context and to Big Data in general. Big Data, Linked Data, Smart Dust, Digital Earth, and e-Science are just some of the names for research trends that surfaced over the last years. While all of them address different visions and needs, they share a common theme: How do we manage massive amounts of heterogeneous data, derive knowledge out of them instead of drowning in information, and how do we make our findings reproducible and reusable by others? In a network of knowledge, topics span across scientific disciplines and the idea of domain ontologies as common agreements seems like an illusion. In this work, we argue that these trends require a radical paradigm shift in ontology engineering away from a small number of authoritative, global ontologies developed top-down, to a high number of local ontologies that are driven by application needs and developed bottom-up out of observation data. Similarly as the early Web was replaced by a social Web in which volunteers produce data instead of purely consuming it, the next generation of knowledge infrastructures has to enable users to become knowledge engineers themselves. Surprisingly, existing ontology engineering frameworks are not well suited for this new perspective. Hence, we propose an observation-driven ontology engineering framework, show how its layers can be realized using specific methodologies, and relate the framework to existing work on geo-ontologies. There has been significant progress transforming semi-structured data about places into knowledge graphs that can be used in a wide variety of geographic information systems such as digital gazetteers or geographic information retrieval systems. For instance, in addition to information about events, actors, and objects, DBpedia contains data about hundreds of thousands of places from Wikipedia and publishes it as Linked Data. Repositories that store data about places are among the most interlinked hubs on the Linked Data cloud. However, most content about places resides in unstructured natural language text, and therefore it is not captured in these knowledge graphs. Instead, place representations are limited to facts such as their population counts, geographic locations, and relations to other entities, for example, headquarters of companies or historical figures. In this paper, we present a novel method to enrich the information stored about places in knowledge graphs using thematic signatures that are derived from unstructured text through the process of topic modeling. As proof of concept, we demonstrate that this enables the automatic categorization of articles into place types defined in the DBpedia ontology e.g., mountain and also provides a mechanism to infer relationships between place types that are not captured in existing ontologies. This method can also be used to uncover miscategorized places, which is a common problem arising from the automatic lifting of unstructured and semi-structured data. To a large degree, the attraction of Big Data lies in the variety of its heterogeneous multi-thematic and multi-dimensional data sources and not merely its volume. To fully exploit this variety, however, requires conflation. This is a two-step process. First, one has to establish identity relations between information entities across different data sources; and second, attribute values have to be merged according to certain procedures that avoid logical contradictions. The first step, also called matching, can be thought of as a weighted combination of common attributes according to some similarity measures. In this work, we propose such a matching based on multiple attributes of Points of Interest (POI) from the Location-based Social Network Foursquare and the local directory service Yelp. While both contain overlapping attributes that can be used for matching, they have specific strengths and weaknesses that make their conflation desirable. For instance, Foursquare offers information about user check-in... Abstract The vision of a Digital Earth calls for more dynamic information systems, new sources of information, and stronger capabilities for their integration. Sensor networks have been identified as a major information source for the Digital Earth, while Semantic Web technologies have been proposed to facilitate integration. So far, sensor data are stored and published using the Observations & Measurements standard of the Open Geospatial Consortium (OGC) as data model. With the advent of Volunteered Geographic Information and the Semantic Sensor Web, work on an ontological model gained importance within Sensor Web Enablement (SWE). In contrast to data models, an ontological approach abstracts from implementation details by focusing on modeling the physical world from the perspective of a particular domain. Ontologies restrict the interpretation of vocabularies toward their intended meaning. The ongoing paradigm shift to Linked Sensor Data complements this attempt. Two questions have to be addressed: (1) ... Ontology design patterns ease the engineering of ontologies, improve their quality, foster reusability, and support the alignment of ontologies by acting as common building blocks or strategies for reoccurring modeling problems. This makes ontology design patterns key enablers of semantic interoperability and, hence, a crucial technology for representing the body of knowledge of such heterogeneous domains as the geosciences. While different types of patterns can be distinguished, existing work on geo-ontology design patterns has solely focused on content patterns, i.e., design solutions for domain classes and relationships. In this work, we propose a logical pattern that addresses a frequent modeling problem that has hampered the development of sophisticated geo-ontologies in the past, namely how to model the quantification over types. We argue for the need for such a pattern, explain why it is difficult to model, demonstrate how to implement it using the Web Ontology Language OWL, and finally show how it can be applied to modeling concepts such as biodiversity. To a large degree, the attraction of Big Data lies in the variety of its heterogeneous multi-thematic and multi-dimensional data sources and not merely its volume. To fully exploit this variety, however, requires conflation. This is a two step process. First, one has to establish identity relations between information entities across the different data sources; and second, attribute values have to be merged according to certain procedures which avoid logical contradictions. The first step, also called matching, can be thought of as a weighted combination of common attributes according to some similarity measures. In this work, we propose such a matching based on multiple attributes of Points of Interests (POI) from the Location-based Social Network Foursquare and the Yelp local directory service. While both contain overlapping attributes that can be use for matching, they have specific strengths and weaknesses which makes their conflation desirable. We present a weighted multi-attribute matching strategy and evaluate its performance. Our strategy can automatically match 97% of randomly selected Yelp POI to their corresponding Foursquare entities. Gazetteers are key components of georeferenced information systems, including applications such as Web-based mapping services. Existing gazetteers lack the capabilities to fully integrate user-contributed and vernacular geographic information, as well as to support complex queries. To address these issues, a next generation gazetteer should leverage formal semantics, harvesting of implicit geographic information -- such as geotagged photos -- as well as models of trust for contributors. In this paper, we discuss these requirements in detail. We elucidate how existing standards can be integrated to realize a gazetteer infrastructure allowing for bottom-up contribution as well as information exchange between different gazetteers. We show how to ensure the quality of user-contributed information and demonstrate how to improve querying and navigation using semantics-based information retrieval. Building on abstract reference models, the Open Geospatial Consortium (OGC) has established standards for storing, discovering, and processing geographical information. These standards act as a basis for the implementation of specific services and Spatial Data Infrastructures (SDI). Research on geo-semantics plays an increasing role to support complex queries and retrieval across heterogeneous information sources, as well as for service orchestration, semantic translation, and on-the-fly integration. So far, this research targets individual solutions or focuses on the Semantic Web, leaving the integration into SDI aside. What is missing is a shared and transparent Semantic Enablement Layer for SDI which also integrates reasoning services known from the Semantic Web. Instead of developing new semantically enabled services from scratch, we propose to create profiles of existing services that implement a transparent mapping between the OGC and the Semantic Web world. Finally, we point out how to combine SDI with linked data. Semantic similarity measurement has been an active research area in GIScience and the Semantic Web for many years. However, implementations of these measures were largely missing, not publicly available, or tailored to specific application needs. To foster the application of similarity reasoning in information retrieval, ontology engineering, and spatial decision support, we implemented the SIM-DL semantic similarity server as well as a plug-in for the popular Protege ontology editor. While SIM-DL has been successfully applied to several application areas, the implemented similarity theory was largely structural, could not handle concept and instance similarity within the same framework, and was based on a Protege version and DIG interface that have been re-engineered over the last years. This paper introduces a new version, called SIM-DLA, engineered from scratch to addresses these shortcomings. It is based on our new similarity theory, can handle inter-instance and inter-concept similarity using the same functions and alignments, and is available for the new Protege version 4.1. Gazetteers are more than basic place name directories containing names and locations for named geographic places. Most of them contain additional information, including a categorization of gazetteer entries using a typing scheme. This paper focuses on the nature of these categorization schemes. We argue that gazetteers can benefit from an ontological approach to typing schemes, providing a formalization that will better support gazetteer applications, maintenance, interoperability, and semi-automatic feature annotation. We discuss the process of developing such an ontology as a modification of an existing feature type thesaurus; the difficulties in mapping from thesauri to ontologies are described in detail. To demonstrate the benefits of a categorization based on ontologies, a new gazetteer Web (and programming) interface is introduced and the impact on gazetteer interoperability is discussed. Place name disambiguation is an important task for improving the accuracy of geographic information retrieval. This task becomes more challenging when the input texts are short. Wikipedia provides information about places and has often been employed for named entity recognition. However, the natural language representation of Wikipedia articles limits more effective use of this rich knowledge base. DBpedia is the Semantic Web version of Wikipedia, which provides structured and machine-understandable knowledge mined from Wikipedia articles. This paper presents an approach for combining Wikipedia and DBpedia to disambiguate place names in short texts. We discuss the pros and cons of the two knowledge bases, and argue that a combination of both performs better than each of them alone. We evaluate our proposed method by conducting experiments against baselines of three established methods. The result indicates that our method has a generally higher precision and recall. While our study employs DBpedia, the proposed method is generic and can be extended to other structured Linked Datasets such as Freebase or Wikidata. The landscape of social media applications is littered with novel approaches to using location information. The latest platform to emerge in this geosocial media realm is Yik Yak, an application that allows users to share geo-tagged, (currently) text-based, and most importantly, anonymous content. The fast adoption of this platform by college students as well as the recent availability of data offers a unique research opportunity. This work takes a first step in exploring this novel type of data through a range of textual, topical, and spatial data exploration methods. We are particularly interested in the question of whether Yik Yak differs from other geosocial data sources such as Twitter. Is it just another location-based social network or does it differ from existing social networks, establishing itself as a valuable resource for feature extraction? While open access is established in the world of academic publishing, open reviews are rare. The Semantic Web journal goes further than just open review by implementing an open and transparent review process in which reviews are publicly available, the assigned editors and reviewers are known by name, and are published together with accepted manuscripts. In this article we introduce the steps to realize such a process from the conceptual design, over the implementation, a overview of the results so far, and up to lessons learned. In this paper, we develop a semantic annotation technique for location-based social networks to automatically annotate all places with category tags which are a crucial prerequisite for location search, recommendation services, or data cleaning. Our annotation algorithm learns a binary support vector machine (SVM) classifier for each tag in the tag space to support multi-label classification. Based on the check-in behavior of users, we extract features of places from i) explicit patterns (EP) of individual places and ii) implicit relatedness (IR) among similar places. The features extracted from EP are summarized from all check-ins at a specific place. The features from IR are derived by building a novel network of related places (NRP) where similar places are linked by virtual edges. Upon NRP, we determine the probability of a category tag for each place by exploring the relatedness of places. Finally, we conduct a comprehensive experimental study based on a real dataset collected from a location-based social network, Whrrl. The results demonstrate the suitability of our approach and show the strength of taking both EP and IR into account in feature extraction. This paper presents a collaborative 3D GIS to support public participation. Realizing that public-involved decision-making is often a multistage process, the proposed system is designed to provide coherent support for collaborations in the different stages. We differentiate ubiquitous participation and intensive participation and identify their suitable application stages. The proposed system, then, supports both of the two types of participation by providing synchronous and asynchronous collaboration functionalities. Applying the concept of Digital Earth, the proposed system also features a virtual globe-based user interface. Such an interface integrates a variety of data, functions, and services into a unified virtual environment which is delivered to both experts and public participants through the Internet. The system has been designed as a general software framework and can be tailored for specific projects. In this study, we demonstrate it using a scene modeling case and provide a preliminary evalua... The term Volunteered Geographic Information (VGI) describes various layperson-based, geo-collaboration projects to collect, maintain, and visualize information. VGI has been successfully utilized in scenarios such as emergency response and is also increasingly integrated into commercial products. Based on an analysis of existing projects and research, we propose to extend the idea of VGI by introducing Volunteered Geographic Services (VGS). Instead of contributing information, volunteers can request or offer microservices to their local community. We provide a flexible server framework that handles service requests and offers. We also implement a smartphone application developed using Google's Android platform. The server and mobile client are realized following the Linked Data paradigm and using Semantic Web technologies. In this paper, we discuss the idea behind VGS, motivate it using two scenarios, and explain the technical realization. Feature types play a crucial role in understanding and analyzing geographic information. Usually, these types are defined, standardized, and controlled by domain experts and cover geographic features on the mesoscale level, e.g., populated places, forests, or lakes. While feature types also underlie most Location-Based Services (LBS), assigning a consistent typing schema for Points Of Interest (POI) across different data sets is challenging. In case of Volunteered Geographic Information (VGI), types are assigned as tags by a heterogeneous community with different backgrounds and applications in mind. Consequently, VGI research is shifting away from data completeness and positional accuracy as quality measures towards attribute accuracy. As tags can be assigned by everybody and have no formal or stable definition, we propose to study category tags via indirect observations. We extract user check-ins from massive real-world data crawled from Location-based Social Networks to understand the temporal dimension of Points Of Interest. While users may assign different category tags to places, we argue that their temporal characteristics, e.g., opening times, will show distinguishable patterns. This work1 enables an automated integration of sensors and services on the Sensor Web. The Sensor Web is defined as an infrastructure which enables the interoperable usage of sensor resources by providing services for (1) discovery, (2) access, (3) tasking, as well as (4) eventing & alerting [2]. The notion of the Sensor Web has been largely influenced by the developments of OGC's Sensor Web Enablement (SWE) initiative [3], however, there are also other implementations complying to the Sensor Web idea, such as Sensorpedia [4], SensorMap with its underlying SenseWeb infrastructure [5], SensorBase [6], or Cosm2 (formerly known as Pachube). Linked Data (LD) has been an active research area for more than 6 years and many aspects about publishing, retrieving, linking, and cleaning Linked Data have been investigated. There seems to be a broad and general agreement that in principle LD datasets can be very useful for solving a wide variety of problems ranging from practical industrial analytics to highly specific research problems. Having these notions in mind, we started exploring the use of notable LD datasets such as DBpedia, Freebase, Geonames and others for a commercial application. However, it turns out that using these datasets in realistic settings is not always easy. Surprisingly, in many cases the underlying issues are not tech- nical but legal barriers erected by the LD data publishers. In this paper we argue that these barriers are often not justified, detrimental to both data publishers and users, and are often built without much consideration of their consequences. The W3C Semantic Sensor Network Incubator group (the SSN-XG) produced an OWL 2 ontology to describe sensors and observations - the SSN ontology, available at http://purl.oclc.org/NET/ssnx/ssn. The SSN ontology can describe sensors in terms of capabilities, measurement processes, observations and deployments. This article describes the SSN ontology. It further gives an example and describes the use of the ontology in recent research projects. Environmental sensors have continuously improved by becoming smaller, cheaper, and more intelligent over the past years. As consequence of these technological advancements, sensors are increasingly deployed to monitor our environment. The large variety of available sensor types with often incompatible protocols complicates the integration of sensors into observing systems. The standardized Web service interfaces and data encodings defined within OGC\\u2019s Sensor Web Enablement (SWE) framework make sensors available over the Web and hide the heterogeneous sensor protocols from applications. So far, the SWE framework does not describe how to integrate sensors on-the-fly with minimal human intervention. The driver software which enables access to sensors has to be implemented and the measured sensor data has to be manually mapped to the SWE models. In this article we introduce a Sensor Plug & Play infrastructure for the Sensor Web by combining (1) semantic matchmaking functionality, (2) a publish/subscribe mechanism underlying the SensorWeb, as well as (3) a model for the declarative description of sensor interfaces which serves as a generic driver mechanism. We implement and evaluate our approach by applying it to an oil spill scenario. The matchmaking is realized using existing ontologies and reasoning engines and provides a strong case for the semantic integration capabilities provided by Semantic Web research. Ontology-based information publishing, retrieval, reuse, and integration have become popular research topics to address the challenges involved in exchanging data between heterogeneous sources. However, in most cases ontologies are still developed in a centralized top-down manner by a few knowledge engineers. Consequently, the role that developers play in conceptualizing a domain such as the geosciences is disproportional compared with the role of domain experts and especially potential end-users. These and other drawbacks have stimulated the creation of new methodologies focusing around collaboration. Based on a review of existing approaches, this article presents a two-step methodology and implementation to foster collaborative ontology engineering in the geosciences. Our approach consists of the development of a minimalistic core ontology acting as a catalyst and the creation of a virtual collaborative development cycle. Both methodology and prototypical implementation have been tested in the context of the EU-funded ForeStClim project which addresses environmental protection with respect to forests and climate change. The goal of the Sensor Web Enablement (SWE) initiative of the Open Geospatial Consortium (OGC) is the definition of web service interfaces and data encodings to make sensors discoverable, taskable and accessible on the World Wide Web. The SWE specifications enable a standardized communication and interaction with arbitrary types of sensors and sensor systems. The central concepts within OGC's Sensor Web architecture are sensors, observations and features of interest. Sensors and their observations can be registered and stored through the Sensor Observation Service (SOS) to make them accessible for clients. So far, mechanisms are missing which support a semantic matching between features of interest stored in a database and referred to by an observation. The same applies for the matching between observations as sensor outputs and the properties of the features of interest. By taking a use case from disaster management, we outline the challenges and demonstrate how semantically annotated SWE data models and service interfaces support semantic matching. The result is a roadmap towards a semantically enabled sensor plug & play within the Sensor Web. Semantic similarity measurement plays a significant role in semantic interoperability and in information retrieval within the geo domain as it supports the detection of conceptually close but not identical entities. In feature-based models, the similarity measurement is done by comparing common and different features such as parts, attributes and functions. This paper suggests adding thematic roles as an additional type of features to be compared, and shows why and how the usage of thematic roles may prevent wrong function matches. What prevents the Geospatial Semantic Web from taking off is not a missing architecture and protocol stack but, beside other aspects, the question of how Web services can be semi-automatically discovered and whether and to what degree they satisfy user requirements. Two approaches turned out to be useful for semanticenabled geospatial information retrieval: subsumption reasoning and similarity measurement. However, while the former one can be applied to query service ontologies described in OWL-S or WSMO/WSML, most existing similarity theories are not able to cope with logic-based service descriptions. This chapter presents initial results on developing a directed and context-aware similarity measure that compares WSML concept descriptions for overlap and therefore supports retrieval within the upcoming Geospatial Semantic The relation between data, annotations, and schemata seems straightforward at first: Data are annotated with additional meta information according to some schemata in order to expose additional non-intrinsic characteristics relevant to the meaningful interpretation of said data. However, on closer examination, things are not as simple. Focusing on geo-information retrieval, we will try to disentangle the aforementioned relations. We will report from our own experience and from observations gathered by editing papers about ontologies and Linked Data for the Semantic Web journal. Semantic desktops are a novel approach to improve user interfaces by recording, semantically annotating, and learning from the user's activities to create a personalized user experience and improve search. Such activities, however, are restricted to the information universe, i.e., they only cover events on the local desktop. A next step towards smart mobile devices is the integration of those desktop events with the user's activities in the physical world. Establishing such mappings enables the device to draw conclusions from the recorded desktop events to those that the user is likely performing in the physical world. A Personal Information Management (PIM) system can then better assist the user in task planning and routing. In this work, we propose activity ontologies as blueprints to model the user's activities in the physical world, and use these ontologies to link the Semantic Desktop and the information available on the Web of Linked Data. We discuss the principles of designing the activity ontologies and how to employ them to associate local files and applications with complementary information from the Web. We design a specific activity ontology for a conference use case and present a user interface that extends the Zeitgeist Semantic Desktop to evaluate our approach. The concepts of scale is at the core of cartographic abstraction and mapping. It defines which geographic phenomena should be displayed, which type of geometry and map symbol to use, which measures can be taken, as well as the degree to which features need to be exaggerated or spatially displaced. In this work, we present an ontology design pattern for map scaling using the Web Ontology Language (OWL) within a particular extension of the OWL RL profile. We explain how it can be used to describe scaling applications, to reason over scale levels, and geometric representations. We propose an axiomatization that allows us to impose meaningful constraints on the pattern, and, thus, to go beyond simple surface semantics. Interestingly, this includes several functional constraints currently not expressible in any of the OWL profiles. We show that for this specific scenario, the addition of such constraints does not increase the reasoning complexity which remains tractable. Geoportals provide integrated access to geospatial resources, and enable both authorities and the general public to contribute and share data and services. An essential goal of geoportals is to facilitate the discovery of the available resources. Such process heavily relies on the quality of metadata. While multiple metadata standards have been established, data contributers may adopt different standards when sharing their data via the same geoportal. This is especially the case for user-generated content where various terms and topics can be introduced to describe similar datasets. While this heterogeneity provides a wealth of perspectives, it also complicates resource discovery. With the fast development of the Semantic Web technologies, there is a rise of Linked-Data-driven portals. Although these novel portals open up new ways to organizing metadata and retrieving resources, they lack effective semantic search methods. This paper addresses the two challenges discussed above, namely the topic heterogeneity brought by multiple metadata standards as well as the lack of established semantic search in Linked-Data-driven geoportals. To harmonize the metadata topics, we employ a natural language processing method, namely Labeled Latent Dirichlet Allocation (LLDA), and train it using standardized metadata from Data.gov. With respect to semantic search, we construct thematic and geographic matching features from the textual metadata descriptions, and train a regression model via a human participants experiment. We evaluate our methods by examining their performances in addressing the two issues. Finally, we implement a semantics-enabled and Linked-Data-driven prototypical geoportal using a sample dataset from Esri\\u2019s ArcGIS Online. While catchphrases such as big data, smart data, dataintensive science, or smart dust highlight different aspects, they share a common theme: Namely, a shift towards a data-centric perspective in which the synthesis and analysis of data at an ever-increasing spatial, temporal, and thematic resolution promises new insights, while, at the same time, reducing the need for strong domain theories as starting points. In terms of the envisioned methodologies, those catchphrases tend to emphasize the role of predictive analytics, i.e., statistical techniques including data mining and machine learning, as well as supercomputing. Interestingly, however, while this perspective takes the availability of data as a given, it does not answer the question how one would discover the required data in today\\u2019s chaotic information universe, how one would understand which datasets can be meaningfully integrated, and how to communicate the results to humans and machines alike. The Semantic Web addresses these questions. In the following, we argue why the data train needs semantic rails. We point out that making sense of data and gaining new insights works best if inductive and deductive techniques go hand-in-hand instead of competing over the prerogative of interpretation. Similarity measures have a long tradition in fields such as information retrieval, artificial intelligence, and cognitive science. Within the last years, these measures have been extended and reused to measure semantic similarity; i.e., for comparing meanings rather than syntactic differences. Various measures for spatial applications have been de- veloped, but a solid foundation for answering what they measure; how they are best ap- plied in information retrieval; which role contextual information plays; and how similarity values or rankings should be interpreted is still missing. It is therefore difficult to decide which measure should be used for a particular application or to compare results from dif- ferent similarity theories. Based on a review of existing similarity measures, we introduce a framework to specify the semantics of similarity. We discuss similarity-based information retrieval paradigms as well as their implementation in web-based user interfaces for geo- graphic information retrieval to demonstrate the applicability of the framework. Finally, we formulate open challenges for similarity research. Similarity measurement theories play an increasing role in GIScience and especially in information retrieval and integration Existing feature and geometric models have proven useful in detecting close but not identical concepts and entities However, until now none of these theories are able to handle the expressivity of description logics for various reasons and therefore are not applicable to the kind of ontologies usually developed for geographic information systems or the upcoming geospatial semantic web To close the resulting gap between available similarity theories on the one side and existing ontologies on the other, this paper presents ongoing work to develop a context-aware similarity theory for concepts specified in expressive description logics such as $mathcal ALCNR$. 2012 brought a major change to the semantics research community. Discussions on the use and benefits of semantic technologies are shifting away from the why to the how. Surprisingly this more in stakeholder interest is not accompanied by a more detailed understanding of what semantics research is about. Instead of blaming others for their (wrong) expectations, we need to learn how to emphasize the paradigm shift proposed by semantics research while abstracting from technical details and advocate the added value in a way that relates to the immediate needs of individual stakeholders without overselling. This paper highlights some of the major ingredients to prepare your next Semantics Elevator Talk. Building on abstract reference models, the Open Geospatial Consortium (OGC) has established standards for storing, discovering, and processing geographical information. These standards act as basis for the implementation of specic services and Spatial Data Infrastructures (SDI). Research on geo-semantics plays an increasing role to support complex queries and retrieval across heterogeneous information sources, as well as for service orchestration, semantic translation, and on-the-y integration. So far, this research targets individual solutions or focuses on the Semantic Web, leaving the integration into SDI aside. What is missing is a shared and transparent semantic enablement layer for Spatial Data Infrastructures which also integrates reasoning services known from the Semantic Web. Focusing on Sensor Web Enablement (SWE), we outline how Spatial Data Infrastructures in general can benet from such a semantic enablement layer. Instead of developing new semantically enabled services from scratch, we propose to create proles of existing services that implement a transparent mapping between the OGC and the Semantic Web world. Semantic similarity measurement gained attention over the last years as a non-standard inference service for various kinds of knowledge representations including description logics. Most existing similarity measures compute an undirected overall similarity, i.e., they do not take the context of the similarity query into account. If they do, the notion of context is usually reduced to the selection of particular concepts for comparison (instead of comparing all concepts within an examined ontology). The importance of context in deriving meaningful similarity judgments is beyond question and has been examined within recent research. This paper argues that there are several kinds of contexts. Each of them has its own impact on the resulting similarity values, but also on their interpretation. To support this view, the paper introduces definitions for the examined contexts and illustrates their influence by example. The research field of scientometrics is concerned with measuring and analyzing science. In practice, this is often done by restricting the impact of publications, journals, and researchers to a mere frequency. However, scientific activities (co-publication, citation, labor mobility) display clear spatiotemporal patterns, and such patterns have rarely been considered in traditional scientometrics. In this work we focus on the study of citations and present a spatiotemporal scientometrics framework to measure the citation impact of research output by taking physical space, place, and time into account. Specifically, we use the statistics of categorical places (institutions, cities, and countries), spatiotemporal kernel density estimations, cartograms, distance distribution curves, and point-pattern analysis to identify spatiotemporal citation patterns. Moreover, we propose a series of s-indices, such as S_institution-index, S_city-index, and S_country-index to evaluate a scientist's impact as a complement to non-spatial citation indicators, e.g., h-index and g-index. In addition, we have developed an interactive web application which allows users to visually explore research topics, authors, publications, as well as the spread of citations through space and time. Our work offers insights on the role of location in scientific knowledge diffusion. The semantic integration of heterogeneous, spatiotemporal information is a major challenge for achieving the vision of a multi-thematic and multi-perspective Digital Earth. The Semantic Web technology stack has been proposed to address the integration problem by knowledge representation languages and reasoning. However approaches such as the Web Ontology Languages (OWL) were developed with decidability in mind. They do not integrate well with established modeling paradigms in the geosciences that are dominated by numerical and geometric methods. Additionally, work on the Semantic Web is mostly feature-centric and a field-based view is difficult to integrate. A layer specifying the transition from observation data to classes and relations is missing. In this work we combine OWL with geometric and topological language constructs based on similarity spaces. Our approach provides three main benefits. First, class constructors can be built from a larger palette of mathematical operations based on vector algebra. Second, it affords the representation of prototype-based classes. Third, it facilitates the representation of classes derived from machine learning classifiers that utilize a multi-dimensional feature space. Instead of following a one-size-fits-all approach, our work allows one to derive contextualized OWL ontologies by reification of observation data. With the increasing amount of geographic information available on the Internet, searching, browsing, and organizing such information has become a major challenge within the field of Geographic Information Science (GIScience). As all information is ultimately for and from human beings, the methodologies applied to retrieve and organize this information should correlate with human similarity judgments. Semantic similarity measurement, which originated in psychology, is a methodology fulfilling this requirement and supporting geographic information retrieval. The following special issue presents work on semantic similarity measurement from different perspectives, including cognitive science, information retrieval, and ontology engineering, with a focus on applications in GIScience. It originated in the Workshop on Semantic Similarity Measurement and Geospatial Applications held in conjunction with COSIT 2007, the International Conference on Spatial Information Theory (http://www.cosit.info/). A substantial part of the workshop contributions addressed the need for similarity measurement in geographic information retrieval, including applications in web service discovery, knowledge management, and emergency scenarios. The call for papers to this issue was based on these workshop contributions, discussions, and results (http://musil.uni-muenster.de), but open to any submissions on the role of semantic similarity in GIScience. Eleven papers were submitted and then Geographic location is a key component for information retrieval on the Web, recommendation systems in mobile computing and social networks, and place-based integration on the Linked Data cloud. Previous work has addressed how to estimate locations by named entity recognition, from images, and via structured data. In this paper, we estimate geographic regions from unstructured, non geo-referenced text by computing a probability distribution over the Earth\\u2019s surface. Our methodology combines natural language processing, geostatistics, and a data-driven bottom-up semantics. We illustrate its potential for mapping geographic regions from non geo-referenced text. Reference to places is a central but largely underexposed problem of information science. Place has been a major object of research in many domains including Geography, Cognitive Science and Geographic Information Science. However, Geographic Information Systems (GIS) have been built solely on space reference systems creating a gap between human conceptualization and machine representation. While reference to space only partially captures reference to place, most existing definitions of place either reduce the latter to the former or lack a formal characterization of how places are constructed. In a spatial coordinate system, locations are referenced by angles and distances to other referents. In this paper, we suggest that place reference systems can be built based on localizing things (locatums) involved in simulated activities relative to other involved referents (locators). We propose a formal theory about relevant types of activities and their involved participants, and show how place referents can be identified and localized by choosing locators and locatum among the participants. We formally derive an ontology of places, publish a corresponding OWL version, and demonstrate how to compute a market place and a vantage place in a GIS. Recent years have witnessed a large increase in the amount of information available from the Web and many other sources. Such an information deluge presents a challenge for individuals who have to identify useful information items to complete particular tasks in hand. Information value theory IVT from economics and artificial intelligence has provided some guidance on this issue. However, existing IVT studies often focus on monetary values, while ignoring the spatiotemporal properties which can play important roles in everyday tasks. In this paper, we propose a theoretical framework for task-oriented information value measurement. This framework integrates IVT with the space-time prism from time geography and measures the value of information based on its impact on an individual\\u2019s space-time prisms and its capability of improving task planning. We develop and formalize this framework by extending the utility function from space-time accessibility studies and elaborate it using a simplified example from time geography. We conduct a simulation on a real-world transportation network using the proposed framework. Our research could be applied to improving information display on small-screen mobile devices e.g., smartwatches by assigning priorities to different information items. GeoLink is one of the building block projects within EarthCube, a major effort of the National Science Foundation to establish a next-generation knowledge infrastructure for geosciences. As part of this effort, GeoLink aims to improve data retrieval, reuse, and integration of seven geoscience data repositories through the use of ontologies. In this paper, we report on the GeoLink modular ontology, which consists of an interlinked collection of ontology design patterns engineered as the result of a collaborative modeling effort. We explain our design choices, present selected modeling details, and discuss how data integration can be achieved using the patterns while respecting the existing heterogeneity within the participating repositories. A major focus in the design of Semantic Web ontology languages used to be on finding a suitable balance between the expressivity of the language and the tractability of reasoning services defined over this language. This focus mirrors the original vision of a Web composed of machine readable and understandable data. Similarly to the classical Web a few years ago, the attention is recently shifting towards a user-centric vision of the Semantic Web. Essentially, the information stored on the Web is from and for humans. This new focus is not only reflected in the fast growing Linked Data Web but also in the increasing influence of research from cognitive science, human computer interaction, and machine-learning. Cognitive aspects emerge as an essential ingredient for future work on knowledge acquisition, representation, reasoning, and interactions on the Semantic Web. Visual interfaces have to support semantic-based retrieval and at the same time hide the complexity of the underlying reasoning machinery from the user. Analogical and similarity-based reasoning should assist users in browsing and navigating through the rapidly increasing amount of information. Instead of pre-defined conceptualizations of the world, the selection and conceptualization of relevant information has to be tailored to the user's context on-the-fly. This involves work on ontology modularization and context-awareness, but also approaches from ecological psychology such as affordance theory which also plays an increasing role in robotics and AI. During the Dagstuhl Seminar 12221 we discussed the most promising ways to move forward on the vision of bringing findings from cognitive science to the Semantic Web, and to create synergies between the different areas of research. While the seminar focused on the use of cognitive engineering for a user-centric Semantic Web, it also discussed the reverse direction, i.e., how can the Semantic Web work on knowledge representation and reasoning feed back to the cognitive science community. The Worskhop on Ontology and Semantic Web Patterns (WOP2015, 6th edition) was held on October 11, 2015 in conjunction with the 14th International Semantic Web Conference in Bethlehem, PA, USA. At the workshop, the organizers conducted a discussion with the participants regarding the promises and obstacles of ontology design patterns (ODPs for short). This editorial reports on those discussions. We begin with a brief introduction of ODPs for the unfamiliar reader. In this work we discuss an ontology design pattern for material transformations. It models the relation between products, resources, and catalysts in the transformation process. Our axiomatization goes beyond a mere surface semantics. While we focus on the construction domain, the pattern can also be applied to chemistry and other domains. This paper presents an overview of ongoing work to develop a generic ontology design pattern for observation-based data on the Semantic Web. The core classes and relationships forming the pattern are discussed in detail and are aligned to the DOLCE foundational ontology to improve semantic interoperability and clarify the underlying ontological commitments. The pattern also forms the top-level of the the Semantic Sensor Network ontology developed by the W3C Semantic Sensor Network Incubator Group. The integration of both ontologies is discussed and directions of further work are pointed out. Information plays an important role in disaster response. In the past, there has been a lack of up-to-date information following major disasters due to the limited means of communication. This situation has changed substantially in recent years. With the ubiquity of mobile devices, people experiencing emergency events may still be able to share information via social media and peer-to-peer networks. Meanwhile, volunteers throughout the world are remotely convened by humanitarian organizations to digitize satellite images for the impacted area. These processes produce rich information which presents a new challenge for decision makers who have to interpret large amount of heterogeneous information within limited time. This short paper discusses this problem and outlines a potential solution to prioritizing information in emergency situations. Specifically, we focus on information about road network connectivity, i.e., whether a road segment is still accessible after a disaster. We propose to integrate information value theory with graph theory, and prioritize information items based on their contributions to the successes of potential rescue tasks and to the more accurate estimation of road network connectivity. Finally, we point out directions for future work. GIScience 2016 Short Paper Proceedings Understanding the Mapping Sequence of Online Volunteers in Disaster Response Yingjie Hu and Krzysztof Janowicz {yingjiehu,jano}@geog.ucsb.edu STKO Lab, Department of Geography, University of California, Santa Barbara, USA Abstract In recent years, online volunteers have been actively participating in disaster response, thanks to the advancement of information technologies and the support from humanitarian organizations. One important way in which online volunteers contribute to disaster response is by mapping the a\\u21b5ected area based on remote sensing imagery. Such online mapping generates up-to-date geographic informa- tion which can provide valuable support for the decision making of emergency responders. Typically, the area a\\u21b5ected by an disaster is divided into a number of cells using a grid-based tessellation and each volunteer can select one cell to start the mapping. While this approach coordinates the e\\u21b5orts from many online volunteers, it is unclear in which sequence these grid cells have been mapped. This sequence is important because it determines when the geographic information within a particular cell will become available to emergency responders, which in turn can directly influence the efficiency of rescue tasks and other relief e\\u21b5orts. In this work, we study three online mapping projects which were deployed and utilized in 2015 Nepal, 2016 Ecuador, and 2016 Japan earthquakes to gain insights into the mapping sequences performed by online volunteers. Keywords: Disaster response, crisis mapping, volunteered geographic information Introduction In recent years, online volunteers have been actively involved in disaster response. On the one side, infor- mation and communication technologies allow volunteers to contribute to disaster relief without having to be physically present at the a\\u21b5ected areas. On the other side, humanitarian communities, such as Standby Task Force (Meier, 2012a) and Crisis Mapper (Shanley et al., 2013), play an important role in bringing together online volunteers and coordinating their e\\u21b5orts. With the support from technologies and hu- manitarian organizations, volunteers have made important contributions to 2010 Haiti earthquake (Zook et al., 2010), 2012 Hurricane Sandy in the U.S. (Meier, 2012b), 2013 Typhoon Haiyan in the Philippines (Humanitarian OpenStreetMap Team, 2013), and the 2015 Nepal Earthquake (Hu and Janowicz, 2015). One important way in which volunteers contribute to disaster response is by mapping the a\\u21b5ected areas based on remote sensing images. During the online mapping process, volunteers digitize geographic features which may be missing from the previous maps, as well as update the existing geographic data to reflect the current status (e.g., a road may be blocked after an earthquake). This process generates up- to-date geographic information which can provide valuable support for the decision making of emergency responders. While these volunteer-contributed data may not be of highest quality, they generally satisfy the needs of disaster response (Goodchild and Glennon, 2010). Since many volunteers may be participating in online mapping at the same time, humanitarian orga- nizations often divide the a\\u21b5ected area into cells using a grid-based tessellation. Each online volunteer can then select a grid cell to start the mapping task. While this approach helps avoid editing conflicts and duplications, there is a lack of understanding on the sequence in which online volunteers map the grid cells. Such a sequence is important because it directly determines the time when the geographic information within a particular cell will become available. From our observation, the mapping time di\\u21b5er- ence between two neighboring grid cells can be 4 days and sometimes even longer. In disaster response, the first 72 hours after a disaster have been widely considered as the critical period for rescue tasks. After this period, the survival rate drops dramatically (Fiedrich et al., 2000; Comfort et al., 2004; Ochoa and Santos, 2015). Thus, if the grid cells that contain the critical information for disaster response are mapped first, more people can be potentially saved. Intuitively, selecting cells at random is not an ideal solution, since population, transportation infrastructure, and potential rescue routes should be taken into account. Sensor data is stored and published using OGC\\u2019s Observation & Measurement specifications as underlying data model. With the advent of volunteered geographic information and the Semantic Sensor Web, work on an ontological, i.e. conceptual, model gains importance within the Sensor Web Enablement community. In contrast to a data model, an ontological approach abstracts from implementation details by focusing on modeling the real world from the perspective of a particular domain or application and, hence, restricts the interpretation of the used terminology towards their intended meaning. The shift to linked sensor data, however, requires yet another perspective. Two challenges have to be addressed, (i) how to refer to changing and frequently updated data sets such as stored in Sensor Observation Services using Uniform Resource Identifiers, and (ii) how to establish meaningful links between those data sets, i.e., observations, sensors, features of interest, observed properties, and further participants in the measurement process. In this short paper we focus on the problem of assigning meaningful URIs. Traditional gazetteers are built and maintained by authoritative mapping agencies. In the age of Big Data, it is possible to construct gazetteers in a data-driven approach by mining rich volunteered geographic information (VGI) from the Web. In this research, we build a scalable distributed platform and a high-performance geoprocessing workflow based on the Hadoop ecosystem to harvest crowd-sourced gazetteer entries. Using experiments based on geotagged datasets in Flickr, we find that the MapReduce-based workflow running on the spatially enabled Hadoop cluster can reduce the processing time compared with traditional desktop-based operations by an order of magnitude. We demonstrate how to use such a novel spatial-computing infrastructure to facilitate gazetteer research. In addition, we introduce a provenance-based trust model for quality assurance. This work offers new insights on enriching future gazetteers with the use of Hadoop clusters, and makes contributions in connecting GIS to the cloud computing environment for the next frontier of Big Geo-Data analytics.\",\n",
            "  \"311794911\": \"Some researchers have argued that algebra word problems are difficult for students because they have difficulty in comprehending English. Others have argued that because algebra is a generalization of arithmetic, and generalization is hard, it\\u2019s the use of variables, per se, that cause difficulty for students. Heffernan and Koedinger [9] [10] presented evidence against both of these hypotheses. In this paper we present how to use tutorial log files from an intelligent tutoring system to try to contribute to answering such questions. We take advantage of the Power Law of Learning, which predicts that error rates should fit a power function, to try to find the best fitting mathematical model that predicts whether a student will get a question correct. We decompose the question of \\u201cWhy are Algebra Word Problems Difficult?\\u201d into two pieces. First, is there evidence for the existence of this articulation skill that Heffernan and Koedinger argued for? Secondly, is there evidence for the existence of the skill of \\u201ccomposed articulation\\u201d as the best way to model the \\u201ccomposition effect\\u201d that Heffernan and Koedinger discovered? In this paper, we proposed a new cognitive modeling approach: Instructional Factors Analysis Model (IFM). It belongs to a class of Knowledge-Component-based cognitive models. More specifically, IFM is targeted for modeling student\\u2019s performance when multiple types of instructional interventions are involved and some of them may not generate a direct observation of students\\u2019 performance. We compared IFM to two other pre-existing cognitive models: Additive Factor Models (AFMs) and Performance Factor Models (PFMs). The three methods differ mainly on how a student\\u2019s previous experience on a Knowledge Component is counted into multiple categories. Among the three models, instructional interventions without immediate direct observations can be easily incorporate into the AFM and IFM models. Therefore, they are further compared on two important tasks\\u2014unseen student prediction and unseen step prediction\\u2014and to determine whether the extra flexibility afforded by additional parameters leads to better models, or just to over fitting. Our results suggested that, for datasets involving multiple types of learning interventions, dividing student learning opportunities into multiple categories is beneficial in that IFM out-performed both AFM and PFM models on various tasks. However, the relative performance of the IFM models depends on the specific prediction task; so, experimenters facing a novel task should engage in some measure of model selection. Seeing Language Learning inside the Math: Cognitive Analysis Yields Transfer Kenneth R. Koedinger (koedinger@cmu.edu) Human-Computer Interaction Institute, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15217 USA Elizabeth A. McLaughlin (mimim@cs.cmu.edu) Human-Computer Interaction Institute, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15217 USA (Heffernan & Koedinger, 1997; 1998). Table 1 shows examples of symbolization problems, which ask students to translate a story problem into an algebraic expression. The obvious potential connection between language learning processes and this task is in learning to read and comprehend story problems. While such learning is indeed a significant challenge for elementary students (Cummins, Kintsch, Reusser, & Weimer, 1988), our past data provided evidence that comprehending story problems is no longer a major sticking point for most beginning algebra students. This claim can be illustrated by an analogy to foreign language translation: Translating a story problem to algebra is like translating English to Greek. For an English speaker, the difficulty in translating to Greek is not comprehending the English, but generating the Greek. Similarly, the challenge for older students in a beginning algebra course is much less in understanding the English in which the story problems are written and more in being able to express that understanding algebraically, that is, in the language of algebra. One indication that comprehension of algebra story problems is not a major sticking point for beginning algebra students comes from Heffernan and Koedinger\\u2019s (1998) data showing that students can solve story problems (produce a value for the dependent or \\u201cy\\u201d variable when a value for the independent or \\u201cx\\u201d variable is given) much more accurately (63% correct) than they can symbolize (write an equation relating x and y) a story problem (18% correct). Since solving requires comprehension of the story, the performance difference is suggestive that symbolizing is problematic for students in ways beyond the demands of sentence comprehension. A second indication presents a contrast with a difficulty experienced by Artificial Intelligence systems programmed to solve story problems, namely that of understanding the arithmetic relationships between quantities described in the story (Bobrow, 1968). We created problems where natural implicit descriptions of such relationships (e.g., \\u201cMs. Lindquist teaches 62 girls. Ms. Lindquist teaches b boys.\\u201d) are supplemented (Heffernan & Koedinger, 1997) or replaced (Koedinger, Alibali, & Nathan, 2008) with explicit descriptions (e.g., \\u201cThe number of students Ms. Lindquist teaches is equal to the number of boys plus the number of girls.\\u201d), which are much easier for a program to process. We found, however, that providing such explicit descriptions does not Abstract Achieving and understanding effective transfer of learning requires a careful analysis of the hidden knowledge and skills to be transferred. We present an experiment that tests a subtle prediction of such an analysis. It concluded that a critical difficulty in students\\u2019 learning to translate algebra story problems into symbolic expressions is in learning the grammar of such expressions. We hypothesized that exercises requiring students to substitute one algebraic expression into another would enhance students\\u2019 algebraic grammar knowledge. This hypothesis led to a counter-intuitive prediction that learning to symbolize story problems could be better enhanced through practice on dissimilar looking substitution exercises than through practice on more similar looking story problems. We report on an experimental comparison involving 303 middle school students that supports this prediction. We discuss how having learners externalize a uniform abstract form and get interactive feedback on it may be important factors in enhancing transfer. Keywords: cognitive task analysis; transfer; grammar learning; mathematics education. Introduction Humans learn language before they have a language to use to learn. Might the learning processes that make this amazing feat possible, like the capability to learn grammatical structures through experience without explicit instruction, be useful for other kinds of learning tasks? Once children have acquired language, are the cognitive functions employed in language learning no longer useful? For instance, as students take courses in complex academic topics, like algebra, does all that brain matter for language learning have nothing to do? Or is it possible that some of the same implicit learning mechanisms employed in language learning are useful for learning math and science? This paper does not aim to provide conclusive answers to these questions, however, it does provide a compelling demonstration that grammar learning processes may be important in learning mathematics. Students may engage in such learning without explicit awareness and such implicit learning may be more prevalent in academic learning than is generally recognized (e.g., Alibali & Goldin-Meadow, 1993; Landay & Goldstone, 2007). In earlier work, we performed a cognitive task analysis of the important task domain of \\u201csymbolization\\u201d, that is, the ability to model problem situations or \\u201cstory problems\\u201d in algebraic symbols Is Self-Explanation Always Better? The Effects of Adding Self-Explanation Prompts to an English Grammar Tutor Ruth Wylie (rwylie@cs.cmu.edu) Human-Computer Interaction Institute, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15217 USA Kenneth R. Koedinger (koedinger@cmu.edu) Human-Computer Interaction Institute, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15217 USA Teruko Mitamura (teruko@cs.cmu.edu) Language Technologies Institute, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15217 USA Abstract Several studies have demonstrated the benefits of self- explanation on learning well-defined domains like math, biology, and physics. However, these findings have yet to be replicated in probabilistic domains like second language acquisition. Working with adult English as a Second Language students (n=61) within the domain of the English article system (i.e. teaching students the difference between a dog vs. the dog) we conduct the first experimental study of the effects of prompting self-explanation on second language grammar acquisition. We compare two different modes of self-explanations (free-response and menu-based), each implemented in an intelligent tutoring system, to a control tutor with no explicit self-explanation prompts. Students in all conditions show significant learning gains but contrary to theoretical predictions, the self-explanation tutors did not lead to better learning over the no self-explanation condition. We discuss why and under what specific conditions target- specific practice without self-explanation may be a more effective instructional strategy. Keywords: Self-Explanation Effect; Computer Assisted Language Learning; ESL Grammar Learning Introduction Self-explanation has been shown to be a successful learning strategy for multiple domains, contexts, and learners. One limitation of the existing work is the domains in which it has been tested have all been math and science domains like biology (Chi, et al., 1994), physics (Chi, 1989; Conati & VanLehn, 2000), and geometry (Aleven & Koedinger, 2002), and, to the best of our knowledge, there have never been any experimental studies on the effects of self- explanation on second language grammar acquisition. Thus, an open question exists: is self-explanation truly domain independent (Roy & Chi, 2005) or are there constraints to its applicability? In the original self-explanation studies, Chi et al. (1989) examined students\\u2019 spontaneous self-explanations of a physics text. This work revealed a positive correlation between the number and type of self-explanations and student learning. In subsequent experimental studies, Chi et al. (1994) showed that students who were prompted to self- explain demonstrated greater learning gains than those who were not. Furthermore, Aleven and Koedinger (2002) demonstrated that prompting self-explanations can be an effective learning strategy even when students only select a general problem-solving principle. Within the second language acquisition community, there is a large body of research that looks at implicit versus explicit instruction. A meta-analysis of the relative effectiveness of different types of second language instruction revealed that treatments involving explicit focus on rules were more effective than those that did not (Norris & Ortega, 2000). Thus, self- explanations, which highlight explicit rules, may be beneficial for the second language learner. Our goal was to see if the success of self-explanation could be replicated within second language acquisition. To this end, we developed two tutoring systems with different types of self-explanation prompts and compared student learning gains and learning efficiency scores to a control tutor that had no explicit self-explanation prompts. Results show that while students in all three conditions demonstrate significant pre-post learning gains, students in the self- explanation conditions did no better than those in the control group. In fact, a significant learning efficiency by tutor condition interaction reveals that there may be limits to the benefits of self-explanation. Adding Self-Explanation to an Existing Tutor Self-explanation prompts were added to an existing tutoring system designed to teach the English article system (teaching students the difference between \\u201ca dog\\u201d and \\u201cthe dog\\u201d). In the existing system (Figure 1), developed using the Cognitive Tutoring Authoring Tools (Koedinger, et al., 2004), students select an article (a, an, the, or no article) from a drop-down menu to complete the sentence. They receive immediate feedback on their selections (the answer General and Efficient Cognitive Model Discovery Using a Simulated Student Nan Li (nli1@cs.cmu.edu) Eliane Stampfer (estampfe@cs.cmu.edu) William W. Cohen (wcohen@cs.cmu.edu) Kenneth R. Koedinger (koedinger@cs.cmu.edu) School of Computer Science , Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA 15213 USA Abstract important instructional details may still be overlooked. Au- tomated search methods such as Learning Factor Analysis (LFA) (Cen, Koedinger, & Junker, 2006) are more objective: the algorithm searches through the space of human-provided factors to find a cognitive model that best matches with hu- man data. Although automated search methods have found better models than manual construction, the quality of the dis- covered model depends on the quality of the human-provided factors. If there is a better model that can not be expressed by known factors, LFA will not be able to uncover it. In Li, Matsuda, Cohen, and Koedinger (2011), we have proposed to use the state-of-the-art learning agent, SimStu- dent (Matsuda, Lee, Cohen, & Koedinger, 2009), to auto- matically discover cognitive models without depending on human-provided factors. SimStudent learns skill knowledge from demonstration and problem solving experience. Each skill SimStudent acquires corresponds to a KC in the cogni- tive model. To demonstrate the generality of this approach, we present evaluations of the SimStudent-generated models in three domains: algebra, stoichiometry, and fraction addi- tion. We validate the quality of the cognitive models using hu- man student data as in Koedinger and MacLaren (1997). In- stead of matching with performance data, we use the discov- ered cognitive model to predict human learning curve data. Experimental results show that for algebra and stoichiometry, SimStudent directly finds a better cognitive model than hu- mans. For fraction addition, SimStudent results assist LFA in finding a better cognitive model than a domain expert. We have also carried out an in-depth study using Focused Bene- fits Investigation (FBI) (Koedinger, McLaughlin, & Stamper, 2012) to better understand this machine learning approach, and discussed possible ways of further improvements. In order to better understand how humans acquire knowledge, one of the essential goals in cognitive science is to build a cognitive model of human learning. Moreover, a cognitive model that better matches student behavior will often yield bet- ter instruction in intelligent tutoring systems. However, man- ual construction of such cognitive models is time consuming, and requires domain expertise. Further, manually-constructed models may still miss distinctions in learning which are impor- tant for instruction. Our prior work proposed an approach that finds cognitive models using a state-of-the-art learning agent, SimStudent, and we demonstrated that, for algebra learning, the agent can find a better cognitive model than human experts. To ensure the generality of that proposed approach, we further apply it to three domains: algebra, stoichiometry, and frac- tion addition. To evaluate the quality of the cognitive models discovered, we measured how well the cognitive models fit to student learning curve data. In two of those domains, SimStu- dent directly discovers a cognitive model that predicts human student behavior better than the human-generated model. In fraction addition, SimStudent supported discovery of a better cognitive model in combination with another automated cog- nitive model discovery method. Keywords: cognitive model, machine learning, simulated stu- dent Introduction One of the fundamental goals in cognitive science is to un- derstand human knowledge acquisition. A cognitive model of human learning that fits data would be a significant achieve- ment. This goal also complements with another goal in edu- cation, which is to provide individualized instruction based on students\\u2019 abilities, learning styes, etc. Cognitive mod- els provide intelligent tutoring systems with useful informa- tion on the learning task difficulties and transfer of learning among similar problems. A better cognitive model often leads to more effective tutoring. A cognitive model is a system that can solve problems in the various ways human students can. One common way of representing a cognitive model is a set of knowledge components (KC) (Koedinger & McLaughlin, 2010). The set of KCs includes the component skills, con- cepts, or percepts that a student must learn to be successful on the target tasks. For example, a KC \\u201cdivide\\u201d in algebra encodes how to proceed given problems of the form Nv = N (e.g., \\u22123x = 6), where N stands for a number, and v stands for a variable. Nevertheless, manual construction of cognitive models re- mains time consuming and error prone. Traditional ways to construct cognitive models include structured interviews, think-aloud protocols, and rational analysis. Manual con- struction of cognitive models requires domain expertise, and A Brief Review of SimStudent SimStudent is an intelligent agent that inductively learns skills to solve problems from demonstrated solutions and from problem solving experience. It is a realization of pro- gramming by demonstration (Lau & Weld, 1998) using a variation of the version space algorithm (Mitchell, 1982), in- ductive logic programming (Muggleton & Raedt, 1994), and iterative-deepening depth-first search as underlying learning techniques. For more details, please refer to Matsuda et al. (2009). Recently, in order to build a more human-like in- telligent agent, we have developed a model of representation learning, and integrated it into SimStudent\\u2019s skill acquisition mechanism. How does the development of expertise influence problem solving performance? Specifically, do characteristics of expert behavior such as attention to deep features while problem solving and planning solution strategies emerge simultaneously or sequentially? Further, is there evidence that conceptual understanding of the meaning behind calculations can be attributed to differences in problem solving? An analysis of expert/novice problem solving revealed three stages of expertise. Novices failed to attend to deep features while problem solving (e.g., choosing which values to use in a formula) and failed to demonstrate planning of solution strategies. Advanced novices attended to deep features while carrying out specific problem steps, but did not use conceptual information to plan an effective strategy. Finally, experts attended to deep features and used conceptual information to plan effective solution strategies. An analysis of protocol data suggests that problem solving performance relates to a conceptual understanding of the quantities involved in calculations. A Cognitive Task Analysis of Using Pictures To Support Pre-Algebraic Reasoning Kenneth R. Koedinger (koedinger@cmu.edu) Human-computer Interaction Institute, Carnegie Mellon University 5000 Forbes Ave. Pittsburgh, PA 15213-3890 Atsushi Terao (atsushi@cs.cmu.edu) Human-Computer Interaction Institute, Carnegie Mellon University 5000 Forbes Ave. Pittsburgh, PA 15213-3890 USA Abstract We present an analysis of hypothesized advantages of pictorial representations for improving learning and understanding of pre-algebraic quantitative reasoning. We discuss a Picture Algebra strategy that has been used successfully by 6 th grade students as part of a new middle school mathematics curriculum. This strategy supports students in sense making both as they construct pictorial representations and as they use them to cue appropriate computations. Although we demonstrate that 6 th grade students can use this strategy to successfully solve algebra-level problems, our detailed production rule analysis revealed limitations in our instructional approach and targeted areas for improvement. Introduction As part of a larger effort to develop a 6 th grade mathematics course including both a textbook and Cognitive Tutor software (cf., Koedinger, Anderson, Hadley, & Mark, 1997), we have been exploring the use of pictorial representations to support student reasoning and learning (Rittle-Johnson & Koedinger, 2001). Here we investigate the claim that pictorial representations can help students gain early entry into algebraic reasoning and build a foundation that will facilitate more effective learning of formal algebra. Why might using pictures or diagrams be advantageous? Cognitive scientists have presented arguments and experiments for the advantages of diagrams (e.g., Cheng, 1999; Larkin & Simon, 1987). According to Larkin and Simon (p. 98), a diagram can be superior to a verbal description for solving problems for three reasons. First, diagrams reduce problem-solving search by providing localized groupings of relevant information. Second, diagrams reduce the need for matching symbolic labels. Third, diagrams support perceptual inferences that are often easier than corresponding symbolic inferences. Others have presented arguments for the use of diagrams for mathematics instruction in particular. The mathematics standards of the National Council of Teachers of Mathematics (NCTM, 2000) recommends use of pictures to support students in developing a conceptual understanding of mathematics. Pictorial representations are used extensively in Asian curricula (cf., Singapore Ministry, 1999). This usage may be a factor in the success of Asian countries on international mathematics assessments (TIMSS, 1996). Despite these arguments for the advantages of pictures, there is also reason for caution. One argument for the use of alternative representations, like pictures, is that traditional instruction focuses too much on error- prone rote learning. However, students may also acquire rote procedures when learning to use alternative representations. Further, learning an alternative representation takes time that might be better spent learning the standard representation. In this paper we introduce the Picture Algebra strategy, present student data on the use of it, and discuss a production rule model of the strategy and implications for transfer and instructional design. Picture Algebra Try to solve the Cans problem shown in Table 1 and reflect on the strategy that you use to do so. We have informally observed that many adults do not directly infer what arithmetic operations are needed to solve this problem. Instead, most begin by translating the problem statement to one or more algebraic equations, for instance, x + (x + 9) + (x + 17) = 227. They then perform transformations on the equation to arrive at a solution. Although a few use other means (cf., Hall et al., 1989), most find this problem difficult without the use of algebraic equations. In other words, this is arguably an algebra problem that we might expect to be out of reach of students without algebra instruction, for instance, 6 th graders. Figure 1A shows a 6 th grade student\\u2019s solution to this problem using a Picture Algebra strategy that was taught to students as part of our middle school mathematics curriculum. Like other problem-solving strategies, Picture Algebra can be described in two phases: a representation phase and a solution phase. In the representation phase, the student first translates the Standard intelligent tutoring systems give immediate feedback on whether students' answers are correct. This prevents unproductive floundering, but may also prevent students from engaging deeply with their misconceptions. This paper presents a prototype intelligent tutoring system with grounded feedback that supports students in evaluating and correcting their own errors. In a think-aloud study with five fifth-graders, students used the grounded feedback to self-correct, and solved more fraction addition problems with the tutor than with paper and pencil. These preliminary results are encouraging and motivate experimental work in this area. Key Misconceptions in Algebraic Problem Solving Julie L. Booth (juliebooth@cmu.edu) Kenneth R. Koedinger (koedinger@cmu.edu) Human Computer Interaction Institute, Carnegie Mellon University Pittsburgh, PA 15213 USA problems (Anderson, 1989; Van Lehn & Jones, 1993). Use of incorrect procedures is common when learning Algebra (Lerch, 2004; Sebrechts, Enright, Bennett, & Martin, 1996), and by nature, this behavior inhibits accurate solution of problems. One reason why use of these incorrect strategies may persist is that many of the procedures that students attempt to use are ones that will lead to a successful solution for some problem situations. Unfortunately, without adequate knowledge of the problem features, students are unable to distinguish between the situations in which the strategy will work and the ones where it is not applicable. For example, one common strategy students have for solving equations is that if they want to remove a term from the equation, they subtract it from both sides of the equation. This works just fine for removing 4 from the equation x + 4 = 13. However, when they encounter equations like x \\u2013 4 = 13, many students still try to subtract 4 from both sides to solve the problem. One explanation for this mistake is that those students may be deficient in their conceptual knowledge of negative signs. If they don\\u2019t process the fact that the negative sign modifies the 4 and is a necessary part of the \\u201cterm\\u201d they are trying to remove, they ignore it, to the detriment of their goal of solving the problem. Having good conceptual knowledge may thus be necessary for students to solve equations correctly, as deep strategy construction relies on inclusion of sufficient information about the problem features that make them appropriate or inappropriate. Unfortunately, for students with incorrect or incomplete conceptual knowledge about problem features, shallow strategies, such as the one described above, will likely prevail. Abstract The current study examines how holding misconceptions about key problem features affects students\\u2019 ability to solve algebraic equations correctly and to learn correct procedures for problem solution. Algebra I students learning to solve simple equations using the Cognitive Tutor curriculum (Koedinger, Anderson, Hadley, & Mark, 1997) completed a pretest and posttest designed to evaluate their conceptual understanding of problem features (including the equals sign and negative signs) as well as their equation solving skill. Results indicate that students who begin the lesson with misconceptions about the meaning of the equals sign or negative signs solve fewer equations correctly at pretest, and also have difficulty learning how to solve them. However, improving their knowledge of those features over the course of the lesson increases their learning of correct procedures. Keywords: algebraic problem solving; conceptual knowledge; learning; mathematics education Introduction Learners are not blank slates. Each student brings prior knowledge into a lesson, and that knowledge can greatly influence what the student will gain from the experience. Possession of good content knowledge has been associated with advantages in memory (Chi, 1978; Tenenbaum, Tehan, Stewart, & Christensen, 1999), generation of inferences (Chi, Hutchinson, & Robin, 1989), categorization (Chi, Feltovich, & Glaser, 1981), strategy use and acquisition (Gaultney, 1995; Alexander & Schwanenflugel, 1994), and reasoning skills (Gobbo & Chi, 1986; Johnson, Scott, & Mervis, 2004). It follows, then, that students who do not possess this important knowledge are at a disadvantage for successful problem solving or for learning new information. In the domain of algebraic problem solving, one type of prior knowledge that is key to learning is conceptual understanding of features in the problem (e.g., equals sign, variables, like terms, negative signs, etc.). We operationally define conceptual knowledge of these features as not just recognizing the symbols or carrying out an operation, but understanding the function of the feature in the equation and how changing the location of the feature would affect the overall problem. In the next two sections, we describe why a lack of understanding about these features could interfere with students\\u2019 performance and learning on procedural equation-solving problems. Why conceptual knowledge of problem features should affect students\\u2019 learning Instruction on procedural problems can take many forms, including demonstration by a teacher, study of written worked examples, hint messages from a computerized tutor, or feedback from a teacher or computer program. What all of these instructional techniques have in common is that in all cases, students need to be able align the presented information with the information in the problems they are or will be attempting to solve. Carrying out this alignment is not trivial. Often times, people focus on literal similarity between the two sources -- which makes it difficult to identify the crucial features that have meaningful similarity -- as opposed to the overall structural similarity, which enables extraction of causal principles (Gentner, 1989) Why conceptual knowledge of problem features should affect students\\u2019 performance Previous research has proposed that student misconceptions or gaps in conceptual knowledge of Algebra lead to use of incorrect, buggy procedures for solving Helping Students Know \\u2018Further\\u2019 \\u2013 Increasing the Flexibility of Students\\u2019 Knowledge Using Symbolic Invention Tasks Ido Roll (idoroll@cmu.edu) Human Computer Interaction Institute, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA Vincent Aleven (aleven@cs.cmu.edu) Human Computer Interaction Institute, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA Kenneth R. Koedinger (koedinger@cmu.edu) Human Computer Interaction Institute and Department of Psychology, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA Abstract evaluate a set of examples with regard to one aspect of the data. Figure 1 shows an example of such a task, in which students are asked to invent a method for comparing the variability of two datasets, in order to choose the more consistent one (i.e., where data is \\u201ccloser together\\u201d). Following the invention attempt, students receive direct instruction on canonical methods and practice them. For example, following the task detailed in Figure 1, students receive instruction on Mean Absolute Deviation and practice applying it. While students often fail to invent general valid methods, research suggests that this experience prepares them to better learn independently from learning opportunities that follow the instruction (Schwartz & Martin, 2004; Kapur, 2008). IPL tasks use contrasting cases to direct students' attention to deep features of the domain. Rather than analyzing a single dataset, as commonly done in show- and-practice problems, IPL tasks ask students to compare two or more sets of data that vary along a single deep feature. For example, the two sets in Figure 1 have the same average and sample size but differ in their range. The contrasting cases also give students a baseline against which to evaluate their inventions, since their intuitive comparison of the cases is often clear and correct (Schwartz, Sears, & Chang, 2007). The invention activity itself, prior to instruction, can be divided into two parts. First, students analyze the contrasting cases and rank them intuitively according to the target construct (e.g., variability). We refer to this stage as comparative reasoning, since students reason about the task by comparing the different cases. The second part is the design of mathematical methods, in which students attempt to invent general valid methods that rank the cases in the same way as their intuitive ranking. We refer to this stage as generative reasoning, since students generate symbolic methods to quantitatively compare the contrasting cases. Table 1 shows a summary of the IPL process. In the current study, we begin to unpack the IPL process and its effects. Our first research question evaluates the different roles of comparative vs. generative Invention as Preparation for Learning (IPL) is a teaching strategy in which students attempt to develop novel solutions prior to receiving instruction (Schwartz & Taylor, 2004). This method was previously shown to prepare students to learn independently from future learning opportunities that build upon the materials learned in class. We began unpacking the IPL process by identifying its components and evaluating the contribution of generative reasoning (in the form of symbolic invention) on top of comparative reasoning (in the form of ranking alternatives). An in-vivo study in 6 middle-school classes with 105 students found that generative reasoning is an essential component of IPL. Furthermore, we found that students who attempted to invent symbolic models during the IPL process (generative reasoning) were able to invent new strategies during the post-test. At the same time, students who completed the IPL process without designing symbolic methods were in need for worked-out examples in order to solve new-strategy problems in the post-test. We propose a mechanism that explains how invention leads to the observed increased flexibility in students\\u2019 knowledge. Keywords: Invention as Preparation for Learning; Preparation for Future learning; Transfer; Comfort Zone; Generative Reasoning. Introduction Invention as Preparation for Learning (IPL) is a teaching strategy that uses constructivist instructional methods and direct instruction in a complementary fashion (Schwartz & Martin, 2004). First, students are asked to invent general methods (and their mathematical expressions) to The Bouncers Trampoline Company tests their trampolines by dropping a 100 lb weight from 15 feet. They measure how many feet the weight bounces back into the air. They do several trials for each trampoline. Here are the results for two of their trampolines: Trampoline A: {1 3 5 7 9} Trampoline B: {3 4 5 6 7} Which trampoline is more consistent, that is, its test results are closer together? Figure 1: The trampoline IPL task. SimStudent is an educational software infrastructure which is designed to leverage the tutor effect in an on-line learning environment. Tutor effect is the phenomenon that students learn when they teach others. SimStudent allows students to learn by teaching a computer agent instead of their peers. SimStudent is a lively computer agent that inductively learns skills through its own tutored-problem solving experience. SimStudent is integrated into an on-line learning environment where students can interactively tutor SimStudent in how to solve equations [1]. Giving effective help is an important collaborative skill that leads to improved learning for both the help-giver and help-receiver. Adding intelligent tutoring to student interaction may be one effective way of assisting students in giving and receiving better help. However, such systems have proven difficult to implement, in part due to the challenges of modeling productive dialogue in a collaborative activity. We present a theoretical model of good helping behavior in a peer tutoring context, and validate the model using student tutoring data, linking optimal and buggy behaviors to learning outcomes. We discuss the implications of the model with respect to providing intelligent tutoring for peer tutoring. Helping students' improve their metacognitive and self-regulation skills holds the potential to improve students' ability to learn independently. Yet, to date, there are relatively few success stories of helping students enhance their metacognitive skills using interactive learning environments. In this paper we describe the Self-Assessment Tutor, an intelligent tutoring system for improving the accuracy of the judgments students make regarding their own knowledge. A classroom evaluation of the Self-Assessment Tutor with 84 students found that students improved their ability to identify their strengths while working with the Self-Assessment Tutor. In addition, students transferred the improved self-assessment skills to corresponding sections in the Geometry Cognitive Tutor. However, students often failed to identify their knowledge deficits a-priori and failed to update their assessments following unsuccessful solution attempts. This study contributes to theories of Self-Assessment and provides support for the viability of improving metacognitive skills using intelligent tutoring systems. The Effect of Prior Conceptual Knowledge on Procedural Performance and Learning in Algebra Julie L. Booth (juliebooth@cmu.edu) Kenneth R. Koedinger (koedinger@cmu.edu) Human Computer Interaction Institute, Carnegie Mellon University Pittsburgh, PA 15213 USA Robert S. Siegler (rs7k@andrew.cmu.edu) Department of Psychology, Carnegie Mellon University Pittsburgh, PA 15213 USA Keywords: Psychology; Education; concepts and categories; learning; problem-solving; human experimentation Introduction Errors are inevitable when individuals are first learning any skill; solving algebraic equations is no exception. Students often use incorrect procedures when learning Algebra (e.g., Lerch, 2004), and use of incorrect procedures has been hypothesized to be attributable to misunderstandings or gaps in students\\u2019 conceptual knowledge of Algebra (Anderson, 1989; Van Lehn & Jones, 1993). Experiment 1 in the current study tests this hypothesis; Experiment 2 examines whether these pretest misconceptions also affect learning of algebraic problem- solving Methods In Experiment 1, 112 middle and high school students using the Algebra I Cognitive Tutor, a self-paced intelligent tutor system (Koedinger, Anderson, Hadley, & Mark, 1997), completed a test assessing their conceptual and procedural knowledge of algebra before beginning the unit on solving two-step linear equations; there were two forms of the test and half of the students were randomly assigned to receive each. In Experiment 2, the 97 students who completed the tutor unit took the opposite form of the test. Results and Discussion Students in Experiment 1who had a good concept of the equals sign made equals sign-related errors (e.g., performing operations to only one side of an equation, dropping the equals sign from the equation) on fewer problems (10%) than those with a poor concept of the equal sign (21%; t(110) = 2.07, p < .05). Similarly, those with a good concept of like terms combined unlike terms in fewer problems (1%) than those with poorer knowledge of like terms (9%; t(110) = 2.59, p < .01). Lack of conceptual knowledge of certain features was also correlated with fewer procedural problems solved correctly (negatives (R(112) =.38, p < .01), like terms (R(112) =.32, p < .01) and equals sign (R(112) =.59, p < .01); lack of equals sign knowledge is predictive of performance beyond the other two misconceptions (21% variance added; t(109) = 5.95, p < .01) In Experiment 2, improved equals sign knowledge correlated with improvement in problem-solving (controlling for the amount of procedural improvement possible), (partial R(97) = .23, p < .05). In addition, two marginally significant trends were found suggesting that pretest knowledge of negatives and of like terms predicted students\\u2019 learning on transfer problems (partial Rs(97) = .17, ps < .10). Results from this study indicate that a lack of knowledge about certain conceptual features is associated with use of related incorrect procedures when solving equations. These misconceptions lead students to solve less problems correctly and, in some cases, to learn less from instruction unless they are corrected. This suggests that improving students\\u2019 knowledge of the conceptual features that underlie Algebra may be necessary for robust learning to occur. Acknowledgments Funding for this research is provided by the National Science Foundation, Grant Number SBE-0354420 to the Pittsburgh Science of Learning Center (PSLC, http://www.learnlab.org). References Anderson, J.R. (1989). The analogical origins of errors in problem solving. In D. Klahr & K. Kotovsky (Eds). Complex information processing: The impact of Herbert A. Simon. Hillsdale, NJ, England: Lawrence Erlbaum Associates, Inc. Koedinger, K.R., Anderson, J.R., Hadley, W.H., & Mark, M.A. (1997). Intelligent tutoring goes to school in the big city. International Journal of Artificial Intelligence in Education, 8, 30-43. Lerch, C. M. (2004). Control decisions and personal beliefs: Their effect on solving mathematical problems. Journal of Mathematical Behavior, 23, 21-36. Van Lehn, K., & Jones, R.M. (1993). What mediates the self-explanation effect? Knowledge gaps, schemas, or analogies? In M. Polson (Ed.) Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society (pp. 1034-1039). To study the impact of extrinsic motivational intervention, a competitive Game Show was integrated into an on-line learning environment where students learn algebra equation solving by teaching a synthetic peer learner, called SimStudent. In the Game Show, a pair of SimStudents competed with each other by solving challenging problems to achieve higher ratings. To evaluate the effectiveness of the Game Show in the context of learning by teaching, we conducted a classroom study with 141 students in 7thto 9th grade. The results showed that to facilitate students' learning, the Game Show setting must be carefully designed so that (1) the Game Show goal and learning goal are aligned, and (2) it fosters a symbiotic scenario in which both winners and losers of the game show learn. An ideal scenario for educational research is to perform an experiment, report and publish results, make the results and data available for verification, and finally allow the data to be used in follow up experiments or for secondary analyses. Unfortunately, this scenario often fails after the results are published. Researchers move on to new data and the old data may linger on a legacy server for a short while before disappearing or becoming impossible to comprehend. Managing the dataset lifecycle is a way to address this problem. DataShop (http://pslcdatashop.org) is a central hub for data management of educational data, and in this paper we show how DataShop fits into the dataset lifecycle. We explore the potential of bringing together the advantages of computer games and the physical world to increase engagement, collaboration and learning. We introduce EarthShake: A tangible interface and mixed-reality game consisting of an interactive multimodal earthquake table, block towers and a computer game synchronized with the physical world via depth camera sensing. EarthShake helps kids discover physics principles while experimenting with real blocks in a physical environment supported with audio and visual feedback. Students interactively make predictions, see results, grapple with disconfirming evidence and formulate explanations in forms of general principles. We report on a preliminary user study with 12 children, ages 4-8, indicating that EarthShake produces large and significant learning gains, improvement in explanation of physics concepts, and clear signs of productive collaboration and high engagement. Systems for smart authoring of automated tutors, like SimStudent, have been mostly applied in well-defined problem-solving domains where little real-world background knowledge is needed, like math. Here we explore the generality of these methods by considering a very different task, article selection in English, where little problem-solving is done, but where complex prior perceptual skills and large amounts of background knowledge are needed. This background knowledge includes the ability to parse text and the extensive understanding of semantics of English words and phrases. We show that good performance can be obtained by coupling SimStudent with appropriate broad-coverage linguistic tools. Performance can be improved further on this task by extending one of the learning mechanisms used by SimStudent so that it will accept less-accurate production rule conditions, and prioritize learned production rules by accuracy. Experimental results show that the extended SimStudent successfully learns the tutored article selection grammar rules, and can be used to discover a student model that predicts human student behavior as well as the human-generated model. One of the key factors that affects automated tutoring systems in making instructional decisions is the quality of the student model built in the system. A student model is a model that can solve problems in various ways as human students. A good student model that matches with student behavior patterns often provides useful information on learning task difficulty and transfer of learning between related problems, and thus often yields better instruction on intelligent tutoring systems. However, traditional ways of constructing such models are often time consuming, and may still miss distinctions in content and learning that have important instructional implications. Automated methods can be used to find better student models, but usually require some engineering effort, and can be hard to interpret. In this paper, we propose an automated approach that finds student models using a clustering algorithm based on automaticallygenerated problem content features. We demonstrate the proposed approach using an algebra dataset. Experimental results show that the discovered model is as good as one of the best existing models, which is a model found by a previous automated approach, but without the knowledge engineering effort. This paper presents a developmental model of students' acquisition of competence in quantitative and algebraic problem solving. A key notion underlying the developmental model is a distinction between grounded and abstract representations. Grounded representations, like story problems, are more concrete and familiar, closer to physical objects and everyday events. Abstract representations, like symbolic equations, are concise and easy to manipulate, but are distanced from any physical objects of reference. The complementary computational characteristics of grounded and abstract representations lead to hypotheses about the order of skill acquisition. In prior research, the authors demonstrated that early in the development of algebraic competence, the advantages of grounded representations outweigh those of abstract representations--for simpler problems, students are better at story problems than the analogous equations. This paper presents two studies that test the hypothesis that later in algebra development, the advantages of abstract representations emerge--for more complex problems, students are better at equations than the analogous story problems. Includes 6 tables, 7 figures, and 16 references. (Author/WRM) ******************************************************************************** Reproductions supplied by EDRS are the best that can be made from the original document. ******************************************************************************** Algebra Problem Solving Development 1 A Developmental Model of Algebra Problem Solving: Trade-offs Between Grounded and Abstract Representations Kenneth R. Koedinger M :BLE 1 PERMISSION TO REPRODUCE AND DISSEMINATE THIS MATERIAL HAS BEEN GRANTED BY ktiadj TO THE EDUCATIONAL RESOURCES INFORMATION CENTER (ERIC) U.S. DEPARTMENT OF EDUCATION Office of Educational Research and Improvement EDUCATIONAL RESOURCES INFORMATION CENTER (ERIC) This document has been reproduced as ived from the person or organization originating it. Minor changes have been made to improve reproduction quality. Points of view or opinions stated in this document do not necessarily represent official OERI position cr policy. Koedinger, K. R., Alibali, M. W., Nathan, M. J. (1999). A developmental model of algebra problem solving: Trade-offs between grounded and abstract representations. Paper prepared for the annual meeting of the American Educational Research Association, Montreal. Algebra Problem Solving Development 2 ABSTRACT We present a developmental model of students' acquisition of competence in quantitative and algebraic problem solving. A key notion underlying our developmental model is a distinction between grounded and abstract representations. Grounded representations, like story problems, are more concrete and familiar, closer to physical objects and everyday events. Abstract representations, like symbolic equations, are concise and easy to manipulate, but are distanced from any physical objects of reference. The complementary computational characteristics of grounded and abstract representations lead to hypotheses about the order of skill acquisition. In prior research (Koedinger & Nathan, 1999), we demonstrated that early in the development of algebraic competence, the advantages of grounded representations outweigh those of abstract representations for simpler problems, students are better at story problems than the analogous equations. This paper presents two studies that test the hypothesis that later in algebra development, the advantages of abstract representations emerge for more complex problems, students are better at equations than the analogous story problems.We present a developmental model of students' acquisition of competence in quantitative and algebraic problem solving. A key notion underlying our developmental model is a distinction between grounded and abstract representations. Grounded representations, like story problems, are more concrete and familiar, closer to physical objects and everyday events. Abstract representations, like symbolic equations, are concise and easy to manipulate, but are distanced from any physical objects of reference. The complementary computational characteristics of grounded and abstract representations lead to hypotheses about the order of skill acquisition. In prior research (Koedinger & Nathan, 1999), we demonstrated that early in the development of algebraic competence, the advantages of grounded representations outweigh those of abstract representations for simpler problems, students are better at story problems than the analogous equations. This paper presents two studies that test the hypothesis that later in algebra development, the advantages of abstract representations emerge for more complex problems, students are better at equations than the analogous story problems. Algebra Problem Solving Development 3 INTRODUCTION This paper presents a developmental model of students at different levels of competence in quantitative and algebraic problem solving. In prior research, we employed the difficulty factors assessment (DFA) methodology to explore early algebra problem solving, and identified effects that contradict common beliefs and current instructional practices (Koedinger & Tabachneck, 1995; Koedinger & Mac Laren, 1997; Koedinger & Nathan, 1999). In this paper, we review these results and present new results about student problem solving at higher levels of competence. Together these results on difficulty factors in algebra problem solving provide a picture of student development from arithmetic competence through various distinguishable levels of algebraic competence. A key notion underlying our developmental model is a distinction between grounded and abstract representations. Grounded representations are ones that are more concrete, closer to physical objects and everyday events. In the context of quantitative reasoning, real world problem situations or \\\"story problems\\\" are more grounded than symbolic equations because they use familiar words and refer to familiar physical objects and events'. For example, consider the following story problem. Ted works as a waiter. He worked 6 hours in one day and also got $66 in tips. If he made $81.90 that day, how much per hour does Ted make? Given some experience with money and waiters, the words, objects and events described in this problem are relatively familiar to students. Students' understanding of the quantitative relationships described is thus grounded in these familiar terms. 1 How grounded a particular problem is for a particular student depends on that student's past experiences with the particular objects and events in a problem as well as the words or symbols used to refer to them. To the Algebra Problem Solving Development 4 Abstract representations, in contrast, are short and concise, leaving out any direct indication of the physical objects and events being referred to. Consider the following algebraic representation of the story problem above.representations, in contrast, are short and concise, leaving out any direct indication of the physical objects and events being referred to. Consider the following algebraic representation of the story problem above. x * 6 + 66 = 81.90 The equation is clearly shorter and more concise than the story problem above. Besides the numbers common to both problems (6, 66, 81.90) there are only four other characters in the equation (x, *, +, .) whereas there are 100 other characters in the story. What is left out, however, is any reference to the familiar objects and events like hourly wages and tips. Furthermore, the terminology is different. The \\\"words\\\" in the algebraic sentence (i.e., x, *, +, =) are less familiar than the phrases expressing analogous meanings in the story (i.e., how much, hours in one day, also got, he made). The difference between grounded and abstract representations is not as discrete as these examples might indicate. There are multiple levels of intermediate groundedness or abstractness. Consider the following word problem, which is devoid of situational content: Starting with some number, if I multiply it by 6 and then add 66, I get 81.90. What number did I start with? It is intermediate in abstractness between the story and the equation, having 78 characters besides the numbers. It is also intermediate in its groundedness. While it is missing familiar references to money and waiters, it contains words that are more familiar (i.e., some number, multiply, add, get) than the characters expressing analogous meaning in the equation (i.e., x, It is difficult to build intelligent tutoring systems in the domain of programming due to the complexity and variety of possible answers. To simplify this process, we have constructed a language-independent canonicalized model for programming solutions. This model allows for much greater overlap across different students than a basic text model, which enables more self-sustaining hint generation methods in programming tutors. The rich interaction space of many educational games presents a challenge for designers and researchers who strive to help players achieve specific learning outcomes. Giving players a large amount of freedom over how they perform a complex game task makes it difficult to anticipate what they will do. In order to address this issue designers must ask: what are students doing in my game? And does it embody what I intended them to learn? To answer these questions, designers need methods to expose the details of student play. We describe our approach for automatic extraction of conceptual features from logs of student play sessions within an open educational game utilizing a two-dimensional context-free grammar. We demonstrate how these features can be used to cluster student solutions in the educational game RumbleBlocks. Using these clusters, we explore the range of solutions and measure how many students use the designers\\u2019 envisioned solution. Equipped with this information, designers and researchers can focus redesign efforts to areas in the game where discrepancies exist between the designers\\u2019 intentions and player experiences. Students can use an educational system\\u2019s help in unexpected ways. For example, they may bypass abstract hints in search of a concrete solution. This behavior has traditionally been labeled as a form of gaming or help abuse. We propose that some examples of this behavior are not abusive and that bottom-out hints can act as worked examples. We create a model for distinguishing good student use of bottom-out hints from bad student use of bottom-out hints by means of logged response times. We show that this model not only predicts learning, but captures behaviors related to self-explanation. The goal of our research is to investigate whether a Cognitive Tutor can be made more effective by extending it to help students acquire help-seeking skills. We present a preliminary model of help-seeking behavior that will provide the basis for a Help-Seeking Tutor Agent. The model, implemented by 57 production rules, captures both productive and unproductive help-seeking behavior. As a first test of the model\\u2019s efficacy, we used it off-line to evaluate students\\u2019 help-seeking behavior in an existing data set of student-tutor interactions, We found that 72% of all student actions represented unproductive help-seeking behavior. Consistent with some of our earlier work (Aleven & Koedinger, 2000) we found a proliferation of hint abuse (e.g., using hints to find answers rather than trying to understand). We also found that students frequently avoided using help when it was likely to be of benefit and often acted in a quick, possibly undeliberate manner. Students\\u2019 help-seeking behavior accounted for as much variance in their learning gains as their performance at the cognitive level (i.e., the errors that they made with the tutor). These findings indicate that the help-seeking model needs to be adjusted, but they also underscore the importance of the educational need that the Help-Seeking Tutor Agent aims to address. Effective instructional design requires navigating the tradeoff between providing helpful cues to the correct solutions and supplying hints that ultimately detract from what students learn. The present study manipulated the correlations between superficial features and the correct solutions in a set of training problems in the domain of exploratory data analysis and examined their effect on novices with no prior knowledge of statistics. Students who were trained on problems with these spurious correlations performed more poorly on posttest problems lacking these associations, making errors in the direction predicted by the misleading features. The theoretical and educational implications of the outcomes of this practice are discussed. Introduction When learning to solve problems based on a new concept, students often are influenced by the surface features of the problems (Chi, Feltovich, & Glaser, 1981; Ross, 1984, 1987; Palmer, 1997). While this reliance on superficial features may facilitate the quick retrieval of previously successful solution methods that are helpful to the current problem, it can also hinder students\\u2019 abstraction of the deeper concepts that can be transferred to more distant problems. Instructional treatments that draw upon the impact of specific instances, such as learning from workedout examples (Sweller & Cooper, 1985; Zhu & Simon, 1987) or case studies (Kolodner, 1993), consequently may risk having their effectiveness undermined by students\\u2019 oversensitivity to surface similarity. A variety of interventions involving explicit instruction have been examined as possible remedies to alleviate this problem. Clement and his colleagues capitalize on the power of concrete details in advocating the use of \\u201cbridging analogies\\u201d that gradually develop the fundamental principles from the superficial features by providing a sequence of progressively more abstract analogues to the original \\u201canchor\\u201d (Clement, Brown, & Zietsman, 1989; Brown, 1992; Clement, 1993). Other research on analogies underscores the value of the familiar \\u201ccompare-andcontrast\\u201d injunction in highlighting key relationships and conceptual structures within the examples being studied. As described by Gentner and her colleagues, analogical encoding or mutual alignment between simultaneously juxtaposed examples promotes the abstraction and transfer of general principles (Loewenstein, Thompson, & Gentner, 1999; Thompson, Gentner, & Loewenstein, 2000; Loewenstein & Gentner, 2001; Kurtz, Miao, & Gentner, 2001). Similarly, Schwartz and Bransford (1998) endorse the analysis of contrasting cases to boost students\\u2019 ability to identify the features that differentiate those cases, as well as their comprehension of a subsequent explanation of principles involving those features. To maximize the potential benefit of instructing the learner to generalize across examples, one should select those examples according to principles that will further highlight the deep concepts and diminish the superficial similarities. The multitude of studies that find minimal transfer when irrelevant features such as the situational context change suggest that participants may be encoding and using these features as cues for their problem-solving strategies (Gick & Holyoak, 1983; Catrambone & Holyoak, 1989; see Barnett & Ceci, 2002, for review). Interpreting these results through an associationist framework yields the implication that varying these features during training would prevent these cues from becoming too strongly learned in the first place. Especially if students may already exhibit predispositions toward noticing and utilizing certain superficial features that they find more salient, instructors should beware of allowing those superficial features to become misleadingly helpful during their problem-solving experiences. Rather, the goal should be to select examples that minimize spurious correlations between irrelevant features and the correct solution method, thereby reducing the noise so that the student has the opportunity to learn the conceptual relationships in the problems. This principle 228 would apply not just to the situational context, but to the peripheral content in the example problems as well. Other implications that derive from associative learning theory are that even if students recognize that these spuriously correlated cues are not meaningful, they may still need to inhibit these associations when solving the problems, an especially costly burden under the high cognitive load that accompanies difficult problems. Forming associations with multiple cues may also result in decreased strength for the association to the target cue, if the learning is now spread across many cues. Further, if these superficial features are too predictive of the correct solution strategy, their associations may grow so strong as to block the learning of the association to the relevant feature. To test the applicability of this theory, the experiment described here investigated the impact of such spurious correlations on subsequent problem-solving performance in the domain of exploratory data analysis. Method Participants Eighteen undergraduate students from Carnegie Mellon University were recruited to participate in the experiment. Two participants left the experiment due to visa restrictions on their eligibility for payment. None of the participants had any statistics background beyond high-school mathematics, and all were fluent in English. Design The experiment employed a between-subjects design, with participants randomly assigned to condition. Nine students completed the condition incorporating spurious correlations with superficial features in the training problems (\\u201cspurious\\u201d or \\u201cS\\u201d condition), and seven students completed the condition in which these features were allowed to vary across representation types (\\u201cvaried\\u201d or \\u201cV\\u201d condition). Materials Problem-Sorting Task Six word problems were written with different combinations of cover story themes (cars, sports, crime) and solution methods (boxplot, scatterplot, contingency table). Each problem was typed on a separate 2.5\\\" x 5.5\\\" card so that participants could easily sort the cards into different groups. Skills Assessment A paper-and-pencil test consisting of 26 multiple-choice questions was constructed to determine participants\\u2019 knowledge of relevant statistical definitions and their skills at interpreting and selecting the appropriate type of data display to answer a given question (histogram, boxplot, scatterplot, contingency table). Data-Analysis Training Problems Sixty problems were written for the training phase of the experiment, thirty per condition. Each problem consisted of a dataset and cover story requiring the student to construct one of the following data representations: pie chart, histogram, side-by-side boxplots, scatterplot, or contingency table. The superficial features manipulated were the cover-story theme, the wording of the question, and the number and combination of variable types presented in the dataset. In the \\u201cS\\u201d condition, all problems requiring the same type of data representation were accompanied by a cover story of the same theme, with the question of interest always phrased in the same way. For each representation type, every problem except one was presented with the same number and combination of variable types in the dataset (categorical or quantitative), with the variables presented in the same order. The exception provided three variables so that Scondition participants would gain some experience having to decide which variables were relevant to the problem. Table 1 shows the mappings between each of these features and the representation type. As shown below, all the boxplot problems used the same cover story and wording: Gosset College is looking for patterns in its course evaluations to find ways to improve its introductory classes. Is there a difference in overall course ratings between small classes and large classes? Bonferroni University is performing a grade audit to determine if it has experienced grade inflation in the last ten years. Is there a difference in grades between the class of 1990 and the class of 2000? These examples each provided a dataset containing one quantitative and one categorical variable. Table 1: Superficial Features for Problems in \\u201cSpurious\\u201d Condition Representation Cover Story Theme Question Wording Variables in dataset Pie Chart demographics \\u201cWhat percentage...\\u201d 1 categorical Histogram entertainment \\u201cHow would you describe the features of the distribution of...\\u201d 1 quantitative Boxplot academics \\u201cIs there a difference between... in ...\\u201d 1 quantitative, 1 categorical Scatterplot money \\u201cIs there an association between... and ...\\u201d 2 quantitative Contingency Table health \\u201cIs there a significant effect of ... on whether...\\u201d 2 categorical It is becoming a standard technique to use learning curves as part of evaluation of intelligent tutoring systems [1,2,3], but such learning curves require a method for attributing errors. That is, the method must determine for each error a student makes what \\u201cknowledge component\\u201d in the student model is to blame. To this point, alternative methods for error attribution have not been systematically investigated. We implemented four alternative methods for error attribution --two temporal heuristics and two location-temporal heuristics. We employed two evaluation standards --a statistical standard for measuring model fit and parsimony and the Kappa technique for measuring inter-observer reliability. We looked to see which method better met the \\u201clearning-curve standard\\u201d that is, led to better prediction of students' changes in error rate over time. Second, we asked if the codes generated by the methods better met the \\u201chuman-match standard\\u201d, that is, were they like error attributions made by human coders. Both evaluation standards led to better results for the location-temporal heuristic methods than the temporal heuristic methods. Interestingly, we found that two of the methods proposed were better at error attribution, according to the learning curve standard, than the original cognitive model of the intelligent tutoring system. Overall, these results suggest that the heuristics proposed and implemented in this paper can generally aid learning curve analysis and perhaps, more generally, the design of student models The examination of user data as a basis for developing production models of user behavior has been a major focus in the PAT Algebra I Tutor\\u2019s development. In recent work, we have investigated relationships between related tasks and the solution strategies displayed by students. To solve a PAT Algebra I problem, students must complete several related arithmetic and algebraic tasks. The sequences in which these tasks are completed suggest problem-solving strategies of students. We have observed a characteristic pattern of students\\u2019 success rates on related tasks. We have also observed that students\\u2019 success on specific skills (e.g. constructing a symbolic representation) may differ depending on whether students previously carried out related tasks in the same problem (e.g. solving an analogous arithmetic question). This information has important implications for our user model and our modeling approach. Intelligent tutors have the potential to be used in supporting learning from collaboration, but there are few results demonstrating their positive effects in this domain. One of the main challenges in automated support for collaboration is the machine classification of dialogue, giving the system an ability to know when and how to intervene. We have developed an automated detector of conceptual content that is used as a basis for providing adaptive prompts to peer tutors in high-school algebra. We conducted an after-school study with 61 participants where we compared this adaptive support to two nonadaptive support conditions, and found that adaptive prompts significantly increased conceptual help and peer tutor domain learning. The amount of conceptual help students gave, as determined by either human coding or machine classification, was predictive of learning. Thus, machine classification was effective both as a basis for feedback and predictor of success. Traditional experimental paradigms have focused on executing experiments in a lab setting and eventually moving successful findings to larger experiments in the field. However, data from field experiments can also be used to inform new lab experiments. Now, with the advent of large student populations using internetbased learning software, online experiments can serve as a third setting for experimental data collection. In this paper, we introduce the Super Experiment Framework (SEF), which describes how internet-scale experiments can inform and be informed by classroom and lab experiments. We apply the framework to a research project implementing learning games for mathematics that is collecting hundreds of thousands of data trials weekly. We show that the framework allows findings from the lab-scale, classroom-scale and internet-scale experiments to inform each other in a rapid complementary feedback loop. Developing intelligent tutoring systems from student solution data is a promising approach to facilitating more widespread application of tutors. In principle, tutor feedback can be generated by matching student solution attempts to stored intermediate solution states, and next-step hints can be generated by finding a path from a student's current state to a correct solution state. However, exact matching of states and paths does not work for many domains, like programming, where the number of solution states and paths is too large to cover with data. It has previously been demonstrated that the state space can be substantially reduced using canonicalizing operations that abstract states. In this paper, we show how solution paths can be constructed from these abstract states that go beyond the paths directly observed in the data. We describe a domain-independent algorithm that can automate hint generation through use of these paths. Through path construction, less data is needed for more complete hint generation. We provide examples of hints generated by this algorithm in the domain of programming. How to best sequence instruction in a collection of basic facts is a problem often faced by intelligent tutoring systems. To solve this problem, the following work details two tests of a system to provide drill practice (test trials with feedback) for foreign language vocabulary learning using a practice schedule determined to be optimal according to a cognitive model. In the first test, students chose between an optimized version and a version that merely cycled the vocabulary items. Examination of the time on task data revealed a preference for practice based on the decisions of the cognitive model. In the second test, the system was used to train the component parts of Chinese characters and measure the transfer of knowledge to subsequent learning of Chinese characters. Chinese character learning was improved for students with the relevant optimized training. SimStudent is a machine-learning agent that learns cognitive skills by demonstration. It was originally developed as a building block of the Cognitive Tutor Authoring Tools (CTAT), so that the authors do not have to build a cognitive model by hand, but instead simply demonstrate solutions for SimStudent to automatically generate a cognitive model. The SimStudent technology could then be used to model human students' performance as well. To evaluate the applicability of SimStudent as a tool for modeling real students, we applied SimStudent to a genuine learning log gathered from classroom experiments with the Algebra I Cognitive Tutor. Such data can be seen as the human students' \\\"demonstrations\\\" of how to solve problems. The results from an empirical study show that SimStudent can indeed model human students' performance. After training on 20 problems solved by a group of human students, a cognitive model generated by SimStudent explained 82% of the problem-solving steps performed correctly by another group of human students. The PACT Geometry tutor has been designed, with guidance from mathematics educators, to be an integrated part of a complete, new-standards-oriented course for high-school geometry. We conducted a formative evaluation of the third \\\"geometric properties\\\" lesson and saw significant student learning gains. We also found that students were better able to provide numerical answers to problems than to articulate the reasons that are presumably involved in finding these answers. This suggests that students may provide answers using superficial (and possibly unreliable) visual associations rather than reason logically from definitions and conjectures. To combat this type of shallow learning, we are developing a new version of the tutor's third lesson, aimed at getting students to reason more deliberately with definitions and theorems as they work on geometry problems. In the new version, students are required to state a reason for their answers, which they can select from a Glossary of geometry definitions and theorems. We will conduct an experiment to test whether providing tutoring on reasoning will transfer to better performance on answer giving. Key success criteria for an ITS authoring tool are that (1) the tool supports the creation of effective tutoring systems, (2) the tool can be used to build tutors across a wide range of application domains, (3) authoring with the tool is cost-effective, (4) the tool supports easy deployment and delivery of tutors in a variety of technical contexts, (5) tutors created with the tool are maintainable, and (6) if tutors are used in a research context, the tool must support research-related functionality. The Cognitive Tutor Authoring Tools (CTAT) address all of these requirements to a substantial degree, fully meeting most of them. CTAT supports the creation of both Cognitive Tutors (Koedinger & Corbett, 2006) and a newer type of tutors called example-tracing tutors. This paper focuses on the latter. Example-tracing tutors evaluate student behavior by flexibly comparing it against examples of correct and incorrect problem-solving behaviors. Example-tracing tutors are capable of sophisticated tutoring behaviors: they provide step-by-step guidance on complex problems while recognizing multiple student strategies and maintaining multiple interpretations of student behavior. On that basis, they should be deemed intelligent tutoring systems. Example-tracing tutors can be built without programming, through drag-and-drop techniques and programming by demonstration. Exampletracing tutors have been built and used in real educational settings for a wide range of application areas. Development time estimates from a large number of projects that have used CTAT suggest that CTAT improves the cost-effectiveness of ITS development by a factor of 4-8, compared to \\u201chistorical\\u201d estimates of tutor development time. Although there is a lot of variability in these kinds of estimates, they nonetheless support our hope that lowering the skill requirements for tutor creation is a key step toward widespread use of ITS technology. The main contributions of the work are the example-tracing tutor technology and tools for building these types of tutors without programming. Students often fail to learn crucial distinctions between different representations of data. For instance, many students learning about scatterplots consistently create representations which have the surface features of scatterplots but with informational content more appropriate for discrete bar graphs. Schwartz and Bransford (1998) have found that combining feature-based conceptual instruction with contrasting cases is an effective way to help students make conceptual distinctions. We adapt their approach to the domain of data representation and incorporate it into a cognitive tutoring curriculum. We show that this new curriculum improves learning more than a curriculum where the contrasts are not present. This paper describes the design and evaluation of two features in an Intelligent Tutoring System designed to facilitate a deeper conceptual understanding of domain principles in conjunction with the development of procedural skills. The first feature described here relates to the timing of feedback. Some researchers have argued that immediate corrective feedback, as embodied in many cognitive tutors, can block the exercise of activities that may enable students to gain a deeper conceptual understanding of a domain. These include self-monitoring, error detection, and error correction skills. We compare an immediate feedback tutor with a tutor that allows students to reflect on problem solving outcomes, and engage in error detection and correction activities. The other feature reported here is a component of declarative instruction. We assess the use of Example Walkthroughs as a comprehension-fostering tool. Prior to procedural practice, Example Walkthroughs step students through the study of example problems and guide them to reflect on the reasoning involved in going from a problem statement to a solution. An evaluation has shown that the best learning outcomes were associated with a combination of immediate feedback and Example Walkthroughs. There are indications that a combination of lower cognitive load during procedural practice and a robust and accurate encoding of declarative concepts contributed to the observed outcomes. Accelerated future learning, in which learning proceeds more effectively and more rapidly because of prior learning, is considered to be one of the most interesting measures of robust learning A growing body of studies have demonstrated that some instructional treatments lead to accelerated future learning However, little study has focused on under- standing the learning mechanisms that yield accelerated future learning In this paper, we present a computational model that demonstrates accelerated future learning through the use of machine learning techniques for feature recognition In order to understand the behavior of the proposed model, we conducted a controlled simulation study with four alternative versions of the model to investigate how both better prior knowledge learning and better learning strategies might independently yield accelerated future learning We measured the learning outcomes of the models by rate of learning and the fit to the pattern of errors made by real students We found out that both stronger prior knowledge and a better learning strategy can speed up the learning process Some model variations generate human-like error patterns, but others learn to avoid errors more quickly than students. This study examined the effectiveness of an educational data mining method --Learning Factors Analysis (LFA) --on improving the learning efficiency in the Cognitive Tutor curriculum. LFA uses a statistical model to predict how students perform in each practice of a knowledge component (KC), and identifies over-practiced or under-practiced KCs. By using the LFA findings on the Cognitive Tutor geometry curriculum, we optimized the curriculum with the goal of improving student learning efficiency. With a control group design, we analyzed the learning performance and the learning time of high school students participating in the Optimized Cognitive Tutor geometry curriculum. Results were compared to students participating in the traditional Cognitive Tutor geometry curriculum. Analyses indicated that students in the optimized condition saved a significant amount of time in the optimized curriculum units, compared with the time spent by the control group. There was no significant difference in the learning performance of the two groups in either an immediate post test or a two-week-later retention test. Findings support the use of this data mining technique to improve learning efficiency with other computer-tutor-based curricula. Research has shown that students\\u2019 problem-solving actions vary in type and duration. Among other causes, this behavior is a result of strategies that are driven by different goals. We describe a first version of a computational cognitive model that explains the origin of these strategies and identifies the tendencies of students towards different learning goals. Our model takes into account (i) interpersonal differences, (ii) an estimation of the student\\u2019s knowledge level, and (iii) current feedback from the tutor, in order to predict the next action of the student \\u2013 a solution, a guess or a help request. Our long-term goal is to use identification of the students\\u2019 strategies and their efficiency in order to better understand the learning process and to improve the metacognitive learning skills of the students. Is learning by solving problems better than learning from worked-out examples? Using a machine-learning program that learns cognitive skills from examples, we have conducted a study to compare three learning strategies: learning by solving problems with feedback and hints from a tutor, learning by generalizing worked-out examples exhaustively, and learning by generalizing worked-out examples only for the skills that need to be generalized. The results showed that learning by tutored problem solving outperformed other learning strategies. The advantage of tutored problem solving was mostly due to the error detection and correction that was available only when skills were applied incorrectly. The current study also suggested that learning certain kinds of conditions to apply rules only for appropriate situations is quite difficult. Collaborative activities, like peer tutoring, can be beneficial for student learning, but only when students are supported in interacting effectively. Constructing intelligent tutors for collaborating students may be an improvement over fixed forms of support that do not adapt to student behaviors. We have developed an intelligent tutor to improve the help that peer tutors give to peer tutees by encouraging them to explain tutee errors and to provide more conceptual help. The intelligent tutor must be able to classify the type of peer tutor utterance (is it next step help, error feedback, both, or neither?) and the quality (does it contain conceptual content?). We use two techniques to improve automated classification of student utterances: incorporating domain context, and incorporating students' self-classifications of their chat actions. The domain context and self-classifications together significantly improve classification of student dialogue over a baseline classifier for help type. Using domain features alone significantly improves classification over baseline for conceptual content. Intelligent Tutoring Systems have been shown to be effective in a number of domains, but they remain hard to build, with estimates of 200-300 hours of development per hour of instruction. Two goals of the Cognitive Tutor Authoring Tools (CTAT) project are to (a) make tutor development more efficient for both programmers and non-programmers and (b) produce scientific evidence indicating which tool features lead to improved efficiency. CTAT supports development of two types of tutors, Cognitive Tutors and Example-Tracing Tutors, which represent different trade-offs in terms of ease of authoring and generality. In preliminary small-scale controlled experiments involving basic Cognitive Tutor development tasks, we found efficiency gains due to CTAT of 1.4 to 2 times faster. We expect that continued development of CTAT, informed by repeated evaluations involving increasingly complex authoring tasks, will lead to further efficiency gains. Students approach the learning opportunity offered by intelligent tutoring systems with a variety of goals and attitudes. These goals and attitudes can substantially affect students' behavior within the tutor, and how much the student learns. One behavior that has been found to be associated with poorer learning is gaming the system, where a student attempts to complete problems and advance through an educational task by systematically taking advantage of properties and regularities in the system used to complete that task. It has been hypothesized that students game the system because of performance goals. In this paper, however, we find that the frequency of gaming the system does not correlate to a known measure of performance goals; instead, gaming is correlated to disliking computers and the tutor. Performance goals, by contrast, are shown to be associated with working slowly and avoiding errors, and are found to not be correlated to differences in learning outcomes. Increasing widespread use of educational technologies is producing vast amounts of data. Such data can be used to help advance our understanding of student learning and enable more intelligent, interactive, engaging, and effective education. In this article, we discuss the status and prospects of this new and powerful opportunity for data-driven development and optimization of educational technologies, focusing on intelligent tutoring systems We provide examples of use of a variety of techniques to develop or optimize the select, evaluate, suggest, and update functions of intelligent tutors, including probabilistic grammar learning, rule induction, Markov decision process, classification, and integrations of symbolic search and statistical inference. Toward a Model of Learning Data Representations Ryan Shaun Baker (rsbaker@cmu.edu) Human-Computer Interaction Institute, Carnegie Mellon University Pittsburgh, PA 15213 USA Albert T. Corbett (corbett+@cmu.edu) Human-Computer Interaction Institute, Carnegie Mellon University Pittsburgh, PA 15213 USA Kenneth R. Koedinger (koedinger@cmu.edu) Human-Computer Interaction Institute, Carnegie Mellon University Pittsburgh, PA 15213 USA Abstract The use of graphs to represent and reason about data is of growing importance in pre-high school mathematics curricula. This study examines middle school students\\u2019 skills in reasoning about three graphical representations: histograms, scatterplots and stem-and-leaf plots. Students were asked to interpret graphs, select an appropriate graph type to represent a relationship and to generate graphs. Accuracy levels varied substantially across the three tasks and three graph types. The overall pattern of results is largely explained by the varying ease of transfer of student knowledge from a simpler graph type, based on surface similarity. Introduction External graphical representations are of considerable importance in problem solving. Considerable research has taken place over the last two decades on the different mechanisms through which graphical representations assist their users in drawing inferences (Larkin & Simon, 1987; Stenning, Cox, and Oberlander 1989). In this paper we take up the use of representations at a very early point \\u2013 at the point when a student is just learning to generate and interpret a representation \\u2013 and ask what some of the major challenges are in learning these skills. There has been growing interest in attempting to teach these skills to students as young as those in the third through eighth grades 1 (NCTM 2000), but there is considerable evidence as well that these skills have not yet been developed by many undergraduates (Tabachneck, Leonardo, and Simon 1994). We take up this subject in the context of developing a cognitive model of how novices generate and interpret some of the simpler representations used in data analysis. This model is designed with production-rule logic, in ACT-R (Anderson 1993). In this process, we hope to follow in the footsteps of some of the successful cognitive models of novices developed in other domains such as algebra problem solving (Koedinger & MacLaren 1997). One area which might considerably influence students\\u2019 performance on these tasks is transfer of the knowledge students already have of generating and interpreting other Between the ages of 7 and 13. representations. Since students are taught different sets of representations at different grade levels (NCTM 2000), it is quite plausible that an important model for learning new representations will be the representations encountered earlier. Previous research into when transfer occurs shows that transfer can happen between exercises taking place in different representations, through mechanisms such as analogy, and that transfer can occur between similar processes (Novick 1988, Novick and Holyoak 1991, Singley and Anderson 1989). Hence, we seek to find out if and how these processes extend to the very first stages of learning how to use and generate a representation. We are interested both in positive transfer, and in overgeneralization, where knowledge is transferred inappropriately. Scanlon\\u2019s (1993) research in the use of representations for physics problem-solving provides some excellent examples of overgeneralization in the interpretation of different graphical representations. Additionally, other research has shown that misconceptions in physics, arising from overgeneralization of previously learned knowledge, causes long-term difficulties in correctly learning new material. How best to deal with such misconceptions is an active question in the research literature, with some arguing for a curricular strategy which acknowledges the appropriate contexts for certain conceptions and helps students see when they are inappropriate (NRC 1999). In this paper, we present results and analysis of a empirical study we conducted in this domain, investigating novice performance (with an eye towards transfer effects) on interpreting, generating, and selecting representations important to early data analysis. Domain Representations This study focuses on three graphical representations of data: histograms, scatterplots, and stem-and-leaf plots. A histogram depicts a frequency distribution, as displayed in Figure 1. A set of interval categories (as in Figure 1) are represented in the X axis, and the frequency of each category is represented by the height of the corresponding vertical bar. A stem-and-leaf plot, shown in Figure 2, also Scaffolding learners, i.e. helping learners to attain tasks they could not accomplish without support, entails the notion of fading, i.e. reducing the scaffolding for learners to become more and more self-regulated. Fading implies to tailor support for collaboration, such as collaboration scripts, to the particular needs of the specific collaborators. In computer supported collaborative learning (CSCL) settings, support can be designed in a very restrictive and inflexible fashion; at the same time computerized settings open new possibilities for the realization of adaptive support as they enable automation of analysis and feedback mechanisms. In this symposium we present new technical approaches and latest empirical research on possibilities and limitations of adaptive support for learners in CSCL settings. SimStudent is a machine-learning agent that learns cognitive skills by demonstration. It was originally developed as a building block of the Cognitive Tutor Authoring Tools (CTAT), so that the authors do not have to build a cognitive model by hand, but instead simply demonstrate solutions for SimStudent to automatically generate a cognitive model. The SimStudent technology could then be used to model human students' performance as well. To evaluate the applicability of SimStudent as a tool for modeling real students, we applied SimStudent to a genuine learning log gathered from classroom experiments with the Algebra I Cognitive Tutor. Such data can be seen as the human students' \\\"demonstrations\\\" of how to solve problems. The results from an empirical study show that SimStudent can indeed model human students' performance. After training on 20 problems solved by a group of human students, a cognitive model generated by SimStudent explained 82% of the problem-solving steps performed correctly by another group of human students. The Self-Assessment Tutor (SAT) is an add-on component to Cognitive Tutors that supports self-assessment in four steps: prediction, attempt, reflection, and projection. The SAT encourages students to self-assess their ability spontaneously while problem solving, and to use help resources accordingly. For that reason its episodes precede the students' work with the Cognitive Tutor, which itself remains unchanged. The SAT offers detailed feedback and help function to support the Self-Assessment process. A complementary instruction is given to students before working with the SAT. We hypothesize that working with the SAT will encourage students to self-assess on subsequent problems requiring similar skills, and thus will promote learning. A classroom evaluation of SAT is currently in progress. This report describes an adaptation of the Practical Algebra Tutor, \\\"PAT,\\\" for college-level developmental mathematics, and initial evaluations of PAT at two college sites. PAT is a software learning environment that presents students with real-world problem situations, modern mathematical representational tools to analyze these situations, and constant background support from a \\\"cognitive tutor\\\"-- an intelligent computer tutor based on the ACT theory of cognition. Previous classroom evaluations showed that high-school students using PAT outperformed students in regular classes by 15% on standardized tests, and by 100% on assessments of authentic problem-solving. In two colleges, PAT students performed more than 50% better than students in regular classes on a performance-based assessment. This assessment requires students to use mathematical representations to analyze a real-world problem situation. It captures the reform objectives of the PAT approach, which are consistent with new national standards for mathematics. In recent years, a number of systems have been developed to detect differences in how students choose to use intelligent tutoring systems, and the attitudes and goals which underlie these decisions. These systems, when trained using data from human observations and questionnaires, can detect specific behaviors and attitudes with high accuracy. However, such data is time-consuming to collect, especially across an entire tutor curriculum. Therefore, to deploy a detector of behaviors or attitudes across an entire tutor curriculum, the detector must be able to transfer to a new tutor lesson without being re-trained using data from that lesson. In this paper, we present evidence that detectors of gaming the system can transfer to new lessons without re-training, and that training detectors with data from multiple lessons improves generalization, beyond just the gains from training with additional data. The order of problems presented to students is an important variable that affects learning effectiveness. Previous studies have shown that solving problems in a blocked order, in which all problems of one type are completed before the student is switched to the next problem type, results in less effective performance than does solving the problems in an interleaved order. While results are starting to accumulate, we have little by way of precise understanding of the cause of such effect. Using a machine-learning agent that learns cognitive skills from examples and problem solving experience, SimStudent, we conducted a controlled simulation study in three math and science domains (i.e., fraction addition, equation solving and stoichiometry) to compare two problem orders: the blocked problem order, and the interleaved problem order. The results show that the interleaved problem order yields as or more effective learning in all three domains, as the interleaved problem order provides more or better opportunities for error detection and correction to the learning agent. The study shows that learning when to apply a skill benefits more from interleaved problem orders, and suggests that learning how to apply a skill benefits more from blocked problem orders. The purpose of the current study is to test whether we could create a system where students can learn by teaching a live machine-learning agent, called SimStudent. SimStudent is a computer agent that interactively learns cognitive skills through its own tutored-problem solving experience. We have developed a game-like learning environment where students learn algebra equations by tutoring SimStudent. While Simulated Students, Teachable Agents and Learning Companion systems have been created, our study is unique that it genuinely learns skills from student input. This paper describes the overview of the learning environment and some results from an evaluation study. The study showed that after tutoring SimStudent, the students improved their performance on equation solving. The number of correct answers on the error detection items was also significantly improved. On average students spent 70.0 minutes on tutoring SimStudent and used an average of 15 problems for tutoring. Previous research has shown that self-explanation can be supported effectively in an intelligent tutoring system by simple means such as menus. We now focus on the hypothesis that natural language dialogue is an even more effective way to support self-explanation. We have developed the Geometry Explanation Tutor, which helps students to state explanations of their problem-solving steps in their own words. In a classroom study involving 71 advanced students, we found that students who explained problem-solving steps in a dialogue with the tutor did not learn better overall than students who explained by means of a menu, but did learn better to state explanations. Second, examining a subset of 700 student explanations, students who received higher-quality feedback from the system made greater progress in their dialogues and learned more, providing some measure of confidence that progress is a useful intermediate variable to guide further system development. Finally, students who tended to reference specific problem elements in their explanations, rather than state a general problem-solving principle, had lower learning gains than other students. Such explanations may be indicative of an earlier developmental level. A cognitive model is a set of production rules or skills encoded in intelligent tutors to model how students solve problems. It is usually generated by brainstorming and iterative refinement between subject experts, cognitive scientists and programmers. In this paper we propose a semi-automated method for improving a cognitive model called Learning Factors Analysis that combines a statistical model, human expertise and a combinatorial search. We use this method to evaluate an existing cognitive model and to generate and evaluate alternative models. We present improved cognitive models and make suggestions for improving the intelligent tutor based on those models. A step in ITS often involve multiple skills. Thus a step requiring a conjunction of skills is harder than steps that require requiring each individual skill only. We developed two Item-Response Models --- Additive Factor Model (AFM) and Conjunctive Factor Model (CFM) --- to model the conjunctive skills in the student data sets. Both models are compared on simulated data sets and a real assessment data set. We showed that CFM was as good as or better than AFM in the mean cross validation errors on the simulated data. In the real data set CFM is not clearly better. However, AFM is essentially performing as a conjunctive model. Research has shown that students\\u2019 help-seeking behavior is far from being ideal. In trying to make it more efficient, 27 students using the Geometry Cognitive Tutor regularly received individual online instructions. The instruction to the HELP group, aimed to improve their help-seeking behavior, included a walk-through metacognitive example. The CONTROL group received \\u201cplacebo instruction\\u201d with a similar walk-through but without the help-seeking content. In two subsequent weeks, the HELP group used the system\\u2019s hints more frequently than the CONTROL group. However, we didn\\u2019t observe a significant difference in the learning outcomes. These results suggest that appropriate instruction can improve help-seeking behavior in ITS usage. Further evaluation should be performed in order to design better instruction and improve learning. Self-explanation is an effective instructional strategy for improving problem solving in math and science domains. However, our previous studies, within the domain of second language grammar learning, show self-explanation to be no more effective than simple practice; perhaps the metalinguistic challenges involved in explaining using one's non-native language are hampering the potential benefits. An alternative strategy is tutoring using analogical comparisons, which reduces language difficulties while continuing to encourage feature focusing and deep processing. In this paper, we investigate adult English language learners learning the English article system (e.g. the difference between \\u201ca dog\\u201d and \\u201cthe dog\\u201d). We present the results of a classroom-based study (N=99) that compares practice-only to two conditions that facilitate deep processing: self-explanation with practice and analogy with practice. Results show that students in all conditions benefit from the instruction. However, students in the practice-only condition complete the instruction in significantly less time leading to greater learning efficiency. Possible explanations regarding the differences between language and science learning are discussed. Previous studies have shown that self-explanation is an effective metacognitive strategy and can be supported effectively by intelligent tutoring systems. It is plausible however that students may learn even more effectively when stating explanations in their own words and when receiving tutoring focused on their explanations. We are developing the Geometry Explanation Tutor in order to test this hypothesis. This system helps students, through a restricted form of dialogue, to construct general explanations of problem-solving steps in their own words. We conducted a pilot study in which the tutor was used for two class periods in a junior high school. The data from this study suggest that the techniques that we chose to implement the dialogue system, namely a knowledge-based approach to natural language understanding and classification of student explanations, are up to the task. There are a number of ways in which the system could be improved within the current architecture. Recent research has indicated that misuse of intelligent tutoring software is correlated with substantially lower learning. Students who frequently engage in behavior termed \\u201cgaming the system\\u201d (behavior aimed at obtaining correct answers and advancing within the tutoring curriculum by systematically taking advantage of regularities in the software\\u2019s feedback and help) learn only 2/3 as much as similar students who do not engage in such behaviors. We present a machine-learned Latent Response Model that can identify if a student is gaming the system in a way that leads to poor learning. We believe this model will be useful both for re-designing tutors to respond appropriately to gaming, and for understanding the phenomenon of gaming better. The aim of this study is to build an intelligent authoring environment for Cognitive Tutors in which the author need not manually write a cognitive model. Writing a cognitive model usually requires days of programming and testing even for a well-trained cognitive scientist. To achieve our goal, we have built a machine learning agent \\u2013 called a Simulated Student \\u2013 that automatically generates a cognitive model from sample solutions demonstrated by the human domain expert (i.e., the author). This paper studies the effectiveness and generality of the Simulated Student. The major findings include (1) that the order of training problems does not affect a quality of the cognitive model at the end of the training session, (2) that ambiguities in the interpretation of demonstrations might hinder machine learning, and (3) that more detailed demonstration can both avoid difficulties with ambiguity and prevent search complexity from growing to impractical levels. 1 Human Computer Interaction Institute, {mazda, sewall, koedinger}@cs.cmu.edu} 2 Machine Learning Department, wcohen@cs.cmu.edu This material is based upon a work supported by National Science Foundation award number REC-0537198 Data analysis has become a topic of increasing emphasis within middle school mathematics in the last few years, especially in the recent recommendations by the National Council of Teachers of Mathematics (NCTM 2000). In order to better inform efforts to expand data analysis\\u2019s role in middle school curricula, we have begun the development of a cognitive model of student thinking in this domain. Proper representation of data is an essential part of the process of data analysis (Larkin and Simon 1987) -therefore, we have focused on modeling how students learn to generate and interpret some of the important and widely-used representations of data, such as histograms and scatterplots. Knowledge tracing (KT)[1] has been used in various forms for adaptive computerized instruction for more than 40 years. However, despite its long history of application, it is difficult to use in domain model search procedures, has not been used to capture learning where multiple skills are needed to perform a single action, and has not been used to compute latencies of actions. On the other hand, existing models used for educational data mining (e.g. Learning Factors Analysis (LFA)[2]) and model search do not tend to allow the creation of a \\u201cmodel overlay\\u201d that traces predictions for individual students with individual skills so as to allow the adaptive instruction to automatically remediate performance. Because these limitations make the transition from model search to model application in adaptive instruction more difficult, this paper describes our work to modify an existing data mining model so that it can also be used to select practice adaptively. We compare this new adaptive data mining model (PFA, Performance Factors Analysis) with two versions of LFA and then compare PFA with standard KT. This paper discusses the refinement of the algebraic expression-writing rules for the PAT Algebra I tutor, between 1992 and 1997. Direct observation of students in class, step-by-step PAT tutor protocols tracing student behavior, and statistical analysis of protocol data, have all informed our understanding of students' skills, and our refinement of this part of PAT's production model. Students often use available help facilities in an unproductive fashion. To improve students' help-seeking behavior we built the Help Tutor \\u2013 a domain-independent agent that can be added as an adjunct to Cognitive Tutors. Rather than making help-seeking decisions for the students, the Help Tutor teaches better help-seeking skills by tracing students actions on a (meta)cognitive help-seeking model and giving students appropriate feedback. In a classroom evaluation the Help Tutor captured help-seeking errors that were associated with poorer learning and with poorer declarative and procedural knowledge of help seeking. Also, students performed less help-seeking errors while working with the Help Tutor. However, we did not find evidence that they learned the intended help-seeking skills, or learned the domain knowledge better. A new version of the tutor that includes a self-assessment component and explicit help-seeking instruction, complementary to the metacognitive feedback, is now being evaluated. Transfer of learning to new or different contexts has always been a chief concern of education because unlike training for a specific job, education must establish skills without knowing exactly how those skills might be called upon. Research on transfer can be difficult, because it is often superficially unclear why transfer occurs or, more frequently, does not, in a particular paradigm. While initial results with Learning Factors Transfer (LiFT) analysis (a search procedure using Performance Factors Analysis, PFA) show that more predictive models can be built by paying attention to these transfer factors [1, 2], like proceeding models such as AFM (Additive Factors Model) [3], these models rely on a Q-matrix analysis that treats skills as discrete units at transfer. Because of this discrete treatment, the models are more parsimonious, but may lose resolution on aspects of component transfer. To improve understanding of this transfer, we develop new logistic regression model variants that predict learning differences as a function of the context of learning. One advantage of these models is that they allow us to disentangle learning of transferable knowledge from the actual transfer performance episodes. Background.\\u00a0High school and college students demonstrate a verbal, or textual, advantage whereby beginning algebra problems in story format are easier to solve than matched equations (Koedinger & Nathan, 2004). Adding diagrams to the stories may further facilitate solution (Hembree, 1992; Koedinger & Terao, 2002). However, diagrams may not be universally beneficial (Ainsworth, 2006; Larkin & Simon, 1987).  Aims.\\u00a0To identify developmental and individual differences in the use of diagrams, story, and equation representations in problem solving. When do diagrams begin to aid problem-solving performance? Does the verbal advantage replicate for younger students?  Sample.\\u00a0Three hundred and seventy-three students (121 sixth, 117 seventh, 135 eighth grade) from an ethnically diverse middle school in the American Midwest participated in Experiment 1. In Experiment 2, 84 sixth graders who had participated in Experiment 1 were followed up in seventh and eighth grades.  Method.\\u00a0In both experiments, students solved algebra problems in three matched presentation formats (equation, story, story + diagram).  Results.\\u00a0The textual advantage was replicated for all groups. While diagrams enhance performance of older and higher ability students, younger and lower-ability students do not benefit, and may even be hindered by a diagram's presence.  Conclusions.\\u00a0The textual advantage is in place by sixth grade. Diagrams are not inherently helpful aids to student understanding and should be used cautiously in the middle school years, as students are developing competency for diagram comprehension during this time. Exploratory Learning Environments (ELE) facilitate scientific inquiry tasks in which learners attempt to develop or uncover underlying scientific or mathematical models. Unlike step-based Intelligent Tutoring Systems (ITS), and due to task characteristics and pedagogical philosophy, ELE offer little support at the domain level. Lacking adequate support, ELE often fail to deliver on their promise. We describe the Invention Lab, a system that combines the benefits of ELE and ITS by offering adaptive support in a relatively unconstrained environment. The Invention Lab combines modeling techniques to assess students' knowledge at the domain and inquiry levels. The system uses this information to design new tasks in real time, thus adapting to students' needs while maintaining critical features of the inquiry process. Data from an in-class evaluation study illustrates how the Invention Lab helps students develop sophisticated mathematical models and improve their scientific inquiry behavior. Implications for intelligent support in ELE are discussed. We present an approach to designing intelligent tutoring systems, termed the Difficulty Factors Approach. In this approach, the designer investigates, at each iteration of the design cycle, which skills and concepts are difficult for students, and what factors underlie those difficulties. We show that this approach complements existing design principles, producing data that helps designers apply principles in context. We also show that by continuing to investigate student difficulties throughout the design process, it is possible to discover difficulty factors initially obscured by other difficulty factors. We give an example of the application of the Difficulty Factors Approach in the context of the development of a cognitive tutor lesson on scatterplots. Given the important role that meta-cognitive processes play in learning, intelligent tutoring systems should not only provide domain-specific assistance, but should also aim to help students in acquiring meta-cognitive skills. As a step toward this goal, we have constructed a Help Tutor, aimed at improving students' help-seeking skill. The Help Tutor is based on a cognitive model of students' desired help-seeking processes, as they work with a Cognitive Tutor (Aleven et al., 2004). To provide meta-cognitive tutoring in conjunction with cognitive tutoring, we designed an architecture in which the Help Tutor and a Cognitive Tutor function as independent agents, to facilitate re-use of the Help Tutor. Pilot tests with four students showed that students improved their help-seeking behavior significantly while working with the Help Tutor. The improvement could not be attributed to their becoming more familiar with the domain-specific skills being taught by the tutor. Although students reported afterwards that they welcomed feedback on their help-seeking behavior, they seemed less fond of it when actually advised to act differently while working. We discuss our plans for an experiment to evaluate the impact of the Help Tutor on students' help-seeking behavior and learning, including future learning, after their work with the Help Tutor. In this chapter, we will discuss our work to understand why students game the system. This work leverages models of student gaming, termed \\u201cdetectors\\u201d, which can infer student gaming in log files of student interaction with educational software. These detectors are developed using a combination of human observation and annotation, and educational data mining. We then apply the detectors to large data sets, and analyze the detectors\\u2019 predictions, using discovery with models methods, to study the factors associated with gaming behavior. Within this chapter, we will discuss the work to develop these detectors, and what we have discovered through these analyses based on these detectors. We will discuss evidence for how gaming the system impacts learning and evidence for why students choose to game. We will also discuss attempts to address gaming the system through adaptive scaffolding. Previous studies have demonstrated the learning benefit of personalized language and worked examples. However, previous investigators have primarily been interested in how these interventions support students as they problem solve with no other cognitive support. We hypothesized that personalized language added to a web-based intelligent tutor and worked examples provided as complements to the tutor would improve student (e-)learning. However, in a 2 x 2 factorial study, we found that personalization and worked examples had no significant effects on learning. On the other hand, there was a significant difference between the pretest and posttest across all conditions, suggesting that the online intelligent tutor present in all conditions did make a difference in learning. We conjecture why personalization and, especially, the worked examples did not have the hypothesized effect in this preliminary experiment, and discuss a new study we have begun to further investigate these effects. SimStudent is a machine-learning agent that learns cognitive skills by demonstration. SimStudent was originally built as a building block for Cognitive Tutor Authoring Tools to help an author build a cognitive model without significant programming. In this paper, we evaluate a second use of SimStudent, viz., student modeling for Intelligent Tutoring Systems. The basic idea is to have SimStudent observe human students solving problems. It then creates a cognitive model that can replicate the students' performance. If the model is accurate, it would predict the human students' performance on novel problems. An evaluation study showed that when trained on 15 problems, SimStudent accurately predicted the human students' correct behavior on the novel problems more than 80% of the time. However, the current implementation of SimStudent does not accurately predict when the human students make errors. Intelligent tutoring systems help students acquire cognitive skills by tracing students' knowledge and providing relevant feedback. However, feedback that focuses only on the cognitive level might not be optimal \\u2013 errors are often the result of inappropriate metacognitive decisions. We have developed two models which detect aspects of student faulty metacognitive behavior: A prescriptive rational model aimed at improving help-seeking behavior, and a descriptive machine-learned model aimed at eliminating attempts to \\u201cgame\\u201d the tutor. In a comparison between the two models we found that while both successfully identify gaming behavior, one is better at characterizing the types of problems students game in, and the other captures a larger variety of faulty behaviors. An analysis of students' actions in two different tutors suggests that the help-seeking model is domain independent, and that students' behavior is fairly consistent across classrooms, age groups, domains, and task elements. Intelligent tutoring techniques can successfully improve student learning from collaborative activities, but little is known about why and under what contexts this support is effective. We have developed an intelligent tutor to improve the help that peer tutors give by encouraging them to explain tutee errors and provide more conceptual help. In previous work, we have shown that adaptive support from this \\\"tutor\\\" tutor improves student learning more than randomly selected support. In this paper, we examine this result, looking more closely at the feedback students received, and coding it for relevance to the current situation. Surprisingly, we find that the amount of relevant support students receive is not correlated with their learning; however, there is a positive correlation with learning and students noticing relevant support, and a negative correlation with learning and students ignoring relevant support. Designers of adaptive collaborative learning systems should focus not only on making support relevant, but also engaging. Methods and systems are provided for tutoring a student in solving a problem described in the form of a dialog with a student involving questions posed to the student and analysis of student responses. The method employs a student model for receiving and diagnosing student input and a tutorial model for deciding what new questions to plan to ask the student. The method comprises the following steps: receiving the diagnosis in a tutorial processing module; generating an agenda including at least one question; applying at least one tutorial strategy to the diagnosis; and, providing feedback to the student based on application of the tutorial strategy given the current context as indicated by the agenda data structure. In certain aspects of the method, at least one knowledge remediation dialog and/or at least one knowledge construction dialog can be applied to generate dialog between the tutor and the student. System embodiments are also provided that incorporate the functions of the methods for tutoring a student. In addition, in other embodiments, the present tutoring methods and systems are practiced in connection with instructions stored on a computer-readable medium. Students are often susceptible to surface features when learning to solve problems in a new domain. Providing example problems where salient surface features are spuriously correlated with the same problem type may encourage their use (Ben-Zeev & Star, 2001), whereas increasing the variability among superficial features during training may yield more robust knowledge (Schmidt & Bjork, 1992). To better understand the causes and consequences of this phenomenon, we compared the impact of two instructional regimens embodying these extremes and articulated detailed models of students' surface and deep knowledge resulting from each training procedure, enabling us to distinguish between weak correct knowledge and strong incorrect knowledge. Although ITSs are supposed to adapt to differences among learners, so far, little attention has been paid to how they might adapt to differences in how students learn from help. When students study with an Intelligent Tutoring System, they may receive multiple types of help, but may not comprehend and make use of this help in the same way. To measure the extent of such individual differences, we propose two new logistic regression models, ProfHelp and ProfHelp-ID. Both models extend the Performance Factors Analysis model (Pavlik, Cen & Koedinger, 2009) with parameters that represent the effect of hints on performance on the same step on which the help was given. Both models adjust for general student proficiency, prior practice on knowledge components, and knowledge component difficulty. Multilevel Bayesian implementations of these models were fit to data on student interactions with a geometry ITS, where students received on-demand problem-relevant help ranging from firstlevel hints that facilitate application of principles to specific and immediately actionable bottom-out hints. The model comparison showed that in this dataset students differ in their individual hintprocessing proficiency and these differences depend on hint levels. These results suggest that we can assess specific learning skills, e.g., making sense of instructional text, and in future work we may be able to remediate and improve such skills. Numerous approaches to student modeling have been proposed since the inception of the field more than three decades ago. What the field is lacking completely is comparative analyses of different student modeling approaches. In this paper we compare Cognitive Tutoring to Constraint-Based Modeling (CBM). We present our experiences in implementing a database design tutor using both methodologies and highlight their strengths and weaknesses. We compare their characteristics and argue the differences are often more apparent than real: for specific domains one approach may be favoured over the other, making them viable complementary methods for supporting learning. Measuring the efficacy of ITS can be hard because there are many confounding factors: short, well-isolated studies suffer from insufficient interaction with the system, while longer studies may be affected by the students' other learning activities. Coarse measurements such as pre-and post-testing are often inconclusive. Learning curves are an alternative tool: slope and fit of learning curves show the rate at which the student learns, and reveal how well the system model fits what the student is learning. The downside is that they are extremely sensitive to changes in the system's setup, which arguably makes them useless for comparing different tutors. We describe these problems in detail and our experiences with them. We also suggest some other ways of using learning curves that may be more useful for making such comparisons. Building an intelligent agent that simulates human learning of math and science could potentially benefit both education, by contributing to the understanding of human learning, and artificial intelligence, by advancing the goal of creating human-level intelligence. However, constructing such a learning agent currently requires significant manual encoding of prior domain knowledge; in addition to being a poor model of human acquisition of prior knowledge, manual knowledge-encoding is both time-consuming and error-prone. Recently, we proposed an efficient algorithm that automatically acquires domain-specific prior knowledge in the form of deep features. We integrate this deep feature learner into a machine-learning agent, SimStudent. To evaluate the generality of the proposed approach and the effect of integration on prior knowledge, we carried out a controlled simulation study in three domains, fraction addition, equation solving, and stoichiometry, using problems solved by human students. The results show that the integration reduces SimStudent's dependence over domain-specific prior knowledge, while maintains SimStudent's performance. People learn to read and understand various displays (e.g., tables on webpages and software user interfaces) every day. How do humans learn to process such displays? Can computers be efficiently taught to understand and use such displays? In this paper, we use statistical learning to model how humans learn to perceive visual displays. We extend an existing probabilistic context-free grammar learner to support learning within a two-dimensional space by incorporating spatial and temporal information. Experimental results in both synthetic domains and real world domains show that the proposed learning algorithm is effective in acquiring user interface layout. Furthermore, we evaluate the effectiveness of the proposed algorithm within an intelligent tutoring agent, SimStudent, by integrating the learned display representation into the agent. Experimental results in learning complex problem solving skills in three domains show that the learned display representation is as good as one created by a human expert, in that skill learning using the learned representation is as effective as using a manually created representation. The effectiveness of intelligent tutoring systems at increasing learning might be improved if the systems were combined with collaborative activities that encouraged conceptual elaboration. We extended the Cognitive Tutor Algebra, an intelligent tutoring system for high-school mathematics, with a peer tutoring activity that was designed to encourage interaction, reflection, and accountability. Two types of domain support were provided: adaptive support, which used the intelligent tutor domain models to provide feedback to the peer tutor, and fixed support, which simply consisted of answers to the problems. We compared the two peer tutoring conditions (adaptive or fixed support) to individual use of the cognitive tutor (without peer-tutoring activities). Even though students in the individual condition solved more problems during instruction, we did not find significant differences between the individual and collaborative conditions on learning. However, we found a correlation between tutee impasses and tutor learning. An emerging field of educational data mining (EDM) is building on and contributing to a wide variety of disciplines through analysis of data coming from various educational technologies. EDM researchers are addressing questions of cognition, metacognition, motivation, affect, language, social discourse, etc. using data from intelligent tutoring systems, massive open online courses, educational games and simulations, and discussion forums. The data include detailed action and timing logs of student interactions in user interfaces such as graded responses to questions or essays, steps in rich problem solving environments, games or simulations, discussion forum posts, or chat dialogs. They might also include external sensors such as eye tracking, facial expression, body movement, etc. We review how EDM has addressed the research questions that surround the psychology of learning with an emphasis on assessment, transfer of learning and model discovery, the role of affect, motivation and metacognition on learning, and analysis of language data and collaborative learning. For example, we discuss (1) how different statistical assessment methods were used in a data mining competition to improve prediction of student responses to intelligent tutor tasks, (2) how better cognitive models can be discovered from data and used to improve instruction, (3) how data-driven models of student affect can be used to focus discussion in a dialog-based tutoring system, and (4) how machine learning techniques applied to discussion data can be used to produce automated agents that support student learning as they collaborate in a chat room or a discussion board. WIREs Cogn Sci 2015, 6:333\\u2013353. doi: 10.1002/wcs.1350  For further resources related to this article, please visit the WIREs website.  Conflict of interest: The authors have declared no conflicts of interest for this article. Building on past results establishing a benefit for using handwriting when entering mathematics on the computer, we hypothesize that handwriting as an input modality may be able to provide significant advantages over typing in the mathematics learning domain. We report the results of a study in which middle and high school students used a software tutor for algebra equation solving with either typing or handwriting as the input modality. We found that handwriting resulted in similar learning gains in much less time than typing. We also found students seem to experience a higher degree of transfer in handwriting than in typing based on performance during training. This implies that students could achieve farther goals in an intelligent tutoring system curriculum when they use handwriting interfaces vs. typing. Both of these results encourage future exploration of the use of handwriting interfaces for mathematic instruction online. Bayesian Knowledge Tracing (BKT)[1] is a user modeling method extensively used in the area of Intelligent Tutoring Systems. In the standard BKT implementation, there are only skill-specific parameters. However, a large body of research strongly suggests that student-specific variability in the data, when accounted for, could enhance model accuracy [5,6,8]. In this work, we revisit the problem of introducing student-specific parameters into BKT on a larger scale. We show that student-specific parameters lead to a tangible improvement when predicting the data of unseen students, and that parameterizing students\\u2019 speed of learning is more beneficial than parameterizing a priori knowledge. Assessment of students\\u2019 self-regulated learning (SRL) requires a method for evaluating whether observed actions are appropriate acts of self-regulation in the specific learning context in which they occur. We review research that has resulted in an automated method for context-sensitive assessment of a specific SRL strategy, help seeking while working with an intelligent tutoring system. The method relies on a computer-executable model of the targeted SRL strategy. The method was validated by showing that it converges with other measures of help seeking. Automated feedback on help seeking driven by this method led to a lasting improvement in students\\u2019 help-seeking behavior, although not in domain-specific learning. The method is unobtrusive, is temporally fine-grained, and can be applied on a large scale and over extended periods. The approach could be applied to other SRL strategies besides help seeking. Adaptive collaborative learning support (ACLS) involves collaborative learning environments that adapt their characteristics, and sometimes provide intelligent hints and feedback, to improve individual students\\u2019 collaborative interactions. ACLS often involves a system that can automatically assess student dialogue, model effective and ineffective collaboration, and provide relevant support. While there is evidence that ACLS can improve student learning, little is known about why systems that incorporate ACLS are effective. Does relevant support improve student interactions by providing just-in-time feedback, or do students who believe they are receiving relevant support feel more accountable for the collaboration, and thus more motivated to improve their interactions? In this paper, we describe an adaptive system we have developed to support help-giving during peer tutoring in high school algebra: the Adaptive Peer Tutoring Assistant (APTA). To validate our approach, we conducted a controlled study that demonstrated that our system provided students with more relevant support and was more effective at improving student learning than parallel nonadaptive conditions. Our contributions involve generalizable techniques for implementing ACLS that can function adaptively and effectively, and the finding that adaptive support does indeed improve student learning because of the relevance of the support. Building an intelligent agent that simulates human learning of math and science could potentially benefit both cognitive science, by contributing to the understanding of human learning, and artificial intelligence, by advancing the goal of creating human-level intelligence. However, constructing such a learning agent currently requires manual encoding of prior domain knowledge; in addition to being a poor model of human acquisition of prior knowledge, manual knowledge-encoding is both time-consuming and error-prone. Previous research has shown that one of the key factors that differentiates experts and novices is their different representations of knowledge. Experts view the world in terms of deep functional features, while novices view it in terms of shallow perceptual features. Moreover, since the performance of learning algorithms is sensitive to representation, the deep features are also important in achieving effective machine learning. In this paper, we present an efficient algorithm that acquires representation knowledge in the form of \\\"deep features\\\", and demonstrate its effectiveness in the domain of algebra as well as synthetic domains. We integrate this algorithm into a machine-learning agent, SimStudent, which learns procedural knowledge by observing a tutor solve sample problems, and by getting feedback while actively solving problems on its own. We show that learning \\\"deep features\\\" reduces the requirements for knowledge engineering. Moreover, we propose an approach that automatically discovers student models using the extended SimStudent. By fitting the discovered model to real student learning curve data, we show that it is a better student model than human-generated models, and demonstrate how the discovered model may be used to improve a tutoring system's instructional strategy. We present a new model of skilled performance in geometry proof problem solving called the Diagram Configuration model (DC). While previous models plan proofs in a step-by-step fashion, we observed that experts plan at a more abstract level: They focus on the key steps and skip the less important ones. DC models this abstract planning behavior by parsing geometry problem diagrams into perceptual chunks, called diagram configurations, which cue relevant schematic knowledge. We provide verbal protocol evidence that DC's schemas correspond with the step-skipping inferences experts make in their initial planning. We compare DC with other models of geometry expertise and then, in the final section, we discuss more general implications of our research. DC's reasoning has important similarities with Larkin's (1988) display-based reasoning approach and Johnson-Laird's (1983) mental model approach. DC's perceptually based schemas are a step towards a unified explanation of (1) experts' superior problem-solving effectiveness, (2) experts' superior problem-state memory, and (3) experts' ability, in certain domains, to solve relatively simple problems by pure forward inferencing. We also argue that the particular and efficient knowledge organization of DC challenges current theories of skill acquisition as it presents an end-state of learning that is difficult to explain within such theories. Finally, we discuss the implications of DC for geometry instruction. This article explores how differences in problem representations change both the performance and underlying cognitive processes of beginning algebra students engaged in quantitative reasoning. Contrary to beliefs held by practitioners and researchers in mathematics education, students were more successful solving simple algebra story problems than solving mathematically equivalent equations. Contrary to some views of situated cognition, this result is not simply a consequence of situated world knowledge facilitating problem-solving performance, but rather a consequence of student difficulties with comprehending the formal symbolic representation of quantitative relations. We draw on analyses of students' strategies and errors as the basis for a cognitive process explanation of when, why, and how differences in problem representation affect problem solving. We conclude that differences in external representations can affect performance and learning when one representation is easier to comprehend than anoth... This study investigated the cognitive processes involved in inductive reasoning. Sixteen undergraduates solved quadratic function\\u2013finding problems and provided concurrent verbal protocols. Three fundamental areas of inductive activity were identified: Data Gathering, Pattern Finding, and Hypothesis Generation. These activities are evident in three different strategies that they used to successfully find functions. In all three strategies, Pattern Finding played a critical role not previously identified in the literature. In the most common strategy, called the Pursuit strategy, participants created new quantities from x and y, detected patterns in these quantities, and expressed these patterns in terms of x. These expressions were then built into full hypotheses. The processes involved in this strategy are instantiated in an ACT-based model that simulates both successful and unsuccessful performance. The protocols and the model suggest that numerical knowledge is essential to the detection of patterns and, therefore, to higher-order problem solving. This paper is intended as guidance for those who are familiar with user modeling field but are less fluent in statistical methods. It addresses potential problems with user model selection and evaluation, that are often clear to expert modelers, but are not obvious for others. These problems are frequently a result of a falsely straightforward application of statistics to user modeling (e.g. over-reliance on model fit metrics). In such cases, absolute trust in arguably shallow model accuracy measures could lead to selecting models that are hard-to-interpret, less meaningful, over-fit, and less generalizable. We offer a list of questions to consider in order to avoid these modeling pitfalls. Each of the listed questions is backed by an illustrative example based on the user modeling approach called Performance Factors Analysis (PFA) [9]. Current note-taking applications have been shown to affect the way students take notes. The impact on learning has not been studied. In this paper, we describe a project aimed at addressing how specific features of note-taking tools impact both behavior and performance. We describe our initial results evaluating copy-paste functionality, their implication for design, and future studies. We believe this work has relevance not only for the design of note-taking tools, but for a broader CHI audience. The possibility of leveraging technology to support children's learning in the real world is both appealing and technically challenging. We have been exploring factors in tangible games that may contribute to both learning and enjoyment with an eye toward technological feasibility and scalability. Previous research found that young children learned early physics principles better when interactively predicting and observing experimental comparisons on a physical earthquake table than when seeing a video of the same. Immersing children in the real world with computer vision-based feedback appears to evoke embodied cognition that enhances learning. In the current experiment, we replicated this intriguing result of the mere difference between observing the real world versus a flat-screen. Further, we explored whether a simple and scalable addition of physical control (such as shaking a tablet) would yield an increase in learning and enjoyment. Our 2x2 experiment found no evidence that adding simple forms of hands-on control enhances learning, while demonstrating a large impact of physical observation. A general implication for educational game design is that affording physical observation in the real world accompanied by interactive feedback may be more important than affording simple hands-on control on a tablet. Intelligent tutoring systems have been successful at increasing student mathematics learning, but may be further improved with the addition of collaborative activities. We have extended the Cognitive Tutor Algebra, a successful intelligent tutoring system for individual learning, with a reciprocal peer tutoring activity designed to increase conceptual learning. While using our peer tutoring environment, students take on tutor and tutee roles, and engage in both problem-solving actions and dialogue. In a classroom study, we randomly assigned 62 participants to three conditions (adaptive assistance to peer tutoring, fixed assistance to peer tutoring, and individual learning). All conditions yielded significant learning gains, but there were no differences between conditions in final outcomes. There were significant process differences, however. We assessed student interaction using problem-solving information logged by the intelligent tutoring system and collaborative dialogue captured in a chat window. Our analysis integrated these multiple data sources in order to better understand how collaborative dialogue and problem-solving actions might lead to conceptual learning. This rich data sheds light on how students benefitted from the reciprocal peer tutoring activity: Peer tutors learned when they reflected on tutee problem-solving actions, and tutees learned when the tutor's help was responsive to those actions. We present a methodology for designing better learning environments. In Phase 1, 6th-grade students' (n = 223) prior knowledge was assessed using a difficulty factors assessment (DFA). The assessment revealed that scaffolds designed to elicit contextual, conceptual, or procedural knowledge each improved students' ability to add and subtract fractions. Analyses of errors and strategies along with cognitive modeling suggested potential mechanisms underlying these effects. In Phase 2, we designed an intervention based on scaffolding this prior knowledge and implemented the computer-based lessons in mathematics classes. In Phase 3, we used the DFA and supporting analyses to assess student learning from the intervention. The posttest results suggest that scaffolding conceptual, contextual, and procedural knowledge are promising tools for improving student learning. Although engineering models of user behavior have enjoyed a rich history in HCI, they have yet to have a widespread impact due to the complexities of the modeling process. In this paper we describe a development system in which designers generate predictive cognitive models of user behavior simply by demonstrating tasks on HTML mock-ups of new interfaces. Keystroke-Level Models are produced automatically using new rules for placing mental operators, then implemented in the ACT-R cognitive architecture. They interact with the mock-up through integrated perceptual and motor modules, generating behavior that is automatically quantified and easily examined. Using a query-entry user interface as an example [19], we demonstrate that this new system enables more rapid development of predictive models, with more accurate results, than previously published models of these tasks. The printing press long ago and the computer today have made widespread access to information possible. Learning theorists have suggested, however, that mere information is a poor way to learn. Instead, more effective learning comes through doing. While the most popularized element of today's MOOCs are the video lectures, many MOOCs also include interactive activities that can afford learning by doing. This paper explores the learning benefits of the use of informational assets (e.g., videos and text) in MOOCs, versus the learning by doing opportunities that interactive activities provide. We find that students doing more activities learn more than students watching more videos or reading more pages. We estimate the learning benefit from extra doing (1 SD increase) to be more than six times that of extra watching or reading. Our data, from a psychology MOOC, is correlational in character, however we employ causal inference mechanisms to lend support for the claim that the associations we find are causal. The order of problems presented to students is an important variable that affects learning effectiveness. Previous studies have shown that solving problems in a blocked order, in which all problems of one type are completed before the student is switched to the next problem type, results in less effective performance than does solving the problems in an interleaved order. However, we have no precise understanding of the reason for this effect. In addition to existing theoretical results, we use a machine-learning agent that learns cognitive skills from examples and problem solving experience, SimStudent, to provide a computational model of the problem order question. We conduct a controlled simulation study in three different math and science domains (i.e., fraction addition, equation solving and stoichiometry), where SimStudent is tutored by automatic tutors given problems that have been used to teach human students. We compare two problem orders: the blocked problem order, and the interleaved problem order. The results show that the interleaved problem order yields as effective or more effective learning in all three domains, because the interleaved problem order provides more or better opportunities for error detection and correction to the learning agent. Examination of the agent\\u2019s performance shows that learning when to apply a skill benefits more from interleaved problem orders, and suggests that learning how to apply a skill benefits more from blocked problem orders. Intelligent Tutoring Systems have been shown to be very effective in supporting learning in domains such as mathematics, physics, computer programming, etc. However, they are yet to achieve similar success in tutoring metacognition. While an increasing number of educational technology systems support productive metacognitive behavior within the scope of the system, few attempt to teach skills students need to become better future learners. To that end, we offer a set of empirically-based design principles for metacognitive tutoring. Our starting point is a set of design principles put forward by Anderson et al. (Journal of the Learning Sciences, 4:167-207, 1995) regarding Cognitive Tutors, a family of Intelligent Tutoring Systems. We evaluate the relevance of these principles to the tutoring of help-seeking skills, based on our ongoing empirical work with the Help Tutor. This auxiliary tutor agent is designed to help students learn to make effective use of the help facilities offered by a Cognitive Tutor. While most of Anderson's principles are relevant to the tutoring of help seeking, a number of differences emerge as a result of the nature of metacognitive knowledge and of the need to combine metacognitive and domain-level tutoring. We compare our approach to other metacognitive tutoring systems, and, where appropriate, propose new guidelines to promote the discussion regarding the nature and design of metacognitive tutoring within scaffolded problem- solving environments. This article explores handwriting recognition-based interfaces in intelligent tutoring systems for students learning algebra equations. We explored the consequences for learning through interaction with an educational microworld called Electric Field Hockey (EFH). Like many microworlds, EFH is intended to help students develop a qualitative understanding of the target domain, in this case, the physics of electrical interactions. Through the development and use of a computer model that learns to play EFH, we analyzed the knowledge the model acquired as it applied the game-oriented strategies we observed physics students using. Through learning-by-doing on the standard sequence of tasks, the model substantially improved its EFH playing ability; however, it did so without acquiring any new qualitative physics knowledge. This surprising result led to an experiment that compared students\\u2019 use of EFH with standard-goal tasks against two alternative instructional conditions, specific-path and no-goal, each justified from a different learning theory. Students in the standard-goal condition learned less qualitative physics than did those in the two alternative conditions, which was consistent with the model. The implication for instructional practice is that careful selection and analysis of the tasks that frame microworld use is essential if these programs are to lead to the learning outcomes imagined for them. Theoretically, these results suggest a new interpretation for numerous empirical findings on the effectiveness of no-goal instructional tasks. The standing \\u2018\\u2018reduced cognitive load\\u2019\\u2019 interpretation is contradicted by the success of the specific-path condition, and we offer an alternative knowledge-dependent interpretation. Pen-based Flash Cards Application (\\\"application\\\") offers the flexibility of handwritten input while benefiting a wide set of users to increase their memory retention. It is particularly useful in learning mathematics where typing the material using a keyboard can be difficult. In this study, we describe the observations and major findings in a two-year case study in an eighth-grade geometry class. We found that this application may enhance teacher-student interaction, increase autonomy in students for self-guided learning, and encourage collaborative learning. Secondary teachers across the United States are being asked to use formative assessment data (Black and Wiliam 1998a,b; Roediger and Karpicke 2006) to inform their classroom instruction. At the same time, critics of US government's No Child Left Behind legislation are calling the bill \\\"No Child Left Untested\\\". Among other things, critics point out that every hour spent assessing students is an hour lost from instruction. But, does it have to be? What if we better integrated assessment into classroom instruction and allowed students to learn during the test? We developed an approach that provides immediate tutoring on practice assessment items that students cannot solve on their own. Our hypothesis is that we can achieve more accurate assessment by not only using data on whether students get test items right or wrong, but by also using data on the effort required for students to solve a test item with instructional assistance. We have integrated assistance and assessment in the ASSISTment system. The system helps teachers make better use of their time by offering instruction to students while providing a more detailed evaluation of student abilities to the teachers, which is impossible under current approaches. Our approach for assessing student math proficiency is to use data that our system collects through its interactions with students to estimate their performance on an end-of-year high stakes state test. Our results show that we can do a reliably better job predicting student end-of-year exam scores by leveraging the interaction data, and the model based on only the interaction information makes better predictions than the traditional assessment model that uses only information about correctness on the test items. Online games can serve as research instruments to explore the effects of game design elements on motivation and learning. In our research, we manipulated the design of an online math game to investigate the effect of challenge on player motivation and learning. To test the '1cInverted-U Hypothesis'1d, which predicts that maximum game engagement will occur with moderate challenge, we produced two large-scale (10K and 70K subjects), multi-factor (2x3 and 2x9x8x4x25) online experiments. We found that, in almost all cases, subjects were more engaged and played longer when the game was easier, which seems to contradict the generality of the Inverted-U Hypothesis. Troublingly, we also found that the most engaging design conditions produced the slowest rates of learning. Based on our findings, we describe several design implications that may increase challenge-seeking in games, such as providing feedforward about the anticipated degree of challenge. In this paper we investigate how competition among tutees in the context of learning by teaching affects tutors\\u2019 engagement as well as tutor learning. We conducted this investigation by incorporating a competitive Game Show feature into an online learning environment where students learn to solve algebraic equations by teaching a synthetic peer, called SimStudent. In the Game Show, pairs of SimStudents trained by students beforehand competed against each other by solving challenging problems to attain higher ratings. The results of a classroom study with 141 7th through 9th grade students showed the following: (1) Students improved their proficiency to solve equations after teaching SimStudent, but there was no observed improvement in their conceptual understanding. (2) Overall, the competitive Game Show promoted students\\u2019 extrinsic and intrinsic motivations\\u2014when the competitive Game Show was available, students\\u2019 engagement in tutoring (intrinsic motivation) was increased; students who arguably had a higher desire to win strategically selected opponents with lower proficiency for an easy win (extrinsic motivation). (3) The availability of the competitive Game Show did not affect tutor learning; there was no notable correlation between students\\u2019 motivation (intrinsic or extrinsic) and tutor learning. Based on these findings, we propose design improvements to increase tutor learning. Recent studies have tested the addition of worked examples to tutored problem solving, a more effective instructional approach than the untutored problem solving used in prior worked example research. These studies involved Cognitive Tutors, software designed to support problem solving while minimizing extraneous cognitive load by providing prompts for problem sub-goals, step-based immediate feedback, and context-sensitive hints. Results across eight studies in three different domains indicate that adding examples to Cognitive Tutors is beneficial, particularly for decreasing the instructional time needed and perhaps also for achieving more robust learning outcomes. These studies bolster the practical importance of examples in learning, but are also of theoretical interest. By using a stronger control condition than previous studies, these studies provide a basis for refining Cognitive Load Theory explanations of the benefits of examples. Perhaps, in addition to other reasons, examples may help simply because they more quickly provide novices with information needed to induce generalized knowledge. In What Should Be the Data Sharing Policy of Cognitive Science? Pitt and Tang (2013) make the case for an open data-sharing policy in Cognitive Science and highlight the use of online data repositories to store and share raw research data. One such data repository is the LearnLab DataShop (http://pslcdatashop.org) hosted at Carnegie Mellon University. DataShop is part of LearnLab, a NSF-funded Science of Learning Center started in 2004. DataShop is a major resource for researchers in educational data mining and the learning sciences, including the educational arm of Cognitive Science. DataShop is both an open repository of learning data and a web application for performing exploratory analyses on those data. DataShop specializes in data on the interaction between students and educational software, including online courses, intelligent tutoring systems, virtual labs, online assessment systems, collaborative learning environments, and simulations. As of March 2013, DataShop offers 385 datasets under 116 projects. Across these data sets, there are 97 million software-student transactions, representing over 238,000 student hours. A key feature relevant to the Cognitive Science community is DataShop\\u2019s set of tools for exploring cognitive models both visually and statistically. In DataShop, a cognitive model is a mapping between hypothesized \\u201cknowledge components\\u201d\\u2014a more general term for skill, concept, schema, production rule, misconception, or facet\\u2014and steps in the procedural completion of an online activity. A researcher can define a hypothesized model in a spreadsheet and upload it to DataShop, where it becomes available for analyses. Visual analyses include learning curves and an error report, while statistical analyses include a logistic regression model that describes how well alternative cognitive models predict student learning. DataShop has been valuable to both primary and secondary researchers in the learning sciences fueling over 100 secondary analysis studies and associated papers. For SimStudent is a machine-learning agent initially developed to help novice authors to create cognitive tutors without heavy programming. Integrated into an existing suite of software tools called Cognitive Tutor Authoring Tools (CTAT), SimStudent helps authors to create an expert model for a cognitive tutor by tutoring SimStudent on how to solve problems. There are two different ways to author an expert model with SimStudent. In the context of Authoring by Tutoring, the author interactively tutors SimStudent by posing problems to SimStudent, providing feedback on the steps performed by SimStudent, and also demonstrating steps as a response to SimStudent\\u2019s hint requests when SimStudent cannot perform steps correctly. In the context of Authoring by Demonstration, the author demonstrates solution steps, and SimStudent attempts to induce underlying domain principles by generalizing those worked-out examples. We conducted evaluation studies to investigate which authoring strategy better facilitates authoring and found two key results. First, the expert model generated with Authoring by Tutoring is better and has higher accuracy while maintaining the same level of completeness than the one generated with Authoring by Demonstration. The reason for this better accuracy is that the expert model generated by tutoring benefits from negative feedback provided for SimStudent\\u2019s incorrect production applications. Second, authoring by Tutoring requires less time than Authoring by Demonstration. This enhanced authoring efficiency is partially because (a) when Authoring by Demonstration, the author needs to test the quality of the expert model, whereas the formative assessment of the expert model is done naturally by observing SimStudent\\u2019s performance when Authoring by Tutoring, and (b) the number of steps that need to be demonstrated during tutoring decreases as learning progresses. Current standard interfaces for entering mathematical equations on computers are arguably limited and cumbersome. Mathematics notations have evolved to aid visual thinking and yet text-based interfaces relying on keyboard-and-mouse input do not take advantage of the natural two-dimensional aspects of math. Due to its similarities to paper-based mathematics, pen-based handwriting input may be faster, more efficient, and more preferable for entering mathematics on computers. This paper presents an empirical study that tests this hypothesis. We also explored a multimodal input method combining handwriting and speech because we hypothesize that it may enhance computer recognition and aid user cognition. Novice users were indeed faster, more efficient and enjoyed the handwriting modality more than a standard keyboard-and-mouse mathematics interface, especially as equation length and complexity increased. The multimodal handwriting-plus-speech method was faster and better liked than the keyboard-and-mouse method and was not much worse than handwriting alone. It has been found in recent years that many students who use intelligent tutoring systems game the system, attempting to succeed in the educational environment by exploiting properties of the system rather than by learning the material and trying to use that knowledge to answer correctly. In this paper, we introduce a system which gives a gaming student supplementary exercises focused on exactly the material the student bypassed by gaming, and which also expresses negative emotion to gaming students through an animated agent. Students using this system engage in less gaming, and students who receive many supplemental exercises have considerably better learning than is associated with gaming in the control condition or prior studies. Research on computer-supported collaborative learning has shown that students need support to benefit from collaborative activities. While classical collaboration scripts have been effective in providing such support, they have also been criticized for being coercive and not allowing students to self-regulate their learning. Adaptive collaboration support, which would provide students with assistance when and where they need it, is a possible solution. However, due to limitations of natural language processing, the development of adaptive support based on an analysis of student dialogue is difficult. To facilitate the implementation of adaptive collaboration support, we propose to leverage existing intelligent tutoring technology to provide support based on student problem-solving actions. The present paper gives two examples that demonstrate this approach and reports first experiences from the implementation of the systems in real classrooms. We conclude the paper with a discussion of possible future developments in adaptive collaboration support. Elementary, middle, and high school mathematics teachers (N = 105) ranked a set of mathematics problems based on expectations of their relative problem-solving difficulty. Teachers also rated their levels of agreement to a variety of reform-based statements on teaching and learning mathematics. Analyses suggest that teachers hold a symbol-precedence view of student mathematical development, wherein arithmetic reasoning strictly precedes algebraic reasoning, and symbolic problem-solving develops prior to verbal reasoning. High school teachers were most likely to hold the symbol-precedence view and made the poorest predictions of students' performances, whereas middle school teachers' predictions were most accurate. The discord between teachers' reform-based beliefs and their instructional decisions appears to be influenced by textbook organization, which institutionalizes the symbol-precedence view. Because of their extensive content training, high school teachers may be particularly susceptible to an expe... The ASSISTment system was used by over 600 students in 2004-05 school year as part of their math class. While in [7] we reported student learning within the ASSISTment system, in this paper we focus on the assessment aspect. Our approach is to use data that the system collected through a year to tracking student learning and thus estimate their performance on a high-stake state test (MCAS) at the end of the year. Because our system is an intelligent tutoring system, we are able to log how much assistance students needed to solve problems (how many hints students requested and how many attempts they had to make). In this paper, our goal is to determine if the models we built by taking the assistance information into account could predict students' test scores better. We present some positive evidence that shows our goal is achieved. We believe handwriting input may be able to provide significant advantages over typing, especially in the mathematics learning domain. The use of handwriting may result in decreased extraneous cognitive load on students, and it may provide better support for the two-dimensional spatial components of mathematics when compared to existing typing-based tools. Here we report progress towards the application of a handwriting interface for mathematics learning. We introduce a prototype system that allows students to use handwriting input to solve algebraic equations in an intelligent tutor. We discuss strategies to improve the existing handwriting system and apply it to math learning. Although the recognition accuracy of current handwriting engines may not be at a level suitable for use by students, we hypothesize that this may be realistically improved via advance training of the engine on a large corpus, as well as via techniques similar to co-training. Much problem solving and learning research in math and science has focused on formal representations. Recently researchers have documented the use of unschooled strategies for solving daily problems -- informal strategies which can be as effective, and sometimes as sophisticated, as school-taught formalisms. Our research focuses on how formal and informal strategies interact in the process of doing and learning mathematics. We found that combining informal and formal strategies is more effective than single strategies. We provide a theoretical account of this multiple strategy effect and have begun to formulate this theory in an ACT-R computer model. We show why students may reach common impasses in the use of written algebra, and how subsequent or concurrent use of informal strategies leads to better problem-solving performance. Formal strategies facilitate computation because of their There is evidence suggesting that providing adaptive assistance to collaborative interactions might be a good way of improving the effectiveness of collaborative activities. In this paper, we introduce the Collaborative Tutoring Research Lab (CTRL), a research-oriented framework for adaptive collaborative learning support that enables researchers to combine different types of adaptive support, particularly by using domain-specific models as input to domain-general components in order to create more complex tutoring functionality. Additionally, the framework allows researchers to implement comparison conditions by making it easier to vary single factors of the adaptive intervention. We evaluated CTRL by designing adaptive and fixed support for a peer tutoring setting, and instantiating the framework using those two collaborative scenarios and an individual tutoring scenario. As part of the implementation, we integrated pre-existing components from the Cognitive Tutor Algebra (CTA) with custom-built components. The three conditions were then compared in a controlled classroom study, and the results helped us to contribute to learning sciences research in peer tutoring. CTRL can be generalized to other collaborative scenarios, but the ease of implementation relates to the complexity of the existing components used. CTRL as a framework has yielded a full implementation of an adaptive support system and a controlled evaluation in the classroom. Despite the accumulation of substantial cognitive science research relevant to education, there remains confusion and controversy in the application of research to educational practice. In support of a more systematic approach, we describe the Knowledge-Learning-Instruction (KLI) framework. KLI promotes the emergence of instructional principles of high potential for generality, while explicitly identifying constraints of and opportunities for detailed analysis of the knowledge students may acquire in courses. Drawing on research across domains of science, math, and language learning, we illustrate the analyses of knowledge, learning, and instructional events that the KLI framework affords. We present a set of three coordinated taxonomies of knowledge, learning, and instruction. For example, we identify three broad classes of learning events (LEs): (a) memory and fluency processes, (b) induction and refinement processes, and (c) understanding and sense-making processes, and weshow how these can lead to different knowledge changes and constraints on optimal instructional choices. Adaptive collaborative learning support systems analyze student collaboration as it occurs and provide targeted assistance to the collaborators. Too little is known about how to design adaptive support to have a positive effect on interaction and learning. We investigated this problem in a reciprocal peer tutoring scenario, where two students take turns tutoring each other, so that both may benefit from giving help. We used a social design process to generate three principles for adaptive collaboration assistance. Following these principles, we designed adaptive assistance for improving peer tutor help-giving, and deployed it in a classroom, comparing it to traditional fixed support. We found that the assistance improved the conceptual content of help and the use of interface features. We qualitatively examined how each design principle contributed to the effect, finding that peer tutors responded best to assistance that made them feel accountable for help they gave. Following Computer Aided Instruction systems, 2nd generation tutors are Model-Tracing Tutors (MTTs) (Anderson & Pelletier, 1991) which are intelligent tutoring systems that have been very successful at aiding student learning, but have not reached the level of performance of experienced human tutors (Anderson et al., 1995). To that end, this paper presents a new architecture called ATM (\\\"Adding a Tutorial Model\\\"), which is an extension to model-tracing, that allows these tutors to engage in a dialog that is more like those in which experienced human tutors engage. Specifically, while MTTs provide hints toward doing the next problemsolving step, this 3rd generation of tutors, the ATM architecture, adds the capability to ask questions towards thinking about the knowledge behind the next problem-solving step. We present a new tutor built in ATM, called Ms. Lindquist, which is designed to carry on a tutorial dialog about algebra symbolization. The difference between ATM and MTT is the separate tutorial model that encodes pedagogical content knowledge in the form of different tutorial strategies, which were partially developed by observing an experienced human tutor. Ms. Lindquist has tutored thousands of students at www.AlgebraTutor.org. Future work will reveal if Ms. Lindquist is a better tutor because of the addition of the tutorial model. We investigate the prevalence and learning impact of different types of off-task behavior in classrooms where students are using intelligent tutoring software. We find that within the classrooms studied, no other type of off-task behavior is associated nearly so strongly with reduced learning as \\\"gaming the system\\\": behavior aimed at obtaining correct answers and advancing within the tutoring curriculum by systematically taking advantage of regularities in the software's feedback and help. A student's frequency of gaming the system correlates as strongly to post-test score as the student's prior domain knowledge and general academic achievement. Controlling for prior domain knowledge, students who frequently game the system score substantially lower on a post-test than students who never game the system. Analysis of students who choose to game the system suggests that learned helplessness or performance orientation might be better accounts for why students choose this behavior than lack of interest in the material. This analysis will inform the future re-design of tutors to respond appropriately when students game the system. The aim of this study is to incorporate the technique of programming by demonstration (PBD) into an authoring tool for Cognitive Tutors. The pri- mary motivation of using PBD is to facilitate the authoring of Cognitive Tutors by educators, rather than AI programmers. That is, instead of asking authors to build a cognitive model representing a task to be taught, a machine-learning agent - called the Simulated Student - observes the author performing the target task and induces production rules that replicate the author's performance. FOIL is used to learn conditions appearing in the production rules. An evaluation in an example domain of algebra equation solving shows that observing 10 prob- lems solved in 44 steps induced 9 correct and 1 wrong production rules. Two of the correctly induced rules were overly general hence produced redundant solu- tions. The goal of our research is to investigate whether a Cognitive Tutor can be made more effective by extending it to help students acquire help-seeking skills. We present a preliminary model of help-seeking behavior that will provide the basis for a Help-Seeking Tutor Agent. The model, implemented by 57 production rules, captures both productive and unproductive help-seeking behavior. As a first test of the model's efficacy, we used it off-line to evaluate students' help-seeking behavior in an existing data set of student-tutor interactions, We found that 72% of all student actions represented unproductive help-seeking behavior. Consistent with some of our earlier work (Aleven & Koedinger, 2000) we found a proliferation of hint abuse (e.g., using hints to find answers rather than trying to understand). We also found that students frequently avoided using help when it was likely to be of benefit and often acted in a quick, possibly undeliberate manner. Students' help-seeking behavior accounted for as much variance in their learning gains as their performance at the cognitive level (i.e., the errors that they made with the tutor). These findings indicate that the help-seeking model needs to be adjusted, but they also underscore the importance of the educational need that the Help-Seeking Tutor Agent aims to address. Symbolization is the ability to translate a real world situation into the language of algebra. We believe that symbolization is the single most important skill students learn in high school algebra. We present research on what makes this skill difficult and report the discovery of a \\\"hidden\\\" skill in symbolization. Contrary to past research that has emphasized that symbolization is difficult due to both comprehension difficulties and the abstract nature of variables, we found that symbolization is difficult because it is the articulation in the \\\"foreign\\\" language of \\\"algebra\\\". We also present Ms. Lindquist, an Intelligent Tutoring System (ITS) designed to carry on a tutorial dialog about symbolization. Ms. Lindquist has a separate tutorial model encoding pedagogical content knowledge in the form of different tutorial strategies, which were partially developed by observing an experienced human tutor. We discuss aspects of this human tutor's method that can be modeled well by Ms. Lindquist. Finally, we present an early formative showing that students can learn from the dialogs Ms. Lindquist is able to engage student in. Ms. Lindquist has tutored over 600 students at www.AlgebraTutor.org. This paper reports on a large-scale experiment introducing and evaluating intelligent tutoring in an urban High School setting. Critical to the success of this project has been a client-centered design approach that has matched our client's expertise in curricular objectives and classroom teaching with our expertise in artificial inte lligence and cognitive psychology. The Pittsburgh Urban Mathematics Project (PUMP) has produced an algebra curriculum that is centrally focused on mathematical analysis of real world situations and the use of computational tools. We have built an intelligent tutor, called PAT, that su pports this curriculum and has been made a regular part of 9th grade Algebra in 3 Pittsburgh schools. In the 1993-94 school year, we evaluated the effect of the PUMP curriculum and PAT tutor use. On average the 470 students in experimental classes outperformed students in comparison classes by 15% on standardized tests and 100% on tests targeting the PUMP objectives. This study provides further evidence that laboratory tutoring systems can be scaled up and made to work, both technically and pedagogically, in real and unforgiving settings like urban high schools. Middle school mathematics teachers are often forced to choose between assisting students\\u2019 development and assessing students\\u2019 abilities because of limited classroom time available. To help teachers make better use of their time, a web-based system, called the Assistment system, was created to integrate assistance and assessment by offering instruction to students while providing a more detailed evaluation of their abilities to the teacher than is possible under current approaches. An initial version of the Assistment system was created and used in May, 2004 with approximately 200 students and over 1000 students currently use it once every two weeks. The hypothesis is that Assistments can assist students while also assessing them. This chapter describes the Assistment system and some preliminary results. Secondary teachers across the country are being asked to use formative assessment data to inform their classroom instruction. At the same time, critics of No Child Left Behind are calling the bill \\\"No Child Left Untestedo emphasizing the negative side of assessment, in that every hour spent assessing students is an hour lost from instruction. Or does it have to be? What if we better integrated assessment into the classroom, and we allowed students to learn during the test? Maybe we could even provide tutoring on the steps of solving problems. Our hypothesis is that we can achieve more accurate assessment by not only using data on whether students get test items right or wrong, but by also using data on the effort required for students to learn how to solve a test item. We provide evidence for this hypothesis using data collected with our E-ASSISTment system by more than 600 students over the course of the 2004-2005 school year. We also show that we can track student knowledge over time using modern longitudinal data analysis techniques. In a separate paper [9], we report on the ASSISTment system's architecture and scalability, while this paper is focused on how we can reliably assess student learning. This paper presents the interaction design of, and demonstration of technical feasibility for, intelligent tutoring systems that can accept handwriting input from students. Handwriting and pen input offer several affordances for students that traditional typing-based interactions do not. To illustrate these affordances, we present evidence, from tutoring mathematics, that the ability to enter problem solutions via pen input enables students to record algebraic equations more quickly, more smoothly (fewer errors), and with increased transfer to non-computer-based tasks. Furthermore our evidence shows that students tend to like pen input for these types of problems more than typing. However, a clear downside to introducing handwriting input into intelligent tutors is that the recognition of such input is not reliable. In our work, we have found that handwriting input is more likely to be useful and reliable when context is considered, for example, the context of the problem being solved. We present an intelligent tutoring system for algebra equation solving via pen-based input that is able to use context to decrease recognition errors by 18% and to reduce recognition error recovery interactions to occur on one out of every four problems. We applied user-centered design principles to reduce the negative impact of recognition errors in the following ways: (1) though students handwrite their problem-solving process, they type their final answer to reduce ambiguity for tutoring purposes, and (2) in the small number of cases in which the system must involve the student in recognition error recovery, the interaction focuses on identifying the student's problem-solving error to keep the emphasis on tutoring. Many potential recognition errors can thus be ignored and distracting interactions are avoided. This work can inform the design of future systems for students using pen and sketch input for math or other topics by motivating the use of context and pragmatics to decrease the impact of recognition errors and put user focus on the task at hand. The increasing integration of education and technology has led to the development of a range of note-taking applications. Our project's goal is to provide empirical data to guide the design of such note-taking applications by evaluating the behavioral and learning outcomes of different note-taking functionality. The study reported here compares note-taking using a text editor and four interaction techniques. The two standard techniques are typing and copy-paste. The two novel techniques are restricted copy-paste and menu-selection, intended to increase attention and processing respectively. Hypothesized learning gains from the novel techniques were not observed. As implemented these techniques were less efficient and appeared to be more frustrating to use. However, data regarding differences in both note-taking efficiency and learning suggest several important implications for selection-based note-taking applications, such as pasting and highlighting. Our results also indicate that students have strong opinions regarding their note-taking practices, which may complicate potentially beneficial interventions. Self-explanation is an instructional strategy that has shown to be beneficial for math and science learning. However, it remains an open question whether these benefits will extend to other domains like second language grammar learning. Within the domain of the English article system (teaching students when to use a, an, the, or no article at all), we compare two computer-based tutoring conditions in an in vivo classroom study. In the article choice condition, students select the correct article to complete the sentence. In the explanation choice condition, students are given a sentence with the correct article highlighted and choose the rule or feature that best explains the article use. Students (N=101) in both conditions show significant learning on both procedural (article choice) and declarative (explanation choice) tasks. Not surprisingly, we found that declarative instruction (explanation choice) led to significant learning of explanations, while procedural practice (article choice) led to significant learning of the procedures. More interestingly, we also found evidence of cross-type transfer such that declarative practice led to procedural gains and procedural practice led to better understanding of the declarative rules. In general the effects of prompted self-explanation appeared somewhat stronger than those of procedural practice. Intelligent tutoring systems are quite difficult and time inten- sive to develop. In this paper, we describe a method and set of software tools that ease the process of cognitive task analysis and tutor development by allowing the author to demonstrate, instead of programming, the behav- ior of an intelligent tutor. We focus on the subset of our tools that allow authors to create \\\"Pseudo Tutors\\\" that exhibit the behavior of intelligent tu- tors without requiring AI programming. Authors build user interfaces by di- rect manipulation and then use a Behavior Recorder tool to demonstrate al- ternative correct and incorrect actions. The resulting behavior graph is an- notated with instructional messages and knowledge labels. We present some preliminary evidence of the effectiveness of this approach, both in terms of reduced development time and learning outcome. Pseudo Tutors have now been built for economics, analytic logic, mathematics, and language learn- ing. Our data supports an estimate of about 25:1 ratio of development time to instruction time for Pseudo Tutors, which compares favorably to the 200:1 estimate for Intelligent Tutors, though we acknowledge and discuss limitations of such estimates. Some students, when working in interactive learning environments, attempt to \\\"game the system\\\", attempting to succeed in the environment by exploiting properties of the system rather than by learning the material and trying to use that knowledge to answer correctly. In this paper, we present a system that can accurately detect whether a student is gaming the system, within a Cognitive Tutor mathematics curricula. Our detector also distinguishes between two distinct types of gaming which are associated with different learning outcomes. We explore this detector's generalizability, and find that it transfers successfully to both new students and new tutor lessons. Prompted self-explanation is a successful intervention for many domains. However, in our previous work within the domain of second language grammar learning, we found no advantage for self-explanation over practice alone. Here, we continue testing the generality of self-explanation through the development of an adaptive self-explanation tutor and report on results of a classroom evaluation (N=92) in which we compare the adaptive tutor to a practice-only tutor. We investigate both procedural and declarative knowledge acquisition as well as long-term retention. Results show that while self-explanation takes more time than practice alone, it leads to greater learning of declarative knowledge. However, there are no differences between conditions on immediate or long-term retention measures of procedural knowledge. Learning to think is about transfer. The scope of transfer is essentially a knowledge representation question. Experiences during learning can lead to alternative latent representations of the acquired knowledge, not all of which are equally useful. Productive learning facilitates a general representation that yields accurate behavior in a large variety of new situations, thus enabling transfer. This chapter explores two hypotheses. First, learning to think happens in pieces and these pieces, or knowledge components, are the basis of a mechanistic explanation of transfer. This hypothesis yields an instructional engineering prescription: that scientific methods of cognitive task analysis can be used to discover these knowledge components, and the resulting cognitive models can be used to redesign instruction so as to foster better transfer. The second hypothesis is that symbolic languages act as agents of transfer by focusing learning on abstract knowledge components that can enhance thinking across a wide variety of situations. The language of algebra is a prime example and we use it to illustrate (1) that cognitive task analysis can reveal knowledge components hidden to educators; (2) that such components may be acquired, like first language grammar rules, implicitly through practice; (3) that these components may be \\u201cbig ideas\\u201d not in their complexity but in terms of their usefulness as they produce transfer across contexts; and (4) that domain-specific knowledge analysis is critical to effective application of domain-general instructional strategies. We have built Sim Student, a computational model of learning, and applied it as a peer learner that allows students to learn by teaching. Using Sim Student, we study the effect of tutor learning. In this paper, we discuss an empirical classroom study where we evaluated whether asking students to provide explanations for their tutoring activities facilitates tutor learning -- the self-explanation effect for tutor learning. The results showed that students in the self-explanation condition displayed the same amount of learning gain as students in the non-self-explanation condition, but with a significantly smaller number of problems tutored (during the same time). The study also showed an apparent increase in effectiveness relative to a prior study, which is arguably due to improvement of the system based on the iterative system-engineering effort. We show how data visualization and modeling tools can be used with human input to improve student models. We present strategies for discovering potential flaws in existing student models and use them to identify improvements in a Geometry model. A key discovery was that the student model should distinguish problem steps requiring problem decomposition planning and execution from problem steps requiring just execution of problem decomposition plans. This change to the student model better fits student data not only in the original data set, but also in two other data sets from different sets of students. We also show how such student model changes can be used to modify a tutoring system, not only in terms of the usual student model effects on the tutor's problem selection, but also in driving the creation of new problems and hint messages. We are building an intelligent authoring tool for Cognitive Tutors, a highly successful form of computer-based tutoring. The primary target users (the authors) are educators who are not familiar with cognitive task analysis and AI programming, which are essential tasks in building Cognitive Tutors. Instead of asking authors to write a cognitive model by hand, a Simulated Student embedded in the authoring tool lets an author demonstrate how to perform the tasks in the subject domain, for instance, solving an algebra equation. The Simulated Student observes an author\\u2019s demonstration and induces a set of production rules that replicate the demonstrated performances. Correct production rules, as well as production rules that are incorrect but similar to those a human student might produce, can be directly embedded in the Cognitive Tutor. We give a preliminary evaluation of an implemented Simulated Students based on inductive logic programming and path-finding. ASSISTments is a web-based math tutor designed to address the need for timely student assessment while simultaneously providing instruction, thereby avoiding lost instruction time that typically occurs during assessment. This article presents a quasi-experiment that evaluates whether ASSISTments use has an effect on improving middle school students\\u2019 year-end test scores. The data was collected from 1240 seventh graders in three treatment schools and one comparison school. Post-test (7th grade year-end test) results indicate, after adjusting for the pre-test (6th grade year-end test), that students in the treatment schools significantly outperformed students in the comparison school and the difference was especially present for special education students. A usage analysis reveals that greater student use of ASSISTments is associated with greater learning consistent with the hypothesis that it is useful as a tutoring system. We also found evidence consistent with the hypothesis that teachers adapt their whole class instruction based on overall RumbleBlocks was developed at the Entertainment Technology Center (ETC) to teach engineering principles of tower stability to children ages 4\\u20137. The game features tower construction, tower piece removal, and tower comparison levels which were designed with feedback from early childhood educators and learning researchers, and iteratively improved with feedback from child play tests. This paper emphasizes the development process, and initial formative play tests with children. It was developed using the Unity3D game engine, allowing for export as a stand-alone application, web player, or to mobile devices. First results are promising in terms of educational effectiveness, with more studies planned for the future. Research in machine learning is making it possible for instructional developers to perform formative evaluations of different curricula using simulated students (VanLehn, Ohlsson & Nason, 1993). Experiments using simulated students can help clarify issues of instructional design, such as when a complex skill can be better learned by being broken into components. This paper describes two formative evaluations using simulated students that shed light on the potential benefits and limitations of mastery learning. Using an ACT-R based cognitive model (Anderson & Lebiere, 1998) we show that while mastery learning can contribute to success in some cases (Corbett & Anderson, 1995), it may actually impede learning in others. Mastery learning was crucial to learning success in an experiment comparing a traditional early algebra curriculum to a novel one presenting verbal problems first. However, in a second experiment, an instructional manipulation that contradicts mastery learning led to greater success than one consistent with it. In that experiment learning was better when more difficult problems were inserted earlier in the instructional sequence. Such problems are more difficult not because they have more components but because they cannot be successfully solved using shallow procedures that work on easier problems. Combining peer tutoring with an intelligent tutoring system (ITS) holds the promise of augmenting the current benefits of the ITS. We designed and implements a peer tutoring approach as a addition to the Cognitive Tutor Algebra (CTA), an ITS for high school algebra. We then used 30 students to evaluate the potential of the peer tutoring addition to increase learning. Although students learned and interacted positively. peer tutors lacked the necessary expertise to adequately help their tutees. The present research investigated whether immediate metacognitive feedback on students\\u2019 help-seeking errors can help students acquire better help-seeking skills. The Help Tutor, an intelligent tutor agent for help seeking, was integrated into a commercial tutoring system for geometry, the Geometry Cognitive Tutor. Study 1, with 58 students, found that the real-time assessment of students\\u2019 help-seeking behavior correlated with other independent measures of help seeking, and that the Help Tutor improved students\\u2019 help-seeking behavior while learning Geometry with the Geometry Cognitive Tutor. Study 2, with 67 students, evaluated more elaborated support that included, in addition to the Help Tutor, also help-seeking instruction and support for self-assessment. The study replicated the effect found in Study 1. It was also found that the improved help-seeking skills transferred to learning new domain-level content during the month following the intervention, while the helpseeking support was no longer in effect. Implications for metacognitive tutoring are discussed. 2010 Elsevier Ltd. All rights reserved. Cognitive Tutors are proven effective learning environments, but are still not as effective as one-on-one human tutoring. We describe an environment (ALPS) designed to engage students in question-asking during problem solving. ALPS integrates Cognitive Tutors with Synthetic Interview (SI) technology, allowing students to type free-form questions and receive pre-recorded video clip answers. We performed a Wizard-of-Oz study to evaluate the feasibility of ALPS and to design the question-and-answer database for the SI. In the study, a human tutor played the SI\\u2019s role, reading the students\\u2019 typed questions and answering over an audio/video channel. We examine the rate at which students ask questions, the content of the questions, and the events that stimulate questions. We found that students ask questions in this paradigm at a promising rate, but there is a need for further work in encouraging them to ask deeper questions that may improve knowledge encoding and learning. Although many studies demonstrate large learning gains when instruction includes diagrams, diagrams do not always lead to improved outcomes. How can instructional designers know whether a given diagram will enhance learning? We have developed a framework of three factors that influence the effectiveness of a diagram in a particular learning situation: the learning objective, the design of the visual representation and the cognitive processing of the learner. In a randomized-design study conducted in a college chemistry class, we found that instruction that included diagrams created with this framework led to enhanced performance on open-ended transfer items compared to traditional instruction, particularly for low-performing students. We propose that a concept-based cognitive theory of multimedia learning that includes a conceptual working memory component may explain why the efficacy of diagrams depends heavily on the prior knowledge of the learner as well as the conceptual information available in the representation. Cognitive tutors have been shown to increase student learning in long-term classroom studies but would become even more effective if they provided collaborative support and metacognitive tutoring. Reconceptualizing an established tutoring system as a research platform to test different collaborative and metacognitive interventions would lead to gains in learning research. In this paper, we define a component-based architecture for such a platform, drawing from previous theoretical frameworks for tutoring systems. We then describe two practical implementation challenges not typically addressed by these frameworks. We detail our efforts to extend a cognitive tutor and evaluate our progress in terms of flexibility, control, and practicality. In this paper we report the progress of our ongoing project exploring the adaptation of handwriting recognition-based interfaces for applications in intelligent tutoring systems for students learning algebra equation-solving. The research is motivated by the hypothesis that handwriting as an input modality may be able to provide significant advantages over typing in the mathematics learning domain. We review the literature of existing handwriting systems for mathematic applications and evaluations of handwriting recognition accuracy. We describe our approach and report results to date in exploring the use of handwriting recognition in interfaces for math learning, from both a technical and a pedagogical perspective. We have found that handwriting input can provide benefits to students learning math, and continue to pursue further technical and pedagogical enhancements. Advanced educational technologies are developing rapidly and online MOOC courses are becoming more prevalent, creating an enthusiasm for the seemingly limitless data-driven possibilities to affect advances in learning and enhance the learning experience. For these possibilities to unfold, the expertise and collaboration of many specialists will be necessary to improve data collection, to foster the development of better predictive models, and to assure models are interpretable and actionable. The big data collected from MOOCs needs to be bigger, not in its height (number of students) but in its width more meta-data and information on learners' cognitive and self-regulatory states needs to be collected in addition to correctness and completion rates. This more detailed articulation will help open up the black box approach to machine learning models where prediction is the primary goal. Instead, a data-driven learner model approach uses fine grain data that is conceived and developed from cognitive principles to build explanatory models with practical implications to improve student learning. When should instruction provide or withhold assistance? In three empirical studies, we have investigated whether worked examples, a high-assistance approach, studied in conjunction with tutored problems to be solved, a mid-level assistance approach, can lead to better learning. Contrary to prior results with untutoredproblem solving, a low-assistance approach, we found that worked examples alternating with isomorphic tutored problems did not produce more learning gains than tutored problems alone. However, the examples group across the three studies learned more efficiently than the tutored-alone group. Our studies, in conjunction with past studies, suggest that mid-level assistance leads to better learning than either lower or higher level assistance. However, while our results are illuminating, more work is needed to develop predictive theory for what combinations of assistance yield the most effective and efficient learning. Authoring tools for Intelligent Tutoring Systems are especially valuable if they not only provide a rich set of options for the efficient authoring of tutoring systems but also support controlled experiments in which the added educational value of new tutor features is evaluated. The Cognitive Tutor Authoring Tools (CTAT) provide both. Using CTAT, real-world \\\"Example-Tracing Tutors\\\" can be created without programming. CTAT also provides various kinds of support for controlled experiments, such as administration of different experimental treatments, logging, and data analysis. We present two case studies in which Example-Tracing Tutors created with CTAT were used in classroom experiments. The case studies illustrate a number of new features in CTAT: Use of Macromedia Flash MX 2004 for creating tutor interfaces, extensions to the Example-Tracing Engine that allow for more flexible tutors, a Mass Production facility for more efficient template-based authoring, and support for controlled experiments. In our on-going endeavor to teach students better help-seeking skills we designed a three-pronged Help-Seeking Support Environment that includes (a) classroom instruction (b) a Self-Assessment Tutor, to help students evaluate their own need for help, and (c) an updated version of the Help Tutor, which provides feedback with respect to students' help-seeking behavior, as they solve problems with the help of an ITS. In doing so, we attempt to offer a comprehensive help-seeking suite to support the knowledge, skills, and dispositions students need in order to become more effective help seekers. In a classroom evaluation, we found that the Help-Seeking Support Environment was successful in improving students' declarative help-seeking knowledge, but did not improve students' learning at the domain level or their help-seeking behavior in a paper-and-pencil environment. We raise a number of hypotheses in an attempt to explain these results. We question the current focus of metacognitive tutoring, and suggest ways to reexamine the role of help facilities and of metacognitive tutoring within ITSs. Studies have shown that both the act of note-taking and the use of notes for review can promote learning. Many note-taking applications have been developed for computer-based learning content. In general, they include advanced annotation functionality, and are geared toward supporting collaboration and discussion. Though these devices have been shown to change note-taking behavior, their effect on learning has not been evaluated. The goal of our research is to evaluate the effect of specific features of note-taking applications on behavior and learning, in order to develop guidelines for advanced note-taking applications that promote learning. These applications could be used as the basis for a variety of educational activities, including collaboration. In this paper, we present the results of an experiment evaluating a basic feature of note-taking technology: copy-paste. Our findings indicate that copy-paste functionality can be detrimental to learning. We describe potential implications of these results for the developers of note-taking applications Using data from an existing pre-algebra computer-based tutor, we analyzed the covariance of item-types with the goal of describing a more effective way to assign skill labels to item-types. Analyzing covariance is important because it allows us to place the skills in a related network in which we can identify the role each skill plays in learning the overall domain. This placement allows more effective and automatic assignment of skills to item- types. To analyze covariance we used POKS (partial order knowledge structures) to analyze item-type outcome relationships and Pearson correlation to capture item-type duration relationships. Hierarchical agglomerative clustering of these item-types was also performed using both outcome and duration covariance patterns. These analyses allowed us to propose improved skill labeling that removes irrelevant item-types, clusters related types, and clarifies the optimal temporal ordering of these clusters during practice. Intelligent tutoring systems are highly interactive learning environments that have been shown to improve upon typical classroom instruction. Cognitive Tutors are a type of intelligent tutor based on cognitive psychology theory of problem solving and learning. Cognitive Tutors provide a rich problem-solving environment with tutorial guidance in the form of step-by-step feedback, specific messages in response to common errors, and on-demand instructional hints. They also select problems based on individual student performance. The learning benefits of these forms of interactivity are supported, to varying extents, by a growing number of results from experimental studies. As Cognitive Tutors have matured and are being applied in new subject-matter areas, they have been used as a research platform and, particularly, to explore interactive methods to support metacognition. We review experiments with Cognitive Tutors that have compared different forms of interactivity and we reinterpret their results as partial answers to the general question: How should learning environments balance information or assistance giving and withholding to achieve optimal student learning? How best to achieve this balance remains a fundamental open problem in instructional science. We call this problem the \\u201cassistance dilemma\\u201d and emphasize the need for further science to yield specific conditions and parameters that indicate when and to what extent to use information giving versus information withholding forms of interaction. The research reported in this paper focuses on the hypothesis that an intelligent tutoring system that provides guidance with respect to students' meta-cognitive abilities can help them to become better learners. Our strategy is to extend a Cognitive Tutor (Anderson, Corbett, Koedinger, & Pelletier, 1995) so that it not only helps students acquire domain-specific skills, but also develop better general help-seeking strategies. In developing the Help Tutor, we used the same Cognitive Tutor technology at the meta-cognitive level that has been proven to be very effective at the cognitive level. A key challenge is to develop a model of how students should use a Cognitive Tutor's help facilities. We created a preliminary model, implemented by 57 production rules that capture both effective and ineffective help-seeking behavior. As a first test of the model's efficacy, we used it off-line to evaluate students' help-seeking behavior in an existing data set of student-tutor interactions. We then refined the model based on the results of this analysis. Finally, we conducted a pilot study with the Help Tutor involving four students. During one session, we saw a statistically significant reduction in students' meta-cognitive error rate, as determined by the Help Tutor's model. These preliminary results inspire confidence as we gear up for a larger-scale controlled experiment to evaluate whether tutoring on help seeking has a positive effect on students' learning outcomes. Content creation is a large component of the cost of creating educational software. Estimates are that approximately 200 hours of development time are required for every hour of instruction. We present an authoring tool designed to reduce this cost as it helps to refine and maintain content. The ASSISTment Builder is a tool designed to effectively create, edit, test, and deploy tutor content. The Web-based interface simplifies the process of tutor construction to allow users with little or no programming experience to develop content. We show the effectiveness of our Builder at reducing the cost of content creation to 40 hours for every hour of instruction. We describe new features that work toward supporting the life cycle of ITS content creation through maintaining and improving content as it is being used by students. The variabilization feature allows the user to reuse tutoring content across similar problems. The Student Comments feature provides a way to maintain and improve content based on feedback from users. The Most Common Wrong Answer feature provides a way to refine remediation based on the users' answers. This paper describes our attempt to support the life cycle of content creation. We present a vision for learning environments, called Science Learning Spaces, that are rich in engaging content and activities, provide constructive experiences in scientific process skills, and are as instructionally effective as a personal tutor. A Science Learning Space combines three independent software systems: 1) lab/field simulations in which experiments are run and data is collected, 2) modeling/construction tools in which data representations are created, analyzed and presented, and 3) tutor agents that provide just-in-time assistance in higher order skills like experimental strategy, representational tool choice, conjecturing, and argument. We believe that achieving this ambitious vision will require collaborative efforts facilitated by a component-based software architecture. We have created a feasibility demonstration that serves as an example and a call for further work toward achieving this vision. In our demonstration, we combined 1) the Active Illustrations lab simulation environment, 2) the Belvedere argumentation environment, and 3) a modeltracing Experimentation Tutor Agent. We illustrate student interaction in this Learning Space and discuss the requirements, advantages, and challenges in creating one. Seeking the right level of help at the right time can support learning. However, in the context of online problem-solving environments, it is still not entirely clear which help-seeking strategies are desired. We use fine-grained data from 38 high school students who worked with the Geometry Cognitive Tutor for 2 months to better understand the associations between specific help-seeking patterns and learning. We evaluate how students\\u2019 help-seeking behaviors on each step in a tutored problem are associated with their success on subsequent steps that require the same skills. Analyzing learning at the skill level allows us to compare different help-seeking patterns within a single student, controlling for between-student variations. Overall, asking for help on challenging steps is associated with productive learning, and overusing help is associated with poorer learning. However, contrary to many help-seeking theories, avoiding help (and failing repeatedly) is associated with better learning than seeking hel... This paper review the 10-year history of tutor development based on the ACT theory (Anderson, 1983,1993). We developed production system models in ACT ofhow students solved problems in LISP, geometry, and algebra. Computer tutors were developed around these cognitive models. Construction ofthese tutors was guided by a set of eight principles loosely based on the ACT theory. Early evaluations of these tutors usually but not always showed significant achievement gains. Best-case evaluations showed that students could achieve at least the same level of proficiency as conventional instruction in one third the time. Empirical studies showed that students were learning skills in production-rule units and that the best tutorial interaction style was one in which the tutor provides immediate feedback, consisting of short and directed error messages. The tutors appear to work better if they present themselves to students as nonhuman tools to assist learning rather than as emulations of human tutors. Students working with these tutors display transfer to other environments to the degree that they can map the tutor environment into the test environment. These experiences have coalesced into a new system for developing and deploying tutors. This system involves first selecting a problem-solving interface, then constructing a curriculum under the guidance of a domain expert, then designing a cognitive model for solving problems in that environment, then building instruction around the productions in that model, and finally deploying the tutor in the classroom. New tutors are being built in this system to achieve the NCTM standards for high school mathematics in an urban setting. (http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA312246) Adaptive collaborative learning support (ACLS) may be better than fixed forms of support at increasing learning from collaboration. While much existing adaptive assistance has focused on providing explicit feedback directly to the relevant student, we propose a two-dimensional design space which explores alternative methods of adaptive assistance that are implicit, indirect, or both. We investigated the viability of these ideas using data collected in a classroom evaluation of an ACLS system for peer tutoring which incorporated the design ideas in a manner that provided cognitive support to peer tutors. In this paper, we discuss how students interacted with the different forms of feedback, and propose a second iteration of the assistance that involves collaborative support in addition to domain support. Student modeling plays a critical role in developing and improving instruction and instructional technologies. We present a technique for automated improvement of student models that leverages the DataShop repository, crowd sourcing, and a version of the Learning Factors Analysis algorithm. We demonstrate this method on eleven educational technology data sets from intelligent tutors to games in a variety of domains from math to second language learning. In at least ten of the eleven cases, the method discovers improved models based on better test-set prediction in cross validation. The improvements isolate flaws in the original student models, and we show how focused investigation of flawed parts of models leads to new insights into the student learning process and suggests specific improvements for tutor design. We also discuss the great potential for future work that substitutes alternative statistical models of learning from the EDM literature or alternative model search algorithms. In this work we are investigating the learning benefits of e-Learning principles (a) within the context of a web-based intelligent tutor and (b) in the \\u201cwild,\\u201d that is, in real classroom (or homework) usage, outside of a controlled laboratory. In the study described in this paper, we focus on the benefits of politeness, as originally formulated by Brown and Levinson and more recently studied by Mayer and colleagues. We test the learning benefits of a stoichiometry tutor that provides polite problem statements, hints, and error messages as compared to one that provides more direct feedback. Although we find a small, but not significant, trend toward the polite tutor leading to better learning gains, our findings do not replicate that of Wang et al., who found significant learning gains through polite tutor feedback. While we hypothesize that an e-Learning principle such as politeness may not be robust enough to survive the transition from the lab to the \\u201cwild,\\u201d we will continue to experiment with the polite stoichiometry tutor. The Pittsburgh Science of Learning Center (PSLC) is developing a data storage and analysis facility, called DataShop. It currently handles log data from 6 full-year tutoring systems and dozens of smaller, experimental tutoring systems. DataShop requires a representation of log data that supports a variety of tutoring systems, atheoretical analyses and theoretical analyses. The theory-based analyses are strongly related to student modeling, so the lessons learned in developing the DataShop's representation may apply to student modeling in general. This report discusses the representation originally used by the DataShop, the problems encountered, and how the key concept of \\\"step\\\" evolved to meet these challenges. Traditionally, intelligent tutoring systems have provided feedback on the basis of a so-called expert model. Expert model tutors incorporate production rules associated with error free and efficient task performance. These systems intervene with corrective feedback as soon as a student deviates from a solution path. This thesis explores the effects of providing feedback on the basis of a so-called intelligent novice cognitive model. An intelligent novice tutor allows students to make errors, and provides guidance through the exercise of error detection and correction skills. The underlying cognitive model in such a tutor includes both rules associated with solution generation, and rules relating to error detection and correction. There are two pedagogical motivations for feedback based on an intelligent novice model. First, novice performance is often error prone and students may need error detection and correction skills in order to succeed in real world tasks. Second, the opportunity to reason about the causes and consequences of errors may allow students to form a better model of the behavior of domain operators. Learning outcomes associated with the two models were experimentally evaluated. Results show that learners who receive intelligent novice feedback demonstrate better learning overall, including better retention and transfer performance than students receiving expert model based feedback. Another focus of the research described here has been to help students form a robust and accurate encoding of declarative knowledge prior to procedural practice with an intelligent tutoring system. Examples have been widely used as a component of declarative instruction. However, research suggests that the effectiveness of examples is limited by the fact that inferences concerning the specific conditions under which operators may be applicable are only implicit in most examples, and may not be apparent to students without self-explanation. This thesis explores the effectiveness of a technique referred to in this thesis as example walkthroughs. Example walkthroughs interactively guide students through the study of examples. They present question prompts that help students make the inferences necessary to select problem solving operators that will lead to a solution. Students make these inferences by responding to multiple choice prompts. Evaluations suggest that example walkthroughs may provide a cost effective way to boost learning outcomes in intelligent tutoring systems. Personalised environments such as adaptive educational systems can be evaluated and compared using performance curves. Such summative studies are useful for determining whether or not new modifications enhance or degrade performance. Performance curves also have the potential to be utilised in formative studies that can shape adaptive model design at a much finer level of granularity. We describe the use of learning curves for evaluating personalised educational systems and outline some of the potential pitfalls and how they may be overcome. We then describe three studies in which we demonstrate how learning curves can be used to drive changes in the user model. First, we show how using learning curves for subsets of the domain model can yield insight into the appropriateness of the model's structure. In the second study we use this method to experiment with model granularity. Finally, we use learning curves to analyse a large volume of user data to explore the feasibility of using them as a reliable method for fine-tuning a system's model. The results of these experiments demonstrate the successful use of performance curves in formative studies of adaptive educational systems. This article explores the complementary strengths and weaknesses of grounded and abstract representations in the domain of early algebra. Abstract representations, such as algebraic symbols, are concise and easy to manipulate but are distanced from any physical referents. Grounded representations, such as verbal descriptions of situations, are more concrete and familiar, and they are more similar to physical objects and everyday experience. The complementary computational characteristics of grounded and abstract representations lead to trade-offs in problem-solving performance. In prior research with high school students solving relatively simple problems, Koedinger and Nathan (2004) demonstrated performance benefits of grounded representations over abstract representations\\u2014students were better at solving simple story problems than the analogous equations. This article extends this prior work to examine both simple and more complex problems in two samples of college students. On complex problems with two references to the unknown, a \\u201csymbolic advantage\\u201d emerged, such that students were better at solving equations than analogous story problems. Furthermore, the previously observed \\u201cverbal advantage\\u201d on simple problems was replicated. We thus provide empirical support for a trade-off between grounded, verbal representations, which show advantages on simpler problems, and abstract, symbolic representations, which show advantages on more complex problems. Content creation is a large component of the cost of creating educational software. For intelligent tutoring systems, estimates are that approximately 200 hours are required for every hour of instruction. We present an authoring tool designed to reduce this cost. The ASSISTment Builder is a tool that is designed to effectively create, edit, test, and deploy pseudo-tutor content. The web-based interface simplifies the process of tutor construction to allow users with little or no programming experience to develop content. Previously, we have shown the effectiveness of our Builder at reducing costs to 30 hours for every hour of instruction. In this paper, we replicate this experiment and report our new results for the cost. We also describe new features that work towards supporting the life cycle of ITS content creation through maintaining and improving content as it is being used by students. One of the fundamental goals of artificial intelligence is to understand and develop intelligent agents that simulate human-level intelligence. A lot of effort has been made to develop intelligent agents that simulate human learning of math and science, e.g., for use in cognitive tutors. However, constructing such a learning agent currently requires manual encoding of prior domain knowledge for each domain and even for each level of problem difficulty, which hurts the generality of the learning agent and is less cognitively plausible. Li et al. (2012) recently proposed an efficient algorithm that acquires representation knowledge in the form of \\u201cdeep features,\\u201d and use the acquired representation to automatically generate feature predicates to assist future learning. The authors demonstrated the generality of the proposed approach across multiple domains. The results showed that by integrating this algorithm into a simulated student, SimStudent, the extended agent achieves efficient skill acquisition, while requiring less prior knowledge engineering effort, and being a more realistic model of the state of prior knowledge of novice algebra students. In this work, we further explore the generality of the proposed approach within one domain, but across multiple difficulty levels. The results indicates that the new, extended SimStudent is able to acquire skill knowledge of harder problems using only its learned problem representations, while the original SimStudent requires its domain-specific prior knowledge to be engineered explicitly to handle these harder problems. The extended SimStudent\\u2019s performance is shown to match and even exceed the original as the complexity of problems increases. This paper describes an application of a machine-learning agent, SimStudent, as a teachable peer learner that allows a student to learn by teaching. SimStudent has been integrated into APLUS (Artificial Peer Learning environment Using SimStudent), an on-line game-like learning environment. The first classroom study was conducted in local public high schools to test the effectiveness of APLUS for learning linear algebra equations. In the study, learning by teaching (i.e., APLUS) was compared with learning by tutored-problem solving (i.e., Cognitive Tutor). The results show that the prior knowledge has a strong influence on tutor learning - for students with insufficient training on the target problems, learning by teaching may have limited benefits compared to learning by tutored problem solving. It was also found that students often use inappropriate problems to tutor SimStudent that did not effectively facilitate the tutor learning. Our 1997 article in IJAIED reported on a study that showed that a new algebra curriculum with an embedded intelligent tutoring system (the Algebra Cognitive Tutor) dramatically enhanced high-school students\\u2019 learning. The main motivation for the study was to demonstrate that intelligent tutors that have cognitive science research embedded in them could have real impact in schools. This study was one of the first large-scale classroom evaluations of the integrated use of an Intelligent Tutoring System (ITS) in high schools. A core challenge was figuring out how to embed this new technology into a curriculum and into the existing social context of schools. A key element of the study design was to include multiple kinds of assessments, including standardized test items and items measuring complex problem solving and use of representations. The results were powerful: \\u201cOn average the 470 students in experimental classes outperformed students in comparison classes by 15 % on standardized tests and 100 % on tests targeting the [course] objectives.\\u201d We suggested that the study was evidence \\u201cthat laboratory tutoring systems can be scaled up and made to work, both technically and pedagogically, in real and unforgiving settings like urban high schools.\\u201d Since this study, many more classroom studies comparing instruction that includes an ITS against business as usual have been conducted, often showing advantages for the ITS-enhanced curricula. More rigorous randomized field trials are now more commonplace, but the approach of using multiple assessments in large-scale randomized field trials has not caught on. Cognitive task analysis will remain fundamental to the success of ITSs. A key remaining question for ITS is to find out how they can be used most effectively to support open-ended problem solving, either online or offline. Given all the recent excitement around Massive Open Online Courses (MOOCs), it is interesting to note that our field of Artificial Intelligence in Education has been making huge, less recognized, progress with impact on millions of students and with the majority of those students finishing the course! Deep analysis of domain content yields novel insights and can be used to produce better courses. Aspects of such analysis can be performed by applying AI and statistical algorithms to student data collected from educational technology and better cognitive models can be discovered and empirically validated in terms of more accurate predictions of student learning. However, can such improved models yield improved student learning? This paper reports positively on progress in closing this loop. We demonstrate that a tutor unit, redesigned based on data-driven cognitive model improvements, helped students reach mastery more efficiently. In particular, it produced better learning on the problem-decomposition planning skills that were the focus of the cognitive model improvements. Help seeking is an important process in self-regulated learning (SRL). It may influence learning with intelligent tutoring systems (ITSs), because many ITSs provide help, often at the student\\u2019s request. The Help Tutor was a tutor agent that gave in-context, real-time feedback on students\\u2019 help-seeking behavior, as they were learning with an ITS. Key goals were to help students become better self-regulated learners and help them achieve better domain-level learning outcomes. In a classroom study, feedback on help seeking helped students to use on-demand help more deliberately, even after the feedback was no longer given, but not to achieve better learning outcomes. The work made a number of contributions, including the creation of a knowledge-engineered, rule-based, executable model of help seeking that can drive tutoring. We review these contributions from a contemporary perspective, with a theoretical analysis, a review of recent empirical literature on help seeking with ITSs, and methodological suggestions. Although we do not view on-demand, principle-based help during tutored problem solving as being as important as we once did, we still view it as helpful under certain circumstances, and recommend that it be included in ITSs. We view the goal of helping students become better self-regulated learners as one of the grand challenges in ITSs research today. A global computer network method and apparatus for creating and deploying many types of intelligent tutoring systems across many different platforms is disclosed. A run time engine supports state graph pseudo tutors and JESS model tracing cognitive tutors, in both a client and server context. An Assistment Builder enables development, testing and deployment of the pseudo tutors, generally the tutorials formed of teacher composed problems. The system simplifies the process of tutorial construction to allow educator users with little or no ITS experience to develop content of problems and teaching strategies (i.e., format of problems including hints, messages and sequencing of related problems). The system provides a Web these tutorials. A reporting component is Web based and allows for live database reporting to teachers, showing how their students are performing. Automated analysis and reporting of experimental tutorials developed by teachers is included. In this thesis, we propose a machine learning based framework called Learning Factors Analysis (LFA) to address the problem of discovering a better cognitive model from student learning data. This problem has both significant real world impact and high academic interest. A cognitive model is a binary matrix representation of how students solve domain problems. It is the key component of Cognitive Tutors, an award-winning computer-based math curriculum that grows out of the extensive research in artificial intelligence at Carnegie Mellon. However, discovering a better matrix representation is a structure learning problem of uncovering the hidden layer of a multi-layer probabilistic graphic model with all variables being discrete. The LFA framework we developed takes an innovative machine learning process that brings human expertise into the discovery loop. It addresses four research questions that one builds upon its predecessor. Accordingly, four techniques are developed to solve each problem. The first question is how to represent and evaluate a cognitive model. We brought in the concept of Q-matrix from Psychometrics and developed a pair of latent variable models\\u2014Additive Factor Model and Conjunctive Factor model\\u2014that predict student performance by student prior knowledge, task difficulty and task learning rates. The second question is how to bring human expertise into the discovery of the latent skill variables. We introduced a technique for subject experts labeling latent factors and developed three graph operators\\u2014add, merge and split to incorporate the latent factors in the existing graphical structure. The third question is how to improve a cognitive model given extensive human labeling. We introduced the concept of P-matrix and developed a penalized combinatorial search built on top of the latent variable models. The search mechanism semi-automatically improves existing cognitive models by \\\"smartly\\\" choosing features from the P-matrix and incorporating them into the Q-matrix. The penalty imposed on the search criteria helps to avoid over fitting the data. The fourth question is how to automate the latent variable discovery process without human involvement. We used Exponential Principal Component Analysis that decomposes student-task matrix into a student-skill matrix and a skill-item matrix. We then compared its performance with LFA. At the end of the thesis, we discuss several applications of LFA to improve student learning. We applied LFA to student learning data and used an LFA-improved cognitive model to save students 10%\\u201330% learning time across several units in a curriculum without hurting their learning performance. The company that markets Cognitive Tutor has started to use improved cognitive models for the 2008 version of the products onward. The estimated timesaving for all U.S. students who are using the Tutor is more than two million hours per year in total. In 2009, we reported on a new Intelligent Tutoring Systems (ITS) technology, example-tracing tutors, that can be built without programming using the Cognitive Tutor Authoring Tools (CTAT). Creating example-tracing tutors was shown to be 4\\u20138 times as cost-effective as estimates for ITS development from the literature. Since 2009, CTAT and its associated learning management system, the Tutorshop, have been extended and have been used for both research and real-world instruction. As evidence that example-tracing tutors are an effective and mature ITS paradigm, CTAT-built tutors have been used by approximately 44,000 students and account for 40 % of the data sets in DataShop, a large open repository for educational technology data sets. We review 18 example-tracing tutors built since 2009, which have been shown to be effective in helping students learn in real educational settings, often with large pre/post effect sizes. These tutors support a variety of pedagogical approaches, beyond step-based problem solving, including collaborative learning, educational games, and guided invention activities. CTAT and other ITS authoring tools illustrate that non-programmer approaches to building ITS are viable and useful and will likely play a key role in making ITS widespread. The present invention relates generally to a system and method that bring together the advantages of computer games and the physical world to increase engagement, collaboration and learning. The system and method can be used with a myriad of physical setups and can be used for many different content areas in education. In one embodiment, a mixed reality interaction is facilitated with an EarthShake\\u2122 game presented on a display. The game is synchronized with a tangible interface comprising a physical object and a sensor capable of detecting a change in the condition of the physical object. The system and method help kids discover scientific and other learning principles while experimenting with real objects in a physical environment supported with audio and visual feedback. Students interactively make predictions, see results, grapple with disconfirming evidence and formulate explanations in forms of general principles. Student mistakes are often not random but, rather, reflect thoughtful yet incorrect strategies. In order for educational technologies to make full use of students' performance data to estimate the knowledge of a student, it is important to model not only the conceptions but also the misconceptions that a student's particular pattern of successes and errors may indicate. The student models that drive the \\\"outer loop\\\" of Intelligent Tutoring Systems typically do not represent or track misconceptions. Here, we present a method of representing misconceptions in the Knowledge Component models, or Q-Matrices, that are used by student models to estimate latent knowledge. We show, in a case study on a fraction arithmetic dataset, that incorporating a misconception into the Knowledge Component model dramatically improves the overall model's fit to data. We also derive qualitative insights from comparing predicted learning curves across models that incorporate varying misconception-related parameters. Finally, we show that the inclusion of a misconception in the Knowledge Component model can yield individual student estimates of misconception strength that are significantly correlated with out-of-tutor measures of student errors. Online Education: A Unique Opportunity for Cognitive Scientists to Integrate Research and Practice Joseph Jay Williams (joseph_williams@berkeley.edu) Alexander Renkl (renkl@psychologie.uni-freiburg.de) Department of Psychology, University of California at Berkeley Psychological Institute, University of Freiberg Ken Koedinger (koedinger@cmu.edu) John Stamper (jstamper@cs.cmu.edu) Human Computer Interaction Institute, Carnegie Mellon University Human Computer Interaction Institute, Carnegie Mellon University Abstract provides an unprecedented opportunity to simultaneously carry out basic and applied research. When experimental manipulations correspond to comparisons of instructional effectiveness, stimuli are educational materials, and dependent measures are students\\u2019 learning outcomes, experimental methodology from cognitive science can be used to iteratively improve the pedagogical principles incorporated into online educational resources. Rather than years of laboratory research that \\u201csuggest\\u201d instructional principles or are eventually followed by a laborious classroom study, the steps from basic to translational research are greatly simplified. Moreover, the product of research programs that investigate online learning is not only new scientific knowledge, but specific products that concretely instantiate theories and learning principles. These proven and iteratively improved resources can be provided directly to students for use as they exhibit such great fidelity to research context. And using the Internet, they can be disseminated to hundreds of thousands of students over an extended period of time, all across the world \\u2013 a clear contribution of cognitive scientists to public education. That there is a rapid expansion of online education is much better understood than what its consequences will be. This symposium considers that one key feature of \\u201creal-world\\u201d education that takes place on the Internet is that it provides a high level of experimental control and automatic data collection & analysis, which can support cognitive science research that was previously only possible in laboratory settings and small-scale educational environments. The presenters discuss the unprecedented opportunities online learning provides for conducting research in ecologically valid contexts: linking existing laboratory experiments to relevant online contexts, personalizing adaptive instruction, embedding in vivo research studies of education, and using the vast amount of high quality data available. The product of such work is not only theories and empirical discoveries that better characterize learning, but also the opportunity to directly translate these into practical benefits to students through concrete improvements to educational resources. Keywords: learning; education; technology; online education; online learning; e-learning; intelligent tutors; educational data mining; Laboratory experiments and classrooms rarely overlap in the physical world, much to the chagrin of educational psychologists. But researchers increasingly use computers and the Internet to run experiments, and the recent explosion of online education now brings student learning into the very same digital medium. Hundreds of thousands of students take Massive Open Online Courses (MOOCs from Coursera, Udacity, EdX) at the university level, use websites like www.mathtutor.cmu.edu, www.khanacademy.org or www.mathalicious.com that are populated with K-12 videos & interactive exercises, not to mention a host of supplementary online educational resources that can be delivered over devices as ubiquitous as smartphones. These web-based resources offer the potential for extensive novel research on learning (Anderson, 2008; Ally, 2004; Linn et al, 2004; Pea, 2003). One distinctive feature is the possibility of embedding in vivo randomized experimental comparisons (Koedinger et al, 2012) into these (now) ecologically valid online educational environments. Furthermore, unlike educational environments in bricks- and-mortar education, in a digital medium there can be precise control over materials and instructions, and systematic collection of data from large samples (Stamper et al, 2012). In addition to investigating learning processes in authentic educational contexts, studying online learning Mapping laboratory studies to online educational settings In the context of his research on the role of explanation in learning, Joseph Jay Williams presents a perspective from basic experimental psychology on finding fruitful connections between lab experiments and experimental manipulations embedded in authentic online educational resources. He discusses the interplay and transitions between typical lab experiments (research on explaining membership in artificial categories, Williams & Lombrozo, 2010), to online studies using convenience samples from Amazon Mechanical Turk (in which explaining promotes learning of mathematics, Williams et al, 2012), and experiments implemented using identical mathematics exercises on Khan Academy\\u2019s educational platform, but with real students who visit the site for genuine help with authentic schoolwork. This approach blends rigorous experimentation in contexts with different levels of control, rapid iterative improvement, and the development of an ecologically valid educational resource. Such web resources serve a basic research goal, as (for example) the structure and dynamics of an interactive video or exercise reflects a concrete and empirically supported instantiation of theoretical principles Authoring Intelligent Tutoring Systems is expensive and time consuming. To reduce costs, the Cognitive Tutor Authoring Tools and the Example-Tracing Tutor paradigm were developed to make the tutor authoring process more efficient. Under this paradigm, tutors are constructed by demonstrating behavior directly in a tutor interface, reducing the need for programming expertise. This paper evaluates the efficiency of authoring a tutor with SimStudent, an extension to the Example-Tracing paradigm that is designed to produce greater generality in less time by induction from past demonstrations and feedback. We found that authoring an algebra tutor in SimStudent is faster than Example-Tracing while maintaining equivalent final model quality. Furthermore, we found that the SimStudent model generalizes beyond the problems that were used to author it. Over the past several years, several extensions to Bayesian knowledge tracing have been proposed in order to improve predictions of students\\u2019 in-tutor and post-test performance. One such extension is Contextual Guess and Slip, which incorporates machine-learned models of students\\u2019 guess and slip behaviors in order to enhance the overall model\\u2019s predictive performance [Baker et al. 2008a]. Similar machine learning approaches have been introduced in order to detect specific problem-solving steps during which students most likely learned particular skills [Baker, Goldstein, and Heffernan in press]. However, one important class of features that have not been considered in machine learning models used in these two techniques is metrics of item and skill difficulty, a key type of feature in other assessment frameworks [e.g Hambleton, Swaminathan, & Rogers, 1991; Pavlik, Cen, & Koedinger 2009]. In this paper, a set of engineered features that quantify skill difficulty and related skill-level constructs are investigated in terms of their ability to improve models of guessing, slipping, and detecting moment-by-moment learning. Supervised machine learning models that have been trained using the new skill-difficulty features are compared to models from the original contextual guess and slip and moment-by-moment learning detector work. This includes performance comparisons for predicting students\\u2019 in-tutor responses, as well as post-test responses, for a pair of Cognitive Tutor data sets. Our tutoring system for fraction addition uses dynamic pictorial representations that reflect student-inputted quantities. However, students had difficulty interpreting the pictorial feedback. Surprisingly, we found that including symbolic numbers with the pictures decreased performance. We hypothesize that students\\u2019 difficulty may stem from insufficient domain knowledge, or insufficient metacognitive skills to use conceptual knowledge to check their work. Shallow learning as a pathway for successful learning both for tutors and tutees Noboru Matsuda (mazda@cs.cmu.edu), Evelyn Yarzebinski (eey2@cs.cmu.edu), Victoria Keiser (keiser@cs.cmu.edu), Rohan Raizada (rohan@cs.cmu.edu), William W. Cohen (wcohen@cs.cmu.edu) School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA Gabriel Stylianides (gabriel.stylianides@education.ox.ac.uk) Department of Education, University of Oxford 15 Norham Gardens, Oxford, OX2 6PY, UK Kenneth R. Koedinger (krk@cs.cmu.edu) School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA Abstract game-like learning environment, called APLUS (Artificial Peer Learning environment Using SimStudent). The current version of APLUS allows students to learn Algebra equa- tions by teaching SimStudent. Using APLUS, we have con- ducted a number of classroom studies to advance cognitive and social theories of tutor learning (Matsuda, Keiser, et al., 2012; Matsuda et al., 2011). The goal of this paper is to investigate the relationship be- tween tutee- and tutor-learning. As previous empirical stud- ies show (e.g., Cohen, 1994), peer tutoring is known to be beneficial both for tutors and tutees. We thus hypothesize that there must be a strong correlation between SimStu- dent\\u2019s and human students\\u2019 learning. We are particularly interested in how a tutee\\u2019s shallow learning affects tutor learning. When tutoring, the tutor might fail to detect the tutee\\u2019s shallow learning by observing the tutee\\u2019s satisfactory performance at the surface level without actually probing for underlying deep understanding of the domain knowledge. However, if there is actually a symbiotic rela- tionship between tutee and tutor learning, then the tutee\\u2019s shallow learning should be detrimental to tutor learning. We are also interested in studying how tutee errors help not only tutee but also tutor learning. In a previous experi- ment, we studied a theoretical account of the impact of cor- rective feedback on SimStudent\\u2019s learning (Matsuda, et al., 2008). We found that committing errors and receiving ex- plicit corrective feedback facilitates tutee learning. On the other hand, it is also known that (human) students learn by explaining erroneous worked-out examples (Grosse & Renkl, 2006; Siegler, 2002). Therefore, tutee errors would also help tutors learn when tutors explain errors committed by tutees. The cognitive fidelity of SimStudent has been demonstrated especially in the way it makes human-like induction errors to learn incorrect skills and hence makes human-like errors when solving problems (Matsuda, Lee, Cohen, & Koedinger, 2009). Therefore, using SimStudent to understand how tutee errors affect tutor learning would be a valid research methodology. SimStudent is a computational model of learning with its cognitive fidelity of learning being demonstrated especially in the way it makes human-like errors. Using SimStudent as a teachable agent in an interactive peer-learning environment, we have investigated how tutee (i.e., SimStudent) learning af- fected tutor (i.e., human student) learning. In this paper, we are particularly interested in how tutees\\u2019 shallow learning af- fects tutor learning. We are also interested in how the errors that the tutee makes affect tutor learning. The results show that teaching SimStudent on a fixed set of problems makes students easy to tutor SimStudent, which in turn helps stu- dents learn, but is likely to allow SimStudent to commit shal- low learning, which is harmful for tutor learning. It is thus crucial to let the student detect SimStudent\\u2019s shallow learning and extend teaching until SimStudent and the student achieve satisfactory competence. Keywords: Learning by teaching; teachable agent; SimStu- dent; shallow learning; learning from errors. Introduction Studying the effect of learning by teaching through the use of teachable-agent technology is a rapidly growing research field. There have been a number of teachable agents used in empirical classroom studies, for example, Betty\\u2019s Brain (Biswas, Leelawong, Schwartz, Vye, & Vanderbilt, 2005) and TAAG (Pareto, Arvemo, Dahl, Haake, & Gulz, 2011). Researchers have explored different aspects of the effect of tutor learning, including learning meta-cognitive skills for self-regulated learning (Biswas, Jeong, Kinnebrew, Sulcer, & Roscoe, 2010), the protege effect (Chase, Chin, Oppezzo, & Schwartz, 2009), the adaptive assistance (Walker, Rummel, & Koedinger, 2009), and the effect of self-explanation (Matsuda, Keiser, et al., 2012). The teachable agent we have developed is called SimStu- dent. SimStudent is a machine-learning agent that learns procedural problem-solving skills from examples (Matsuda, Cohen, Sewall, Lacerda, & Koedinger, 2008). SimStudent can be interactively tutored (aka, learning from tutored problem-solving), and has been integrated into an on-line, In a previous study on a physics dataset from the Andes tutor, we found that the simple location heuristic was better at making error attribution than the simple temporal heuristic when evaluated on the learning curve standard. In this study, we investigated the generality of performance of the simple location heuristic and the simple temporal heuristic in the math domain to see if previous results generalized to other Intelligent Tutoring System domains. In support of past results, we found that the simple location heuristic provided a better goodness of fit to the learning curve standard, that is, it was better at performing error attribution than the simple temporal heuristic. One observation is that for tutors where the knowledge components can be determined by the interface location in which an action appears, using the simple location heuristic is likely to show better results than the simple temporal heuristic. It is possible that the simple temporal heuristic is better in situations where the different problem subgoals can be associated with a single location. However, our prior results with a physics data set indicated that even in such situations the simple location heuristic may be better. Further research should explore this issue. We analyze naturally occurring datasets from student use of educational technologies to explore a long-standing question of the scope of transfer of learning. We contrast a faculty theory of broad transfer with a component theory of more constrained transfer. To test these theories, we develop statistical models of them. These models use latent variables to represent mental functions that are changed while learning to cause a reduction in error rates for new tasks. Strong versions of these models provide a common explanation for the variance in task difficulty and transfer. Weak versions decouple difficulty and transfer explanations by describing task difficulty with parameters for each unique task. We evaluate these models in terms of both their prediction accuracy on held-out data and their power in explaining task difficulty and learning transfer. In comparisons across eight datasets, we find that the component models provide both better predictions and better explanations than the faculty models. Weak model variations tend to improve generalization across students, but hurt generalization across items and make a sacrifice to explanatory power. More generally, the approach could be used to identify malleable components of cognitive functions, such as spatial reasoning or executive functions. A growing body of research suggests that accounting for studentspecific variability in educational data can improve modeling accuracy and may have implications for individualizing instruction. The Additive Factors Model (AFM), a logistic regression model used to fit educational data and discover/refine skill models of learning, contains a parameter that individualizes for overall student ability but not for student learning rate. Here, we show that adding a per-student learning rate parameter to AFM overall does not improve predictive accuracy. In contrast, classifying students into three \\u201clearning rate\\u201d groups using residual error patterns, and adding a per-group learning rate parameter to AFM, substantially and consistently improves predictive accuracy across 8 datasets spanning the domains of Geometry, Algebra, English grammar, and Statistics. In a subset of datasets for which there are preand post-test data, we observe a systematic relationship between learning rate group and pre-topost-test gains. This suggests there is both predictive power and external validity in modeling these distinct learning rate groups. Additive Factors Model (AFM) and Performance Factors Analysis (PFA) are two popular models of student learning that employ logistic regression to estimate parameters and predict performance. This is in contrast to Bayesian Knowledge Tracing (BKT) which uses a Hidden Markov Model formalism. While all three models tend to make similar predictions, they differ in their parameterization of student learning. One key difference is that BKT has parameters for the slipping rates of learned skills, whereas the logistic models do not. Thus, the logistic models assume that as students get more practice their probability of correctly answering monotonically converges to 100%, whereas BKT allows monotonic convergence to lower probabilities. In this paper, we present a novel modification of logistic regression that allows it to account for situations resulting in false negative student actions (e.g., slipping on known skills). We apply this new regression approach to create two new methods AFM+Slip and PFA+Slip and compare the performance of these new models to traditional AFM, PFA, and BKT. We find that across five datasets the new slipping models have the highest accuracy on 10-fold cross validation. We also find evidence that the slip parameters better enable the logistic models to fit steep learning rates, rather than better fitting the tail of learning curves as we expected. Lastly, we explore the use of high slip values as an indicator of skills that might benefit from skill label refinement. We find that after refining the skill model for one dataset using this approach the traditional model fit improved to be on par with the slip model. Intelligent tutors have often been used mainly to teach students. In the ASSISTments project, we have emphasized using the intelligent tutoring system as an assessment system that just so happens to provide instructional assistance during the test. In this chapter we review and summarize some of the main studies we have done with the system. Usually its believed that assessment get harder if students are allowed to learn during the test, as its then like try to hit a moving target. So our results are surprising, that by providing tutoring to students while they are assessed we actually improve the assessment of students\\u2019 knowledge. We also review out attempts to give teachers feedback based on fine grained skill models. Overall, we conclude that using intelligent tutoring systems to do assessment seems like a reasonable way of dealing with the dilemma that every minute spent testing students takes time away from instruction. Key word: Intelligent Tutoring System, online tutoring, assessment, student modeling Can experimenting with three-dimensional (3D) physical objects in mixed-reality environments produce better learning and enjoyment than flat-screen two-dimensional (2D) interaction? We explored this question with EarthShake: a mixed-reality game bridging physical and virtual worlds via depth-camera sensing, designed to help children learn basic physics principles. In this paper, we report on a controlled experiment with 67 children, 4--8 years old, that examines the effect of observing physical phenomena and collaboration (pairs vs. solo). A follow-up experiment with 92 children tests whether adding simple physical control, such as shaking a tablet, improves learning and enjoyment. Our results indicate that observing physical phenomena in the context of a mixed-reality game leads to significantly more learning and enjoyment compared to screen-only versions. However, there were no significant effects of adding simple physical control or having students play in pairs vs. alone. These results and our gesture analysis provide evidence that children's science learning can be enhanced through experiencing physical phenomena in a mixed-reality environment. Outcomes and Mechanisms of Transfer in Invention Activities Ido Roll (ido@phas.ubc.ca) Carl Wieman Science Education Initiative, University of British Columbia 6224 Agricultural Road, Vancouver, BC V6T-1R9, Canada Vincent Aleven (aleven@cs.cmu.edu) Human Computer Interaction Institute, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA Kenneth R. Koedinger (koedinger@cmu.edu) Human Computer Interaction Institute and Department of Psychology, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA Abstract Invention activities are structured tasks in which students create mathematical methods that attempt to capture deep properties of data (e.g., variability), prior to receiving instruction on canonical methods (e.g., mean deviation). While experiments have demonstrated the learning benefits of invention activities, the mechanisms of transfer remain unknown. We address this question by evaluating the role of design in invention activities, identifying what knowledge is acquired during invention activities, and how it is applied in transfer tasks. A classroom experiment with 92 students compared the full invention process to one in which students evaluate predesigned methods. Results show that students in the full invention condition acquired more adaptive knowledge, yet not necessarily better procedural knowledge or invention skills. We suggest a mechanism that explains what knowledge invention attempts produce, how that knowledge is productively modified in subsequent instruction, and how it improves performance on some measures of transfer but not others. Keywords: Invention activities, transfer, intelligent tutoring systems, modular knowledge, generation. Introduction Invention activities ask students to design and evaluate mathematical methods that capture deep properties of given examples. For instance, the task in Figure 1 asks students to invent a general method for calculating variability. Invention activities are designed to augment and precede traditional teacher-led instruction (Roll, Aleven, & Koedinger, 2009; Schwartz & Martin, 2004). Following the invention attempt, whether successful or not, students receive instruction on canonical methods for the same problem (\\u201cshow\\u201d), and apply these methods to different problems (\\u201cpractice\\u201d). For example, after inventing measures of variability, students receive show-and-practice instruction on Mean Deviation (MD), that is, the mean absolute difference from the mean. One key aspect of invention activities is the use of contrasting cases (Chase, Shemwell, & Schwartz, 2010). Contrasting cases are carefully designed examples that emphasize target features by changing just those and no other features. For example, the contrasting cases shown in Figure 1 (middle) emphasize distribution while fixing sample size, range, average, etc. The Invention Lab facilitates invention activities in three steps (Roll, Aleven, & Koedinger, 2010). Students are first asked to rank the contrasting cases according to the target property (e.g, the variability of the left graph is lower than that of the right graph, see Figure 1). Students then design a mathematical method and calculate the target property for the given data (e.g., design \\u201crange / N\\u201d and apply it to both data sets). Last, students evaluate their method by comparing its inferred ranking to the initial qualitative ranking. Once the invented method ranks the contrasting cases successfully, students are given new data to work with. Students often make progress during the invention process, yet they rarely invent a valid general method (Schwartz & Martin, 2004). Classroom evaluations found that the combination of invention activities and show-and- practice instruction improves performance on transfer measures, compared with show-and- practice alone, controlling for overall time on task (Schwartz & Martin 2004; Roll et al., 2009; Kapur, 2008). However, while the positive effect of invention activities is well documented, not enough is known about how this effect is This paper describes a novel method to create a quantitative model of an educational content domain of related practice item-types using learning curves. By using a pairwise test to search for the relationships between learning curves for these item-types, we show how the test results in a set of pairwise transfer relationships that can be expressed in a Q-matrix domain model. Creating these Q-matrices for various test criteria we show that the new domain model results in consistently better learning curve fits as shown by cross- validation. Further, the Q-matrices produced can be used by educators or curriculum designers to gain a richer, more integrated perspective on concepts in the domain. The model may also have implications for tracing student knowledge more effectively to sequence practice in tutoring/training software. A Computational Model of How Learner Errors Arise from Weak Prior Knowledge oboru Matsuda (noboru.matsuda@cs.cmu.edu) Andrew Lee (andrewlee@cmu.edu) William W. Cohen (wcohen@cs.cmu.edu) Kenneth R. Koedinger (koedinger@cmu.edu) School of Computer Science, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh, PA 15213 USA Abstract How do differences in prior conceptual knowledge affect the nature and rate of learning? To answer this question, we built a computational model of learning, called SimStudent, and conducted a controlled simulation study to investigate how learning a complex skill changes when the system is given \\u201cweak\\u201d domain-general vs. \\u201cstrong\\u201d domain-specific prior knowledge. We measured SimStudent\\u2019s learning outcomes as the rate of learning, the accuracy of learned skills (test scores), and the fit to the pattern of errors made by real students. We found that when the \\u201cweak\\u201d prior knowledge is given, not only the accuracy of learned skills decreases, but also the learning rate significantly slows down. The accuracy of predicting student errors increased significantly \\u2013 namely, SimStudent with the weak prior knowledge made the same errors that real students commonly make. These modeling results help explain empirical results connecting prior knowledge and student learning (Booth & Koedinger, 2008). Keywords: Computational model of learning; machine learning; SimStudent; weak prior knowledge; patterns of student errors; mathematics education. Introduction In this paper, we present an innovative application of a synthetic student for modeling the error-prone process of student learning in a complex problem-solving domain. Previous studies have shown that student misconceptions or flaws in their prior knowledge not only directly cause errors in solving problems (VanLehn & Jones, 1993), but may also affect learning. For instance, Booth and Koedinger (2008) demonstrated that particular limitations in prior knowledge (e.g., treating terms in an equation as though terms and numbers were equivalent concepts) were correlated with particular strategic errors later in instruction (e.g., subtracting 4 from both sides of x- 4=13). The presumed causal connection is that the nature of student prior knowledge changes the learning process and thus leads to differences in the problem-solving knowledge that is acquired. But what is this learning process and how is it affected by differences in prior knowledge? A classic result from Chi, Feltovich, and Glaser\\u2019s study (1981) that experts categorize problems with deep solution- relevant features while novices categorize problems with shallow, perceptually apparent, features is also relevant to our endeavor. Also, Novick and Holyak (1991) found that domain expertise is a significant predictor of analogical transfer, but general analogical reasoning skill is not. We are ultimately interested in understanding how a novice goes from only being aware of shallow features to learning to encode problems in terms of deep features. Our strategy toward tackling this important question is to create a computational model of the learning process in complex math and science domains and to use fine-grain data from student learning over time to constrain model development. Our first steps involve demonstrating how a computational model of learning can learn when given shallow (or \\u201cweak\\u201d) knowledge, how such learning is slower than when deep (or \\u201cstrong\\u201d) knowledge is available, and how learning based on shallow/weak knowledge better predicts patterns of real student errors. In this study, we focus on the process of learning problem-solving skills from examples, where students generalize examples to inductively learn skills to solve problems. We are particularly interested in errors that are made by applying incorrect skills, and our computational model explains the processes of learning such incorrect skills as incorrect induction from examples. A number of models of student errors have been proposed (Brown & Burton, 1978; Langley & Ohlsson, 1984; Sleeman, Kelly, Martinak, Ward, & Moore, 1989; Weber, 1996; Young & O'Shea, 1981). Our effort builds on the past works by exploring how differences in prior knowledge affect the nature of the incorrect skills acquired and the errors derived. We hypothesize that incorrect generalizations are more likely when students have weaker, more general prior knowledge for encoding incoming information. This knowledge is typically perceptually grounded and is in contrast to deeper or more abstract encoding knowledge. An example of such perceptually grounded prior knowledge is to recognize 3 in x/3 simply as a number instead of as a denominator. Such an interpretation might lead students to learn an inappropriate generalization such as \\u201cmultiply both sides by a number in the left hand side of the equation\\u201d after observing x/3=5 gets x=15. If this generalization gets applied to an equation like 4x=2, the error of multiplying both sides by 4 is produced. We call this type of perceptually grounded prior knowledge \\u201cweak\\u201d prior knowledge in a similar sense as Newell and Simon\\u2019s weak reasoning methods (1972). Weak knowledge can apply across domains and can yield successful results prior to domain-specific instruction. However, in contrast to \\u201cstrong\\u201d domain-specific knowledge, weak knowledge is more likely to lead to incorrect conclusions. The goal of the present paper is to investigate an impact of the prior knowledge on learning problem-solving skills The FaCT (Fact and Concept Training) System: A New Tool Linking Cognitive Science with Educators Philip I Pavlik Jr. (ppavlik@cs.cmu.edu), Human Computer Interaction Institute Nora Presson (presson@cmu.edu), Psychology Department Giancarlo Dozzi (gdozzi@gmail.com), Human Computer Interaction Institute Sue-mei Wu (suemei@andrew.cmu.edu), Modern Languages Department Brian MacWhinney (macw@cmu.edu), Psychology Department Kenneth Koedinger (koedinger@cmu.edu), Human Computer Interaction Institute Pittsburgh Science of Learning Center Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA 15213 USA FaCT System projects in foreign language vocabulary (the Chinese I and II work described below) and in French word gender identification (a newer project described in this paper). The system also has clear applications in domains such as chemistry nomenclature, basic facts in math (i.e. times tables), history facts, and geography facts. Abstract The FaCT (Fact and Concept Training) System provides a general platform for delivering practice in the form of discrete flashcard-like drills. The system optimizes practice schedules according to model-based predictions and can be used to deliver various types of assessment. The system\\u2019s features satisfy the real world goals of educators using a theory-driven approach that gives researchers control over the model of practice delivery. For educators it provides web deployment, automatic reporting of student practice and assessment, and the ability to tailor content for specific curricular needs. For researchers it provides data export to MySQL, pluggable model architecture, and generalized model fitting algorithms. Keywords: Instruction. Memory; Skill; Practice; Sample Student Run Computer-Aided The longest running installation of the FaCT system has been in introductory Chinese I and II at Carnegie Mellon University, in cooperation with the Pittsburgh Science of Learning Center. Students in these classes are assigned a portion of their grade for using the system for vocabulary practice for a minimum of 15 minutes per week. For their weekly assignments students navigate to the URL for the system (demonstration versions are available at http://optimallearning.org/demos/). At this point the Java browser plug-in (freely available at the Sun Microsystems website) loads the system applet and a login window is displayed. Students complete the login according to the instructions. After these preliminaries, the instructions for practice are displayed, and practice begins after the user presses a button. Chinese practice trials may occur in two canonical forms, passive presentation (a study trial) and drill presentation (a test trial that includes corrective review for failure). While there are options for fixed-schedule practice sessions that randomize items and conditions (called assessment sessions in FaCT), students in Chinese are administered practice according to efficiency predictions dynamically computed from an equation-based ACT-R model of declarative memory. The overall premise of this practice scheduling is that there is an ideal spacing between successive presentations of each fact item or skill exemplar. In the model, the best spacing is characterized by a tradeoff between the advantage of spaced practice (wider temporal spacing between repetitions of an item improves the long- term recall gains for each practice) and the disadvantage of wider spacing (wider spacing decreases recall during drill practice, causing slower recall and more failures, which require costly review feedback). The sequence of trials that students in Chinese I and II experience is determined by an algorithm derived from this System Overview The FaCT System is a general-purpose application to provide practice for learners in various domains. Practice in these domains takes the form of a sequence of discrete drill trials, each of which includes immediate corrective feedback for errors. This sequence of practice trials is selected with an algorithm that uses a cognitive model of skill learning and forgetting to predict the optimal item to practice for each trial. Although the system currently uses the ACT-R model for its declarative memory predictions and trial selections (Anderson & Schooler, 1991; Pavlik Jr. & Anderson, 2005), the FaCT architecture is designed to house any model that produces dependent measures that can be used to select practice (e.g., latency and probability correct). The FaCT System is written mainly as a Java applet and is delivered over the web to learners and experimental subjects when they navigate to a webpage where the Java applet is located. This paper introduces the system capabilities and some preliminary data. The primary goal of this research has been to develop a flexible, configurable vehicle for testing cognitive theories of practice and applying these theories to advance concrete educational goals. The facts and concepts the system is designed to train are single-step problems rather than the multi-step ones trained by more complex cognitive tutors (Koedinger & Corbett, 2006). Despite their simplicity, these facts and concepts are important in many real world domains. Specifically, this paper summarizes\",\n",
            "  \"2003185464\": \"We propose a parameter shrinkage adaptation framework to estimate models with only a limited set of adaptation data to improve accuracy for automatic speech recognition, by regularizing an objective function with a sum of parameterwise power q constraint. For the first attempt, we formulate ridge maximum likelihood linear regression (MLLR) and ridge constraint MLLR (CMLLR) with an element-wise square sum constraint to regularize the objective functions of the conventional MLLR and CMLLR, respectively. Tested on the 5k-WSJ0 task, the proposed ridge MLLR and ridge CMLLR algorithms give significant word error rate reduction from the errors obtained with standard MLLR and CMLLR in an utterance-by-utterance unsupervised adaptation scenario. We study separation between models of speech attributes. A good measure of separation usually serves as a key indicator of the discrimination power of these speech models because it can often be used to indirectly determine the performance of speech recognition and verification systems. In this study, we use a probabilistic distance, called generalized log likelihood ratio (GLLR), to measure the separation between a model of a target speech attribute and models of its competing attributes. We illustrate five applications to compare separations among models obtained over multiple levels of discrimination capabilities, at various degrees of acoustic definitions and resolutions, under mismatched training and testing conditions, and with different training criteria and speech parameters. We demonstrate that the well-known GLLR distance and its corresponding histograms also provide a good utility to qualitatively and quantitatively characterize the properties of trained models without performing large scale speech recognition and verification experiments. The Gaussian mixture model (GMM)-based method has dominated the field of voice conversion (VC) for last decade. However, the converted spectra are excessively smoothed and thus produce muffled converted sound. In this study, we improve the speech quality by enhancing the dependency between the source (natural sound) and converted feature vectors (converted sound). It is believed that enhancing this dependency can make the converted sound closer to the natural sound. To this end, we propose an integrated maximum a posteriori and mutual information (MAPMI) criterion for parameter generation on spectral conversion. Experimental results demonstrate that the quality of converted speech by the proposed MAPMI method outperforms that by the conventional method in terms of formal listening test. Temporal envelope is the primary acoustic cue used in most cochlear implant (CI) speech processors to elicit speech perception for patients fitted with CI devices. Envelope compression narrows down envelope dynamic range and accordingly degrades speech understanding abilities of CI users, especially under challenging listening conditions (e.g., in noise). A new adaptive envelope compression (AEC) strategy was proposed recently, which in contrast to the traditional static envelope compression, is effective at enhancing the modulation depth of envelope waveform by making best use of its dynamic range and thus improving the intelligibility of envelope-based speech. The present study further explored the effect of adaptation rate in envelope compression on the intelligibility of compressed-envelope based speech. Moreover, since noise reduction is another essential unit in modern CI systems, the compatibility of AEC and noise reduction was also investigated. In this study, listening experiments were carried out by presenting vocoded sentences to normal hearing listeners for recognition. Experimental results demonstrated that the adaptation rate in envelope compression had a notable effect on the speech intelligibility performance of the AEC strategy. By specifying a suitable adaptation rate, speech intelligibility could be enhanced significantly in noise compared to when using static envelope compression. Moreover, results confirmed that the AEC strategy was suitable for combining with noise reduction to improve the intelligibility of envelope-based speech in noise. GMAPA specifies the weight of prior density based on the SNR of the testing speech signals.GMAPA is capable of performing environment-aware speech enhancement.When the SNR is high, GMAPA adopts a small weight to prevent overcompensations.When the SNR is low, GMAPA uses a large weight to avoid disturbance of the restoration.Results show that GMAPA outperforms related approaches in objective and subjective evaluations. Spectral restoration methods for speech enhancement aim to remove noise components in noisy speech signals by using a gain function in the spectral domain. How to design the gain function is one of the most important parts for obtaining enhanced speech with good quality. In most studies, the gain function is designed by optimizing a criterion based on some assumptions of the noise and speech distributions, such as minimum mean square error (MMSE), maximum likelihood (ML), and maximum a posteriori (MAP) criteria. The MAP criterion shows advantage in obtaining a more reliable gain function by incorporating a suitable prior density. However, it has a problem as several studies showed: although MAP based estimator effectively reduces noise components when the signal-to-noise ratio (SNR) is low, it brings large speech distortion when the SNR is high. For solving this problem, we have proposed a generalized maximum a posteriori spectral amplitude (GMAPA) algorithm in designing a gain function for speech enhancement. The proposed GMAPA algorithm dynamically specifies the weight of prior density of speech spectra according to the SNR of the testing speech signals to calculate the optimal gain function. When the SNR is high, GMAPA adopts a small weight to prevent overcompensations that may result in speech distortions. On the other hand, when the SNR is low, GMAPA uses a large weight to avoid disturbance of the restoration caused by measurement noises. In our previous study, it has been proven that the weight of the prior density plays a crucial role to the GMAPA performance, and the weight is determined based on the SNR in an utterance-level. In this paper, we propose to compute the weight with the consideration of time-frequency correlations that result in a more accurate estimation of the gain function. Experiments were carried out to evaluate the proposed algorithm on both objective tests and subjective tests. The experimental results obtained from objective tests indicate that GMAPA is promising compared to several well-known algorithms at both high and low SNRs. The results of subjective listening tests indicate that GMAPA provides significantly higher sound quality than other speech enhancement algorithms. For hearing aid (HA) devices, speech enhancement (SE) is an essential unit aiming to improve signal-to-noise ratio (SNR) and quality of speech signals. Previous studies, however, indicated that user experience with current HAs was not fully satisfactory in noisy environments, suggesting that there is still room for improvement of SE in HA devices. This study proposes a novel discriminative post-filter (DPF) approach to further enhance the SNR and quality of SE processed speech signals. The DPF uses a filter to increase the energy contrast (discrimination) of speech and noise segments in a noisy utterance. In this way, SNR and sound quality of speech signals can be improved, and annoying musical noises can be suppressed. To verify the effectiveness of DPF, the present study integrates DPF with a previously proposed generalized maximum a posteriori spectral amplitude estimation (GMAPA) SE method. Experimental results demonstrated that when comparing to GMAPA alone, this integration can further improve output SNR and perceptual evaluation of speech quality (PESQ) scores and effectively suppress musical noises across various noisy conditions. Due to its low-complexity, low-latency, and high-performance, DPF can be suitably integrated in HA devices, where computational efficiency, power consumption, and effectiveness are major considerations. Speech enhancement is an important segment in digital hearing aids, which aims to improve signal-to-noise (SNR) level of received speech signals and thus enhance speech intelligibility for hearing-loss individuals. Recently, we proposed a generalized maximum a posteriori spectral amplitude (GMAPA) speech enhancement algorithm. The proposed GMAPA algorithm has been confirmed effective in a series of objective evaluations and speech recognition tests. In this study, we conduct experiments and observe that GMAPA also provides clear long-term SNR increases in a simulated hearing-aids testing condition. The result demonstrates that GMAPA can be suitably applied in digital hearing aids. The conventional NMF-based speech enhancement algorithm analyzes the magnitude spectrograms of both clean speech and noise in the training data via NMF and estimates a set of spectral basis vectors. These basis vectors are used to span a space to approximate the magnitude spectrogram of the noise-corrupted testing utterances. Finally, the components associated with the clean-speech spectral basis vectors are used to construct the updated magnitude spectrogram, producing an enhanced speech utterance. Considering that the rich spectral-temporal structure may be explored in local frequency and time-varying spectral patches, this study proposes a segmental NMF (SNMF) speech enhancement scheme to improve the conventional frame-wise NMF-based method. Two algorithms are derived to decompose the original nonnegative matrix associated with the magnitude spectrogram; the first algorithm is used in the spectral domain and the second algorithm is used in the temporal domain. When using the decomposition processes, noisy speech signals can be modeled more precisely, and spectrograms regarding the speech part can be constituted more favorably compared with using the conventional NMF-based method. Objective evaluations using perceptual evaluation of speech quality (PESQ) indicate that the proposed SNMF strategy increases the sound quality in noise conditions and outperforms the well-known MMSE log-spectral amplitude (LSA) estimation. Recently, we proposed an ensemble speaker and speaking environment modeling (ESSEM) approach to enhance the robustness of automatic speech recognition (ASR) under adverse conditions. The ESSEM framework comprises two phases, offline and online phases. In the offline phase, we prepare an environment structure that is formed by multiple sets of hidden Markov models (HMMs). Each HMM set represents a particular speaker and speaking environment. In the online phase, ESSEM estimates a mapping function to transform the prepared environment structure to a set of HMMs for the unknown testing condition. In this study, we incorporate the soft margin estimation (SME) to increase the discriminative power of the environment structure in the offline stage and therefore enhance the overall ESSEM performance. We evaluated the performance on the Aurora-2 connected digit database. With the SME refined environment structure, ESSEM provides better performance than the original framework. By using our best online mapping function, ESSEM achieves a word error rate (WER) of 4.62%, corresponding to 14.60% relative WER reduction (from 5.41% to 4.62%) over the best baseline performance of 5.41% WER. We propose an acoustic segment model (ASM) approach to incorporating temporal information into speaker modeling in text-independent speaker recognition. In training, the proposed framework first estimates a collection of ASM-based universal background models (UBMs). Multiple sets of speaker-specific ASMs are then obtained by adapting the ASM-based UBMs with speaker-specific enrollment data. A novel usage of language models of the ASM units is also proposed to characterize transitions among ASMs. In the testing phase the ASM sets for the claimed speaker and UBMs, along with a bigram ASM language model, are used to calculate detection scores for each given test utterance. We report on speaker recognition experiments using the NIST 2001 SRE database. The results clearly indicate that the proposed ASM-based method achieves a notable improvement over the GMM-based speaker modeling in which no temporal modeling is considered. Moreover, a further error reduction is obtained by integrating the language model, another inclusion of temporal properties made possibly by ASM based speaker modeling. The performance of Wi-Fi positioning systems degrades severely when the user orientation differs between locating and training phases. This paper proposes a novel approach based on histogram equalization (HEQ) to compensate for an orientation mismatch in robust Wi-Fi localization. The proposed method involves converting the temporal\\u2013spatial radio signal strength into a reference function (i.e., equalizing the histogram). By using equalized signals, the proposed algorithm improves the robustness of location estimation, even in the presence of mismatch orientation. The advantages of the proposed algorithm over traditional methods are that the assumption of user behavior is not required, and a digital compass does not need to be embedded on a mobile device. Experiments conducted in Wi-Fi networks demonstrated the effectiveness of the proposed algorithm. The results show that the proposed algorithm outperforms the orientation classifier method and provides comparable positioning accuracy to the compass-assisted approach. Maximum likelihood-based trajectory mapping considering global variance (MLGV-based trajectory mapping) has been proposed for improving the quality of the converted speech of Gaussian mixture model-based voice conversion (GMM-based VC). Although the quality of the converted speech is significantly improved, the computational cost of the online conversion process is also increased because there is no closed form solution for parameter generation in MLGV-based trajectory mapping, and an iterative process is generally required. To reduce the online computational cost, we propose to incorporate GV in the training phase of GMM-based VC. Then, the conversion process can simply adopt ML-based trajectory mapping (without considering GV in the conversion phase), which has a closed form solution. In this way, it is expected that the quality of the converted speech can be improved without increasing the online computational cost. Our experimental results demonstrate that the proposed method yields a significant improvement in the quality of the converted speech comparing to the conventional GMM-based VC method. Meanwhile, comparing to MLGV-based trajectory mapping, the proposed method provides comparable converted speech quality with reduced computational cost in the conversion process. This paper proposes a generalized maximum a posteriori spectral amplitude (GMAPA) algorithm to spectral restoration for speech enhancement. The proposed GMAPA algorithm dynamically adjusts the scale of prior information to calculate the gain function for spectral restoration. In higher signal-to-noise ratio (SNR) conditions, GMAPA adopts a smaller scale to prevent overcompensations that may result in speech distortions. On the other hand, in lower SNR conditions, GMAPA uses a larger scale to enable the gain function to more effectively remove noise components from noisy speech. We also develop a mapping function to optimally determine the prior information scale according to the SNR of speech utterances. Two standardized speech databases, Aurora-4 and Aurora-2, are used to conduct objective and recognition evaluations, respectively, to test the proposed GMAPA algorithm. For comparison, three conventional spectral restoration algorithms are also evaluated; they are minimum mean-square error spectral estimator (MMSE), maximum likelihood spectral amplitude estimator (MLSA), and maximum a posteriori spectral amplitude estimator (MAPA). The experimental results first confirm that GMAPA provides better objective evaluation scores than MMSE, MLSA, and MAPA in lower SNR conditions, with comparable scores to MLSA in higher SNR conditions. Moreover, our recognition results indicate that GMAPA outperforms the three conventional algorithms consistently over different testing conditions. Received signal strength (RSS) in Wi-Fi networks is commonly employed in indoor positioning systems; however, device diversity is a fundamental problem in such RSS-based systems. The variation in hardware is inevitable in the real world due to the tremendous growth in recent years of new Wi-Fi devices, such as iPhones, iPads, and Android devices, which is expected to continue. Different Wi-Fi devices performed differently in respect to the RSS values even at a fixed location, thus degrading localization performance significantly. This study proposes an enhanced approach, called spatial mean normalization (SMN), to design localization systems that are robust against heterogeneous devices. The main idea of SMN is to remove the spatial mean of RSS to compensate for the shift effect resulted from device diversity. The proposed algorithm was evaluated on an indoor Wi-Fi environment, where realistic RSS measurements were collected through heterogeneous laptops and smart phones. Experimental results demonstrate the effectiveness of SMN. Results show that SMN outperforms previous positioning features for heterogeneous devices. The narrow dynamic range for speech perception partially accounts for the poor speech understanding abilities of hearing-impaired patients fitted with cochlear implants, particularly in challenging listening conditions, e.g., in reverberation. Wide dynamic range compression is designed to compress speech signal into the usable hearing dynamic range of implanted patients; however, it normally uses a static compression based strategy. An adaptive envelope compression (AEC) strategy was recently proposed for speech processing in cochlear implants. It implemented the envelope compression as close to linear as possible, while confined the compressed amplitude envelope within the pre-set dynamic range. This study further assessed its effect to improve speech perception in reverberation. Vocoder simulation experiment showed that, when narrowed down to a small dynamic range, the AEC-processed sentences could yield a higher intelligibility score in reverberation than the static compression processed sentences. Designing suitable prior distributions is important for MAP-based methods.We propose a framework to characterize local information of acoustic environments.With the local information, suitable prior distributions can be designed.Four algorithms to specify hyper-parameters for prior distributions are derived.Results confirm the advantage of using local information to MAP-based methods. The maximum a posteriori (MAP) criterion is popularly used for feature compensation (FC) and acoustic model adaptation (MA) to reduce the mismatch between training and testing data sets. MAP-based FC and MA require prior densities of mapping function parameters, and designing suitable prior densities plays an important role in obtaining satisfactory performance. In this paper, we propose to use an environment structuring framework to provide suitable prior densities for facilitating MAP-based FC and MA for robust speech recognition. The framework is constructed in a two-stage hierarchical tree structure using environment clustering and partitioning processes. The constructed framework is highly capable of characterizing local information about complex speaker and speaking acoustic conditions. The local information is utilized to specify hyper-parameters in prior densities, which are then used in MAP-based FC and MA to handle the mismatch issue. We evaluated the proposed framework on Aurora-2, a connected digit recognition task, and Aurora-4, a large vocabulary continuous speech recognition (LVCSR) task. On both tasks, experimental results showed that with the prepared environment structuring framework, we could obtain suitable prior densities for enhancing the performance of MAP-based FC and MA. Anchorperson segment detection enables efficient video content indexing for information retrieval. Anchorperson detection based on audio analysis has gained popularity due to lower computational complexity and satisfactory performance. This paper presents a robust framework using a hybrid I-vector and deep neural network (DNN) system to perform anchorperson detection based on audio streams of video content. The proposed system first applies I-vector to extract speaker identity features from the audio data. With the extracted speaker identity features, a DNN classifier is then used to verify the claimed anchorperson identity. In addition, subspace feature normalization (SFN) is incorporated into the hybrid system for robust feature extraction to compensate the audio mismatch issues caused by recording devices. An anchorperson verification experiment was conducted to evaluate the equal error rate (EER) of the proposed hybrid system. Experimental results demonstrate that the proposed system outperforms the state-of-the-art hybrid I-vector and support vector machine (SVM) system. Moreover, the proposed system was further enhanced by integrating SFN to effectively compensate the audio mismatch issues in anchorperson detection tasks. An ensemble speaker and speaking environment modeling (ESSEM) approach was recently developed. This ESSEM process consists of offline and online phases. The offline phase establishes an environment structure using speech data collected under a wide range of acoustic conditions, whereas the online phase estimates a set of acoustic models that matches the testing environment based on the established environment structure. Since the estimated acoustic models accurately characterize particular testing conditions, ESSEM can improve the speech recognition performance under adverse conditions. In this work, we propose two maximum a posteriori (MAP) based algorithms to improve the online estimation part of the original ESSEM framework. We first develop MAP-based environment structure adaptation to refine the original environment structure. Next, we propose to utilize the MAP criterion to estimate the mapping function of ESSEM and enhance the environment modeling capability. For the MAP estimation, three types of priors are derived; they are the clustered prior (CP), the sequential prior (SP), and the hierarchical prior (HP) densities. Since each prior density is able to characterize specific acoustic knowledge, we further derive a combination mechanism to integrate the three priors. Based on the experimental results on the Aurora-2 task, we verify that using the MAP-based online mapping function estimation can enable ESSEM to achieve better performance than using the maximum-likelihood (ML) based counterpart. Moreover, by using an integration of the online environment structuring adaptation and mapping function estimation, the proposed MAP-based ESSEM framework is found to provide the best performance. Compared with our baseline results, MAP-based ESSEM achieves an average word error rate reduction of 15.53% (5.41 to 4.57%) under 50 testing conditions at a signal-to-noise ratio (SNR) of 0 to 20 dB over the three standardized testing sets. Data Sparsity incurs serious concern in collaborative filtering (CF). This issue is especially critical for newly launched CF applications where observed ratings are too scarce to learn a good model to predict missing values. There could be, however, information from other related domains which are with relatively denser data that can be utilized. This paper proposes a transfer-learning based approach that exploits probabilistic matrix factorization model trained with variational expectation-maximization (VIM) to resolve data sparsity by using information from multiple auxiliary domains. We conduct experiments on several data combination and report significant improvements over state-of-the-art transfer-based models for collaborative filtering. The results also show that our framework is the only solution that can achieve acceptable performance when each user has only one single rating. The code of our model is available at https://github.com/Kublai-Jing/TIC https://github.com/Kublai-Jing/TIC. We propose an ensemble speaker and speaking environment modeling (ESSEM) approach to characterizing environments in order to enhance performance robustness of automatic speech recognition systems under adverse conditions. The ESSEM process comprises two phases, the offline and the online. In the offline phase, we prepare an ensemble speaker and speaking environment space formed by a collection of super-vectors. Each super-vector consists of the entire set of means from all the Gaussian mixture components of a set of hidden Markov models that characterizes a particular environment. In the online phase, with the ensemble environment space prepared in the offline phase, we estimate the super-vector for a new testing environment based on a stochastic matching criterion. In this paper, we focus on methods for enhancing the construction and coverage of the environment space in the offline phase. We first demonstrate environment clustering and partitioning algorithms to structure the environment space well; then, we propose a minimum classification error training algorithm to enhance discrimination across environment super-vectors and therefore broaden the coverage of the ensemble environment space. We evaluate the proposed ESSEM framework on the Aurora2 connected digit recognition task. Experimental results verify that ESSEM provides clear improvement over a baseline system without environmental compensation. Moreover, the performance of ESSEM can be further enhanced by using well-structured environment spaces. Finally, we confirm that ESSEM gives the best overall performance with an environment space refined by an integration of all techniques. In this paper, we propose a semantic naive Bayes classifier (SNBC) to improve the conventional naive Bayes classifier (NBC) by incorporating \\u201cdocument-level\\u201d semantic information for document classification (DC). To capture the semantic information from each document, we develop semantic feature extraction and modeling algorithms. For semantic feature extraction, we first apply a log-Bilinear document modeling (LBDM) algorithm to transform each word into a semantic vector, and then apply principal component analysis (PCA) to the semantic space formed by the word vectors to extract a set of semantic features for each document. For semantic modeling, a semantic model is constructed using the semantic features of the training documents. In the testing phase, SNBC systematically integrates the semantic model and the conventional NBC to perform DC. The results of experiments on the 20 News-groups and WebKB datasets confirm that, with the semantic score, SNBC consistently outperforms NBC with various language modeling approaches. Recently, an ensemble speaker and speaking environment modeling (ESSEM) framework was proposed to enhance automatic speech recognition performance under adverse conditions. In the online phase of ESSEM, the prepared environment structure in the offline stage is transformed to a set of acoustic models for the target testing environment by using a mapping function. In the original ESSEM framework, the mapping function parameters are estimated based on a maximum likelihood (ML) criterion. In this study, we propose to use a maximum a posteriori (MAP) criterion to calculate the mapping function to avoid a possible over-fitting problem that can degrade the accuracy of environment characterization. For the MAP estimation, we also study two types of prior densities, namely, clustered prior and hierarchical prior, in this paper. On the Aurora-2 task using either type of prior densities, MAP-based ESSEM can achieve better performance than ML-based ESSEM, especially under low SNR conditions. When comparing to our best baseline results, the MAP-based ESSEM achieves a 14.97% (5.41% to 4.60%) word error rate reduction in average at a signal to noise ratio of 0dB to 20dB over the three testing sets. Recently an ensemble speaker and speaking environment modeling (ESSEM) approach to characterizing unknown testing environments was studied for robust speech recognition. Each environment is modeled by a super-vector consisting of the entire set of mean vectors from all Gaussian densities of a set of HMMs for a particular environment. The super-vector for a new testing environment is then obtained by an affine transformation on the ensemble super-vectors. In this paper, we propose a minimum classification error training procedure to obtain discriminative ensemble elements, and a super-vector clustering technique to achieve refined ensemble structures. We test these two extentions to ESSEM on Aurora2. In a per-utterance unsupervised adaptation mode we achieved an average WER of 4.99% from OdB to 20 dB conditions with these two extentions when compared with a 5.51% WER obtained with the ML-trained gender-dependent baseline. To our knowledge this represents the best result reported in the literature on the Aurora2 connected digit recognition task. We propose a rescoring framework for speech recognition that incorporates acoustic phonetic knowledge sources. The scores corresponding to all knowledge sources are generated from a collection of neural network based classifiers. Rescoring is then performed by combining different knowledge scores and they are used to reorder candidate strings provided by state-of-the-art HMM-based speech recognizers. We report on continuous phone recognition experiments using the TIMIT database. Our results indicate that classifying manners and places of articulation provides additional information in rescoring, and improved accuracies over our best baseline speech recognizers are achieved using both context-independent and context-dependent phone models. The same technique can be extended to lattice rescoring and large vocabulary continuous speech recognition. A 16 x 16 programmable analog radial-basis-function (RBF) based classifier is demonstrated. The distribution of each feature is modeled by a Gaussian function, which is realized by a proposed floating-gate bump circuit having bell-shaped transfer characteristics. The maximum likelihood, mean, and variance of the distribution are stored in floating-gate transistors and are independently programmable. By cascading these floating-gate bump circuits, the overall transfer characteristics approximate a multivariate Gaussian distribution with a diagonal covariance matrix. An array of these circuits constitutes a compact RBF-based classifier. When followed by a winner-take-all circuit, the analog classifier can implement vector quantization. Automatic gender identification is implemented on a 16 x 16 analog vector quantizer chip as one possible audio application of this work. The performance of the analog classifier is comparable to that of digital counter -parts. The proposed approach can be at least two orders of magnitude more power efficient than the digital microprocessors at the same task. Recently, we proposed an ensemble speaker and speaking environment modeling (ESSEM) framework to characterize speaker variability and speaking environments. In contrast to multi-style training, ESSEM uses single-style training to prepare multiple sets of environment-specific acoustic models. The ensemble of these acoustic models forms a prior structure of the environment for flexible prediction of unknown environment during testing. In this study, we present methods to further improve the precision for model characterization. We first study a weighted N-best information technique to well utilize the N-best transcription hypothesis in an unsupervised adaptation manner. Next, we introduce cohort selection and environment space adaptation techniques to online improve the resolution and coverage of the prior structure. With an integration of the proposed methods, we further improve the ESSEM performance over our previous study. On the Aurora-2 task, ESSEM achieves an average word error rate (WER) of 4.64%, corresponding to a 15.64% relative WER reduction over our best baseline result (5.50% to 4.64% WER) obtained with multi-condition training. We present methods of detector design in the Automatic Speech Attribute Transcription project. This paper details the results of a student-led, cross-site collaboration between Georgia Institute of Technology, The Ohio State University and Rutgers University. The work reported in this paper describes and evaluates the detection-based ASR paradigm and discusses phonetic attribute classes, methods of detecting framewise phonetic attributes and methods of combining attribute detectors for ASR. We use Multi-Layer Perceptrons, Hidden Markov Models and Support Vector Machines to compute confidence scores for several prescribed sets of phonetic attribute classes. We use Conditional Random Fields (CRFs) and knowledge-based rescoring of phone lattices to combine framewise detection scores for continuous phone recognition on the TIMIT database. With CRFs, we achieve a phone accuracy of 70.63%, outperforming the baseline and enhanced HMM systems, by incorporating all of the attribute detectors discussed in the paper. Index Terms: Detection-based ASR Eigenvoice techniques have been proposed to provide rapid speaker adaptation with very limited adaptation data, but the performance may be saturated when more adaptation data become available. This is because in these techniques an eigenspace with reduced dimensionality is established by properly utilizing the a priori knowledge from the large quantity of training data. The reduced dimensionality of the eigenspace requires less adaptation data to estimate the model parameters for the new speaker, but also makes it less easy to obtain more precise models with more adaptation data. In this paper, a new segmental eigenvoice approach is proposed, in which the eigenspace can be further segmented into N subeigenspaces by properly classifying the model parameters into N clusters. These N subeigenspaces can help to construct a more delicate eigenspace and more precise models when more adaptation data are available. It will be shown that there can be at least mixture-based, model-based and feature-based segmental eigenvoice approaches. Not only improved performance can be obtained, but these different approaches can be properly integrated to offer better performance. Two further approaches leading to improved segmental eigenvoice techniques with even better performance are also proposed. The experiments were performed with both a large vocabulary and a small vocabulary recognition tasks. Use of a linear projection (LP) function to transform multiple sets of acoustic models into a single set of acoustic models is proposed for characterizing testing environments for robust automatic speech recognition. The LP function is an extension of the linear regression (LR) function used in maximum likelihood linear regression (MLLR) and maximum a posteriori linear regression (MAPLR) by incorporating local information in the ensemble acoustic space to enhance the environment modeling capacity. To estimate the nuisance parameters of the LP function, we developed maximum likelihood LP (MLLP) and maximum a posteriori LP (MAPLP) and derived a set of integrated prior (IP) densities for MAPLP. The IP densities integrate multiple knowledge sources from the training set, previously seen speech data, current utterance, and a prepared tree structure. We evaluated the proposed MLLP and MAPLP on the Aurora-2 database in an unsupervised model adaptation manner. Experimental results show that the LP function outperforms the LR function with both ML- and MAP-based estimates over different test conditions. Moreover, because the MAP-based estimate can handle over-fittings well, MAPLP has clear improvements over MLLP. Compared to the baseline result, MAPLP provides a significant 10.99% word error rate reduction. Voice conversion (VC) using artificial neural networks (ANNs) has shown its capability to produce better sound quality of the converted speech than that using Gaussian mixture model (GMM). Although ANN-based VC works reasonably well, there is still room for further improvement. One of the promising ways is to adopt the successful techniques in statistical model-based parameter generation (SMPG), such as trajectory-based mapping approaches that are originally designed for GMM-based VC and hidden Markov model (HMM)-based speech synthesis. This study presents a probabilistic interpretation for ANN-based VC. In this way, ANN-based VC can easily incorporate the successful techniques in SMPG. Experimental results demonstrate that the performance of ANN-based VC can be effectively improved by two trajectory-based mapping techniques (maximum likelihood parameter generation (MLPG) algorithm and maximum likelihood-based trajectory mapping considering global variance (referred to as MLGV)), compared to the conventional ANN-based VC with frame-based mapping and the GMM-based VC with the MLPG algorithm. Moreover, ANN-based VC with the trajectory-based mapping techniques can achieve comparable performance when compared to the state-of-the-art GMM-based VC with the MLGV algorithm. This paper investigates the use of the speech parameter generation (SPG) algorithm, which has been successfully adopted in deep neural network (DNN)-based voice conversion (VC) and speech synthesis (SS), for incorporating temporal information to improve the deep denoising auto-encoder (DDAE)-based speech enhancement. In our previous studies, we have confirmed that DDAE could effectively suppress noise components from noise corrupted speech. However, because DDAE converts speech in a frame by frame manner, the enhanced speech shows some level of discontinuity even though context features are used as input to the DDAE. To handle this issue, this study proposes using the SPG algorithm as a post-processor to transform the DDAE processed feature sequence to one with a smoothed trajectory. Two types of temporal information with SPG are investigated in this study: static-dynamic and context features. Experimental results show that the SPG with context features outperforms the SPG with static-dynamic features and the baseline system, which considers context features without SPG, in terms of standardized objective tests in different noise types and SNRs. This paper investigates the transportation and vehicular modes classification by using big data from smartphone sensors. The three types of sensors used in this paper include the accelerometer, magnetometer, and gyroscope. This study proposes improved features and uses three machine learning algorithms including decision trees, K-nearest neighbor, and support vector machine to classify the user\\u2019s transportation and vehicular modes. In the experiments, we discussed and compared the performance from different perspectives including the accuracy for both modes, the executive time, and the model size. Results show that the proposed features enhance the accuracy, in which the support vector machine provides the best performance in classification accuracy whereas it consumes the largest prediction time. This paper also investigates the vehicle classification mode and compares the results with that of the transportation modes. Recently, channel state information (CSI) has been adopted as an enhanced wireless channel measurement instead of received signal strength (RSS) for indoor WiFi positioning systems. However, although CSI contains richer location information, a challenging problem is the severe dynamic range and fluctuation among the high-dimensional channels, which may degrade accuracy and cause overfitting problems. This paper proposes a novel algorithm for improved fingerprinting-based indoor localization. The proposed algorithm decomposes the CSI sequence using the multilevel discrete wavelet transform (MDWT) and normalizes the wavelet coefficients by employing histogram equalization. The robust features were then extracted by reconstructing CSI through the inverse MDWT of the normalized coefficients. We demonstrate the effectiveness of the proposed algorithm through experiments. The results show that the proposed algorithm outperforms traditional RSS, CSI, and two CSI-based algorithms, FIFS and MIMO. This paper proposes a novel framework that integrates audio and visual information for speech enhancement. Most speech enhancement approaches consider audio features only to design filters or transfer functions to convert noisy speech signals to clean ones. Visual data, which provide useful complementary information to audio data, have been integrated with audio data in many speech-related approaches to attain more effective speech processing performance. This paper presents our investigation into the use of the visual features of the motion of lips as additional visual information to improve the speech enhancement capability of deep neural network (DNN) speech enhancement performance. The experimental results show that the performance of DNN with audio-visual inputs exceeds that of DNN with audio inputs only in four standardized objective evaluations, thereby confirming the effectiveness of the inclusion of visual information into an audio-only speech enhancement framework. We propose a flexible framework for spectral conversion (SC) that facilitates training with unaligned corpora. Many SC frameworks require parallel corpora, phonetic alignments, or explicit frame-wise correspondence for learning conversion functions or for synthesizing a target spectrum with the aid of alignments. However, these requirements gravely limit the scope of practical applications of SC due to scarcity or even unavailability of parallel corpora. We propose an SC framework based on variational auto-encoder which enables us to exploit non-parallel corpora. The framework comprises an encoder that learns speaker-independent phonetic representations and a decoder that learns to reconstruct the designated speaker. It removes the requirement of parallel corpora or phonetic alignments to train a spectral conversion system. We report objective and subjective evaluations to validate our proposed method and compare it to SC methods that have access to aligned corpora.\",\n",
            "  \"2104129307\": \"The capacity to recognize faces under varied poses is a fundamental human ability that presents a unique challenge for computer vision systems. Compared to frontal face recognition, which has been intensively studied and has gradually matured in the past few decades, Pose-Invariant Face Recognition (PIFR) remains a largely unsolved problem. However, PIFR is crucial to realizing the full potential of face recognition for real-world applications, since face recognition is intrinsically a passive biometric technology for recognizing uncooperative subjects. In this article, we discuss the inherent difficulties in PIFR and present a comprehensive review of established techniques. Existing PIFR methods can be grouped into four categories, that is, pose-robust feature extraction approaches, multiview subspace learning approaches, face synthesis approaches, and hybrid approaches. The motivations, strategies, pros/cons, and performance of representative approaches are described and compared. Moreover, promising directions for future research are discussed. Scene classification plays an important role in multimedia information retrieval. Since local features are robust to image transformation, they have been used extensively for scene classification. However, it is difficult to encode the spatial relations of local features in the classification process. To solve this problem, Geometric Local Features Integration(GLFI) is proposed. By segmenting a scene image into a set of regions, a so-called Region Adjacency Graph(RAG) is constructed to model their spatial relations. To measure the similarity of two RAGs, we select a few discriminative templates and then use them to extract the corresponding discriminative graphlets(connected subgraphs of an RAG). These discriminative graphlets are further integrated by a boosting strategy for scene classification. Experiments on five datasets validate the effectiveness of our GLFI. Architectural style classification differs from standard classification tasks due to the rich inter-class relationships between different styles, such as re-interpretation, revival, and territoriality. In this paper, we adopt Deformable Part-based Models (DPM) to capture the morphological characteristics of basic architectural components and propose Multinomial Latent Logistic Regression (MLLR) that introduces the probabilistic analysis and tackles the multi-class problem in latent variable models. Due to the lack of publicly available datasets, we release a new large-scale architectural style dataset containing twenty-five classes. Experimentation on this dataset shows that MLLR in combination with standard global image features, obtains the best classification results. We also present interpretable probabilistic explanations for the results, such as the styles of individual buildings and a style relationship network, to illustrate inter-class relationships. This paper presents a novel algorithm for computing graph edit distance (GED) in image categorization. This algorithm is purely structural, i.e., it needs only connectivity structure of the graph and does not draw on node or edge attributes. There are two major contributions: (1) Introducing edge direction histogram (EDH) to characterize shape features of images. It is shown that GED can be employed as distance of EDHs. This algorithm is completely independent on cost function which is difficult to be defined exactly. (2) Computing distance of EDHs with earth mover distance (EMD) which takes neighborhood bins into account so as to compute distance of EDHs correctly. A set of experiments demonstrate that the newly presented algorithm is available for classifying and clustering images and is immune to the planar rotation of images. Compared with GED from spectral seriation, our algorithm can capture the structure change of graphs better and consume 12.79% time used by the former one. The average classification rate is 5% and average clustering rate is 25% higher than the spectral seriation method. Multimedia searching and management have become popular due to demanding applications and competition among companies. Despite the increase in interest, there is no existing book covering basic knowledge on state-of-the-art techniques within the field. Semantic Mining Technologies for Multimedia Databases provides an introduction to the most recent techniques in multimedia semantic mining necessary to researchers new to the field. This book serves as an important reference in multimedia for academicians, multimedia technologists and researchers, and academic libraries. Manifold learning algorithms have been demonstrated to be effective for hyperspectral data dimension reduction (DR). However, the low dimensional feature representation resulted by traditional manifold learning algorithms can not preserve the nonnegativity property of the hyperspectral data, which leads inconsistency with the psychological intuition of \\u201ccombining parts to form a whole\\u201d. In this paper, we propose a nonnegative discriminative manifold learning (NDML) algorithm for hyperspectral data DR, which yields a discriminative and low dimensional feature representation, with psychological and physical evidence in the human brain. Our method benefits from both the nonnegative matrix factorization (NMF) algorithm and the discriminative manifold learning (DML) algorithm. We apply the NDML algorithm to hyperspectral remote sensing image classification on HYDICE dataset. Experimental results confirm the efficiency of the proposed NDML algorithm, compared with some existing manifold learning based DR methods. In this paper, we propose a robust distracter-resistant tracking approach by learning a discriminative metric that adaptively learns the importance of features on-the-fly. The proposed metric is elaborately designed for the tracking problem by forming a margin objective function which systematically includes distance margin maximization and reconstruction error constraint that acts as a force to push distracters away from the positive space and into the negative space. Due to the variety of negative samples in the tracking problem, we specifically introduce the similarity propagation technique that gives distracters a second force from the negative space. Consequently, the discriminative metric obtained helps to preserve the most discriminative information to separate the target from distracters while ensuring the stability of the optimal metric. We seamlessly combine it with the popular L1 minimization tracker. Our tracker is therefore not only resistant to distracters, but also inherits the merit of occlusion robustness from the L1 tracker. Quantitative comparisons with several state-of-the-art algorithms have been conducted in many challenging video sequences. The results show that our method resists distracters excellently and achieves superior performance. Non-negative tensor factorization (NTF) has attracted great attention in the machine learning community. In this paper, we extend traditional non-negative tensor factorization into a supervised discriminative decomposition, referred as Supervised Non-negative Tensor Factorization with Maximum-Margin Constraint (SNTFM2). SNTFM2 formulates the optimal discriminative factorization of non-negative tensorial data as a coupled least-squares optimization problem via a maximum-margin method. As a result, SNTFM2 not only faithfully approximates the tensorial data by additive combinations of the basis, but also obtains a strong generalization power to discriminative analysis (in particular for classification in this paper). The experimental results show the superiority of our proposed model over state-of-the-art techniques on both toy and real world data sets. This paper presents a specialised Bayesian model for analysing the covariance of data that are observed in the form of matrices, which is particularly suitable for images. Compared to existing general-purpose covariance learning techniques, we exploit the fact that the variables are organised as an array with two sets of ordered indexes, which induces innate relationship between the variables. Specifically, we adopt a factorised structure for the covariance matrix. The covariance of two variables is represented by the product of the covariance of the two corresponding rows and that of the two columns. The factors, i.e. the row-wise and column-wise covariance matrices are estimated by Bayesian inference with sparse priors.  Empirical study has been conducted on image analysis. The model first learns correlations between the rows and columns in an image plane. Then the correlations between individual pixels can be inferred by their locations. This scheme utilises the structural information of an image, and benefits the analysis when the data are damaged or insufficient. Glaucoma is an optic nerve disease resulting in loss of vision. There are two common types of glaucoma: open angle glaucoma and angle closure glaucoma. Glaucoma type classification is important in glaucoma diagnosis. Ophthalmologists examine the iridocorneal angle between iris and cornea to determine the glaucoma type. However, manual classification/grading of the iridocorneal angle images is subjective and time consuming. To save workload and facilitate large-scale clinical use, it is essential to determine glaucoma type automatically. In this paper, we propose to use focal biologically inspired feature for the classification. The iris surface is located to determine the focal region. The association between focal biologically inspired feature and angle grades is built. The experimental results show that the proposed method can correctly classify 85.2% images from open angle glaucoma and 84.3% images from angle closure glaucoma. The accuracy could be improved close to 90% with more images included in the training. The results show that the focal biologically inspired feature is effective for automatic glaucoma type classification. It can be used to reduce workload of ophthalmologists and diagnosis cost. Developing reduced reference image quality assessment (RR-IQA) plays a vital role in dealing with the prediction of the visual quality of distorted images. However, most of existing methods fail to take color information into consideration, although the color distortion is significant for the increasing color images. To solve the aforementioned problem, this paper proposed a novel IQA method which focuses on the color distortion. In particular, we extract color features based on the model of color fractal structure. Then the color and structure features are mapped into visual quality using the support vector regression. Experimental results on the LIVE II database demonstrate that the proposed method has a good consistency with the human perception especially on images with color distortion. This paper studies dimensionality reduction in a weakly supervised setting, in which the preference relationship between examples is indicated by weak cues. A novel framework is proposed that integrates two aspects of the large margin principle (angle and distance), which simultaneously encourage angle consistency between preference pairs and maximize the distance between examples in preference pairs. Two specific algorithms are developed: an alternating direction method to learn a linear transformation matrix and a gradient boosting technique to optimize a non-linear transformation directly in the function space. Theoretical analysis demonstrates that the proposed large margin optimization criteria can strengthen and improve the robustness and generalization performance of preference learning algorithms on the obtained low-dimensional subspace. Experimental results on real-world datasets demonstrate the significance of studying dimensionality reduction in the weakly supervised setting and the effectiveness of the proposed framework. Speckle noise is problematic in optical coherence tomography (OCT). With the fast scan rate, swept source OCT scans the same position in the retina for multiple times rapidly and computes an average image from the multiple scans for speckle reduction. However, the eye movement poses some challenges. In this paper, we propose a new method for speckle reduction from multiply-scanned OCT slices. The proposed method applies a preliminary speckle reduction on the OCT slices and then registers them using a global alignment followed by a local alignment based on fast iterative diamond search. After that, low rank matrix completion using bilateral random projection is utilized to iteratively estimate the noise and recover the underlying clean image. Experimental results show that the proposed method achieves average contrast to noise ratio 15.65, better than 13.78 by the baseline method used currently in swept source OCT devices. The technology can be embedded into current OCT machines to enhance the image quality for subsequent analysis. Single image superresolution (SR) aims to construct a high-resolution version from a single low-resolution (LR) image. The SR reconstruction is challenging because of the missing details in the given LR image. Thus, it is critical to explore and exploit effective prior knowledge for boosting the reconstruction performance. In this paper, we propose a novel SR method by exploiting both the directional group sparsity of the image gradients and the directional features in similarity weight estimation. The proposed SR approach is based on two observations: 1) most of the sharp edges are oriented in a limited number of directions and 2) an image pixel can be estimated by the weighted averaging of its neighbors. In consideration of these observations, we apply the curvelet transform to extract directional features which are then used for region selection and weight estimation. A combined total variation regularizer is presented which assumes that the gradients in natural images have a straightforward group sparsity structure. In addition, a directional nonlocal means regularization term takes pixel values and directional information into account to suppress unwanted artifacts. By assembling the designed regularization terms, we solve the SR problem of an energy function with minimal reconstruction error by applying a framework of templates for first-order conic solvers. The thorough quantitative and qualitative results in terms of peak signal-to-noise ratio, structural similarity, information fidelity criterion, and preference matrix demonstrate that the proposed approach achieves higher quality SR reconstruction than the state-of-the-art algorithms. Abstract Inspired by the theory of Leitner\\u05f3s learning box from the field of psychology, we propose DropSample, a new method for training deep convolutional neural networks (DCNNs), and apply it to large-scale online handwritten Chinese character recognition (HCCR). According to the principle of DropSample, each training sample is associated with a quota function that is dynamically adjusted on the basis of the classification confidence given by the DCNN softmax output. After a learning iteration, samples with low confidence will have a higher frequency of being selected as training data; in contrast, well-trained and well-recognized samples with very high confidence will have a lower frequency of being involved in the ongoing training and can be gradually eliminated. As a result, the learning process becomes more efficient as it progresses. Furthermore, we investigate the use of domain-specific knowledge to enhance the performance of DCNN by adding a domain knowledge layer before the traditional CNN. By adopting DropSample together with different types of domain-specific knowledge, the accuracy of HCCR can be improved efficiently. Experiments on the CASIA-OLHDWB 1.0, CASIA-OLHWDB 1.1, and ICDAR 2013 online HCCR competition datasets yield outstanding recognition rates of 97.33%, 97.06%, and 97.51% respectively, all of which are significantly better than the previous best results reported in the literature. Dictionary learning is a method of acquiring a collection of atoms for subsequent signal representation. Due to its excellent representation ability, dictionary learning has been widely applied in multimedia and computer vision. However, conventional dictionary learning algorithms fail to deal with multi-modal datasets. In this paper, we propose an online multi-modal robust non-negative dictionary learning (OMRNDL) algorithm to overcome this deficiency. Notably, OMRNDL casts visual tracking as a dictionary learning problem under the particle filter framework and captures the intrinsic knowledge about the target from multiple visual modalities, e.g., pixel intensity and texture information. To this end, OMRNDL adaptively learns an individual dictionary, i.e., template, for each modality from available frames, and then represents new particles over all the learned dictionaries by minimizing the fitting loss of data based on M-estimation. The resultant representation coefficient can be viewed as the common semantic representation of particles across multiple modalities, and can be utilized to track the target. OMRNDL incrementally learns the dictionary and the coefficient of each particle by using multiplicative update rules to respectively guarantee their non-negativity constraints. Experimental results on a popular challenging video benchmark validate the effectiveness of OMRNDL for visual tracking in both quantity and quality. A novel level set method (LSM) with shape priors is proposed to implement a shape-driven image segmentation. By using image moments, we deprive the shape priors of position, scale and angle information, consequently obtain the aligned shape priors. Considering that the shape priors sparsely distribute into the observation space, we utilize the locality preserving projections (LPP) to map them into a low dimensional subspace in which the probability distribution is predicted by using kernel density estimation. Finally, a new energy functional with shape priors is developed by combining the negative log-probability of shape priors with other data-driven energy items. We assess the proposed LSM on the synthetic, medical and natural images. The experimental results show that it is superior to the pure data-driven LSMs and the representative LSM with shape priors. A novel level set method (LSM) with the constraint of shape priors is proposed to implement a selective image segmentation. Firstly, the shape priors are aligned by using image moment to deprive the spatial related information. Secondly, the aligned shape priors are projected into the subspace expanded by using locality preserving projection to measure the similarity between the shapes. Finally, a new energy functional is built by combing data-driven and shape-driven energy items to implement a selective image segmentation method. We assess the proposed method and some representative LSMs on the synthetic, medical and natural images, the results suggest that the proposed one is superior to the pure data-driven LSMs and the representative LSMs with shape priors. Non-negative matrix factorization (NMF) has been a popular data analysis tool and has been widely applied in computer vision. However, conventional NMF methods cannot adaptively learn grouping structure from a dataset. This paper proposes a non-negative low-rank and group-sparse matrix factorization (NLRGS) method to overcome this deficiency. Particularly, NLRGS captures the relationships among examples by constraining rank of the coefficients meanwhile identifies the grouping structure via group sparsity regularization. By both constraints, NLRGS boosts NMF in both classification and clustering. However, NLRGS is difficult to be optimized because it needs to deal with the low-rank constraint. To relax such hard constraint, we approximate the low-rank constraint with the nuclear norm and then develop an optimization algorithm for NLRGS in the frame of augmented Lagrangian method(ALM). Experimental results of both face recognition and clustering on four popular face datasets demonstrate the effectiveness of NLRGS in quantities. Images are usually associated with multiple labels and comprised of multiple views, due to each image containing several objects (e.g. a pedestrian, bicycle and tree) and multiple visual features (e.g. color, texture and shape). Currently available tools tend to use either labels or features for classification, but both are necessary to describe the image properly. There have been recent successes in using vector-valued functions, which construct matrix-valued kernels, to explore the multilabel structure in the output space. This has motivated us to develop multi-view vector-valued manifold regularization (MV3MR) in order to integrate multiple features. MV3MR exploits the complementary properties of different features, and discovers the intrinsic local geometry of the compact support shared by different features, under the theme of manifold regularization. We validate the effectiveness of the proposed MV3MR methodology for image classification by conducting extensive experiments on two challenge datasets, PASCAL VOC' 07 and MIR Flickr. In this paper, we present a set of simple and efficient regularized logistic regression algorithms to predict tags of m usic. We first vector-quantize the delta MFCC features using k-means and construct \\u201cbag-of-words\\u201d representation for each song. We then learn the parameters of these logistic regression algorithms from the \\u201cbag-of-words\\u201d vectors and ground truth labels in the training set. At test time, the prediction confidence by the linear classifiers can be used to rank the songs for music annotation and retrieval tasks. Thanks to the convex property of the objective functions, we adopt an efficient and scalable generalized gradient method to learn the parameters, with global optimum guaranteed. And we show that these efficient algorithms achieve stateof-the-art performance in annotation and retrieval tasks evaluated on CAL-500. Classifier design is a fundamental problem in pattern recognition. A variety of pattern classification methods such as the nearest neighbor (NN) classifier, support vector machine (SVM), and sparse representation-based classification (SRC) have been proposed in the literature. These typical and widely used classifiers were originally developed from different theory or application motivations and they are conventionally treated as independent and specific solutions for pattern classification. This paper proposes a novel pattern classification framework, namely, representative vector machines (or RVMs for short). The basic idea of RVMs is to assign the class label of a test example according to its nearest representative vector. The contributions of RVMs are twofold. On one hand, the proposed RVMs establish a unified framework of classical classifiers because NN, SVM, and SRC can be interpreted as the special cases of RVMs with different definitions of representative vectors. Thus, the underlying relationship among a number of classical classifiers is revealed for better understanding of pattern classification. On the other hand, novel and advanced classifiers are inspired in the framework of RVMs. For example, a robust pattern classification method called discriminant vector machine (DVM) is motivated from RVMs. Given a test example, DVM first finds its ${k}$ -NNs and then performs classification based on the robust M-estimator and manifold regularization. Extensive experimental evaluations on a variety of visual recognition tasks such as face recognition (Yale and face recognition grand challenge databases), object categorization (Caltech-101 dataset), and action recognition (Action Similarity LAbeliNg) demonstrate the advantages of DVM over other classifiers. Police agencies have been collecting an increasing amount of information to better understand patterns in criminal activity. Recently there is a new trend in using the data collected to predict where and when crime will occur. Crime prediction is greatly beneficial because if it is done accurately, police practitioner would be able to allocate resources to the geographic areas most at risk for criminal activity and ultimately make communities safer. In this paper, we discuss a new four-order tensor representation for crime data. The tensor encodes the longitude, latitude, time, and other relevant incidents. Using the tensor data structure, we propose the Empirical Discriminative Tensor Analysis (EDTA) algorithm to obtain sufficient discriminative information while minimizing empirical risk simultaneously. We examine the algorithm on the crime data collected in one Northeastern city. EDTA demonstrates promising results compared to other existing methods in real world scenarios. This report presents results from the Video Person Recognition Evaluation held in conjunction with the 11th IEEE International Conference on Automatic Face and Gesture Recognition. Two experiments required algorithms to recognize people in videos from the Point-and-Shoot Face Recognition Challenge Problem (PaSC). The first consisted of videos from a tripod mounted high quality video camera. The second contained videos acquired from 5 different handheld video cameras. There were 1401 videos in each experiment of 265 subjects. The subjects, the scenes, and the actions carried out by the people are the same in both experiments. Five groups from around the world participated in the evaluation. The video handheld experiment was included in the International Joint Conference on Biometrics (IJCB) 2014 Handheld Video Face and Person Recognition Competition. The top verification rate from this evaluation is double that of the top performer in the IJCB competition. Analysis shows that the factor most effecting algorithm performance is the combination of location and action: where the video was acquired and what the person was doing. Fisher's linear discriminant analysis (LDA), one of the most popular dimensionality reduction algorithms for classification, has three particular problems: it fails to find the nonlinear structure hidden in the high dimensional data; it assumes all samples contribute equivalently to reduce dimension for classification; and it suffers from the matrix singularity problem. In this paper, we propose a new algorithm, termed Discriminative Locality Alignment (DLA), to deal with these problems. The algorithm operates in the following three stages: first, in part optimization, discriminative information is imposed over patches, each of which is associated with one sample and its neighbors; then, in sample weighting, each part optimization is weighted by the margin degree, a measure of the importance of a given sample; and finally, in whole alignment, the alignment trick is used to align all weighted part optimizations to the whole optimization. Furthermore, DLA is extended to the semi-supervised case, i.e., semi-supervised DLA (SDLA), which utilizes unlabeled samples to improve the classification performance. Thorough empirical studies on the face recognition demonstrate the effectiveness of both DLA and SDLA. Manifold learning has been demonstrated to be an effective way to discover the intrinsic geometrical structure of a number of samples. In this paper, a new manifold learning algorithm, Local Coordinates Alignment (LCA), is developed based on the alignment technique. LCA first obtains the local coordinates as representations of a local neighborhood by preserving the proximity relations on the patch which is Euclidean; and then the extracted local coordinates are aligned to yield the global embeddings. To solve the out of sample problem, the linearization of LCA (LLCA) is also proposed. Empirical studies on both synthetic data and face images show the effectiveness of LCA and LLCA in comparing with existing manifold learning algorithms and linear subspace methods. This paper presents a novel algorithm for computing graph edit distance (GED) in image categorization. This algorithm is purely structural, i.e., it needs only connectivity structure of the graph and does not draw on node or edge attributes. There are two major contributions: (1) Introducing edge direction histogram (EDH) to characterize shape features of images. It is shown that GED can be employed as distance of EDHs. This algorithm is completely independent on cost function which is difficult to be defined exactly. (2) Computing distance of EDHs with earth mover distance (EMD) which takes neighborhood bins into account so as to compute distance of EDHs correctly. A set of experiments demonstrate that the newly presented algorithm is available for classifying and clustering images and is immune to the planar rotation of images. Compared with GED from spectral seriation, our algorithm can capture the structure change of graphs better and consume 12.79% time used by the former one. The average classification rate is 5% and average clustering rate is 25% higher than the spectral seriation method. In this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability $ ho in [0,0.5)$ , and the random label noise can be class-conditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate $ ho$ . We show that the rate is upper bounded by the conditional probability $P(hat{Y}|X)$ of the noisy sample. Consequently, the rate can be estimated, because the upper bound can be easily reached in classification problems. Experimental results on synthetic and real datasets confirm the efficiency of our methods. Image denoising is a fundamental problem in computer vision and image processing that holds considerable practical importance for real-world applications. The traditional patch-based and sparse coding-driven image denoising methods convert 2D image patches into 1D vectors for further processing. Thus, these methods inevitably break down the inherent 2D geometric structure of natural images. To overcome this limitation pertaining to the previous image denoising methods, we propose a 2D image denoising model, namely, the dictionary pair learning (DPL) model, and we design a corresponding algorithm called the DPL on the Grassmann-manifold (DPLG) algorithm. The DPLG algorithm first learns an initial dictionary pair (i.e., the left and right dictionaries) by employing a subspace partition technique on the Grassmann manifold, wherein the refined dictionary pair is obtained through a sub-dictionary pair merging. The DPLG obtains a sparse representation by encoding each image patch only with the selected sub-dictionary pair. The non-zero elements of the sparse representation are further smoothed by the graph Laplacian operator to remove the noise. Consequently, the DPLG algorithm not only preserves the inherent 2D geometric structure of natural images but also performs manifold smoothing in the 2D sparse coding space. We demonstrate that the DPLG algorithm also improves the structural SIMilarity values of the perceptual visual quality for denoised images using the experimental evaluations on the benchmark images and Berkeley segmentation data sets. Moreover, the DPLG also produces the competitive peak signal-to-noise ratio values from popular image denoising algorithms. In recent years, transfer learning has attracted much attention in multimedia. In this paper, we propose an efficient transfer dimensionality reduction algorithm called transfer discriminative Logmaps (TDL). TDL finds a common feature so that 1) the quadratic distance between the distribution of the training set and that of the testing set is minimized and 2) specific knowledge of the training samples can be conveniently delivered to or shared with the testing samples. Drawing on this common feature in the representation space, our objective is to develop a linear subspace in which discriminative and geometric information can be exploited. TDL adopts the margin maximization to identify discriminative information between different classes, while Logmaps is used to preserve the local-global geodesic distance as well as the direction information. Experiments carried out on both synthetic and real-word image datasets show the effectiveness of TDL for cross-domain face recognition and web image annotation. What are the primitives of visual perception? The early feature-analysis theory insists on it being a local-to-global process which has acted as the foundation of most computer vision applications for the past 30 years. The early holistic registration theory, however, considers it as a global-to-local process, of which Chen\\u2019s theory of topological perceptual organization (TPO) has been strongly supported by psychological and physiological proofs. In this paper, inspired by Chen\\u2019s theory, we propose a novel visual organization, termed computational topological perceptual organization (CTPO), which pioneers the early holistic registration in computational vision. Empirical studies on synthetic datasets prove that CTPO is invariant to global transformation such as translation, scaling, rotation and insensitive to topological deformation. We also extend it to other applications by integrating it with local features. Experiments show that our algorithm achieves competitive performance compared with some popular algorithms. Accurate gait recognition from video is a complex process involving heterogenous features, and is still being developed actively. This article introduces a novel framework, called GC2F, for effective and efficient gait recognition and classification. Adopting a \\u201drefinement-and-classification\\u201d principle, the framework comprises two components: 1) a classifier to generate advanced probabilistic features from low level gait parameters; and 2) a hidden classifier layer (based on multilayer perceptron neural network) to model the statistical properties of different subject classes. To validate our framework, we have conducted comprehensive experiments with a large test collection, and observed significant improvements in identification accuracy relative to other state-of-the-art approaches. Pedestrian tracking in multi-camera is an important task in intelligent visual surveillance system, but it suffers from the problem of large appearance variations of the same person under different cameras. Inspired by the success of existing view transformation model in multi-view gait recognition, we present a novel view transformation model based approach named shared dictionary learning with group sparsity to address the problem. It projects the pedestrian appearance feature descriptor in probe view into the gallery one before feature descriptors matching. In this case, L 1, \\u221e regularization over the latent embedding ensure the lower reconstruction error and more stable feature descriptors generation, comparing with the existing Singular Value Decomposition. Although the overall optimization function is not global convex, the Nesterovs optimal gradient scheme ensure the efficiency and reliability. Experiments on VIPeR dataset show that our approach reaches the state-of-the-art performance. Vectored data frequently occur in a variety of fields, which are easy to handle since they can be mathematically abstracted as points residing in a Euclidean space. An appropriate distance metric in the data space is quite demanding for a great number of applications. In this paper, we pose robust and tractable metric learning under pairwise constraints that are expressed as similarity judgements between data pairs. The major features of our approach include: 1) it maximizes the gap between the average squared distance among dissimilar pairs and the average squared distance among similar pairs; 2) it is capable of propagating similar constraints to all data pairs; and 3) it is easy to implement in contrast to the existing approaches using expensive optimization such as semidefinite programming. Our constrained metric learning approach has widespread applicability without being limited to particular backgrounds. Quantitative experiments are performed for classification and retrieval tasks, uncovering the effectiveness of the proposed approach. Technological advances in sensor manufacture, communication, and computing are stimulating the development of new applications that are transforming traditional vision systems into pervasive intelligent camera networks. The analysis of visual cues in multi-camera networks enables a wide range of applications, from smart home and of\\ufb01ce automation to large area surveillance and traf\\ufb01c surveillance. While dense camera networks - in which most cameras have large overlapping \\ufb01elds of view - are well studied, we are mainly concerned with sparse camera networks. A sparse camera network undertakes large area surveillance using as few cameras as possible, and most cameras have non-overlapping \\ufb01elds of view with one another. The task is challenging due to the lack of knowledge about the topological structure of the network, variations in the appearance and motion of speci\\ufb01c tracking targets in different views, and the dif\\ufb01culties of understanding composite events in the network. In this review paper, we present a comprehensive survey of recent research results to address the problems of intra-camera tracking, topological structure learning, target appearance modeling, and global activity understanding in sparse camera networks. A number of current open research issues are discussed. Conditional random fields (CRFs) are a flexible yet powerful probabilistic approach and have shown advantages for popular applications in various areas, including text analysis, bioinformatics, and computer vision. Traditional CRF models, however, are incapable of selecting relevant features as well as suppressing noise from noisy original features. Moreover, conventional optimization methods often converge slowly in solving the training procedure of CRFs, and will degrade significantly for tasks with a large number of samples and features. In this paper, we propose robust CRFs (RCRFs) to simultaneously select relevant features. An optimal gradient method (OGM) is further designed to train RCRFs efficiently. Specifically, the proposed RCRFs employ the $ell _{1}$ norm of the model parameters to regularize the objective used by traditional CRFs, therefore enabling discovery of the relevant unary features and pairwise features of CRFs. In each iteration of OGM, the gradient direction is determined jointly by the current gradient together with the historical gradients, and the Lipschitz constant is leveraged to specify the proper step size. We show that an OGM can tackle the RCRF model training very efficiently, achieving the optimal convergence rate $O(1/k^{vphantom {R^{R^{.}}}2})$ (where $k$ is the number of iterations). This convergence rate is theoretically superior to the convergence rate $O(1/k)$ of previous first-order optimization methods. Extensive experiments performed on three practical image segmentation tasks demonstrate the efficacy of OGM in training our proposed RCRFs. To measure the fuzziness of fuzzy sets, this paper introduces a distance-based and a fuzzy entropy-based measurements. Then these measurements are generalized to measure the fuzziness of fuzzy partition, namely partition fuzziness. According to the relationship between the validity of fuzzy partition and its partition fuzziness, a family of cluster validity functions is proposed based on the modified partition fuzziness. The new cluster validity functions overcome the increasing tendency of the traditional partition fuzziness with the cluster number, which provides an effective analysis methodology for fuzzy cluster validity. The experimental results with different testing data sets illustrate the effectiveness, reliability, sensitivity and applicability of the proposed cluster validity function. Modeling relationship between visual words in feature encoding is important in image classification. Recent methods consider this relationship in either image or feature space, and most of them incorporate only pairwise relationship (between visual words). However, in situations involving large variability in images, one cannot capture intrinsic invariance of intra-class images using low-order pairwise relationship. The result is not robust to larger variations in images. In addition, as the number of potential pairings grows exponentially with the number of visual words, the task of learning becomes computationally expensive. To overcome these two limitations, we propose an efficient classification framework that exploits high-order topology of visual words in the feature space, as follows. First, we propose a search algorithm that seeks dependence between the visual words. This dependence is used to construct higher order topology in the feature space. Then, the local features are encoded according to this higher order topology to improve the image classification. Experiments involving four common data sets, namely PASCAL VOC 2007, 15 Scenes, Caltech 101, and UIUC Sport Event, demonstrate that the dependence search significantly improves the efficiency of higher order topological construction, and consequently increases the image classification in all these data sets. Image quality assessment (IQA) algorithms are important for image-processing systems. And structure information plays a significant role in the development of IQA metrics. In contrast to existing structure driven IQA algorithms that measure the structure information using the normalized image or gradient amplitudes, we present a new Local Structure Divergence (LSD) index based on the local structures contained in an image. In particular, we exploit the steering kernels to describe local structures. Afterward, we estimate the quality of a given image by calculating the symmetric Kullback-Leibler divergence (SKLD) between kernels of the reference image and the distorted image. Experimental results on the LIVE database II show that LSD performs consistently with the human perception with a high confidence, and outperforms representative structure driven IQA metrics across various distortions. In many real world applications, different features (or multiview data) can be obtained and how to duly utilize them in dimension reduction is a challenge. Simply concatenating them into a long vector is not appropriate because each view has its specific statistical property and physical interpretation. In this paper, we propose a multiview stochastic neighbor embedding (m-SNE) that systematically integrates heterogeneous features into a unified representation for subsequent processing based on a probabilistic framework. Compared with conventional strategies, our approach can automatically learn a combination coefficient for each view adapted to its contribution to the data embedding. Also, our algorithm for learning the combination coefficient converges at a rate of O(1/k2), which is the optimal rate for smooth problems. Experiments on synthetic and real datasets suggest the effectiveness and robustness of m-SNE for data visualization and image retrieval. Abstract-Graph edit distance (GED) is widely applied to similarity measurement of graphs in inexact graph matching. Due to the difficulty of defining cost functions reasonably, we do research on two GED algorithms without cost function definition: the first is combining edge direction histogram (EDH) and earth mover\\u2019s distance (EMD) to estimate the GED; the second is introducing hidden Markov model (HMM) and Kullback-Leibler distance (KLD) into GED algorithm. These algorithms are evaluated theoretically and experimentally, and are compared with the GED from spectral seriation, one of the leading methods for computing GED with cost functions. Theoretical comparison shows that the proposed two cost function free GED algorithms have less complexity and characterize graph structure more effectively than spectral seriation method. Experimental results on image classification demonstrate that time occupied by the EDH-based method is 4.4% that of the spectral seriation method with the same correct classification rate, and correct classification rate of HMM-based method is 3.4% greater than that of the other two methods with 3.3% the time consumed by spectral seriation method. Clustering rate of these three methods is basically the same, but HMM-based and EDH-based methods only consume 3.17% and 5.43% the time of spectral seriation method. We present a new method for tracking human pose by employing max-margin Markov models. Representing a human body by part-based models, such as pictorial structure, the problem of pose tracking can be modeled by a discrete Markov random field. Considering max-margin Markov networks provide an efficient way to deal with both structured data and strong generalization guarantees, it is thus natural to learn the model parameters using the max-margin technique. Since tracking human pose needs to couple limbs in adjacent frames, the model will introduce loops and will be intractable for learning and inference. Previous work has resorted to pose estimation methods, which discard temporal information by parsing frames individually. Alternatively, approximate inference strategies have been used, which can overfit to statistics of a particular data set. Thus, the performance and generalization of these methods are limited. In this paper, we approximate the full model by introducing an ensemble of two tree-structured sub-models, Markov networks for spatial parsing and Markov chains for temporal parsing. Both models can be trained jointly using the max-margin technique, and an iterative parsing process is proposed to achieve the ensemble inference. We apply our model on three challengeable data sets, which contains highly varied and articulated poses. Comprehensive experimental results demonstrate the superior performance of our method over the state-of-the-art approaches. This paper comprehensively reviews the recent development of image deblurring, including nonblind/blind, spatially invariant/variant deblurring techniques. Indeed, these techniques share the same objective of inferring a latent sharp image from one or several corresponding blurry images, while the blind deblurring techniques are also required to derive an accurate blur kernel. Considering the critical role of image restoration in modern imaging systems to provide high-quality images under complex environments such as motion, undesirable lighting conditions, and imperfect system components, image deblurring has attracted growing attention in recent years. From the viewpoint of how to handle the illposedness which is a crucial issue in deblurring tasks, existing methods can be grouped into ve categories: Bayesian inference framework, variational methods, sparse representation-based methods, homographybased modeling, and region-based methods. In spite of achieving a certain level of development, image deblurring, especially the blind case, is limited in its success by complex application conditions which make the blur kernel hard to obtain and be spatially variant. We provide a holistic understanding and deep insight into image deblurring in this review. An analysis of the empirical evidence for representative methods, practical issues, as well as a discussion of promising future directions are also presented. In recent years, a great many methods of learning from multi-view data by considering the diversity of different views have been proposed. These views may be obtained from multiple sources or different feature subsets. In trying to organize and highlight similarities and differences between the variety of multi-view learning approaches, we review a number of representative multi-view learning algorithms in different areas and classify them into three groups: 1) co-training, 2) multiple kernel learning, and 3) subspace learning. Notably, co-training style algorithms train alternately to maximize the mutual agreement on two distinct views of the data; multiple kernel learning algorithms exploit kernels that naturally correspond to different views and combine kernels either linearly or non-linearly to improve learning performance; and subspace learning algorithms aim to obtain a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Though there is significant variance in the approaches to integrating multiple views to improve learning performance, they mainly exploit either the consensus principle or the complementary principle to ensure the success of multi-view learning. Since accessing multiple views is the fundament of multi-view learning, with the exception of study on learning a model from multiple views, it is also valuable to study how to construct multiple views and how to evaluate these views. Overall, by exploring the consistency and complementary properties of different views, multi-view learning is rendered more effective, more promising, and has better generalization ability than single-view learning. Canonical correlation analysis (CCA) has proven an effective tool for two-view dimension reduction due to its profound theoretical foundation and success in practical applications. In respect of multi-view learning, however, it is limited by its capability of only handling data represented by two-view features, while in many real-world applications, the number of views is frequently many more. Although the ad hoc way of simultaneously exploring all possible pairs of features can numerically deal with multi-view data, it ignores the high order statistics (correlation information) which can only be discovered by simultaneously exploring all features. Therefore, in this work, we develop tensor CCA (TCCA) which straightforwardly yet naturally generalizes CCA to handle the data of an arbitrary number of views by analyzing the covariance tensor of the different views. TCCA aims to directly maximize the canonical correlation of multiple (more than two) views. Crucially, we prove that the main problem of multi-view canonical correlation maximization is equivalent to finding the best rank- $1$ approximation of the data covariance tensor, which can be solved efficiently using the well-known alternating least squares (ALS) algorithm. As a consequence, the high order correlation information contained in the different views is explored and thus a more reliable common subspace shared by all features can be obtained. In addition, a non-linear extension of TCCA is presented. Experiments on various challenge tasks, including large scale biometric structure prediction, internet advertisement classification, and web image annotation, demonstrate the effectiveness of the proposed method Compressed sensing (CS) and 1-bit CS cannot directly recover quantized signals and require time consuming recovery. In this paper, we introduce \\textit{Hamming compressed sensing} (HCS) that directly recovers a k-bit quantized signal of dimensional $n$ from its 1-bit measurements via invoking $n$ times of Kullback-Leibler divergence based nearest neighbor search. Compared with CS and 1-bit CS, HCS allows the signal to be dense, takes considerably less (linear) recovery time and requires substantially less measurements ($mathcal O(log n)$). Moreover, HCS recovery can accelerate the subsequent 1-bit CS dequantizer. We study a quantized recovery error bound of HCS for general signals and \\\"HCS+dequantizer\\\" recovery error bound for sparse signals. Extensive numerical simulations verify the appealing accuracy, robustness, efficiency and consistency of HCS. Labeling of sequential data is a prevalent meta-problem for a wide range of real world applications. While the first-order Hidden Markov Models (HMM) provides a fundamental approach for unsupervised sequential labeling, the basic model does not show satisfying performance when it is directly applied to real world problems, such as part-of-speech tagging (PoS tagging) and optical character recognition (OCR). Aiming at improving performance, important extensions of HMM have been proposed in the literatures. One of the common key features in these extensions is the incorporation of proper prior information. In this paper, we propose a new extension of HMM, termed diversified Hidden Markov Models (dHMM), which utilizes a diversity-encouraging prior over the state-transition probabilities and thus facilitates more dynamic sequential labellings. Specifically, the diversity is modeled by a continuous determinantal point process prior, which we apply to both unsupervised and supervised scenarios. Learning and inference algorithms for dHMM are derived. Empirical evaluations on benchmark datasets for unsupervised PoS tagging and supervised OCR confirmed the effectiveness of dHMM, with competitive performance to the state-of-the-art. Recent years have witnessed the explosive growth of online social networks (OSNs). They provide powerful IT-innovations for online social activities such as organizing contacts, publishing content, and sharing interests between friends who may never meet before. As more and more people become active users of OSNs, one may ponder questions such as (1) Do OSNs indeed improve our sociability? (2) To what extent can we expand our offline social spectrum in OSNs? (3) Can we identify some interesting user behaviors in OSNs? Our work in this paper attempts to answer these interesting questions. First, we systematically validate the existence of a new Dunbar@?s number in OSNs, which is ranging from 200 to 300 empirically. To reach this, we conduct local-structure analysis as well as user-interaction analysis on extensive real-world OSNs. Second, based on this new number, we divide OSN users into two categories: the rational and the aggressive, and find that rational users intend to develop close and reciprocated relationship, whereas aggressive users have no consistent behaviors. Third, we propose a simple model to highlight the constraints of time and cognition that may affect the evolution of OSNs heavily. Finally, we discuss the potential use of our findings for viral marketing and privacy management in OSNs. The Vapnik-Chervonenkis (VC) dimension plays an important role in statistical learning theory. In this paper, we propose the discretized VC dimension obtained by discretizing the range of a real function class. Then, we point out that Sauer's Lemma is valid for the discretized VC dimension. We group the real function classes having the in\\u00afnite VC dimension into four categories by using the discretized VC dimension. As a byproduct, we present the equidistantly discretized VC dimension by introducing an equidistant partition to segmenting the range of a real function class. Finally, we obtain the error bounds for real function classes based on the discretized VC dimensions in the PAC-learning framework. Granger causal analysis has been an important tool for causal analysis for time series in various fields, including neuroscience and economics, and recently it has been extended to include instantaneous effects between the time series to explain the contemporaneous dependence in the residuals. In this paper, we assume that the time series at the true causal frequency follow the vector autoregressive model. We show that when the data resolution becomes lower due to subsampling, neither the original Granger causal analysis nor the extended one is able to discover the underlying causal relations. We then aim to answer the following question: can we estimate the temporal causal relations at the right causal frequency from the subsampled data? Traditionally this suffers from the identifiability problems: under the Gaussianity assumption of the data, the solutions are generally not unique. We prove that, however, if the noise terms are non-Gaussian, the underlying model for the highfrequency data is identifiable from subsampled data under mild conditions. We then propose an Expectation-Maximization (EM) approach and a variational inference approach to recover temporal causal relations from such subsampled data. Experimental results on both simulated and real data are reported to illustrate the performance of the proposed approaches. We analyze the local Rademacher complexity of empirical risk minimization-based multi-label learning algorithms, and in doing so propose a new algorithm for multi-label learning. Rather than using the trace norm to regularize the multi-label predictor, we instead minimize the tail sum of the singular values of the predictor in multi-label learning. Benefiting from the use of the local Rademacher complexity, our algorithm, therefore, has a sharper generalization error bound. Compared with methods that minimize over all singular values, concentrating on the tail singular values results in better recovery of the low-rank structure of the multi-label predictor, which plays an important role in exploiting label correlations. We propose a new conditional singular value thresholding algorithm to solve the resulting objective function. Moreover, a variance control strategy is employed to reduce the variance of variables in optimization. Empirical studies on real-world data sets validate our theoretical results and demonstrate the effectiveness of the proposed algorithm for multi-label learning. This paper studies the active learning problem in crowd-sourcing settings, where multiple imperfect annotators with varying levels of expertise are available for labeling the data in a given task. Annotations collected from these labelers may be noisy and unreliable, and the quality of labeled data needs to be maintained for data mining tasks. Previous solutions have attempted to estimate individual users' reliability based on existing knowledge in each task, but for this to be effective each task requires a large quantity of labeled data to provide accurate estimates. In practice, annotation budgets for a given task are limited, so each instance can be presented to only a few users, each of whom can only label a few examples. To overcome data scarcity we propose a new probabilistic model that transfers knowledge from abundant unlabeled data in auxiliary domains to help estimate labelers' expertise. Based on this model we present a novel active learning algorithm that: a) simultaneously selects the most informative example and b) queries its label from the labeler with the best expertise. Experiments on both text and image datasets demonstrate that our proposed method outperforms other state-of-the-art active learning methods. Upon extensive studies, text clustering remains a critical challenge in data mining community. Even by various techniques proposed to overcome some of these challenges, there still exist problems when dealing with weakly related or even noisy features. In response to this, we propose a DIssembleASsemble (DIAS) framework for text clustering. DIAS employs simple random feature sampling to disassemble highdimensional text data and gains diverse structural knowledge. This also does good to avoiding the bulk of noisy features. Then the multi-view knowledge is assembled by weighted Information-theoretic Consensus Clustering (ICC) in order to gain a high-quality consensus partitioning. Extensive experiments on eight real-world text data sets demonstrate the advantages of DIAS over other widely used methods. In particular, DIAS shows strengths in learning from very weak basic partitionings. In addition, it is the natural suitability to distributed computing that makes DIAS become a promising candidate for big text clustering. Most object tracking methods only exploit a single quantization of an image space: pixels, superpixels, or bounding boxes, each of which has advantages and disadvantages. It is highly unlikely that a common optimal quantization level, suitable for tracking all objects in all environments, exists. We therefore propose a hierarchical appearance representation model for tracking, based on a graphical model that exploits shared information across multiple quantization levels. The tracker aims to find the most possible position of the target by jointly classifying the pixels and superpixels and obtaining the best configuration across all levels. The motion of the bounding box is taken into consideration, while Online Random Forests are used to provide pixel- and superpixel-level quantizations and progressively updated on-the-fly. By appropriately considering the multilevel quantizations, our tracker exhibits not only excellent performance in non-rigid object deformation handling, but also its robustness to occlusions. A quantitative evaluation is conducted on two benchmark datasets: a non-rigid object tracking dataset (11 sequences) and the CVPR2013 tracking benchmark (50 sequences). Experimental results show that our tracker overcomes various tracking challenges and is superior to a number of other popular tracking methods. Image-based 3-D human pose recovery is usually conducted by retrieving relevant poses with image features. However, it suffers from the high dimensionality of image features and the low efficiency of the retrieving process. Particularly for multiview data, the integration of different types of features is difficult. In this paper, a novel approach is proposed to recover 3-D human poses from silhouettes. This approach improves traditional methods by adopting multiview locality-sensitive sparse coding in the retrieving process. First, it incorporates a local similarity preserving term into the objective of sparse coding, which groups similar silhouettes to alleviate the instability of sparse codes. Second, the objective function of sparse coding is improved by integrating multiview data. The experimental results show that the retrieval error has been reduced by 20% to 50%, which demonstrate the effectiveness of the proposed method. Query difficulty estimation is a useful tool for content-based image retrieval. It predicts the performance of the search result of a given query, and thus it can guide the pseudo relevance feedback to rerank the image search results, and can be used to re-write the given query by suggesting \\\"easy\\\" alternatives. This paper presents a query difficulty estimation guided image retrieval system. The system initially estimates the difficulty of a given query image by analyzing both the query image and the retrieved top ranked images. Different search strategies are correspondingly applied to improve the retrieval performance. In multi-label learning, each sample is associated with several labels. Existing works indicate that exploring correlations between labels improve the prediction performance. However, embedding the label correlations into the train- ing process significantly increases the problem size. Moreo ver, the mapping of the label structure in the feature space is not clear. In this paper, we propose a novel multi-label learning method \\\"Structured Decomposition + Group Sparsity (SDGS)\\\". In SDGS, we learn a feature subspace for each label from the structured decomposition of the training data, and predict the labels of a new sample from its group sparse representation on the multi-subspace obtained from the struc- tured decomposition. In particular, in the training stage, we decompose the data matrix X2 R np as X = P k=1 L i +S, wherein the rows of L i associated with samples that belong to label i are nonzero and consist a low-rank matrix, while the other rows are all-zeros, the residual S is a sparse matrix. The row space of Li is the feature subspace corresponding to label i. This decomposition can be efficiently obtained via randomized optimization. In the pr ediction stage, we es- timate the group sparse representation of a new sample on the multi-subspace via group lasso. The nonzero representation coefficients tend to concentra te on the subspaces of labels that the sample belongs to, and thus an effective prediction can be obtained. We evaluate SDGS on several real datasets and compare it with popular methods. Results verify the effectiveness and effic iency of SDGS. This paper represents a new level set method for multiregional image segmentation. It employs the Gabor filter bank to extract local geometrical features and builds the pixel tensor representation whose dimensionality is reduced by using the offline tensor analysis. Then multiphase level set functions are evolved in the tensor field to detect the boundaries of the corresponding image. The proposed method has three main advantages as follows. Firstly, employing the Gabor filter bank, the model is more robust against the salt-and-pepper noise. Secondly, the pixel tensor representation comprehensively depicts the information of pixels, which results in a better performance on the non-homogenous image segmentation. Thirdly, the model provides a uniform equation for multiphase level set functions to make it more practical. We apply the proposed method to synthetic and medical images respectively, and the results indicate that the proposed method is superior to the typical region-based level set method. Low rank representation (LRR) has shown promising performance for various computer vision applications such as face clustering. Existing algorithms for solving LRR usually depend on its two-variable formulation which contains the original data matrix. In this paper, we develop a fast LRR solver called FaLRR, by reformulating LRR as a new optimization problem with regard to factorized data (which is obtained by skinny SVD of the original data matrix). The new formulation benefits the corresponding optimization and theoretical analysis. Specifically, to solve the resultant optimization problem, we propose a new algorithm which is not only efficient but also theoretically guaranteed to obtain a globally optimal solution. Regarding the theoretical analysis, the new formulation is helpful for deriving some interesting properties of LRR. Last but not least, the proposed algorithm can be readily incorporated into an existing distributed framework of LRR for further acceleration. Extensive experiments on synthetic and real-world datasets demonstrate that our FaLRR achieves order-of-magnitude speedup over existing LRR solvers, and the efficiency can be further improved by incorporating our algorithm into the distributed framework of LRR. Slow feature analysis (SFA) extracts slowly varying signals from input data and has been used to model complex cells in the primary visual cortex (V1). It transmits information to both ventral and dorsal pathways to process appearance and motion information, respectively. However, SFA only uses slowly varying features for local feature extraction, because they represent appearance information more effectively than motion information. To better utilize temporal information, we propose temporal variance analysis (TVA) as a generalization of SFA. TVA learns a linear transformation matrix that projects multidimensional temporal data to temporal components with temporal variance. Inspired by the function of V1, we learn receptive fields by TVA and apply convolution and pooling to extract local features. Embedded in the improved dense trajectory framework, TVA for action recognition is proposed to: 1) extract appearance and motion features from gray using slow and fast filters, respectively; 2) extract additional motion features using slow filters from horizontal and vertical optical flows; and 3) separately encode extracted local features with different temporal variances and concatenate all the encoded features as final features. We evaluate the proposed TVA features on several challenging data sets and show that both slow and fast features are useful in the low-level feature extraction. Experimental results show that the proposed TVA features outperform the conventional histogram-based features, and excellent results can be achieved by combining all TVA features. To perform unconstrained face recognition robust to variations in illumination, pose and expression, this paper presents a new scheme to extract \\u201cMulti-Directional Multi-Level Dual-Cross Patterns\\u201d (MDML-DCPs) from face images. Specifically, the MDML-DCPs scheme exploits the first derivative of Gaussian operator to reduce the impact of differences in illumination and then computes the DCP feature at both the holistic and component levels. DCP is a novel face image descriptor inspired by the unique textural structure of human faces. It is computationally efficient and only doubles the cost of computing local binary patterns, yet is extremely robust to pose and expression variations. MDML-DCPs comprehensively yet efficiently encodes the invariant characteristics of a face image from multiple levels into patterns that are highly discriminative of inter-personal differences but robust to intra-personal variations. Experimental results on the FERET, CAS-PERL-R1, FRGC 2.0, and LFW databases indicate that DCP outperforms the state-of-the-art local descriptors (e.g., LBP, LTP, LPQ, POEM, tLBP, and LGXP) for both face identification and face verification tasks. More impressively, the best performance is achieved on the challenging LFW and FRGC 2.0 databases by deploying MDML-DCPs in a simple recognition scheme. Saliency propagation has been widely adopted for identifying the most attractive object in an image. The propagation sequence generated by existing saliency detection methods is governed by the spatial relationships of image regions, i.e., the saliency value is transmitted between two adjacent regions. However, for the inhomogeneous difficult adjacent regions, such a sequence may incur wrong propagations. In this paper, we attempt to manipulate the propagation sequence for optimizing the propagation quality. Intuitively, we postpone the propagations to difficult regions and meanwhile advance the propagations to less ambiguous simple regions. Inspired by the theoretical results in educational psychology, a novel propagation algorithm employing the teaching-to-learn and learning-to-teach strategies is proposed to explicitly improve the propagation quality. In the teaching-to-learn step, a teacher is designed to arrange the regions from simple to difficult and then assign the simplest regions to the learner. In the learning-to-teach step, the learner delivers its learning confidence to the teacher to assist the teacher to choose the subsequent simple regions. Due to the interactions between the teacher and learner, the uncertainty of original difficult regions is gradually reduced, yielding manifest salient objects with optimized background suppression. Extensive experimental results on benchmark saliency datasets demonstrate the superiority of the proposed algorithm over twelve representative saliency detectors. Variations in the appearance of a tracked object, such as changes in geometry/photometry, camera viewpoint, illumination, or partial occlusion, pose a major challenge to object tracking. Here, we adopt cognitive psychology principles to design a flexible representation that can adapt to changes in object appearance during tracking. Inspired by the well-known Atkinson-Shiffrin Memory Model, we propose MUlti-Store Tracker (MUSTer), a dual-component approach consisting of short- and long-term memory stores to process target appearance memories. A powerful and efficient Integrated Correlation Filter (ICF) is employed in the short-term store for short-term tracking. The integrated long-term component, which is based on keypoint matching-tracking and RANSAC estimation, can interact with the long-term memory and provide additional information for output control. MUSTer was extensively evaluated on the CVPR2013 Online Object Tracking Benchmark (OOTB) and ALOV++ datasets. The experimental results demonstrated the superior performance of MUSTer in comparison with other state-of-art trackers. Video-based human pose recovery is usually conducted by retrieving relevant poses using image features. In the retrieving process, the mapping between 2D images and 3D poses is assumed to be linear in most of the traditional methods. However, their relationships are inherently non-linear, which limits recovery performance of these methods. In this paper, we propose a novel pose recovery method using non-linear mapping with multi-layered deep neural network. It is based on feature extraction with multimodal fusion and back-propagation deep learning. In multimodal fusion, we construct hypergraph Laplacian with low-rank representation. In this way, we obtain a unified feature description by standard eigen-decomposition of the hypergraph Laplacian matrix. In back-propagation deep learning, we learn a non-linear mapping from 2D images to 3D poses with parameter fine-tuning. The experimental results on three data sets show that the recovery error has been reduced by 20%\\u201325%, which demonstrates the effectiveness of the proposed method. One underlying assumption of the conventional multi-view learning algorithms is that all examples can be successfully observed on all the views. However, due to various failures or faults in collecting and pre-processing the data on different views, we are more likely to be faced with an incomplete-view setting, where an example could be missing its representation on one view (i.e., missing view) or could be only partially observed on that view (i.e., missing variables). Low-rank assumption used to be effective for recovering the random missing variables of features, but it is disabled by concentrated missing variables and has no effect on missing views. This paper suggests that the key to handling the incomplete-view problem is to exploit the connections between multiple views, enabling the incomplete views to be restored with the help of the complete views. We propose an effective algorithm to accomplish multi-view learning with incomplete views by assuming that different views are generated from a shared subspace. To handle the large-scale problem and obtain fast convergence, we investigate a successive over-relaxation method to solve the objective function. Convergence of the optimization technique is theoretically analyzed. The experimental results on toy data and real-world data sets suggest that studying the incomplete-view problem in multi-view learning is significant and that the proposed algorithm can effectively handle the incomplete views in different applications. In this paper, we propose a new approach to overcome the representation and matching problems in age invariant face recognition. First, a new maximum entropy feature descriptor (MEFD) is developed that encodes the microstructure of facial images into a set of discrete codes in terms of maximum entropy. By densely sampling the encoded face image, sufficient discriminatory and expressive information can be extracted for further analysis. A new matching method is also developed, called identity factor analysis (IFA), to estimate the probability that two faces have the same underlying identity. The effectiveness of the framework is confirmed by extensive experimentation on two face aging datasets, MORPH (the largest public-domain face aging dataset) and FGNET. We also conduct experiments on the famous LFW dataset to demonstrate the excellent generalizability of our new approach. Over these years, Correlation Filter-based Trackers (CFTs) have aroused increasing interests in the field of visual object tracking, and have achieved extremely compelling results in different competitions and benchmarks. In this paper, our goal is to review the developments of CFTs with extensive experimental results. 11 trackers are surveyed in our work, based on which a general framework is summarized. Furthermore, we investigate different training schemes for correlation filters, and also discuss various effective improvements that have been made recently. Comprehensive experiments have been conducted to evaluate the effectiveness and efficiency of the surveyed CFTs, and comparisons have been made with other competing trackers. The experimental results have shown that state-of-art performance, in terms of robustness, speed and accuracy, can be achieved by several recent CFTs, such as MUSTer and SAMF. We find that further improvements for correlation filter-based tracking can be made on estimating scales, applying part-based tracking strategy and cooperating with long-term tracking methods. Face images appearing in multimedia applications, e.g., social networks and digital entertainment, usually exhibit dramatic pose, illumination, and expression variations, resulting in considerable performance degradation for traditional face recognition algorithms. This paper proposes a comprehensive deep learning framework to jointly learn face representation using multimodal information. The proposed deep learning structure is composed of a set of elaborately designed convolutional neural networks (CNNs) and a three-layer stacked auto-encoder (SAE). The set of CNNs extracts complementary facial features from multimodal data. Then, the extracted features are concatenated to form a high-dimensional feature vector, whose dimension is compressed by SAE. All of the CNNs are trained using a subset of 9,000 subjects from the publicly available CASIA-WebFace database, which ensures the reproducibility of this work. Using the proposed single CNN architecture and limited training data, 98.43% verification rate is achieved on the LFW database. Benefitting from the complementary information contained in multimodal data, our small ensemble system achieves higher than 99.0% recognition rate on LFW using publicly available training set. Low-rank structure have been profoundly studied in data mining and machine learning. In this paper, we show a dense matrix X's low-rank approximation can be rapidly built from its left and right random projections Y 1 = XA 1 and Y 2 = XT A 2 , or bilateral random projection (BRP). We then show power scheme can further improve the precision. The deterministic, average and deviation bounds of the proposed method and its power scheme modification are proved theoretically. The effectiveness and the efficiency of BRP based low-rank approximation is empirically verified on both artificial and real datasets. This paper proposes Tensor Rank One Discriminant Analysis (TR1DA) in which general tensors are input for pattern classification. TR1DA is based on Differential Scatter Discriminant Criterion (DSDC) and Tensor Rank One Analysis (TR1A). DSDC is a generalization of the Fisher discriminant criterion. It ensures convergence during training stage. TR1A is a method for adapting general tensors as input to DSDC. The benefits of TR1DA include: (1) a natural way of representing data without losing structure information, i.e., the information about the relative positions of pixels or regions; (2) a reduction in the small sample size problem which occurs in conventional discriminant learning because the number of training samples is much less than the dimensionality of the feature space; (3) a better convergence during the training procedure. We use a graph-embedding framework to generalize TR1DA in manifold learning-based feature selection algorithms, such as locally linear embedding, ISOMAP, and the Laplace eigenmap. We also kernelize TR1DA to nonlinear problems. TR1DA is then demonstrated to outperform traditional subspace methods, such as principal component analysis and linear discriminant analysis. Statistical modeling of wavelet subbands has frequently been used for image recognition and retrieval. However, traditional wavelets are unsuitable for use with images containing distributed discontinuities, such as edges. Shearlets are a newly developed extension of wavelets that are better suited to image characterization. Here, we propose novel texture classification and retrieval methods that model adjacent shearlet subband dependences using linear regression. For texture classification, we use two energy features to represent each shearlet subband in order to overcome the limitation that subband coefficients are complex numbers. Linear regression is used to model the features of adjacent subbands; the regression residuals are then used to define the distance from a test texture to a texture class. Texture retrieval consists of two processes: the first is based on statistics in contourlet domains, while the second is performed using a pseudo-feedback mechanism based on linear regression modeling of shearlet subband dependences. Comprehensive validation experiments performed on five large texture datasets reveal that the proposed classification and retrieval methods outperform the current state-of-the-art. Hyperspectral unmixing is a hot topic in signal and image processing. A high-dimensional data can be decomposed into two non-negative low-dimensional matrices by Non-negative matrix factorization(NMF). However, the algorithm has many local solutions because of the non-convexity of the objective function. Some algorithms solve this problem by adding auxiliary constraints, such as sparse. The sparse NMF has good performance but the result is unstable and sensitive to noise. Using the structural information for the unmixing approaches can make the decomposition stable. Someone used a clustering based on Euclidean distance to guide the decomposition and obtain good performance. The Euclidean distance is just used to measure the straight line distance of two points, and the ground objects usually obey certain statistical distribution. It's difficult to measure the difference between the statistical distributions comprehensively by Euclidean distance. KL divergence is a better metric. In this paper, we propose a new approach named KL divergence constrained NMF which measures the statistical distribution difference using KL divergence instead of the Euclidean distance. It can improve the accuracy of structured information by using the KL divergence in the algorithm. Experimental results based on synthetic and real hyperspectral data show the superiority of the proposed algorithm with respect to other state-of-the-art algorithms. Face recognition is one of the biometric identification methods with the highest potential. The existing face recognition algorithms relying on the texture information of face images are affected greatly by the variation of expression, scale and illumination. Whereas the algorithms based on the shape topology weaken the influence of illumination to some extent, but the impact of expression, scale and illumination on face recognition is still unsolved. To this end, we propose a new method for face recognition by integrating texture information with shape information, called biview face recognition algorithm. The texture models are constructed by using subspace learning methods and shape topologies are formed by building graphs for face images. The proposed biview face recognition method is compared with recognition algorithms merely based on texture or shape information. Experimental results of recognizing faces under the variation of illumination, expression and scale demonstrate that the performance of the proposed biview face recognition outperforms texture-based and shape-based algorithms. This paper proposes a new image quality assessment framework which is based on color perceptual model. By analyzing the shortages of the existing image quality assessment methods and combining the color perceptual model, the general framework of color image quality assessment based on the S-CIELAB color space is presented. The S-CIELAB color model, a spatial extension of CIELAB, has an excellent performance for mimicking the perceptual processing of human color vision. This paper incorporates excellent color perceptual characteristics model with the geometrical distortion measurement to assess the image quality. First, the reference and distorted images are transformed into S-CIELAB color perceptual space, and the transformed images are evaluated by existing metric in three color perceptual channels. The fidelity factors of three channels are weighted to obtain the image quality. Experimental results achieved on LIVE database II shows that the proposed methods are in good consistency with human subjective assessment results. The rapid development of computer hardware and Internet technology makes large scale data dependent models computationally tractable, and opens a bright avenue for annotating images through innovative machine learning algorithms. Semisupervised learning (SSL) therefore received intensive attention in recent years and was successfully deployed in image annotation. One representative work in SSL is Laplacian regularization (LR), which smoothes the conditional distribution for classification along the manifold encoded in the graph Laplacian, however, it is observed that LR biases the classification function toward a constant function that possibly results in poor generalization. In addition, LR is developed to handle uniformly distributed data (or single-view data), although instances or objects, such as images and videos, are usually represented by multiview features, such as color, shape, and texture. In this paper, we present multiview Hessian regularization (mHR) to address the above two problems in LR-based image annotation. In particular, mHR optimally combines multiple HR, each of which is obtained from a particular view of instances, and steers the classification function that varies linearly along the data manifold. We apply mHR to kernel least squares and support vector machines as two examples for image annotation. Extensive experiments on the PASCAL VOC'07 dataset validate the effectiveness of mHR by comparing it with baseline algorithms, including LR and HR. Image reranking is effective for improving the performance of a text-based image search. However, existing reranking algorithms are limited for two main reasons: 1) the textual meta-data associated with images is often mismatched with their actual visual content and 2) the extracted visual features do not accurately describe the semantic similarities between images. Recently, user click information has been used in image reranking, because clicks have been shown to more accurately describe the relevance of retrieved images to search queries. However, a critical problem for click-based methods is the lack of click data, since only a small number of web images have actually been clicked on by users. Therefore, we aim to solve this problem by predicting image clicks. We propose a multimodal hypergraph learning-based sparse coding method for image click prediction, and apply the obtained click data to the reranking of images. We adopt a hypergraph to build a group of manifolds, which explore the complementarity of different features through a group of weights. Unlike a graph that has an edge between two vertices, a hyperedge in a hypergraph connects a set of vertices, and helps preserve the local smoothness of the constructed sparse codes. An alternating optimization procedure is then performed, and the weights of different modalities and the sparse codes are simultaneously obtained. Finally, a voting strategy is used to describe the predicted click as a binary event (click or no click), from the images' corresponding sparse codes. Thorough empirical studies on a large-scale database including nearly 330 K images demonstrate the effectiveness of our approach for click prediction when compared with several other methods. Additional image reranking experiments on real-world data show the use of click prediction is beneficial to improving the performance of prominent graph-based image reranking algorithms. In many real applications of machine learning and data mining, we are often confronted with high-dimensional data. How to cluster high-dimensional data is still a challenging problem due to the curse of dimensionality. In this paper, we try to address this problem using joint dimensionality reduction and clustering. Different from traditional approaches that conduct dimensionality reduction and clustering in sequence, we propose a novel framework referred to as discriminative embedded clustering which alternates them iteratively. Within this framework, we are able not only to view several traditional approaches and reveal their intrinsic relationships, but also to be stimulated to develop a new method. We also propose an effective approach for solving the formulated nonconvex optimization problem. Comprehensive analyses, including convergence behavior, parameter determination, and computational complexity, together with the relationship to other related approaches, are also presented. Plenty of experimental results on benchmark data sets illustrate that the proposed method outperforms related state-of-the-art clustering approaches and existing joint dimensionality reduction and clustering methods. Signed networks, in which the relationship between two nodes can be either positive (indicating a relationship such as trust) or negative (indicating a relationship such as distrust), are becoming increasingly common. A plausible model for user behavior analytics in signed networks can be based upon the assumption that more extreme positive and negative relationships are explored and exploited before less extreme ones. Such a model implies that a personalized ranking list of latent links should place positive links on the top, negative links at the bottom, and unknown status links in between. Traditional ranking metrics, e.g., area under the receiver operating characteristic curve (AUC), are however not suitable for quantifying such a ranking list which includes positive, negative, and unknown status links. To address this issue, a generalized AUC (GAUC) which can measure both the head and tail of a ranking list has been introduced. Since GAUC weights each pairwise comparison equally and the calculation of GAUC requires quadratic time, we derive two lower bounds of GAUC which can be computed in linear time and put more emphasis on ranking positive links on the top and negative links at the bottom of a ranking list. Next, we develop two efficient latent link recommendation (ELLR) algorithms in order to recommend links by directly optimizing these two lower bounds, respectively. Finally, we compare these two ELLR algorithms with top-performing baseline methods over four benchmark datasets, among which the largest network has more than 100 thousand nodes and seven million entries. Thorough empirical studies demonstrate that the proposed ELLR algorithms outperform state-of-the-art approaches for link recommendation in signed networks at no cost in efficiency. Because of malfunction or noise in 15 out of the 20 detectors, band 6 (1.628-1.652 \\u03bcm) of the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor aboard the Aqua satellite contains large areas of dead pixel stripes. Therefore, the corresponding high-level products of MODIS are corrupted by this periodic phenomenon. This paper proposes an improved Bayesian dictionary learning algorithm based on the burgeoning compressed sensing theory to solve this problem. Compared with other state-of-the-art methods, the proposed method can adaptively exploit the spectral relations of band 6 and other spectra. The performance of the proposed method is demonstrated by experiments on both simulated Terra and real Aqua images. Sparse coding represents a signal sparsely by using an overcomplete dictionary, and obtains promising performance in practical computer vision applications, especially for signal restoration tasks such as image denoising and image inpainting. In recent years, many discriminative sparse coding algorithms have been developed for classification problems, but they cannot naturally handle visual data represented by multiview features. In addition, existing sparse coding algorithms use graph Laplacian to model the local geometry of the data distribution. It has been identified that Laplacian regularization biases the solution towards a constant function which possibly leads to poor extrapolating power. In this paper, we present multiview Hessian discriminative sparse coding (mHDSC) which seamlessly integrates Hessian regularization with discriminative sparse coding for multiview learning problems. In particular, mHDSC exploits Hessian regularization to steer the solution which varies smoothly along geodesics in the manifold, and treats the label information as an additional view of feature for incorporating the discriminative power for image annotation. We conduct extensive experiments on PASCAL VOC'07 dataset and demonstrate the effectiveness of mHDSC for image annotation. For face recognition, we show that knowing that each subject corresponds to multiple face images can improve classification performance. For domains such as video surveillance, it is easy to deduce which group of images belong to the same subject; in domains such as family album identification, we lose group membership information but there is still a group of images for each subject. We define these two types of problems as multiple faces per subject. In this paper, we propose a Bipart framework to take advantage of this group information in the testing set as well as in the training set. From these two sources of information, two models are learned independently and combined to form a unified discriminative distance space. Furthermore, this framework is generalized to allow both subspace learning and distance metric learning methods to take advantage of this group information. Bipart is evaluated on the multiple faces per subject problem using several benchmark datasets, including video and static image data, subjects of various ages, various lighting conditions, and many facial expressions. Comparisons against state-of-the-art distance and subspace learning methods demonstrate much better performance when utilizing group information with the Bipart framework. Abstract Whereas the transform coding algorithms have been proved to be efficient and practical for grey-level and color images compression, they could not directly deal with the hyperspectral images (HSI) by simultaneously considering both the spatial and spectral domains of the data cube. The aim of this paper is to present an HSI compression and reconstruction method based on the multi-dimensional or tensor data processing approach. By representing the observed hyperspectral image cube to a 3-order-tensor, we introduce a tensor decomposition technology to approximately decompose the original tensor data into a core tensor multiplied by a factor matrix along each mode. Thus, the HSI is compressed to the core tensor and could be reconstructed by the multi-linear projection via the factor matrices. Experimental results on particular applications of hyperspectral remote sensing images such as unmixing and detection suggest that the reconstructed data by the proposed approach significantly preserves the HSI\\u05f3s data quality in several aspects. Lossless data hiding methods usually fail to recover the hidden messages completely when the watermarked images are attacked. Therefore, the robust lossless data hiding (RLDH), or the robust reversible watermarking technique, is urgently needed to effectively improve the recovery performance. To date a couple of methods have been developed; however, they have such drawbacks as poor visual quality and low capacity. To solve this problem, we develop a novel statistical quantity histogram shifting and clustering-based RLDH method or SQH-SC for short. The benefits of SQH-SC in comparison with existing typical methods include: (1) strong robustness against lossy compression and random noise due to the usage of k-means clustering; (2) good imperceptibility and reasonable performance tradeoff due to the consideration of the just noticeable distortion of images; (3) high capacity due to the flexible adjustment of the threshold; and (4) wide adaptability and good stability to different kinds of images. Extensive experimental studies based on natural images, medical images, and synthetic aperture radar (SAR) images demonstrate the effectiveness of the proposed SQH-SC. Slow Feature Analysis (SFA) extracts slowly varying features from a quickly varying input signal [1]. It has been successfully applied to modeling the visual receptive fields of the cortical neurons. Sufficient experimental results in neuroscience suggest that the temporal slowness principle is a general learning principle in visual perception. In this paper, we introduce the SFA framework to the problem of human action recognition by incorporating the discriminative information with SFA learning and considering the spatial relationship of body parts. In particular, we consider four kinds of SFA learning strategies, including the original unsupervised SFA (U-SFA), the supervised SFA (S-SFA), the discriminative SFA (D-SFA), and the spatial discriminative SFA (SD--SFA), to extract slow feature functions from a large amount of training cuboids which are obtained by random sampling in motion boundaries. Afterward, to represent action sequences, the squared first order temporal derivatives are accumulated over all transformed cuboids into one feature vector, which is termed the Accumulated Squared Derivative (ASD) feature. The ASD feature encodes the statistical distribution of slow features in an action sequence. Finally, a linear support vector machine (SVM) is trained to classify actions represented by ASD features. We conduct extensive experiments, including two sets of control experiments, two sets of large scale experiments on the KTH and Weizmann databases, and two sets of experiments on the CASIA and UT-interaction databases, to demonstrate the effectiveness of SFA for human action recognition. Experimental results suggest that the SFA-based approach (1) is able to extract useful motion patterns and improves the recognition performance, (2) requires less intermediate processing steps but achieves comparable or even better performance, and (3) has good potential to recognize complex multiperson activities. In this paper, we kernelize conventional clustering algorithms from a novel point of view. Based on the fully mathematical proof, we first demonstrate that kernel KMeans (KKMeans) is equivalent to kernel principal component analysis (KPCA) prior to the conventional KMeans algorithm. By using KPCA as a preprocessing step, we also generalize Gaussian mixture model (GMM) to its kernel version, the kernel GMM (KGMM). Consequently, conventional clustering algorithms can be easily kernelized in the linear feature space instead of a nonlinear one. To evaluate the newly established KKMeans and KGMM algorithms, we utilized them to the problem of semantic object extraction (segmentation) of color images. Based on a series of experiments carried out on a set of color images, we indicate that both KKMeans and KGMM can offer more elaborate output than the conventional KMeans and GMM, respectively. The target of no-reference (NR) image quality assessment (IQA) is to establish a computational model to predict the visual quality of an image. The existing prominent method is based on natural scene statistics (NSS). It uses the joint and marginal distributions of wavelet coefficients for IQA. However, this method is only applicable to JPEG2000 compressed images. Since the wavelet transform fails to capture the directional information of images, an improved NSS model is established by contourlets. In this paper, the contourlet transform is utilized to NSS of images, and then the relationship of contourlet coefficients is represented by the joint distribution. The statistics of contourlet coefficients are applicable to indicate variation of image quality. In addition, an image-dependent threshold is adopted to reduce the effect of content to the statistical model. Finally, image quality can be evaluated by combining the extracted features in each subband nonlinearly. Our algorithm is trained and tested on the LIVE database II. Experimental results demonstrate that the proposed algorithm is superior to the conventional NSS model and can be applied to different distortions. Cartoon character retrieval is critical for cartoonists to effectively and efficiently make cartoons by reusing existing cartoon data. To successfully achieve these tasks, it is essential to extract visual features to comprehensively represent cartoon characters and accurately estimate dissimilarity between cartoon characters. In this paper, we define three visual features: Hausdorff contour feature (HCF), color histogram (CH) and motion feature (MF), to characterize the shape, color and motion structure information of a cartoon character. The HCF can be referred as intra-features, and the features of CH and MF can be regarded as inter-feature. However, due to the semantic gap, the cartoon retrieval by using these visual features still cannot achieve excellent performance. Since the labeling information has been proven effective to reduce the semantic gap, we introduce a labeling procedure called interactive cartoon labeling (ICL). The labeling information actually reflects user's retrieval purpose. A new dimension reduction tool, termed sparse transfer learning (SPA-TL), is adopted to effectively and efficiently encode user's search intention. In particular, SPA-TL exploits two pieces of knowledge data, i.e., the labeling knowledge contained in labeled data and the data distribution knowledge contained in all samples (labeled and unlabeled). The low-dimensional subspace is obtained by transferring the user feedback knowledge from labeled samples to unlabeled samples by preserving the sample distribution knowledge. Experimental evaluations in cartoon synthesis suggest the effectiveness of the visual features and SPA-TL. Facial sketch synthesis (FSS) is crucial in sketch-based face recognition. This paper proposes an automatic FSS algorithm with local strategy based on embedded hidden Markov model (E-HMM) and selective ensemble (SE). By using E-HMM to model the nonlinear relationship between a photo-sketch patch pair, a series of pseudo-sketch patches, generated based on several learned models for a given photo patch, are integrated with SE strategy to synthesize a finer face pseudo-sketch patch. Finally, the intact pseudo-sketch can be generated by combining all synthesized patches. Experimental results illustrate that the proposed FSS algorithm works well. Sketch-photo synthesis plays an important role in sketch-based face photo retrieval and photo-based face sketch retrieval systems. In this paper, we propose an automatic sketch-photo synthesis and retrieval algorithm based on sparse representation. The proposed sketch-photo synthesis method works at patch level and is composed of two steps: sparse neighbor selection (SNS) for an initial estimate of the pseudoimage (pseudosketch or pseudophoto) and sparse-representation-based enhancement (SRE) for further improving the quality of the synthesized image. SNS can find closely related neighbors adaptively and then generate an initial estimate for the pseudoimage. In SRE, a coupled sparse representation model is first constructed to learn the mapping between sketch patches and photo patches, and a patch-derivative-based sparse representation method is subsequently applied to enhance the quality of the synthesized photos and sketches. Finally, four retrieval modes, namely, sketch-based, photo-based, pseudosketch-based, and pseudophoto-based retrieval are proposed, and a retrieval algorithm is developed by using sparse representation. Extensive experimental results illustrate the effectiveness of the proposed face sketch-photo synthesis and retrieval algorithms. Object detection has been widely studied in the com- puter vision community and it has many real applications, despite its variations, such as scale, pose, lighting, and background. Most classical object detection methods heavily rely on category- based training to handle intra-class variations. In contrast to classical methods that use a rigid category-based representation, exemplar-based methods try to model variations among positives by learning from specific positive samples. However, current existing exemplar-based methods either fail to use any training information or suffer from a significant performance drop when few exemplars are available. In this paper, we design a novel local metric learning approach to well handle exemplar- based object detection task. The main works are two-fold: 1) a novel local metric learning algorithm called exemplar metric learning (EML) is designed and 2) an exemplar-based object detection algorithm based on EML is implemented. We evaluate our method on two generic object detection data sets: UIUC-Car and UMass FDDB. Experiments show that compared with other exemplar-based methods, our approach can effectively enhance object detection performance when few exemplars are available. Factorization-based techniques explain arrays of observations using a relatively small number of factors and provide an essential arsenal for multi-dimensional data analysis. Most factorization models are, however, developed on general arrays of scalar values. For a class of practical data arising from observing spatial signals including images, it is desirable for a model to consider general observations, e.g., handling a vector field and non-exchangeable factors, e.g., handling spatial connections between the columns and the rows of the data. In this paper, a probabilistic model for factorization is proposed. We adopt Bayesian hierarchical modeling and treat the factors as latent random variables. A Markov structure is imposed on the distribution of factors to account for the spatial connections. The model is designed to represent vector arrays sampled from fields of continuous domains. Therefore, a tailored observation model is developed to represent the link between the factor product and the data. The proposed technique has been shown effective in analyzing optical flow fields computed on both synthetic images and real-life videoclips. Face images captured in unconstrained environments usually contain significant pose variation, which dramatically degrades the performance of algorithms designed to recognize frontal faces. This paper proposes a novel face identification framework capable of handling the full range of pose variations within \\u00b190\\u00b0 of yaw. The proposed framework first transforms the original pose-invariant face recognition problem into a partial frontal face recognition problem. A robust patch-based face representation scheme is then developed to represent the synthesized partial frontal faces. For each patch, a transformation dictionary is learnt under the proposed multi-task learning scheme. The transformation dictionary transforms the features of different poses into a discriminative subspace. Finally, face matching is performed at patch level rather than at the holistic level. Extensive and systematic experimentation on FERET, CMU-PIE, and Multi-PIE databases shows that the proposed method consistently outperforms single-task-based baselines as well as state-of-the-art methods for the pose problem. We further extend the proposed algorithm for the unconstrained face verification problem and achieve top-level performance on the challenging LFW data set. Inexact graph matching has been one of the significant research foci in the area of pattern analysis. As an important way to measure the similarity between pairwise graphs error-tolerantly, graph edit distance (GED) is the base of inexact graph matching. The research advance of GED is surveyed in order to provide a review of the existing literatures and offer some insights into the studies of GED. Since graphs may be attributed or non-attributed and the definition of costs for edit operations is various, the existing GED algorithms are categorized according to these two factors and described in detail. After these algorithms are analyzed and their limitations are identified, several promising directions for further research are proposed. Visual reranking is effective to improve the performance of the text-based video search. However, existing reranking algorithms can only achieve limited improvement because of the well-known semantic gap between low-level visual features and high-level semantic concepts. In this article, we adopt interactive video search reranking to bridge the semantic gap by introducing user's labeling effort. We propose a novel dimension reduction tool, termed sparse transfer learning (STL), to effectively and efficiently encode user's labeling information. STL is particularly designed for interactive video search reranking. Technically, it (a) considers the pair-wise discriminative information to maximally separate labeled query relevant samples from labeled query irrelevant ones, (b) achieves a sparse representation for the subspace to encodes user's intention by applying the elastic net penalty, and (c) propagates user's labeling information from labeled samples to unlabeled samples by using the data distribution knowledge. We conducted extensive experiments on the TRECVID 2005, 2006 and 2007 benchmark datasets and compared STL with popular dimension reduction algorithms. We report superior performance by using the proposed STL-based interactive video search reranking. In computer vision and pattern recognition researches, the studied objects are often characterized by multiple feature representations with high dimensionality, thus it is essential to encode that multiview feature into a unified and discriminative embedding that is optimal for a given task. To address this challenge, this paper proposes an ensemble manifold regularized sparse low-rank approximation (EMR-SLRA) algorithm for multiview feature embedding. The EMR-SLRA algorithm is based on the framework of least-squares component analysis, in particular, the low dimensional feature representation and the projection matrix are obtained by the low-rank approximation of the concatenated multiview feature matrix. By considering the complementary property among multiple features, EMR-SLRA simultaneously enforces the ensemble manifold regularization on the output feature embedding. In order to further enhance its robustness against the noise, the group sparsity is introduced into the objective formulation to impose direct noise reduction on the input multiview feature matrix. Since there is no closed-form solution for EMR-SLRA, this paper provides an efficient EMR-SLRA optimization procedure to obtain the output feature embedding. Experiments on the pattern recognition applications confirm the effectiveness of the EMR-SLRA algorithm compare with some other multiview feature dimensionality reduction approaches. HighlightsThis paper proposes a novel EMR-SLRA algorithm for multiview feature embedding.The least-squares component analysis is generalized to multiview version.The ensemble manifold regularization is enforced to explore the complementarity.The group sparsity is introduced to promote the robustness against the noise.An efficient iterative procedure is developed to solve EMR-SLRA. Most existing color-based tracking algorithms utilize the statistical color information of the object as the tracking clues, without maintaining the spatial structure within a single chromatic image. Recently, the researches on the multilinear algebra provide the possibility to hold the spatial structural relationship in a representation of the image ensembles. In this paper, a third-order color tensor is constructed to represent the object to be tracked. Considering the influence of the environment changing on the tracking, the biased discriminant analysis (BDA) is extended to the tensor biased discriminant analysis (TBDA) for distinguishing the object from the background. At the same time, an incremental scheme for the TBDA is developed for the tensor biased discriminant subspace online learning, which can be used to adapt to the appearance variant of both the object and background. The experimental results show that the proposed method can track objects precisely undergoing large pose, scale and lighting changes, as well as partial occlusion. This paper comprehensively surveys the development of face hallucination (FH), including both face super-resolution and face sketch-photo synthesis techniques. Indeed, these two techniques share the same objective of inferring a target face image (e.g. high-resolution face image, face sketch and face photo) from a corresponding source input (e.g. low-resolution face image, face photo and face sketch). Considering the critical role of image interpretation in modern intelligent systems for authentication, surveillance, law enforcement, security control, and entertainment, FH has attracted growing attention in recent years. Existing FH methods can be grouped into four categories: Bayesian inference approaches, subspace learning approaches, a combination of Bayesian inference and subspace learning approaches, and sparse representation-based approaches. In spite of achieving a certain level of development, FH is limited in its success by complex application conditions such as variant illuminations, poses, or views. This paper provides a holistic understanding and deep insight into FH, and presents a comparative analysis of representative methods and promising future directions. Technical analysis with numerous indicators and patterns has been regarded as important evidence for making trading decisions in financial markets. However, it is extremely difficult for investors to find useful trading rules based on numerous technical indicators. This paper innovatively proposes the use of biclustering mining to discover effective technical trading patterns that contain a combination of indicators from historical financial data series. This is the first attempt to use biclustering algorithm on trading data. The mined patterns are regarded as trading rules and can be classified as three trading actions (i.e., the buy, the sell, and no-action signals) with respect to the maximum support. A modified ${K}$ nearest neighborhood ( ${K}$ -NN) method is applied to classification of trading days in the testing period. The proposed method [called biclustering algorithm and the ${K}$ nearest neighbor (BIC- ${K}$ -NN)] was implemented on four historical datasets and the average performance was compared with the conventional buy-and-hold strategy and three previously reported intelligent trading systems. Experimental results demonstrate that the proposed trading system outperforms its counterparts and will be useful for investment in various financial markets. It is a significant challenge to classify images with multiple labels by using only a small number of labeled samples. One option is to learn a binary classifier for each label and use manifold regularization to improve the classification performance by exploring the underlying geometric structure of the data distribution. However, such an approach does not perform well in practice when images from multiple concepts are represented by high-dimensional visual features. Thus, manifold regularization is insufficient to control the model complexity. In this paper, we propose a manifold regularized multitask learning (MRMTL) algorithm. MRMTL learns a discriminative subspace shared by multiple classification tasks by exploiting the common structure of these tasks. It effectively controls the model complexity because different tasks limit one another's search volume, and the manifold regularization ensures that the functions in the shared hypothesis space are smooth along the data manifold. We conduct extensive experiments, on the PASCAL VOC'07 dataset with 20 classes and the MIR dataset with 38 classes, by comparing MRMTL with popular image classification algorithms. The results suggest that MRMTL is effective for image classification. This paper aims to reducing difference between sketches and photos by synthesizing sketches from photos, and vice versa, and then performing sketch-sketch/photo-photo recognition with subspace learning based methods. Pseudo-sketch/pseudo-photo patches are synthesized with embedded hidden Markov model. Because these patches are assembled by averaging their overlapping area in most of the local strategy based methods, which leads to blurring effect to the resulted pseudo-sketch/pseudo-photo, we integrate the patches with image quilting. Experiments are carried out to demonstrate that the proposed method is effective to produce pseudo-sketch/pseudo-photo with high quality and achieve promising recognition results. This paper presents a fast and robust level set method for image segmentation. To enhance the robustness against noise, we embed a Markov random field (MRF) energy function to the conventional level set energy function. This MRF energy function builds the correlation of a pixel with its neighbors and encourages them to fall into the same region. To obtain a fast implementation of the MRF embedded level set model, we explore algebraic multigrid (AMG) and sparse field method (SFM) to increase the time step and decrease the computation domain, respectively. Both AMG and SFM can be conducted in a parallel fashion, which facilitates the processing of our method for big image databases. By comparing the proposed fast and robust level set method with the standard level set method and its popular variants on noisy synthetic images, synthetic aperture radar (SAR) images, medical images, and natural images, we comprehensively demonstrate the new method is robust against various kinds of noises. In particular, the new level set method can segment an image of size 500 \\u00d7 500 within 3 s on MATLAB R2010b installed in a computer with 3.30-GHz CPU and 4-GB memory. Node splitting is an important issue in Random Forest but robust splitting requires a large number of training samples. Existing solutions fail to properly partition the feature space if there are insufficient training data. In this paper, we present semi-supervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and unlabeled data. In particular, we derive a nonparametric algorithm to obtain an accurate quality measure of splitting by incorporating abundant unlabeled data. To avoid the curse of dimensionality, we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyper plane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. The proposed algorithm is compared with state-of-the-art supervised and semi-supervised algorithms for typical computer vision applications such as object categorization and image segmentation. Experimental results on publicly available datasets demonstrate the superiority of our method. No reference (NR) method is the most difficult issue of image quality assessment (IQA), which does not need the original image or its features as reference and only depends on the statistical law of the natural images. So, the NR-IQA is a high -level evaluation for image quality and simulates the complicated subjective process of human beings. This paper presents a NR-IQA metric based on Hidden Markov Tree (HMT) model. First, the HMT is utilized to model natural images, and the statistical properties of the model parameters are analyzed to mimic variation of image degradation. Then, by estimating the deviation degree of the parameters from the statistical law the distortion metric is constructed. Experimental results show that the proposed image quality assessment model is consistent well with the subjective evaluation results, and outperforms the existing models on difference distortions. This paper summaries the state-of-the-art of image quality assessment (IQA) and human visual system (HVS). IQA provides an objective index or real value to measure the quality of the specified image. Since human beings are the ultimate receivers of visual information in practical applications, the most reliable IQA is to build a computational model to mimic the HVS. According to the properties and cognitive mechanism of the HVS, the available HVS-based IQA methods can be divided into two categories, i.e., bionics methods and engineering methods. This paper briefly introduces the basic theories and development histories of the above two kinds of HVS-based IQA methods. Finally, some promising research issues are pointed out in the end of the paper. In recent years, cross-domain learning algorithms have attracted much attention to solve labeled data insufficient problem. However, these cross-domain learning algorithms cannot be applied for subspace learning, which plays a key role in multimedia processing. This paper envisions the cross-domain discriminative subspace learning and provides an effective solution to cross-domain subspace learning. In particular, we propose the cross-domain discriminative locally linear embedding or CDLLE for short. CDLLE connects the training and the testing samples by minimizing the quadratic distance between the distribution of the training samples and that of the testing samples. Therefore, a common subspace for data representation can be preserved. We basically expect the discriminative information to separate the concepts in the training set can be shared to separate the concepts in the testing set as well and thus we have a chance to address above cross-domain problem duly. The margin maximization is duly adopted in CDLLE so the discriminative information for separating different classes can be well preserved. Finally, CDLLE encodes the local geometry of each training samples through a series of linear coefficients which can reconstruct a given sample by its intra-class neighbour samples and thus can locally preserve the intra-class local geometry. Experimental evidence on NUS-WIDE, a popular social image database collected from Flickr, and MSRA-MM, a popular real-world web image annotation database collected from the Internet by using Microsoft Live Search, demonstrates the effectiveness of CDLLE for real-world cross-domain applications. Random forest (RF) is a very important classifier with applications in various machine learning tasks, but its promising performance heavily relies on the size of labeled training data. In this paper, we investigate constructing of RFs with a small size of labeled data and find that the performance bottleneck is located in the node splitting procedures; hence, existing solutions fail to properly partition the feature space if there are insufficient training data. To achieve robust node splitting with insufficient data, we present semisupervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and abundant unlabeled data. In particular, an accurate quality measure of node splitting is obtained by carrying out the kernel-based density estimation, whereby a multiclass version of asymptotic mean integrated squared error criterion is proposed to adaptively select the optimal bandwidth of the kernel. To avoid the curse of dimensionality, we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyperplane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. Our algorithm efficiently avoids overfitting caused by bad initialization and local maxima when compared with conventional margin maximization-based semisupervised methods. We demonstrate the effectiveness of the proposed algorithm by comparing it with state-of-the-art supervised and semisupervised algorithms for typical computer vision applications, such as object categorization, face recognition, and image segmentation, on publicly available data sets. In image classification, the goal is to decide whether an image belongs to a certain category or not. Multiple features are usually employed to comprehend the contents of images substantially for the improvement of classification accuracy. However, it also brings in some new problems that how to effectively combine multiple features together, and how to handle the high-dimensional features from multiple views given the small training set. In this paper, we present a large-margin Gaussian process approach to discover the latent space shared by multiple features. Therefore, multiple features can complement each other in this low-dimensional latent space, which derives a strong discriminative ability from the large-margin principle, and then the following classification task can be effectively accomplished. The resulted objective function can be efficiently solved using the gradient descent techniques. Finally, we demonstrate the advantages of the proposed algorithm on real-world image datasets for discovering discriminative latent space and improving the classification performance. Cumulative foot pressure images represent the 2D ground reaction force during one gait cycle. Biomedical and forensic studies show that humans can be distinguished by unique limb movement patterns and ground reaction force. Considering continuous gait pose images and corresponding cumulative foot pressure images, this paper presents a cascade fusion scheme to represent the potential connections between them and proposes a two-modality fusion based recognition system. The proposed scheme contains two stages: (1) given cumulative foot pressure images, canonical correlation analysis is employed to retrieve corresponding gait pose image candidates in gallery dataset; (2) pedestrian recognition is achieved via small samples matching between retrieved gait pose images and unlabeled ones. The proposed fusion recognition system is not only insensitive to slight changes of environment and the individual users, but also can be extended to multiple biometrics retrieval. Experimental results are conducted on the CASIA gait-footprint dataset, which contains cumulative foot pressure images and its corresponding gait pose image sequence from 88 subjects. Evaluation results suggest the effectiveness of the proposed scheme compared to other related approaches. Distance metric learning (DML) is successful in discovering intrinsic relations in data. However, most algorithms are computationally demanding when the problem size becomes large. In this paper, we propose a discriminative metric learning algorithm, develop a distributed scheme learning metrics on moderate-sized subsets of data, and aggregate the results into a global solution. The technique leverages the power of parallel computation. The algorithm of the aggregated DML (ADML) scales well with the data size and can be controlled by the partition. We theoretically analyze and provide bounds for the error induced by the distributed treatment. We have conducted experimental evaluation of the ADML, both on specially designed tests and on practical image annotation tasks. Those tests have shown that the ADML achieves the state-of-the-art performance at only a fraction of the cost incurred by most existing methods. Object tracking is a fundamental problem in computer vision. Although much progress has been made, object tracking is still a challenging problem as it entails learning an effective model to account for appearance change caused by intrinsic and extrinsic factors. To improve the reliability and effectiveness, this paper presents an approach that explores the combination of graph-based ranking and multiple feature representations for tracking. We construct multiple graph matrices with various types of visual features, and integrate the multiple graphs into a regularization framework to learn a ranking vector. In particular, the approach has exploited temporal consistency by adding a regularization term to constrain the difference between two weight vectors at adjacent frames. An effective iterative optimization scheme is also proposed in this paper. Experimental results on a variety of challenging video sequences show that the proposed algorithm performs favorably against the state-of-the-art visual tracking methods. We present a novel descriptor, called spatio-temporal Laplacian pyramid coding (STLPC), for holistic representation of human actions. In contrast to sparse representations based on detected local interest points, STLPC regards a video sequence as a whole with spatio-temporal features directly extracted from it, which prevents the loss of information in sparse representations. Through decomposing each sequence into a set of band-pass-filtered components, the proposed pyramid model localizes features residing at different scales, and therefore is able to effectively encode the motion information of actions. To make features further invariant and resistant to distortions as well as noise, a bank of 3-D Gabor filters is applied to each level of the Laplacian pyramid, followed by max pooling within filter bands and over spatio-temporal neighborhoods. Since the convolving and pooling are performed spatio-temporally, the coding model can capture structural and motion information simultaneously and provide an informative representation of actions. The proposed method achieves superb recognition rates on the KTH, the multiview IXMAS, the challenging UCF Sports, and the newly released HMDB51 datasets. It outperforms state of the art methods showing its great potential on action recognition. According to the research results reported in the past decades, it is well acknowledged that face recognition is not a trivial task. With the development of electronic devices, we are gradually revealing the secret of object recognition in the primate\\u2019s visual cortex. Therefore, it is time to reconsider face recognition by using biologically inspired features. In this paper, we represent face images by utilizing the C1 units, which correspond to complex cells in the visual cortex, and pool over S1 units by using a maximum operation to reserve only the maximum response of each local area of S1 units. The new representation is termed C1 Face. Because C1 Face is naturally a third-order tensor (or a three dimensional array), we propose three-way discriminative locality alignment (TWDLA), an extension of the discriminative locality alignment, which is a top-level discriminate manifold learning-based subspace learning algorithm. TWDLA has the following advantages: (1) it takes third-order tensors as input directly so the structure information can be well preserved; (2) it models the local geometry over every modality of the input tensors so the spatial relations of input tensors within a class can be preserved; (3) it maximizes the margin between a tensor and tensors from other classes over each modality so it performs well for recognition tasks and (4) it has no under sampling problem. Extensive experiments on YALE and FERET datasets show (1) the proposed C1Face representation can better represent face images than raw pixels and (2) TWDLA can duly preserve both the local geometry and the discriminative information over every modality for recognition. The classic multi-image-based super-resolution (SR) methods typically take global motion pattern to produce one or multiple high-resolution (HR) versions from a set of low-resolution (LR) images. However, due to the influence of aliasing and noise, it is difficult to obtain highly accurate registration with sub-pixel accuracy. Moreover, in practical applications, the global motion pattern is rarely found in the real LR inputs. In this paper, to surmount or at least reduce the aforementioned problems, we develop a novel SR framework for video sequence by extending the traditional 2-dimentional (2D) normalized convolution (NC) to 3-dimentional (3D) case. In the proposed framework, to bypass explicit motion estimation, we estimate a target pixel by taking a weighted average of pixels from its neighborhood. We further up-scale the input video sequence in temporal dimension based on the extended 3D NC and hence more video frames can be generated. Fundamental experiments demonstrate the effectiveness of the proposed SR framework both quantitatively and perceptually. Abstract In automated remote sensing based image analysis, it is important to consider the multiple features of a certain pixel, such as the spectral signature, morphological property, and shape feature, in both the spatial and spectral domains, to improve the classification accuracy. Therefore, it is essential to consider the complementary properties of the different features and combine them in order to obtain an accurate classification rate. In this paper, we introduce a modified stochastic neighbor embedding (MSNE) algorithm for multiple features dimension reduction (DR) under a probability preserving projection framework. For each feature, a probability distribution is constructed based on t-distributed stochastic neighbor embedding (t-SNE), and we then alternately solve t-SNE and learn the optimal combination coefficients for different features in the proposed multiple features DR optimization. Compared with conventional remote sensing image DR strategies, the suggested algorithm utilizes both the spatial and spectral features of a pixel to achieve a physically meaningful low-dimensional feature representation for the subsequent classification, by automatically learning a combination coefficient for each feature. The classification results using hyperspectral remote sensing images (HSI) show that MSNE can effectively improve RS image classification performance. Image super-resolution (SR) reconstruction is essentially an ill-posed problem, so it is important to design an effective prior. For this purpose, we propose a novel image SR method by learning both non-local and local regularization priors from a given low-resolution image. The non-local prior takes advantage of the redundancy of similar patches in natural images, while the local prior assumes that a target pixel can be estimated by a weighted average of its neighbors. Based on the above considerations, we utilize the non-local means filter to learn a non-local prior and the steering kernel regression to learn a local prior. By assembling the two complementary regularization terms, we propose a maximum a posteriori probability framework for SR recovery. Thorough experimental results suggest that the proposed SR method can reconstruct higher quality results both quantitatively and perceptually. In this paper, we focus on developing a novel noise-robust LBP-based texture feature extraction scheme for texture classification. Specifically, two solutions have been proposed to overcome the primary two reasons that cause local binary pattern sensitive to noise. First, a hybrid model is proposed for noise-robust texture description. In this new model, the local primitive micro features are encoded with the texture's global spatial structure to reduce the noise sensitiveness. Second, we design an adaptive quantization algorithm, in which quantization thresholds are choosing adaptively on the basis of the texture's content. Higher noise-tolerance and discriminant power can be obtained in the quantization process. Based on the proposed hybrid texture description model and adaptive quantization algorithm, we develop an adaptive hybrid pattern scheme for noise-robust texture feature extraction. Compared with several state-of-the-art feature extraction schemes, our scheme leads to significant improvement in noisy texture classification. How do we find patterns in author-keyword associations, evolving over timeq Or in data cubes (tensors), with product-branchcustomer sales informationq And more generally, how to summarize high-order data cubes (tensors)q How to incrementally update these patterns over timeq Matrix decompositions, like principal component analysis (PCA) and variants, are invaluable tools for mining, dimensionality reduction, feature selection, rule identification in numerous settings like streaming data, text, graphs, social networks, and many more settings. However, they have only two orders (i.e., matrices, like author and keyword in the previous example). We propose to envision such higher-order data as tensors, and tap the vast literature on the topic. However, these methods do not necessarily scale up, let alone operate on semi-infinite streams. Thus, we introduce a general framework, incremental tensor analysis (ITA), which efficiently computes a compact summary for high-order and high-dimensional data, and also reveals the hidden correlations. Three variants of ITA are presented: (1) dynamic tensor analysis (DTA); (2) streaming tensor analysis (STA); and (3) window-based tensor analysis (WTA). In paricular, we explore several fundamental design trade-offs such as space efficiency, computational cost, approximation accuracy, time dependency, and model complexity. We implement all our methods and apply them in several real settings, such as network anomaly detection, multiway latent semantic indexing on citation networks, and correlation study on sensor measurements. Our empirical studies show that the proposed methods are fast and accurate and that they find interesting patterns and outliers on the real datasets. Studies in neuroscience and biological vision have shown that the human retina has strong computational power, and its information representation supports vision tasks on both ventral and dorsal pathways. In this paper, a new local image descriptor, termed distinctive efficient robust features (DERF), is derived by modeling the response and distribution properties of the parvocellular-projecting ganglion cells in the primate retina. DERF features exponential scale distribution, exponential grid structure, and circularly symmetric function difference of Gaussian (DoG) used as a convolution kernel, all of which are consistent with the characteristics of the ganglion cell array found in neurophysiology, anatomy, and biophysics. In addition, a new explanation for local descriptor design is presented from the perspective of wavelet tight frames. DoG is naturally a wavelet, and the structure of the grid points array in our descriptor is closely related to the spatial sampling of wavelets. The DoG wavelet itself forms a frame, and when we modulate the parameters of our descriptor to make the frame tighter, the performance of the DERF descriptor improves accordingly. This is verified by designing a tight frame DoG, which leads to much better performance. Extensive experiments conducted in the image matching task on the multiview stereo correspondence data set demonstrate that DERF outperforms state of the art methods for both hand-crafted and learned descriptors, while remaining robust and being much faster to compute. For image analysis, an important extension to principal component analysis (PCA) is to treat an image as multiple samples, which helps alleviate the small sample size problem. Various schemes of transforming an image to multiple samples have been proposed. Although having been shown effective in practice, the schemes are mainly based on heuristics and experience. In this paper, we propose a probabilistic PCA model, in which we explicitly represent the transformation scheme and incorporate the scheme as a stochastic component of the model. Therefore fitting the model automatically learns the transformation. Moreover, the learned model allows us to distinguish regions that can be well described by the PCA model from those that need further treatment. Experiments on synthetic images and face data sets demonstrate the properties and utility of the proposed model. Multimodal biometric system utilizes two or more individual modalities, e.g., face, gait, and fingerprint, to improve the recognition accuracy of conventional unimodal methods. However, existing multimodal biometric methods neglect interactions of different modalities during the subspace selection procedure, i.e., the underlying assumption is the independence of different modalities. In this paper, by breaking this assumption, we propose a Geometry Preserving Projections (GPP) approach for subspace selection, which is capable of discriminating different classes and preserving the intra-modal geometry of samples within an identical class. With GPP, we can project all raw biometric data from different identities and modalities onto a unified subspace, on which classification can be performed. Furthermore, the training stage is carried out once and we have a unified transformation matrix to project different modalities. Unlike existing multimodal biometric systems, the new system works well when some modalities are not available. Experimental results demonstrate the effectiveness of the proposed GPP for individual recognition tasks. The desirability of being able to search for specific persons in surveillance videos captured by different cameras has increasingly motivated interest in the problem of person re-identification, which is a critical yet under-addressed challenge in multi-camera tracking systems. The main difficulty of person re-identification arises from the variations in human appearances from different camera views. In this paper, to bridge the human appearance variations across cameras, two coupled dictionaries that relate to the gallery and probe cameras are jointly learned in the training phase from both labeled and unlabeled images. The labeled training images carry the relationship between features from different cameras, and the abundant unlabeled training images are introduced to exploit the geometry of the marginal distribution for obtaining robust sparse representation. In the testing phase, the feature of each target image from the probe camera is first encoded by the sparse representation and then recovered in the feature space spanned by the images from the gallery camera. The features of the same person from different cameras are similar following the above transformation. Experimental results on publicly available datasets demonstrate the superiority of our method. It is important for practical application to design an effective and efficient metric for video quality. The most reliable way is by subjective evaluation. Thus, to design an objective metric by simulating human visual system (HVS) is quite reasonable and available. In this paper, the video quality assessment metric based on visual perception is proposed. Three-dimensional wavelet is utilized to decompose video and then extract features to mimic the multichannel structure of HVS. Spatio-temporal contrast sensitivity function (S-T CSF) is employed to weight coefficient obtained by three-dimensional wavelet to simulate nonlinearity feature of the human eyes. Perceptual threshold is exploited to obtain visual sensitive coefficients after S-T CSF filtered. Visual sensitive coefficients are normalized representation and then visual sensitive errors are calculated between reference and distorted video. Finally, temporal perceptual mechanism is applied to count values of video quality for reducing computational cost. Experimental results prove the proposed method outperforms the most existing methods and is comparable to LHS and PVQM. Abstract Target detection has been of great interest in hyperspectral image analysis. Feature extraction from target samples and counterpart backgrounds consist the key to the problem. Traditional target detection methods depend on comparatively fixed feature for all the pixels under observation. For example, RX employs the same distance measurement for all the pixels. However, the best separation results usually come from certain targets and backgrounds. Theoretically, they are the purest targets and backgrounds pixels, or the constructive endmembers in the subspace model. So using those most representative pixels' feature to train a concentrated subspace is expected to enhance the separability between targets and backgrounds. Meanwhile, applying the discriminative information from these training data to the large testing data which are not in the same feature space and with different data distributions is a challenge. Here, the idea of transfer learning from interactive annotation technique in video is employed. Based on the transfer learning frame, several points are taken into consideration and the proposed method is named as an unsupervised transfer learning based target detection (UTLD) method. Firstly, the extreme target and background pixels are generated from robust outlier detection, providing the input for target samples and background samples in transfer learning. Secondly, pixels are calculated from the root points in a segmentation method with the purpose to preserve the most distribution feature of the backgrounds after reduced dimension. Thirdly, sparse constraint is imposed into the transfer learning procedure. With this constraint, a simpler and more concentrated subspace with clear physical meaning can be constructed. Extensive experiments reveal the performance is comparable to the state-of-art target detection methods. It is important to design an effective and efficient objective metric of the video quality in video processing areas. The most reliable way is subjective evaluation, thus the most reasonable objective metric should adequately consider characteristics of the human visual system (HVS). Visual attention (VA) is one of the essential visual phenomena of HVS, the realization of which relies on the saliency of visual field. Moreover, the saliency of visual field has a great influence on recognition, memorization and subjective evaluation of the image. This paper explores the saliency of visual field for objective quality assessment of videos. The proposed method first uses the VA model to obtain visual saliency map of the distorted video, including color, intensity and motion. Then the salient map is used to weight a structural similarity map between the original and distorted videos to get the final value of the video quality. Experimental results prove that the proposed method achieves a good correlation with subjective valuation. Hyperspectral image classification has attracted extensive research efforts in the recent decade. The main difficulty lies in the few labeled samples versus the high dimensional features. To this end, it is a fundamental step to explore the relationship among different pixels in hyperspectral image classification, toward jointly handing both the lack of label and high dimensionality problems. In the hyperspectral images, the classification task can be benefited from the spatial layout information. In this paper, we propose a hyperspectral image classification method to address both the pixel spectral and spatial constraints, in which the relationship among pixels is formulated in a hypergraph structure. In the constructed hypergraph, each vertex denotes a pixel in the hyperspectral image. And the hyperedges are constructed from both the distance between pixels in the feature space and the spatial locations of pixels. More specifically, a feature-based hyperedge is generated by using distance among pixels, where each pixel is connected with its K nearest neighbors in the feature space. Second, a spatial-based hyperedge is generated to model the layout among pixels by linking where each pixel is linked with its spatial local neighbors. Both the learning on the combinational hypergraph is conducted by jointly investigating the image feature and the spatial layout of pixels to seek their joint optimal partitions. Experiments on four data sets are performed to evaluate the effectiveness and and efficiency of the proposed method. Comparisons to the state-of-the-art methods demonstrate the superiority of the proposed method in the hyperspectral image classification. Graph Laplacian has been widely exploited in traditional graph-based semisupervised learning (SSL) algorithms to regulate the labels of examples that vary smoothly on the graph. Although it achieves a promising performance in both transductive and inductive learning, it is not effective for handling ambiguous examples (shown in Fig. 1). This paper introduces deformed graph Laplacian (DGL) and presents label prediction via DGL (LPDGL) for SSL. The local smoothness term used in LPDGL, which regularizes examples and their neighbors locally, is able to improve classification accuracy by properly dealing with ambiguous examples. Theoretical studies reveal that LPDGL obtains the globally optimal decision function, and the free parameters are easy to tune. The generalization bound is derived based on the robustness analysis. Experiments on a variety of real-world data sets demonstrate that LPDGL achieves top-level performance on both transductive and inductive settings by comparing it with popular SSL algorithms, such as harmonic functions, AnchorGraph regularization, linear neighborhood propagation, Laplacian regularized least square, and Laplacian support vector machine. In this article, we present a novel indexing technique called EMIF (Effective Music Indexing Framework) to facilitate scalable and accurate content based music retrieval. It is designed based on a \\\"classification-and-indexing\\\" principle and consists of two main functionality layers: 1) a novel semantic-sensitive classification to identify input music's category and 2) multiple indexing structures - one local indexing structure corresponds to one semantic category. EMIF's layered architecture not only enables superior search accuracy but also reduces query response time significantly. To evaluate the system, a set of comprehensive experimental studies have been carried out using large test collection and EMIF demonstrates promising performance over state-of-the-art approaches. This paper aims to bridge human hearing and vision from the viewpoint of database search for images or music. The semantic content of an image can be illustrated with music or conversely images can be associated with a piece of music. The theoretical basis of the bridge is synaesthesia, a property of human perception. A prototype cross-media retrieval system is built, using principles established in the neuroscientific study of synaesthesia. Distance metric learning (DML) is a critical factor for image analysis and pattern recognition. To learn a robust distance metric for a target task, we need abundant side information (i.e., the similarity/dissimilarity pairwise constraints over the labeled data), which is usually unavailable in practice due to the high labeling cost. This paper considers the transfer learning setting by exploiting the large quantity of side information from certain related, but different source tasks to help with target metric learning (with only a little side information). The state-of-the-art metric learning algorithms usually fail in this setting because the data distributions of the source task and target task are often quite different. We address this problem by assuming that the target distance metric lies in the space spanned by the eigenvectors of the source metrics (or other randomly generated bases). The target metric is represented as a combination of the base metrics, which are computed using the decomposed components of the source metrics (or simply a set of random bases); we call the proposed method, decomposition-based transfer DML (DTDML). In particular, DTDML learns a sparse combination of the base metrics to construct the target metric by forcing the target metric to be close to an integration of the source metrics. The main advantage of the proposed method compared with existing transfer metric learning approaches is that we directly learn the base metric coefficients instead of the target metric. To this end, far fewer variables need to be learned. We therefore obtain more reliable solutions given the limited side information and the optimization tends to be faster. Experiments on the popular handwritten image (digit, letter) classification and challenge natural image annotation tasks demonstrate the effectiveness of the proposed method. When estimating age, human experts can provide privileged information that encodes the facial attributes of aging, such as smoothness, face shape, face acne, wrinkles, and bags under-eyes. In automatic age estimation, privileged information is unavailable to test images. To overcome this problem, we hypothesize that asymmetric information can be explored and exploited to improve the generalizability of the trained model. Using the learning using privileged information (LUPI) framework, we tested this hypothesis by carefully defining relative attributes for support vector machine (SVM+) to improve the performance of age estimation. We term this specific setting as relative attribute SVM+ (raSVM+), in which the privileged information enables separation of outliers from inliers at the training stage and effectively manipulates slack variables and age determination errors during model training, and thus guides the trained predictor toward a generalizable solution. Experimentally, the superiority of raSVM+ was confirmed by comparing it with state-of-the-art algorithms on the face and gesture recognition research network (FG-NET) and craniofacial longitudinal morphological face aging databases. raSVM+ is a promising development that improves age estimation, with the mean absolute error reaching 4.07 on FG-NET. Co-training is a major multi-view learning paradigm that alternately trains two classifiers on two distinct views and maximizes the mutual agreement on the two-view unlabeled data. Traditional co-training algorithms usually train a learner on each view separately and then force the learners to be consistent across views. Although many co-trainings have been developed, it is quite possible that a learner will receive erroneous labels for unlabeled data when the other learner has only mediocre accuracy. This usually happens in the first rounds of co-training, when there are only a few labeled examples. As a result, co-training algorithms often have unstable performance. In this paper, Hessian-regularized co-training is proposed to overcome these limitations. Specifically, each Hessian is obtained from a particular view of examples; Hessian regularization is then integrated into the learner training process of each view by penalizing the regression function along the potential manifold. Hessian can properly exploit the local structure of the underlying data manifold. Hessian regularization significantly boosts the generalizability of a classifier, especially when there are a small number of labeled examples and a large number of unlabeled examples. To evaluate the proposed method, extensive experiments were conducted on the unstructured social activity attribute (USAA) dataset for social activity recognition. Our results demonstrate that the proposed method outperforms baseline methods, including the traditional co-training and LapCo algorithms. In this paper, we propose a method for the dimensionality reduction (DR) of spectral-spatial features in hyperspectral images (HSIs), under the umbrella of multilinear algebra, i.e., the algebra of tensors. The proposed approach is a tensor extension of conventional supervised manifold-learning-based DR. In particular, we define a tensor organization scheme for representing a pixel's spectral-spatial feature and develop tensor discriminative locality alignment (TDLA) for removing redundant information for subsequent classification. The optimal solution of TDLA is obtained by alternately optimizing each mode of the input tensors. The methods are tested on three public real HSI data sets collected by hyperspectral digital imagery collection experiment, reflective optics system imaging spectrometer, and airborne visible/infrared imaging spectrometer. The classification results show significant improvements in classification accuracies while using a small number of features. This paper presents a novel framework for effective video semantic analysis. This framework has two major components, namely, optical flow tensor (OFT) and hidden Markov models (HMMs). OFT and HMMs are employed because: (1) motion is one of the fundamental characteristics reflecting the semantic information in video, so an OFT-based feature extraction method is developed to make full use of the motion information. Thereafter, to preserve the structure and discriminative information presented by OFT, general tensor discriminant analysis (GTDA) is used for dimensionality reduction. Finally, linear discriminant analysis (LDA) is utilized to further reduce the feature dimension for discriminative motion information representation; and (2) video is a sort of information intensive sequential media characterized by its context-sensitive nature, so the video sequences can be more effectively analyzed by some temporal modeling tools. In this framework, we use HMMs to well model different levels of semantic units (SU), e.g., shot and event. Experimental results are reported to demonstrate the advantages of the proposed framework upon semantic analysis of basketball video sequences, and the cross validations illustrate its feasibility and effectiveness. Least square regression (LSR) is popular in pattern classification. Compared against other matrix factorization based methods, it is simple yet efficient. However, LSR ignores unlabeled samples in the training stage, so the regression error could be large when the labeled samples are insufficient. To solve this problem, the Laplacian regularization can be used to penalize LSR. Extensive theoretical and experimental results have confirmed the validity of Laplacian regularized least square (LapRLS). However, multiple hyper-parameters have been introduced to estimate the intrinsic manifold induced by the regularization, and thus the time consuming cross-validation should be applied to tune these parameters. To alleviate this problem, we assume the intrinsic manifold is a linear combination of a given set of known manifolds. By further assuming the priors of the given manifolds are equivalent, we introduce the entropy maximization penalty to automatically learn the linear combination coefficients. The entropy maximization trades the smoothness off the complexity. Therefore, the proposed model enjoys the following advantages: (1) it is able to incorporate both labeled and unlabeled data into training process, (2) it is able to learn the manifold hyper-parameters automatically, and (3) it approximates the true probability distribution with respect to prescribed test data. To test the classification performance of our proposed model, we apply the model on three well-known human face datasets, i.e. FERET, ORL, and YALE. Experimental results on these three face datasets suggest the effectiveness and the efficiency of the new model compared against the traditional LSR and the Laplacian regularized least squares. The least squares problem is one of the most important regression problems in statistics, machine learning and data mining. In this paper, we present the Constrained Stochastic Gradient Descent (CSGD) algorithm to solve the large-scale least squares problem. CSGD improves the Stochastic Gradient Descent (SGD) by imposing a provable constraint that the linear regression line passes through the mean point of all the data points. It results in the best regret bound $O(log{T})$, and fastest convergence speed among all first order approaches. Empirical studies justify the effectiveness of CSGD by comparing it with SGD and other state-of-the-art approaches. An example is also given to show how to use CSGD to optimize SGD based least squares problems to achieve a better performance. Recent years have witnessed a surge of interest in graph-based transductive image classification. Existing simple graph-based transductive learning methods only model the pairwise relationship of images, however, and they are sensitive to the radius parameter used in similarity calculation. Hypergraph learning has been investigated to solve both difficulties. It models the high-order relationship of samples by using a hyperedge to link multiple samples. Nevertheless, the existing hypergraph learning methods face two problems, i.e., how to generate hyperedges and how to handle a large set of hyperedges. This paper proposes an adaptive hypergraph learning method for transductive image classification. In our method, we generate hyperedges by linking images and their nearest neighbors. By varying the size of the neighborhood, we are able to generate a set of hyperedges for each image and its visual neighbors. Our method simultaneously learns the labels of unlabeled images and the weights of hyperedges. In this way, we can automatically modulate the effects of different hyperedges. Thorough empirical studies show the effectiveness of our approach when compared with representative baselines. Searching for specific persons from surveillance videos captured by different cameras, known as person re-identification, is a key yet under-addressed challenge. Difficulties arise from the large variations of human appearance in different poses, and from the different camera views that may be involved, making low-level descriptor representation unreliable. In this paper, we propose a novel Attribute-Restricted Latent Topic Model (ARLTM) to encode targets into semantic topics. Compared to conventional topic models such as LDA and pLSI, ARLTM performs best by imposing semantic restrictions onto the generation of human specific attributes. We use MCMC EM for model learning. Experimental results show that our method achieves state-of-the-art performance. Multiple Instance Learning (MIL) has been widely applied in practice, such as drug activity prediction, content-based image retrieval. In MIL, a sample, comprised of a set of instances, is called a bag. Labels are assigned to bags instead of instances. The uncertainty of labels on instances makes MIL different from conventional supervised single instance learning (SIL) tasks. Therefore, it is critical to learn an effective mapping to convert an MIL task to an SIL task. In this paper, we present OptMILES by learning the optimal transformation on the bag-to-instance similarity measure, exploring the optimal distance metric between instances, by an alternating minimization training procedure. We thoroughly evaluate the proposed method on both a synthetic dataset and real world datasets by comparing with representative MIL algorithms. The experimental results suggest the effectiveness of OptMILES. With much attention from both academia and industrial communities, visual search reranking has recently been proposed to refine image search results obtained from text-based image search engines. Most of the traditional reranking methods cannot capture both relevance and diversity of the search results at the same time. Or they ignore the hierarchical topic structure of search result. Each topic is treated equally and independently. However, in real applications, images returned for certain queries are naturally in hierarchical organization, rather than simple parallel relation. In this paper, a new reranking method \\u201ctopic-aware reranking (TARerank)\\u201d is proposed. TARerank describes the hierarchical topic structure of search results in one model, and seamlessly captures both relevance and diversity of the image search results simultaneously. Through a structured learning framework, relevance and diversity are modeled in TARerank by a set of carefully designed features, and then the model is learned from human-labeled training samples. The learned model is expected to predict reranking results with high relevance and diversity for testing queries. To verify the effectiveness of the proposed method, we collect an image search dataset and conduct comparison experiments on it. The experimental results demonstrate that the proposed TARerank outperforms the existing relevance-based and diversified reranking methods. Feature extraction is probably the most important stage in image quality evaluation-effective features can well reflect the quality of digital images and vice versa. As a non-redundant sparse representation, contourlet transform can effectively reflect visual characteristics of images, and it can be employed to perceptually capture the difference between images. Motivated by this, this paper first proposes an objective reduced-reference image quality evaluation metric based on contourlet transform. Experiments demonstrate that this new objective metric achieves consistent image quality evaluation results with what gained by subjective evaluation. The existing face sketch-photo synthesis methods trend to lose some vital details more or less. In this paper, we propose a novel sketch-photo synthesis approach based on support vector regression (SVR) to handle this difficulty. First, we utilize an existing method to acquire the initial estimate of the synthesized image. Then, the final synthesized image is obtained by combining the initial estimate and the SVR based high frequency information together to further enhance the quality of synthesized image. Experimental results on the benchmark database and our new constructed database demonstrate that the proposed method can achieve significant improvement on perceptual quality. Moreover, the synthesized face images can obtain higher recognition rate when used in retrieval system. Multiple-instance learning (MIL) has been widely investigated in image annotation for its capability of exploring region-level visual information of images. Recent studies show that, by performing feature mapping, MIL can be cast to a single-instance learning problem and, thus, can be solved by traditional supervised learning methods. However, the approaches for feature mapping usually overlook the discriminative ability and the noises of the generated features. In this paper, we propose an MIL method with discriminative feature mapping and feature selection, aiming at solving this problem. Our method is able to explore both the positive and negative concept correlations. It can also select the effective features from a large and diverse set of low-level features for each concept under MIL settings. Experimental results and comparison with other methods demonstrate the effectiveness of our approach. Image super resolution (SR) is a technique to estimate or synthesize a high resolution (HR) image from one or several low resolution (LR) images. This paper proposes a novel framework for single image super resolution based on sparse representation with high resolution dictionary. Unlike the previous methods, the training set is constructed from the HR images instead of HR-LR image pairs. Due to this property, there is no need to retrain a new dictionary when the zooming factor changed. Given a testing LR image, the patch-based representation coefficients and the desired image are estimated alternately through the use of dynamic group sparsity, the fidelity term and the non-local means regularization. Experimental results demonstrate the effectiveness of the proposed algorithm. How to propagate the label information from labeled examples to unlabeled examples is a critical problem for graph-based semisupervised learning. Many label propagation algorithms have been developed in recent years and have obtained promising performance on various applications. However, the eigenvalues of iteration matrices in these algorithms are usually distributed irregularly, which slow down the convergence rate and impair the learning performance. This paper proposes a novel label propagation method called Fick\\u2019s law assisted propagation (FLAP). Unlike the existing algorithms that are directly derived from statistical learning, FLAP is deduced on the basis of the theory of Fick\\u2019s First Law of Diffusion, which is widely known as the fundamental theory in fluid-spreading. We prove that FLAP will converge with linear rate and show that FLAP makes eigenvalues of the iteration matrix distributed regularly. Comprehensive experimental evaluations on synthetic and practical datasets reveal that FLAP obtains encouraging results in terms of both accuracy and efficiency. Ensemble clustering, also known as consensus clustering, is emerging as a promising solution for multi-source and/or heterogeneous data clustering. The co-association matrix based method, which redefines the ensemble clustering problem as a classical graph partition problem, is a landmark method in this area. Nevertheless, the relatively high time and space complexity preclude it from real-life large-scale data clustering. We therefore propose SEC, an efficient Spectral Ensemble Clustering method based on co-association matrix. We show that SEC has theoretical equivalence to weighted K-means clustering and results in vastly reduced algorithmic complexity. We then derive the latent consensus function of SEC, which to our best knowledge is among the first to bridge co-association matrix based method to the methods with explicit object functions. The robustness and generalizability of SEC are then investigated to prove the superiority of SEC in theory. We finally extend SEC to meet the challenge rising from incomplete basic partitions, based on which a scheme for big data clustering can be formed. Experimental results on various real-world data sets demonstrate that SEC is an effective and efficient competitor to some state-of-the-art ensemble clustering methods and is also suitable for big data clustering. Example learning-based superresolution (SR) algorithms show promise for restoring a high-resolution (HR) image from a single low-resolution (LR) input. The most popular approaches, however, are either time- or space-intensive, which limits their practical applications in many resource-limited settings. In this paper, we propose a novel computationally efficient single image SR method that learns multiple linear mappings (MLM) to directly transform LR feature subspaces into HR subspaces. In particular, we first partition the large nonlinear feature space of LR images into a cluster of linear subspaces. Multiple LR subdictionaries are then learned, followed by inferring the corresponding HR subdictionaries based on the assumption that the LR\\u2013HR features share the same representation coefficients. We establish MLM from the input LR features to the desired HR outputs in order to achieve fast yet stable SR recovery. Furthermore, in order to suppress displeasing artifacts generated by the MLM-based method, we apply a fast nonlocal means algorithm to construct a simple yet effective similarity-based regularization term for SR enhancement. Experimental results indicate that our approach is both quantitatively and qualitatively superior to other application-oriented SR methods, while maintaining relatively low time and space complexity. Existing video concept detectors are generally built upon the kernel based machine learning techniques, e.g., support vector machines, regularized least squares, and logistic regression, just to name a few. However, in order to build robust detectors, the learning process suffers from the scalability issues including the high-dimensional multi-modality visual features and the large-scale keyframe examples. In this paper, we propose parallel lasso (Plasso) by introducing the parallel distributed computation to significantly improve the scalability of lasso (the l1 regularized least squares). We apply the parallel incomplete Cholesky factorization to approximate the covariance statistics in the preprocess step, and the parallel primal-dual interior-point method with the Sherman-Morrison-Woodbury formula to optimize the model parameters. For a dataset with n samples in a d-dimensional space, compared with lasso, Plasso significantly reduces complexities from the original O(d3) for computational time and O(d2) for storage space to O(h2d/m) and O(hd/m) , respectively, if the system has m processors and the reduced dimension h is much smaller than the original dimension d . Furthermore, we develop the kernel extension of the proposed linear algorithm with the sample reweighting schema, and we can achieve similar time and space complexity improvements [time complexity from O(n3) to O(h2n/m) and the space complexity from O(n2) to O(hn/m), for a dataset with n training examples]. Experimental results on TRECVID video concept detection challenges suggest that the proposed method can obtain significant time and space savings for training effective detectors with limited communication overhead. Content-based video copy detection (CBVCD) has attracted increasing attention in recent years. However, video content description and search efficiency are still two challenges in this domain. To cope with these two problems, this paper proposes a novel CBVCD approach with similarity preserving multimodal hash learning (SPM2H). The pre-processed video keyframes are represented as multiple features from different perspectives. SPM2H integrates the multimodal feature fusion and the hashing function learning into a joint framework. Mapping video keyframes into hash codes can conducts fast similarity search in the Hamming space. The experiments show that our approach achieves good performance in accuracy as well as efficiency. We propose an automatic approximation of the intrinsic manifold for general semi-supervised learning problems. Unfortunately, it is not trivial to define an optimization function to obtain optimal hyperparameters. Usually, pure cross-validation is considered but it does not necessarily scale up. A second problem derives from the suboptimality incurred by discrete grid search and overfitting problems. As a consequence, we developed an ensemble manifold regularization (EMR) framework to approximate the intrinsic manifold by combining several initial guesses. Algorithmically, we designed EMR very carefully so that it (a) learns both the composite manifold and the semi-supervised classifier jointly; (b) is fully automatic for learning the intrinsic manifold hyperparameters implicitly; (c) is conditionally optimal for intrinsic manifold approximation under a mild and reasonable assumption; and (d) is scalable for a large number of candidate manifold hyperparameters, from both time and space perspectives. Extensive experiments over both synthetic and real datasets show the effectiveness of the proposed framework. Conventional human action recognition algorithms cannot work well when the amount of training videos is insufficient. We solve this problem by proposing a transfer topic model (TTM), which utilizes information extracted from videos in the auxiliary domain to assist recognition tasks in the target domain. The TTM is well characterized by two aspects: 1) it uses the bag-of-words model trained from the auxiliary domain to represent videos in the target domain; and 2) it assumes each human action is a mixture of a set of topics and uses the topics learned from the auxiliary domain to regularize the topic estimation in the target domain, wherein the regularization is the summation of Kullback-Leibler divergences between topic pairs of the two domains. The utilization of the auxiliary domain knowledge improves the generalization ability of the learned topic model. Experiments on Weizmann and KTH human action databases suggest the effectiveness of the proposed TTM for cross-domain human action recognition. This paper proposes multi-task copula (MTC) that can handle a much wider class of tasks than mean regression with Gaussian noise in most former multi-task learning (MTL). While former MTL emphasizes shared structure among models, MTC aims at joint prediction to exploit inter-output correlation. Given input, the outputs of MTC are allowed to follow arbitrary joint continuous distribution. MTC captures the joint likelihood of multi-output by learning the marginal of each output firstly and then a sparse and smooth output dependency graph function. While the former can be achieved by classical MTL, learning graphs dynamically varying with input is quite a challenge. We address this issue by developing sparse graph regression (SpaGraphR), a non-parametric estimator incorporating kernel smoothing, maximum likelihood, and sparse graph structure to gain fast learning algorithm. It starts from a few seed graphs on a few input points, and then updates the graphs on other input points by a fast operator via coarse-to-fine propagation. Due to the power of copula in modeling semi-parametric distributions, SpaGraphR can model a rich class of dynamic non-Gaussian correlations. We show that MTC can address more flexible and difficult tasks that do not fit the assumptions of former MTL nicely, and can fully exploit their relatedness. Experiments on robotic control and stock price prediction justify its appealing performance in challenging MTL problems. In image classification, the goal was to decide whether an image belongs to a certain category or not. Multiple features are usually employed to comprehend the contents of images substantially for the improvement of classification accuracy. However, it also brings in some new problems that how to effectively combine multiple features together and how to handle the high-dimensional features from multiple views given the small training set. In this paper, we integrate the large-margin idea into the Gaussian process to discover the latent subspace shared by multiple features. Therefore, our approach inherits all the advantages of Gaussian process and large-margin principle. A probabilistic explanation is provided by Gaussian process to embed multiple features into the shared low-dimensional subspace, which derives a strong discriminative ability from the large-margin principle, and thus, the subsequent classification task can be effectively accomplished. Finally, we demonstrate the advantages of the proposed algorithm on real-world image datasets for discovering discriminative latent subspace and improving the classification performance. Person reidentification in a camera network is a valuable yet challenging problem to solve. Existing methods learn a common Mahalanobis distance metric by using the data collected from different cameras and then exploit the learned metric for identifying people in the images. However, the cameras in a camera network have different settings and the recorded images are seriously affected by variability in illumination conditions, camera viewing angles, and background clutter. Using a common metric to conduct person reidentification tasks on different camera pairs overlooks the differences in camera settings; however, it is very time-consuming to label people manually in images from surveillance videos. For example, in most existing person reidentification data sets, only one image of a person is collected from each of only two cameras; therefore, directly learning a unique Mahalanobis distance metric for each camera pair is susceptible to over-fitting by using insufficiently labeled data. In this paper, we reformulate person reidentification in a camera network as a multitask distance metric learning problem. The proposed method designs multiple Mahalanobis distance metrics to cope with the complicated conditions that exist in typical camera networks. We address the fact that these Mahalanobis distance metrics are different but related, and learned by adding joint regularization to alleviate over-fitting. Furthermore, by extending, we present a novel multitask maximally collapsing metric learning (MtMCML) model for person reidentification in a camera network. Experimental results demonstrate that formulating person reidentification over camera networks as multitask distance metric learning problem can improve performance, and our proposed MtMCML works substantially better than other current state-of-the-art person reidentification methods. Semi-supervised ranking is a relatively new and important learning problem inspired by many applications. We propose a novel graph-based regularized algorithm which learns the ranking function in the semi-supervised learning framework. It can exploit geometry of the data while preserving the magnitude of the preferences. The least squares ranking loss is adopted and the optimal solution of our model has an explicit form. We establish error analysis of our proposed algorithm and demonstrate the relationship between predictive performance and intrinsic properties of the graph. The experiments on three datasets for recommendation task and two quantitative structure-activity relationship datasets show that our method is effective and comparable to some other state-of-the-art algorithms for ranking. No single feature can satisfactorily characterize the semantic concepts of an image. Multiview learning aims to unify different kinds of features to produce a consensual and efficient representation. This paper redefines part optimization in the patch alignment framework (PAF) and develops a group sparse multiview patch alignment framework (GSM-PAF). The new part optimization considers not only the complementary properties of different views, but also view consistency. In particular, view consistency models the correlations between all possible combinations of any two kinds of view. In contrast to conventional dimensionality reduction algorithms that perform feature extraction and feature selection independently, GSM-PAF enjoys joint feature extraction and feature selection by exploiting ({l_{2,1}}) -norm on the projection matrix to achieve row sparsity, which leads to the simultaneous selection of relevant features and learning transformation, and thus makes the algorithm more discriminative. Experiments on two real-world image data sets demonstrate the effectiveness of GSM-PAF for image classification. Graph regularized nonnegative matrix factorization (GNMF) decomposes a nonnegative data matrix to the product of two lower-rank nonnegative factor matrices, i.e., and () and aims to preserve the local geometric structure of the dataset by minimizing squared Euclidean distance or Kullback-Leibler (KL) divergence between X and WH. The multiplicative update rule (MUR) is usually applied to optimize GNMF, but it suffers from the drawback of slow-convergence because it intrinsically advances one step along the rescaled negative gradient direction with a non-optimal step size. Recently, a multiple step-sizes fast gradient descent (MFGD) method has been proposed for optimizing NMF which accelerates MUR by searching the optimal step-size along the rescaled negative gradient direction with Newton's method. However, the computational cost of MFGD is high because 1) the high-dimensional Hessian matrix is dense and costs too much memory; and 2) the Hessian inverse operator and its multiplication with gradient cost too much time. To overcome these deficiencies of MFGD, we propose an efficient limited-memory FGD (L-FGD) method for optimizing GNMF. In particular, we apply the limited-memory BFGS (L-BFGS) method to directly approximate the multiplication of the inverse Hessian and the gradient for searching the optimal step size in MFGD. The preliminary results on real-world datasets show that L-FGD is more efficient than both MFGD and MUR. To evaluate the effectiveness of L-FGD, we validate its clustering performance for optimizing KL-divergence based GNMF on two popular face image datasets including ORL and PIE and two text corpora including Reuters and TDT2. The experimental results confirm the effectiveness of L-FGD by comparing it with the representative GNMF solvers. Due to that human eye has different perceptual characteristics for different morphological components, so a novel image quality metric is proposed by incorporating morphological component analysis (MCA) and human visual system (HVS), which is capable of assessing the image with different types of distortion. Firstly, reference and distorted images are decomposed into texture and cartoon components by MCA respectively. Then these components are changed into perceptual features by just noticeable difference (JND) which integrates masking features, luminance adaptation and contrast sensitive function (CSF). Finally, the difference between reference and distorted images' perceptual features is quantified using a pooling strategy, and then the final result of the image quality is obtained. Experimental results demonstrate that the performance of the metric prevail over some existing methods on LIVE database II. The Vapnik-Chervonenkis (VC) dimension is used to measure the complexity of a function class and plays an important role in a variety of fields, including artificial neural networks and machine learning. One major concern is the relationship between the VC dimension and inherent characteristics of the corresponding function class. According to Sauer's lemma, if the VC dimension of an indicator function class F is equal to D, the cardinality of the set FS1N will not be larger than \\u03a3d=0DCNd. Therefore, there naturally arises a question about the VC dimension of an indicator function class: what kinds of elements will be contained in the function class F if F has a finite VC dimension? In this brief, we answer the above question. First, we investigate the structure of the function class F when the cardinality of the set FS1N reaches the maximum value \\u03a3d=0DCNd. Based on the derived result, we then figure out what kinds of elements will be contained in F if F has a finite VC dimension. In hyperspectral remote sensing image classification, multiple features, e.g., spectral, texture, and shape features, are employed to represent pixels from different perspectives. It has been widely acknowledged that properly combining multiple features always results in good classification performance. In this paper, we introduce the patch alignment framework to linearly combine multiple features in the optimal way and obtain a unified low-dimensional representation of these multiple features for subsequent classification. Each feature has its particular contribution to the unified representation determined by simultaneously optimizing the weights in the objective function. This scheme considers the specific statistical properties of each feature to achieve a physically meaningful unified low-dimensional representation of multiple features. Experiments on the classification of the hyperspectral digital imagery collection experiment and reflective optics system imaging spectrometer hyperspectral data sets suggest that this scheme is effective. How do we retrieve cartoon characters accurately? Or how to synthesize new cartoon clips smoothly and efficiently from the cartoon library? Both questions are important for animators and cartoon enthusiasts to design and create new cartoons by utilizing existing cartoon materials. The first key issue to answer those questions is to find a proper representation that describes the cartoon character effectively. In this paper, we consider multiple features from different views, i.e., color histogram, Hausdorff edge feature, and skeleton feature, to represent cartoon characters with different colors, shapes, and gestures. Each visual feature reflects a unique characteristic of a cartoon character, and they are complementary to each other for retrieval and synthesis. However, how to combine the three visual features is the second key issue of our application. By simply concatenating them into a long vector, it will end up with the so-called \\u201ccurse of dimensionality,\\u201d let alone their heterogeneity embedded in different visual feature spaces. Here, we introduce a semisupervised multiview subspace learning (semi-MSL) algorithm, to encode different features in a unified space. Specifically, under the patch alignment framework, semi-MSL uses the discriminative information from labeled cartoon characters in the construction of local patches where the manifold structure revealed by unlabeled cartoon characters is utilized to capture the geometric distribution. The experimental evaluations based on both cartoon character retrieval and clip synthesis demonstrate the effectiveness of the proposed method for cartoon application. Moreover, additional results of content-based image retrieval on benchmark data suggest the generality of semi-MSL for other applications. To facilitate human-robot interactions, human gender information is very important. Motivated by the success of manifold learning for visual recognition, we present a novel clustering-based discriminative locality alignment (CDLA) algorithm to discover the low-dimensional intrinsic submanifold from the embedding high-dimensional ambient space for improving the face gender recognition performance. In particular, CDLA exploits the global geometry through k-means clustering, extracts the discriminative information through margin maximization and explores the local geometry through intra cluster sample concentration. These three properties uniquely characterize CDLA for face gender recognition. The experimental results obtained from the FERET data sets suggest the superiority of the proposed method in terms of recognition speed and accuracy by comparing with several representative methods. Most of the existing graph edit distance (GED) algorithms require cost functions which are difficult to be defined exactly. In this article, we propose a cost function free algorithm for computing GED. It only depends on the distribution of nodes rather than node or edge attributes in graphs. Hidden Markov model (HMM) is employed to model the distribution of feature points and thus dissimilarity measure of graphs can be posed as distance of HMMs. A fast algorithm of Kullback-Leibler Distance, suitable for computing the distance between two probability models, is adopted to compute the distance of HMMs. Experimental results demonstrate that the proposed GED algorithm can characterize the structure variety of graphs effectively and is available for clustering and indexing images of both rigid and nonrigid bodies. \\u00a9 2008 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 18, 209\\u2013218, 2008 Nonnegative matrix factorization (NMF) becomes tractable in polynomial time with unique solution under separability assumption, which postulates all the data points are contained in the conical hull of a few anchor data points. Recently developed linear programming and greedy pursuit methods can pick out the anchors from noisy data and results in a near-separable NMF. But their efficiency could be seriously weakened in high dimensions. In this paper, we show that the anchors can be precisely located from low-dimensional geometry of the data points even when their high dimensional features suffer from serious incompleteness. Our framework, entitled divide-and-conquer anchoring (DCA), divides the high-dimensional anchoring problem into a few cheaper sub-problems seeking anchors of data projections in low-dimensional random spaces, which can be solved in parallel by any near-separable NMF, and combines all the detected low-dimensional anchors via a fast hypothesis testing to identify the original anchors. We further develop two non-iterative anchoring algorithms in 1D and 2D spaces for data in convex hull and conical hull, respectively. These two rapid algorithms in the ultra low dimensions suffice to generate a robust and efficient near-separable NMF for high-dimensional or incomplete data via DCA. Compared to existing methods, two vital advantages of DCA are its scalability for big data, and capability of handling incomplete and high-dimensional noisy data. A rigorous analysis proves that DCA is able to find the correct anchors of a rank-k matrix by solving math cal O(klog k) sub-problems. Finally, we show DCA outperforms state-of-the-art methods on various datasets and tasks. This paper introduces a web image search reranking approach that explores multiple modalities in a graph-based learning scheme. Different from the conventional methods that usually adopt a single modality or integrate multiple modalities into a long feature vector, our approach can effectively integrate the learning of relevance scores, weights of modalities, and the distance metric and its scaling for each modality into a unified scheme. In this way, the effects of different modalities can be adaptively modulated and better reranking performance can be achieved. We conduct experiments on a large dataset that contains more than 1000 queries and 1 million images to evaluate our approach. Experimental results demonstrate that the proposed reranking approach is more robust than using each individual modality, and it also performs better than many existing methods. Articulated human pose estimation in unconstrained conditions is a great challenge. We propose a deep structure that represents a human body in different granularity from coarse-to-fine for better detecting parts and describing spatial constrains between different parts. Typical approaches for this problem just utilize a single level structure, which is difficult to capture various body appearances and hard to model high-order part dependencies. In this paper, we build a three layer Markov network to model the body structure that separates the whole body to poselets (combined parts) then to parts representing joints. Parts at different levels are connected through a parent-child relationship to represent high-order spatial relationships. Unlike other multi-layer models, our approach explores more reasonable granularity for part detection and sophisticatedly designs part connections to model body configurations more effectively. Moreover, each part in our model contains different types so as to capture a wide range of pose modes. And our model is a tree structure, which can be trained jointly and favors exact inference. Extensive experimental results on two challenging datasets show the performance of our model improving or being on-par with state-of-the-art approaches. Highlights We present a multi-layer Markov network for detecting parts in different granularity. A composite model structure is designed for better describing spatial constrains. The proposed model can be trained jointly and favors exact inference. Extensive experiments show superior performance of the proposed method. With the popularity of cameras and rapid development of computer vision technology, vision-based HCI is attracting extensive interests. In this paper, we present a system for controlling avatars by natural body movement via a single web-camera. A pose database and a set of color markers are utilized to make ill-posed vision problem tractable for real game applications. Based on the proposed algorithms for indexing pose samples and estimating human pose, we build a prototype system that is responsive, easy to manipulate and runs automatically in real time. User study shows that the system is user-friendly and provides immersive experiences. Bag-of-Feature (BoF) representations and spatial constraints have been popular in image classification research. One of the most successful methods uses sparse coding and spatial pooling to build discriminative features. However, minimizing the reconstruction error by sparse coding only considers the similarity between the input and codebooks. In contrast, this paper describes a novel feature learning approach for image classification by considering the dissimilarity between inputs and prototype images, or what we called reference basis (RB). First, we learn the feature representation by max-margin criterion between the input and the RB. The learned hyperplane is stored as the relative feature. Second, we propose an adaptive pooling technique to assemble multiple relative features generated by different RBs under the SVM framework, where the classifier and the pooling weights are jointly learned. Experiments based on three challenging datasets: Caltech-101, Scene 15 and Willow-Actions, demonstrate the effectiveness and generality of our framework. Protection against geometric distortions and common image processing operations with blind detection becomes a much challenging task in image watermarking. To achieve this, in this paper we propose a content-based watermarking scheme that combines the invariant feature extraction with watermark embedding by using Tchebichef moments. Harris-Laplace detector is first adopted to extract feature points, and then non-overlapped disks centered at feature points are generated. These disks are invariant to scaling and translation distortions. For each disk, orientation alignment is then performed to achieve rotation invariant. Finally, the watermark is embedded in magnitudes of Tchebichef moments of each disk via dither modulation to realize the robustness to common image processing operations and the blind detection. Thorough simulation results obtained by using the standard benchmark, Stirmark, demonstrate that the proposed method is robust against various geometric distortions as well as common image processing operations and outperforms representative image watermarking schemes. : Fisher's linear discriminant analysis (FLDA) is an important dimension reduction method in statistical pattern recognition. It has been shown that FLDA is asymptotically Bayes optimal under the homoscedastic Gaussian assumption. However, this classical result has the following two major limitations: 1) it holds only for a fixed dimensionality D, and thus does not apply when D and the training sample size N are proportionally large; 2) it does not provide a quantitative description on how the generalization ability of FLDA is affected by D and N. In this paper, we present an asymptotic generalization analysis of FLDA based on random matrix theory, in a setting where both D and N increase and D/N \\u2192 \\u03b3 \\u2208 [0,1). The obtained lower bound of the generalization discrimination power overcomes both limitations of the classical result, i.e., it is applicable when D and N are proportionally large and provides a quantitative description of the generalization ability of FLDA in terms of the ratio \\u03b3 = D/N and the population discrimination power. Besides, the discrimination power bound also leads to an upper bound on the generalization error of binary-classification with FLDA. In this paper, we employ the non-local steering kernel regression to construct an effective regularization term for the single image super-resolution problem. The proposed method seamlessly integrates the properties of local structural regularity and non-local self-similarity existing in natural images, and solves a least squares minimization problem for obtaining the desired high-resolution image. Extensive experimental results on both simulated and real low-resolution images demonstrate that the proposed method can restore compelling results with sharp edges and fine textures. Is it possible to train a learning model to separate tigers from elks when we have 1) labeled samples of leopard and zebra and 2) unlabelled samples of tiger and elk at hand? Cross-domain learning algorithms can be used to solve the above problem. However, existing cross-domain algorithms cannot be applied for dimension reduction, which plays a key role in computer vision tasks, e.g., face recognition and web image annotation. This paper envisions the cross-domain discriminative dimension reduction to provide an effective solution for cross-domain dimension reduction. In particular, we propose the cross-domain discriminative Hessian Eigenmaps or CDHE for short. CDHE connects training and test samples by minimizing the quadratic distance between the distribution of the training set and that of the test set. Therefore, a common subspace for data representation can be well preserved. Furthermore, we basically expect the discriminative information used to separate leopards and zebra can be shared to separate tigers and elks, and thus we have a chance to duly address the above question. Margin maximization principle is adopted in CDHE so the discriminative information for separating different classes (e.g., leopard and zebra here) can be well preserved. Finally, CDHE encodes the local geometry of each training class (e.g., leopard and zebra here) in the local tangent space which is locally isometric to the data manifold and thus CDHE preserves the intraclass local geometry. The objective function of CDHE is not convex, so the gradient descent strategy can only find a local optimal solution. In this paper, we carefully design an evolutionary search strategy to find a better solution of CDHE. Experimental evidence on both synthetic and real word image datasets demonstrates the effectiveness of CDHE for cross-domain web image annotation and face recognition. Using biometric resources to recognize a person has been a recent concentration on computer vision. Previously, biometric research has forced on utilizing iris, finger print, palm print, and shoe print to authenticate and authorized a human. However, these conventional biometric resources suffer from some obviously limitation, such as: strictly distance requirement, too many user cooperation requirement and so on. Compared with the difficulties of utilization through conventional biometric resources, human gait can be easily acquired and utilized in many fields. A human's walk image can reflect the walker's physical characteristics and psychological state, and therefore, the gait feature can be used to recognize a person. In order to achieve better performance of gait recognition we represent the gait image using C1 units, which correspond to the complex cells in human visual cortex, and use a maximum mechanism to keep only the maximum response of each local area of S1 units. To enhance the gait recognition rate, we take the label information into account and utilize the discriminative locality alignment (DLA) method to classify, which is a top level discriminate manifold learning based subspace learning algorithm. Experiment on University of South Florida (USF) dataset shows: (1) the proposed C1Gait+DLA algorithms can achieve better performance than the state-of-art algorithms and (2) DLA can duly preserve both the local geometry and the discriminative information for recognition. The desire of enabling computers to learn semantic concepts from large quantities of Internet videos has motivated increasing interests on semantic video understanding, while video segmentation is important yet challenging for understanding videos. The main difficulty of video segmentation arises from the burden of labeling training samples, making the problem largely unsolved. In this paper, we present a novel nearest neighbor-based label transfer scheme for weakly supervised video segmentation. Whereas previous weakly supervised video segmentation methods have been limited to the two-class case, our proposed scheme focuses on more challenging multiclass video segmentation, which finds a semantically meaningful label for every pixel in a video. Our scheme enjoys several favorable properties when compared with conventional methods. First, a weakly supervised hashing procedure is carried out to handle both metric and semantic similarity. Second, the proposed nearest neighbor-based label transfer algorithm effectively avoids overfitting caused by weakly supervised data. Third, a multi-video graph model is built to encourage smoothness between regions that are spatiotemporally adjacent and similar in appearance. We demonstrate the effectiveness of the proposed scheme by comparing it with several other state-of-the-art weakly supervised segmentation methods on one new Wild8 dataset and two other publicly available datasets. This paper aims to take general tensors as inputs for supervised learning. A supervised tensor learning (STL) framework is established for convex optimization based learning techniques such as support vector machines (SVM) and minimax probability machines (MPM). Within the STL framework, many conventional learning machines can be generalized to take n/sup th/-order tensors as inputs. We also study the applications of tensors to learning machine design and feature extraction by linear discriminant analysis (LDA). Our method for tensor based feature extraction is named the tenor rank-one discriminant analysis (TR1DA). These generalized algorithms have several advantages: 1) reduce the curse of dimension problem in machine learning and data mining; 2) avoid the failure to converge; and 3) achieve better separation between the different categories of samples. As an example, we generalize MPM to its STL version, which is named the tensor MPM (TMPM). TMPM learns a series of tensor projections iteratively. It is then evaluated against the original MPM. Our experiments on a binary classification problem show that TMPM significantly outperforms the original MPM. The use of relative attributes for semantic understanding of images and videos is a promising way to improve communication between humans and machines. However, it is extremely labor- and time-consuming to define multiple attributes for each instance in large amount of data. One option is to incorporate active learning, so that the informative samples can be actively discovered and then labeled. However, most existing active-learning methods select samples one at a time (serial mode), and may therefore lose efficiency when learning multiple attributes. In this paper, we propose a batch-mode active-learning method, called diverse expected gradient active learning. This method integrates an informativeness analysis and a diversity analysis to form a diverse batch of queries. Specifically, the informativeness analysis employs the expected pairwise gradient length as a measure of informativeness, while the diversity analysis forces a constraint on the proposed diverse gradient angle. Since simultaneous optimization of these two parts is intractable, we utilize a two-step procedure to obtain the diverse batch of queries. A heuristic method is also introduced to suppress imbalanced multiclass distributions. Empirical evaluations of three different databases demonstrate the effectiveness and efficiency of the proposed approach. Local binary patterns (LBP) achieve great success in texture analysis, however they are not robust to noise. The two reasons for such disadvantage of LBP schemes are (1) they encode the texture spatial structure based only on local information which is sensitive to noise and (2) they use exact values as the quantization thresholds, which make the extracted features sensitive to small changes in the input image. In this paper, we propose a noise-robust adaptive hybrid pattern (AHP) for noised texture analysis. In our scheme, two solutions from the perspective of texture description model and quantization algorithm have been developed to reduce the feature?s noise sensitiveness. First, a hybrid texture description model is proposed. In this model, the global texture spatial structure which is depicted by a global description model is encoded with the primitive microfeature for texture description. Second, we develop an adaptive quantization algorithm in which equal probability quantization is utilized to achieve the maximum partition entropy. Higher noise-tolerance can be obtained with the minimum lost information in the quantization process. The experimental results of texture classification on two texture databases with three different types of noise show that our approach leads significant improvement in noised texture analysis. Furthermore, our scheme achieves state-of-the-art performance in noisy face recognition. HighlightsA hybrid texture description model is proposed for noise-robust texture modeling.An adaptive quantization algorithm is designed for robust angular space quantization.Based on the new description model and quantization algorithm, we develop the AHP.Experimental results demonstrate the significant improvement achieved by our scheme. Posture segmentation plays an essential role in human motion analysis. The state-of-the-art method extracts sufficiently high-dimensional features from 3D depth images for each 3D point and learns an efficient body part classifier. However, high-dimensional features are memory-consuming and difficult to handle on large-scale training dataset. In this paper, we propose an efficient two-stage dimension reduction scheme, termed biview learning, to encode two independent views which are depth-difference features (DDF) and relative position features (RPF). Biview learning explores the complementary property of DDF and RPF, and uses two stages to learn a compact yet comprehensive low-dimensional feature space for posture segmentation. In the first stage, discriminative locality alignment (DLA) is applied to the high-dimensional DDF to learn a discriminative low-dimensional representation. In the second stage, canonical correlation analysis (CCA) is used to explore the complementary property of RPF and the dimensionality reduced DDF. Finally, we train a support vector machine (SVM) over the output of CCA. We carefully validate the effectiveness of DLA and CCA utilized in the two-stage scheme on our 3D human points cloud dataset. Experimental results show that the proposed biview learning scheme significantly outperforms the state-of-the-art method for human posture segmentation. Compared with other existing methods, the feature point-based image watermarking schemes can resist to global geometric attacks and local geometric attacks, especially cropping and random bending attacks (RBAs), by binding watermark synchronization with salient image characteristics. However, the watermark detection rate remains low in the current feature point-based watermarking schemes. The main reason is that both of feature point extraction and watermark embedding are more or less related to the pixel position, which is seriously distorted by the interpolation error and the shift problem during geometric attacks. In view of these facts, this paper proposes a geometrically robust image watermarking scheme based on local histogram. Our scheme mainly consists of three components: (1) feature points extraction and local circular regions (LCRs) construction are conducted by using Harris-Laplace detector; (2) a mechanism of grapy theoretical clustering-based feature selection is used to choose a set of non-overlapped LCRs, then geometrically invariant LCRs are completely formed through dominant orientation normalization; and (3) the histogram and mean statistically independent of the pixel position are calculated over the selected LCRs and utilized to embed watermarks. Experimental results demonstrate that the proposed scheme can provide sufficient robustness against geometric attacks as well as common image processing operations. Dense feature extraction is becoming increasingly popular in face recognition tasks. Systems based on this approach have demonstrated impressive performance in a range of challenging scenarios. However, improvements in discriminative power come at a computational cost and with a risk of over-fitting. In this paper, we propose a new approach to dense feature extraction for face recognition, which consists of two steps. First, an encoding scheme is devised that compresses high-dimensional dense features into a compact representation by maximizing the intrauser correlation. Second, we develop an adaptive feature matching algorithm for effective classification. This matching method, in contrast to the previous methods, constructs and chooses a small subset of training samples for adaptive matching, resulting in further performance gains. Experiments using several challenging face databases, including labeled Faces in the Wild data set, Morph Album 2, CUHK optical-infrared, and FERET, demonstrate that the proposed approach consistently outperforms the current state of the art. Face recognition by sketches in photos remains a challenging task. Unlike the existing sketch-photo recognition methods, which convert a photo into sketch and then perform the sketch-photo recognition through sketch-sketch recognition, this paper devotes to synthesizing a photo from the sketch and transforming the sketch-photo recognition to photo-photo recognition to achieve better performance in mixture pattern recognition. The contribution of this paper mainly focuses on two aspects: (1) in view of that there are no many research findings of sketch-photo recognition based on the pseudo-photo synthesis and the existing methods require a large set of training samples, which is nearly impossible to achieve for the high cost of sketch acquisition, we make use of embedded hidden Markov model (EHMM), which can learn the nonlinearity of sketch-photo pair with less training samples, to produce pseudo-photos in terms of sketches; and (2) photos and sketches are divided into patches and pseudo-photo is generated by combining pseudo-photo patches, which makes pseudo-photo more recognizable. Experimental results demonstrate that the newly proposed method is effective to identify face sketches in photo set. It is practical to assume that an individual view is unlikely to be sufficient for effective multi-view learning. Therefore, integration of multi-view information is both valuable and necessary. In this paper, we propose the Multi-view Intact Space Learning (MISL) algorithm, which integrates the encoded complementary information in multiple views to discover a latent intact representation of the data. Even though each view on its own is insufficient, we show theoretically that by combing multiple views we can obtain abundant information for latent intact space learning. Employing the Cauchy loss (a technique used in statistical learning) as the error measurement strengthens robustness to outliers. We propose a new definition of multi-view stability and then derive the generalization error bound based on multi-view stability and Rademacher complexity, and show that the complementarity between multiple views is beneficial for the stability and generalization. MISL is efficiently optimized using a novel Iteratively Reweight Residuals (IRR) technique, whose convergence is theoretically analyzed. Experiments on synthetic data and real-world datasets demonstrate that MISL is an effective and promising algorithm for practical applications. Distance metric learning (DML) has received increasing attention in recent years. In this paper, we propose a constrained empirical risk minimization framework for DML. This framework enriches the state-of-the-art studies on both theoretic and algorithmic aspects. Theoretically, we comprehensively analyze the generalization by bounding the sample and the approximation errors with respect to the best model. Algorithmically, we carefully derive an optimal gradient descent by using Nesterov's method, and provide two example algorithms that utilize the logarithmic loss and the smoothed hinge loss, respectively. We evaluate the new framework on data classification and image retrieval experiments. Results show that the new framework has competitive performance compared with the representative DML algorithms, including Xing's method, large margin nearest neighbor classifier, neighborhood component analysis, and regularized metric learning. How do we find patterns in author-keyword associations, evolving over time? Or in Data Cubes, with product-branch-customer sales information? Matrix decompositions, like principal component analysis (PCA) and variants, are invaluable tools for mining, dimensionality reduction, feature selection, rule identification in numerous settings like streaming data, text, graphs, social networks and many more. However, they have only two orders, like author and keyword, in the above example.We propose to envision such higher order data as tensors,and tap the vast literature on the topic. However, these methods do not necessarily scale up, let alone operate on semi-infinite streams. Thus, we introduce the dynamic tensor analysis (DTA) method, and its variants. DTA provides a compact summary for high-order and high-dimensional data, and it also reveals the hidden correlations. Algorithmically, we designed DTA very carefully so that it is (a) scalable, (b) space efficient (it does not need to store the past) and (c) fully automatic with no need for user defined parameters. Moreover, we propose STA, a streaming tensor analysis method, which provides a fast, streaming approximation to DTA.We implemented all our methods, and applied them in two real settings, namely, anomaly detection and multi-way latent semantic indexing. We used two real, large datasets, one on network flow data (100GB over 1 month) and one from DBLP (200MB over 25 years). Our experiments show that our methods are fast, accurate and that they find interesting patterns and outliers on the real datasets. Directly applying single-label classification methods to the multi-label learning problems substantially limits both the performance and speed due to the imbalance, dependence and high dimensionality of the given label matrix. Existing methods either ignore these three problems or reduce one with the price of aggravating another. In this paper, we propose a {0,1} label matrix compression and recovery method termed \\\"compressed labeling (CL)\\\" to simultaneously solve or at least reduce these three problems. CL first compresses the original label matrix to improve balance and independence by preserving the signs of its Gaussian random projections. Afterward, we directly utilize popular binary classification methods (e.g., support vector machines) for each new label. A fast recovery algorithm is developed to recover the original labels from the predicted new labels. In the recovery algorithm, a \\\"labelset distilling method\\\" is designed to extract distilled labelsets (DLs), i.e., the frequently appeared label subsets from the original labels via recursive clustering and subtraction. Given a distilled and an original label vector, we discover that the signs of their random projections have an explicit joint distribution that can be quickly computed from a geometric inference. Based on this observation, the original label vector is exactly determined after performing a series of Kullback-Leibler divergence based hypothesis tests on the distribution about the new labels. CL significantly improves the balance of the training samples and reduces the dependence between different labels. Moreover, it accelerates the learning process by training fewer binary classifiers for compressed labels, and makes use of label dependence via DLs based tests. Theoretically, we prove the recovery bounds of CL which verifies the effectiveness of CL for label compression and multi-label classification performance improvement brought by label correlations preserved in DLs. We show the effectiveness, efficiency and robustness of CL via 5 groups of experiments on 21 datasets from text classification, image annotation, scene classification, music categorization, genomics and web page classification. Target detection is one of the most important applications in hyperspectral remote sensing image analysis. However, the state-of-the-art machine-learning-based algorithms for hyperspectral target detection cannot perform well when the training samples, especially for the target samples, are limited in number. This is because the training data and test data are drawn from different distributions in practice and given a small-size training set in a high-dimensional space, traditional learning models without the sparse constraint face the over-fitting problem. Therefore, in this paper, we introduce a novel feature extraction algorithm named sparse transfer manifold embedding (STME), which can effectively and efficiently encode the discriminative information from limited training data and the sample distribution information from unlimited test data to find a low-dimensional feature embedding by a sparse transformation. Technically speaking, STME is particularly designed for hyperspectral target detection by introducing sparse and transfer constraints. As a result of this, it can avoid over-fitting when only very few training samples are provided. The proposed feature extraction algorithm was applied to extensive experiments to detect targets of interest, and STME showed the outstanding detection performance on most of the hyperspectral datasets. In this paper, we present a novel level set method (LSM) for image segmentation. By utilizing the Bayesian rule, we design a nonlinear adaptive velocity and a probability-weighted stopping force to implement a robust segmentation for objects with weak boundaries. The proposed method is featured by the following three properties: 1) it automatically determines the curve to shrink or expand by utilizing the Bayesian rule to involve the regional features of images; 2) it drives the curve evolve with an appropriate speed to avoid the leakage at weak boundaries; and 3) it reduces the influence of false boundaries, i.e., edges far away from objects of interest. We applied the proposed segmentation method to artificial images, medical images and the BSD-300 image dataset for qualitative and quantitative evaluations. The comparison results show the proposed method performs competitively, compared with the LSM and its representative variants. The bag-of-visual-words representation has been widely used in image retrieval and visual recognition. The most time-consuming step in obtaining this representation is the visual word generation, i.e., assigning visual words to the corresponding local features in a high-dimensional space. Recently, structures based on multibranch trees and forests have been adopted to reduce the time cost. However, these approaches cannot perform well without a large number of backtrackings. In this paper, by considering the spatial correlation of local features, we can significantly speed up the time consuming visual word generation process while maintaining accuracy. In particular, visual words associated with certain structures frequently co-occur; hence, we can build a co-occurrence table for each visual word for a large-scale data set. By associating each visual word with a probability according to the corresponding co-occurrence table, we can assign a probabilistic weight to each node of a certain index structure (e.g., a KD-tree and a K-means tree), in order to re-direct the searching path to be close to its global optimum within a small number of backtrackings. We carefully study the proposed scheme by comparing it with the fast library for approximate nearest neighbors and the random KD-trees on the Oxford data set. Thorough experimental results suggest the efficiency and effectiveness of the new scheme. Conventional regression methods, such as multivariate linear regression (MLR) and its extension principal component regression (PCR), deal well with the situations that the data are of the form of low-dimensional vector. When the dimension grows higher, it leads to the under sample problem (USP): the dimensionality of the feature space is much higher than the number of training samples. However, little attention has been paid to such a problem. This paper first adopts an in-depth investigation to the USP in PCR, which answers three questions: 1) Why is USP produced? 2) What is the condition for USP, and 3) How is the influence of USP on regression. With the help of the above analysis, the principal components selection problem of PCR is presented. Subsequently, to address the problem of PCR, a multivariate multilinear regression (MMR) model is proposed which gives a substitutive solution to MLR, under the condition of multilinear objects. The basic idea of MMR is to transfer the multilinear structure of objects into the regression coefficients as a constraint. As a result, the regression problem is reduced to find two low-dimensional coefficients so that the principal components selection problem is avoided. Moreover, the sample size needed for solving MMR is greatly reduced so that USP is alleviated. As there is no closed-form solution for MMR, an alternative projection procedure is designed to obtain the regression matrices. For the sake of completeness, the analysis of computational cost and the proof of convergence are studied subsequently. Furthermore, MMR is applied to model the fitting procedure in the active appearance model (AAM). Experiments are conducted on both the carefully designed synthesizing data set and AAM fitting databases verified the theoretical analysis. Dimensionality reduction algorithms, which aim to select a small set of efficient and discriminant features, have attracted great attention for human gait recognition and content-based image retrieval (CBIR). In this paper, we present extensions of our recently proposed marginal Fisher analysis (MFA) to address these problems. For human gait recognition, we first present a direct application of MFA, then inspired by recent advances in matrix and tensor-based dimensionality reduction algorithms, we present matrix-based MFA for directly handling 2-D input in the form of gray-level averaged images. For CBIR, we deal with the relevance feedback problem by extending MFA to marginal biased analysis, in which within-class compactness is characterized only by the distances between each positive sample and its neighboring positive samples. In addition, we present a new technique to acquire a direct optimal solution for MFA without resorting to objective function modification as done in many previous algorithms. We conduct comprehensive experiments on the USF HumanID gait database and the Corel image retrieval database. Experimental results demonstrate that MFA and its extensions outperform related algorithms in both applications. In this paper, we introduce the discretized-Vapnik-Chervonenkis (VC) dimension for studying the complexity of a real function class, and then analyze properties of real function classes and neural networks. We first prove that a countable traversal set is enough to achieve the VC dimension for a real function class, whereas its classical definition states that the traversal set is the output range of the function class. Based on this result, we propose the discretized-VC dimension defined by using a countable traversal set consisting of rational numbers in the range of a real function class. By using the discretized-VC dimension, we show that if a real function class has a finite VC dimension, only a finite traversal set is needed to achieve the VC dimension. We then point out that the real function classes, which have the infinite VC dimension, can be grouped into two categories: TYPE-A and TYPE-B. Subsequently, based on the obtained results, we discuss the relationship between the VC dimension of an indicator-output network and that of the real-output network, when both networks have the same structure except for the output activation functions. Finally, we present the risk bound based on the discretized-VC dimension for a real function class that has infinite VC dimension and is of TYPE-A. We prove that, with such a function class, the empirical risk minimization (ERM) principle for the function class is still consistent with overwhelming probability. This is a development of the existing knowledge that the ERM learning is consistent if and only if the function class has a finite VC dimension. To reduce tedious work in cartoon animation, some computer-assisted systems including automatic Inbetweening and cartoon reusing systems have been proposed. In existing automatic Inbetweening systems, accurate correspondence construction, which is a prerequisite for Inbetweening, cannot be achieved. For cartoon reusing systems, the lack of efficient similarity estimation method and reusing mechanism makes it impractical for the users. The semi-supervised graph-based cartoon reusing approach proposed in this paper aims at generating smooth cartoons from the existing data. In this approach, the similarity between cartoon frames can be accurately evaluated by calculating the distance based on local shape context, which is expected to be rotation and scaling invariant. By the semi-supervised algorithm, given an initial frame, the most similar cartoon frames in the cartoon library are selected as candidates of the next frame. The smooth cartoons can be generated by carrying out the algorithm repeatedly to select new cartoon frames after the cartoonists specifying the motion path in a background image. Experimental results of the candidate frame selection in our cartoon dataset suggest the effectiveness of the proposed local shape context for similarity evaluation. The other experiments show the excellent performance on cartoon generation of our approach. There is growing interest in multilabel image classification due to its critical role in web-based image analytics-based applications, such as large-scale image retrieval and browsing. Matrix completion (MC) has recently been introduced as a method for transductive (semisupervised) multilabel classification, and has several distinct advantages, including robustness to missing data and background noise in both feature and label space. However, it is limited by only considering data represented by a single-view feature, which cannot precisely characterize images containing several semantic concepts. To utilize multiple features taken from different views, we have to concatenate the different features as a long vector. However, this concatenation is prone to over-fitting and often leads to very high time complexity in MC-based image classification. Therefore, we propose to weightedly combine the MC outputs of different views, and present the multiview MC (MVMC) framework for transductive multilabel image classification. To learn the view combination weights effectively, we apply a cross-validation strategy on the labeled set. In particular, MVMC splits the labeled set into two parts, and predicts the labels of one part using the known labels of the other part. The predicted labels are then used to learn the view combination coefficients. In the learning process, we adopt the average precision (AP) loss, which is particular suitable for multilabel image classification, since the ranking-based criteria are critical for evaluating a multilabel classification system. A least squares loss formulation is also presented for the sake of efficiency, and the robustness of the algorithm based on the AP loss compared with the other losses is investigated. Experimental evaluation on two real-world data sets (PASCAL VOC\\u2019 07 and MIR Flickr) demonstrate the effectiveness of MVMC for transductive (semisupervised) multilabel image classification, and show that MVMC can exploit complementary properties of different features and output-consistent labels for improved multilabel image classification. Projective non-negative matrix factorization (PNMF) projects high-dimensional non-negative examples X onto a lower-dimensional subspace spanned by a non-negative basis W and considers WT X as their coefficients, i.e., X\\u2248WWT X. Since PNMF learns the natural parts-based representation Wof X, it has been widely used in many fields such as pattern recognition and computer vision. However, PNMF does not perform well in classification tasks because it completely ignores the label information of the dataset. This paper proposes a Discriminant PNMF method (DPNMF) to overcome this deficiency. In particular, DPNMF exploits Fisher's criterion to PNMF for utilizing the label information. Similar to PNMF, DPNMF learns a single non-negative basis matrix and needs less computational burden than NMF. In contrast to PNMF, DPNMF maximizes the distance between centers of any two classes of examples meanwhile minimizes the distance between any two examples of the same class in the lower-dimensional subspace and thus has more discriminant power. We develop a multiplicative update rule to solve DPNMF and prove its convergence. Experimental results on four popular face image datasets confirm its effectiveness comparing with the representative NMF and PNMF algorithms. In recent years, sparse representation has been widely used in object recognition applications. How to learn the dictionary is a key issue to sparse representation. A popular method is to use l 1 norm as the sparsity measurement of representation coefficients for dictionary learning. However, the l 1 norm treats each atom in the dictionary independently, so the learned dictionary cannot well capture the multisubspaces structural information of the data. In addition, the learned subdictionary for each class usually shares some common atoms, which weakens the discriminative ability of the reconstruction error of each subdictionary. This paper presents a new dictionary learning model to improve sparse representation for image classification, which targets at learning a class-specific subdictionary for each class and a common subdictionary shared by all classes. The model is composed of a discriminative fidelity, a weighted group sparse constraint, and a subdictionary incoherence term. The discriminative fidelity encourages each class-specific subdictionary to sparsely represent the samples in the corresponding class. The weighted group sparse constraint term aims at capturing the structural information of the data. The subdictionary incoherence term is to make all subdictionaries independent as much as possible. Because the common subdictionary represents features shared by all classes, we only use the reconstruction error of each class-specific subdictionary for classification. Extensive experiments are conducted on several public image databases, and the experimental results demonstrate the power of the proposed method, compared with the state-of-the-arts. People often take photographs at tourist sites and these pictures usually have two main elements: a person in the foreground and scenery in the background. This type of \\u201csouvenir photo\\u201d is one of the most common photos clicked by tourists. Although algorithms that aid a user-photographer in taking a well-composed picture of a scene exist [Ni et al. 2013], few studies have addressed the issue of properly positioning human subjects in photographs. In photography, the common guidelines of composing portrait images exist. However, these rules usually do not consider the background scene. Therefore, in this article, we investigate human-scenery positional relationships and construct a photographic assistance system to optimize the position of human subjects in a given background scene, thereby assisting the user in capturing high-quality souvenir photos. We collect thousands of well-composed portrait photographs to learn human-scenery aesthetic composition rules. In addition, we define a set of negative rules to exclude undesirable compositions. Recommendation results are achieved by combining the first learned positive rule with our proposed negative rules. We implement the proposed system on an Android platform in a smartphone. The system demonstrates its efficacy by producing well-composed souvenir photos. How do we find all images in a larger set of images which have a specific content? Or estimate the position of a specific object relative to the camera? Image classification methods, like support vector machine (supervised) and transductive support vector machine (semi-supervised), are invaluable tools for the applications of content-based image retrieval, pose estimation, and optical character recognition. However, these methods only can handle the images represented by single feature. In many cases, different features (or multiview data) can be obtained, and how to efficiently utilize them is a challenge. It is inappropriate for the traditionally concatenating schema to link features of different views into a long vector. The reason is each view has its specific statistical property and physical interpretation. In this paper, we propose a high-order distance-based multiview stochastic learning (HD-MSL) method for image classification. HD-MSL effectively combines varied features into a unified representation and integrates the labeling information based on a probabilistic framework. In comparison with the existing strategies, our approach adopts the high-order distance obtained from the hypergraph to replace pairwise distance in estimating the probability matrix of data distribution. In addition, the proposed approach can automatically learn a combination coefficient for each view, which plays an important role in utilizing the complementary information of multiview data. An alternative optimization is designed to solve the objective functions of HD-MSL and obtain different views on coefficients and classification scores simultaneously. Experiments on two real world datasets demonstrate the effectiveness of HD-MSL in image classification. The ultimate goal of distance metric learning is to incorporate abundant discriminative information to keep all data samples in the same class close and those from different classes separated. Local distance metric methods can preserve discriminative information by considering the neighborhood influence. In this paper, we propose a new local discriminative distance metrics (LDDM) algorithm to learn multiple distance metrics from each training sample (a focal sample) and in the vicinity of that focal sample (focal vicinity), to optimize local compactness and local separability. Those locally learned distance metrics are used to build local classifiers which are aligned in a probabilistic framework via ensemble learning. Theoretical analysis proves the convergence rate bound, the generalization bound of the local distance metrics and the final ensemble classifier. We extensively evaluate LDDM using synthetic datasets and large benchmark UCI datasets. Watermarking aims to hide particular information into some carrier but does not change the visual cognition of the carrier itself. Local features are good candidates to address the watermark synchronization error caused by geometric distortions and have attracted great attention for content-based image watermarking. This paper presents a novel feature point-based image watermarking scheme against geometric distortions. Scale invariant feature transform (SIFT) is first adopted to extract feature points and to generate a disk for each feature point that is invariant to translation and scaling. For each disk, orientation alignment is then performed to achieve rotation invariance. Finally, watermark is embedded in middle-frequency discrete Fourier transform (DFT) coefficients of each disk to improve the robustness against common image processing operations. Extensive experimental results and comparisons with some representative image watermarking methods confirm the excellent performance of the proposed method in robustness against various geometric distortions as well as common image processing operations. We address the problem of removing video color tone jitter that is common in amateur videos recorded with hand-held devices. To achieve this, we introduce color state to represent the exposure and white balance state of a frame. The color state of each frame can be computed by accumulating the color transformations of neighboring frame pairs. Then, the tonal changes of the video can be represented by a time-varying trajectory in color state space. To remove the tone jitter, we smooth the original color state trajectory by solving an (L1) optimization problem with PCA dimensionality reduction. In addition, we propose a novel selective strategy to remove small tone jitter while retaining extreme exposure and white balance changes to avoid serious artifacts. Quantitative evaluation and visual comparison with previous work demonstrate the effectiveness of our tonal stabilization method. This system can also be used as a preprocessing tool for other video editing methods. Robust reversible watermarking (RRW) methods are popular in multimedia for protecting copyright, while preserving intactness of host images and providing robustness against unintentional attacks. However, conventional RRW methods are not readily applicable in practice. That is mainly because: 1) they fail to offer satisfactory reversibility on large-scale image datasets; 2) they have limited robustness in extracting watermarks from the watermarked images destroyed by different unintentional attacks; and 3) some of them suffer from extremely poor invisibility for watermarked images. Therefore, it is necessary to have a framework to address these three problems, and further improve its performance. This paper presents a novel pragmatic framework, wavelet-domain statistical quantity histogram shifting and clustering (WSQH-SC). Compared with conventional methods, WSQH-SC ingeniously constructs new watermark embedding and extraction procedures by histogram shifting and clustering, which are important for improving robustness and reducing run-time complexity. Additionally, WSQH-SC includes the property-inspired pixel adjustment to effectively handle overflow and underflow of pixels. This results in satisfactory reversibility and invisibility. Furthermore, to increase its practical applicability, WSQH-SC designs an enhanced pixel-wise masking to balance robustness and invisibility. We perform extensive experiments over natural, medical, and synthetic aperture radar images to show the effectiveness of WSQH-SC by comparing with the histogram rotation-based and histogram distribution constrained methods. The technique of visual saliency detection supports video surveillance systems by reducing redundant information and highlighting the critical, visually important regions. It follows that information about the image might be of great importance in depicting the visual saliency. However, the majority of existing methods extract contrast-like features without considering the contribution of information content. Based on the hypothesis that information divergence leads to visual saliency, a two-stage framework for saliency detection, namely information divergence model (IDM), is introduced in this paper. The term ''information divergence'' is used to express the non-uniform distribution of the visual information in an image. The first stage is constructed to extract sparse features by employing independent component analysis (ICA) and difference of Gaussians (DoG) filter. The second stage improves the Bayesian surprise model to compute information divergence across an image. A visual saliency map is finally obtained from the information divergence. Experiments are conducted on nature image databases, psychological patterns and video surveillance sequences. The results show the effectiveness of the proposed method by comparing it with 13 state-of-the-art visual saliency detection methods. : Content-based medical image retrieval continues to gain attention for its potential to assist radiological image interpretation and decision making. Many approaches have been proposed to improve the performance of medical image retrieval system, among which visual features such as SIFT, LBP, and intensity histogram play a critical role. Typically, these features are concatenated into a long vector to represent medical images, and thus traditional dimension reduction techniques such as locally linear embedding (LLE), principal component analysis (PCA), or laplacian eigenmaps (LE) can be employed to reduce the \\\"curse of dimensionality\\\". Though these approaches show promising performance for medical image retrieval, the feature-concatenating method ignores the fact that different features have distinct physical meanings. In this paper, we propose a new method called multiview locally linear embedding (MLLE) for medical image retrieval. Following the patch alignment framework, MLLE preserves the geometric structure of the local patch in each feature space according to the LLE criterion. To explore complementary properties among a range of features, MLLE assigns different weights to local patches from different feature spaces. Finally, MLLE employs global coordinate alignment and alternating optimization techniques to learn a smooth low-dimensional embedding from different features. To justify the effectiveness of MLLE for medical image retrieval, we compare it with conventional spectral embedding methods. We conduct experiments on a subset of the IRMA medical image data set. Evaluation results show that MLLE outperforms state-of-the-art dimension reduction methods. The luminance of a natural scene is often of high dynamic range (HDR). In this paper, we propose a new scheme to handle HDR scenes by integrating locally adaptive scene detail capture and suppressing gradient reversals introduced by the local adaptation. The proposed scheme is novel for capturing an HDR scene by using a standard dynamic range (SDR) device and synthesizing an image suitable for SDR displays. In particular, we use an SDR capture device to record scene details (i.e., the visible contrasts and the scene gradients) in a series of SDR images with different exposure levels. Each SDR image responds to a fraction of the HDR and partially records scene details. With the captured SDR image series, we first calculate the image luminance levels, which maximize the visible contrasts, and then the scene gradients embedded in these images. Next, we synthesize an SDR image by using a probabilistic model that preserves the calculated image luminance levels and suppresses reversals in the image luminance gradients. The synthesized SDR image contains much more scene details than any of the captured SDR image. Moreover, the proposed scheme also functions as the tone mapping of an HDR image to the SDR image, and it is superior to both global and local tone mapping operators. This is because global operators fail to preserve visual details when the contrast ratio of a scene is large, whereas local operators often produce halos in the synthesized SDR image. The proposed scheme does not require any human interaction or parameter tuning for different scenes. Subjective evaluations have shown that it is preferred over a number of existing approaches. The distinguishment between the object appearance and the background is the useful cues available for visual tracking, in which the discriminant analysis is widely applied. However, due to the diversity of the background observation, there are not adequate negative samples from the background, which usually lead the discriminant method to tracking failure. Thus, a natural solution is to construct an object-background pair, constrained by the spatial structure, which could not only reduce the neg-sample number, but also make full use of the background information surrounding the object. However, this idea is threatened by the variant of both the object appearance and the spatial-constrained background observation, especially when the background shifts as the moving of the object. Thus, an incremental pairwise discriminant subspace is constructed in this paper to delineate the variant of the distinguishment. In order to maintain the correct the ability of correctly describing the subspace, we enforce two novel constraints for the optimal adaptation: (1) pairwise data discriminant constraint and (2) subspace smoothness. The experimental results demonstrate that the proposed approach can alleviate adaptation drift and achieve better visual tracking results for a large variety of nonstationary scenes. Deposited solder paste inspection plays a critical role in surface mounting processes. When detecting solder pastes defects on a printed circuit board, profile measurement-based methods suffer from large system size, high cost, and low speed for inspection, although they provide 3-D information of solder pastes. In contrast, image analysis-based methods facilitate the defect detection process of solder pastes by treating them as a pattern recognition problem. However, existing image analysis methods do not perform well because low-level visual features cannot catch sufficient information for defect detection. This paper proposes a new defect detection scheme for solder pastes based on learning the color biological feature sub-manifold. In particular, we apply the biologically inspired color feature (BICF) to represent the solder paste images, and introduce a new sub-manifold learning method to extract the intrinsic low-dimensional BICF manifold embedded in an extrinsic high-dimensional ambient space. This scheme mimics the function of human visual cortex in recognition tasks, and can separate poor quality solder pastes from good quality ones. We apply the new scheme to our automated optical inspection system, and thorough empirical studies indicate the effectiveness of the new scheme for practical utilization. Bag-of-visual-words (BOVW)-based image representation has received intense attention in recent years and has improved content-based image retrieval (CBIR) significantly. BOVW does not consider the spatial correlation between visual words in natural images and thus biases the generated visual words toward noise when the corresponding visual features are not stable. This article outlines the construction of a visual word co-occurrence matrix by exploring visual word co-occurrence extracted from small affine-invariant regions in a large collection of natural images. Based on this co-occurrence matrix, we first present a novel high-order predictor to accelerate the generation of spatially correlated visual words and a penalty tree (PTree) to continue generating the words after the prediction. Subsequently, we propose two methods of co-occurrence weighting similarity measure for image ranking: Co-Cosine and Co-TFIDF. These two new schemes down-weight the contributions of the words that are less discriminative because of frequent co-occurrences with other words. We conduct experiments on Oxford and Paris Building datasets, in which the ImageNet dataset is used to implement a large-scale evaluation. Cross-dataset evaluations between the Oxford and Paris datasets and Oxford and Holidays datasets are also provided. Thorough experimental results suggest that our method outperforms the state of the art without adding much additional cost to the BOVW model. Face sketch-photo synthesis plays a critical role in many applications, such as law enforcement and digital entertainment. Recently, many face sketch-photo synthesis methods have been proposed under the framework of inductive learning, and these have obtained promising performance. However, these inductive learning-based face sketch-photo synthesis methods may result in high losses for test samples, because inductive learning minimizes the empirical loss for training samples. This paper presents a novel transductive face sketch-photo synthesis method that incorporates the given test samples into the learning process and optimizes the performance on these test samples. In particular, it defines a probabilistic model to optimize both the reconstruction fidelity of the input photo (sketch) and the synthesis fidelity of the target output sketch (photo), and efficiently optimizes this probabilistic model by alternating optimization. The proposed transductive method significantly reduces the expected high loss and improves the synthesis performance for test samples. Experimental results on the Chinese University of Hong Kong face sketch data set demonstrate the effectiveness of the proposed method by comparing it with representative inductive learning-based face sketch-photo synthesis methods. Sparse representation based classification (SRC) has recently been proposed for robust face recognition. To deal with occlusion, SRC introduces an identity matrix as an occlusion dictionary on the assumption that the occlusion has sparse representation in this dictionary. However, the results show that SRC's use of this occlusion dictionary is not nearly as robust to large occlusion as it is to random pixel corruption. In addition, the identity matrix renders the expanded dictionary large, which results in expensive computation. In this paper, we present a novel method, namely structured sparse representation based classification (SSRC), for face recognition with occlusion. A novel structured dictionary learning method is proposed to learn an occlusion dictionary from the data instead of an identity matrix. Specifically, a mutual incoherence of dictionaries regularization term is incorporated into the dictionary learning objective function which encourages the occlusion dictionary to be as independent as possible of the training sample dictionary. So that the occlusion can then be sparsely represented by the linear combination of the atoms from the learned occlusion dictionary and effectively separated from the occluded face image. The classification can thus be efficiently carried out on the recovered non-occluded face images and the size of the expanded dictionary is also much smaller than that used in SRC. The extensive experiments demonstrate that the proposed method achieves better results than the existing sparse representation based face recognition methods, especially in dealing with large region contiguous occlusion and severe illumination variation, while the computational cost is much lower. In biometrics research and industry, it is critical yet a challenge to match infrared face images to optical face images. The major difficulty lies in the fact that a great discrepancy exists between the infrared face image and corresponding optical face image because they are captured by different devices (optical imaging device and infrared imaging device). This paper presents a new approach called common feature discriminant analysis to reduce this great discrepancy and improve optical-infrared face recognition performance. In this approach, a new learning-based face descriptor is first proposed to extract the common features from heterogeneous face images (infrared face images and optical face images), and an effective matching method is then applied to the resulting features to obtain the final decision. Extensive experiments are conducted on two large and challenging optical-infrared face data sets to show the superiority of our approach over the state-of-the-art. In this paper, we study `networked bandits', a new bandit problem where a set of interrelated arms varies over time and, given the contextual information that selects one arm, invokes other correlated arms. This problem remains under-investigated, in spite of its applicability to many practical problems. For instance, in social networks, an arm can obtain payoffs from both the selected user and its relations since they often share the content through the network. We examine whether it is possible to obtain multiple payoffs from several correlated arms based on the relationships. In particular, we formalize the networked bandit problem and propose an algorithm that considers not only the selected arm, but also the relationships between arms. Our algorithm is `optimism in face of uncertainty' style, in that it decides an arm depending on integrated confidence sets constructed from historical data. We analyze the performance in simulation experiments and on two real-world offline datasets. The experimental results demonstrate our algorithm's effectiveness in the networked bandit setting. Latent variable models are powerful dimensionality reduction approaches in machine learning and pattern recognition. However, this kind of methods only works well under a necessary and strict assumption that the training samples and testing samples are independent and identically distributed. When the samples come from different domains, the distribution of the testing dataset will not be identical with the training dataset. Therefore, the performance of latent variable models will be degraded for the reason that the parameters of the training model do not suit for the testing dataset. This case limits the generalization and application of the traditional latent variable models. To handle this issue, a transfer learning framework for latent variable model is proposed which can utilize the distance (or divergence) of the two datasets to modify the parameters of the obtained latent variable model. So we do not need to rebuild the model and only adjust the parameters according to the divergence, which will adopt different datasets. Experimental results on several real datasets demonstrate the advantages of the proposed framework. Multi-frame image super-resolution (SR) aims to utilize information from a set of low-resolution (LR) images to compose a high-resolution (HR) one. As it is desirable or essential in many real applications, recent years have witnessed the growing interest in the problem of multi-frame SR reconstruction. This set of algorithms commonly utilizes a linear observation model to construct the relationship between the recorded LR images to the unknown reconstructed HR image estimates. Recently, regularization-based schemes have been demonstrated to be effective because SR reconstruction is actually an ill-posed problem. Working within this promising framework, this paper first proposes two new regularization items, termed as locally adaptive bilateral total variation and consistency of gradients, to keep edges and flat regions, which are implicitly described in LR images, sharp and smooth, respectively. Thereafter, the combination of the proposed regularization items is superior to existing regularization items because it considers both edges and flat regions while existing ones consider only edges. Thorough experimental results show the effectiveness of the new algorithm for SR reconstruction. We consider recovering d-level quantization of a signal from k-level quantization of linear measurements. This problem has great potential in practical systems, but has not been fully addressed in compressed sensing (CS). We tackle it by proposing k-bit Hamming compressed sensing (HCS). It reduces the decoding to a series of hypothesis tests of the bin where the signal lies in. Each test equals to an independent nearest neighbor search for a histogram estimated from quantized measurements. This method is based on that the distribution of the ratio between two random projections is defined by their intersection angle. Compared to CS and 1-bit CS, k-bit HCS leads to lower cost in both hardware and computation. It admits a trade-off between recovery/measurement resolution and measurement amount and thus is more flexible than 1-bit HCS. A rigorous analysis shows its error bound. Extensive empirical study further justifies its appealing accuracy, robustness and efficiency. Peripapillary atrophy (PPA) is an atrophy of pre-existing retina tissue. Because of its association with eye diseases such as myopia and glaucoma, PPA is an important indicator for diagnosis of these diseases. Experienced ophthalmologists are able to determine the presence of PPA using visual information from the retinal images. However, it is tedious, time consuming and subjective to examine all images especially in a screening program. This paper presents biologically inspired feature (BIF) for the automatic detection of PPA. BIF mimics the process of cortex for visual perception. In the proposed method, a focal region is segmented from the retinal image and the BIF is extracted. As BIF is an intrinsically low dimensional feature embedded in a high dimensional space, it is not suitable to measure the similarity between two BIFs directly based on the Euclidean distance. Therefore, it is necessary to obtain a suitable mapping to reduce the dimensionality. In this paper, we explore sparse transfer learning to transfer the label information from ophthalmologists to the sample distribution knowledge contained in all samples. Selective pair-wise discriminant analysis is used to define two strategies of sparse transfer learning: negative and positive sparse transfer learning. Experimental results show that negative sparse transfer learning is superior to the positive one for this task. The proposed BIF based approach achieves an accuracy of more than 90% in detecting PPA, much better than previous methods. It can be used to save the workload of ophthalmologists and thus reduce the diagnosis costs. Blind image quality assessment (BIQA) aims to predict perceptual image quality scores without access to reference images. State-of-the-art BIQA methods typically require subjects to score a large number of images to train a robust model. However, subjective quality scores are imprecise, biased, and inconsistent, and it is challenging to obtain a large-scale database, or to extend existing databases, because of the inconvenience of collecting images, training the subjects, conducting subjective experiments, and realigning human quality evaluations. To combat these limitations, this paper explores and exploits preference image pairs (PIPs) such as the quality of image ${oldsymbol {I}}_{oldsymbol {a}}$ is better than that of image ${oldsymbol {I}}_{oldsymbol {b}}$ for training a robust BIQA model. The preference label, representing the relative quality of two images, is generally precise and consistent, and is not sensitive to image content, distortion type, or subject identity; such PIPs can be generated at a very low cost. The proposed BIQA method is one of learning to rank. We first formulate the problem of learning the mapping from the image features to the preference label as one of classification. In particular, we investigate the utilization of a multiple kernel learning algorithm based on group lasso to provide a solution. A simple but effective strategy to estimate perceptual image quality scores is then presented. Experiments show that the proposed BIQA method is highly effective and achieves a performance comparable with that of state-of-the-art BIQA algorithms. Moreover, the proposed method can be easily extended to new distortion categories. The detection and identification of target pixels such as certain minerals and man-made objects from hyperspectral remote sensing images is of great interest for both civilian and military applications. However, due to the restriction in the spatial resolution of most airborne or satellite hyperspectral sensors, the targets often appear as subpixels in the hyperspectral image (HSI). The observed spectral feature of the desired target pixel (positive sample) is therefore a mixed signature of the reference target spectrum and the background pixels spectra (negative samples), which belong to various land cover classes. In this paper, we propose a novel supervised metric learning (SML) algorithm, which can effectively learn a distance metric for hyperspectral target detection, by which target pixels are easily detected in positive space while the background pixels are pushed into negative space as far as possible. The proposed SML algorithm first maximizes the distance between the positive and negative samples by an objective function of the supervised distance maximization. Then, by considering the variety of the background spectral features, we put a similarity propagation constraint into the SML to simultaneously link the target pixels with positive samples, as well as the background pixels with negative samples, which helps to reject false alarms in the target detection. Finally, a manifold smoothness regularization is imposed on the positive samples to preserve their local geometry in the obtained metric. Based on the public data sets of mineral detection in an Airborne Visible/Infrared Imaging Spectrometer image and fabric and vehicle detection in a Hyperspectral Mapper image, quantitative comparisons of several HSI target detection methods, as well as some state-of-the-art metric learning algorithms, were performed. All the experimental results demonstrate the effectiveness of the proposed SML algorithm for hyperspectral target detection. We propose a novel method to model human actions by explicitly coding motion and structure features that are separately extracted from video sequences. Firstly, the motion template (one feature map) is applied to encode the motion information and image planes (five feature maps) are extracted from the volume of differences of frames to capture the structure information. The Gaussian pyramid and center-surround operations are performed on each of the six obtained feature maps, decomposing each feature map into a set of subband maps. Biologically inspired features are then extracted by successively applying Gabor filtering and max pooling on each subband map. To make a compact representation, discriminative locality alignment is employed to embed the high-dimensional features into a low-dimensional manifold space. In contrast to sparse representations based on detected interest points, which suffer from the loss of structure information, the proposed model takes into account the motion and structure information simultaneously and integrates them in a unified framework; it therefore provides an informative and compact representation of human actions. The proposed method is evaluated on the KTH, the multiview IXMAS, and the challenging UCF sports datasets and outperforms state-of-the-art techniques on action recognition. In this paper, we propose a new algorithm for partitioning human posture represented by 3D point clouds sampled from the surface of human body. The algorithm is formed as a constrained extension of the recently developed segmentation method, spectral clustering (SC). Two folds of merits are offered by the algorithm: (1) as a nonlinear method, it is able to deal with the situation that data (point cloud) are sampled from a manifold (the surface of human body) rather than the embedded entire 3D space; (2) by using constraints, it facilitates the integration of multiple similarities for human posture partitioning, and it also helps to reduce the limitations of spectral clustering. We show that the constrained spectral clustering (CSC) still can be solved by generalized eigen-decomposition. Experimental results confirm the effectiveness of the proposed algorithm. Semi-supervised clustering aims at boosting the clustering performance on unlabeled samples by using labels from a few labeled samples. Constrained NMF (CNMF) is one of the most significant semi-supervised clustering methods, and it factorizes the whole dataset by NMF and constrains those labeled samples from the same class to have identical encodings. In this paper, we propose a novel soft-constrained NMF (SCNMF) method by softening the hard constraint in CNMF. Particularly, SCNMF factorizes the whole dataset into two lower-dimensional factor matrices by using multiplicative update rule (MUR). To utilize the labels of labeled samples, SCNMF iteratively normalizes both factor matrices after updating them with MURs to make encodings of labeled samples close to their label vectors. It is therefore reasonable to believe that encodings of unlabeled samples are also close to their corresponding label vectors. Such strategy significantly boosts the clustering performance even when the labeled samples are rather limited, e.g., each class owns only a single labeled sample. Since the normalization procedure never increases the computational complexity of MUR, SCNMF is quite efficient and effective in practices. Experimental results on face image datasets illustrate both efficiency and effectiveness of SCNMF compared with both NMF and CNMF. Many existing results on statistical learning theory are based on the assumption that samples are independently and identically distributed (i.i.d.). However, the assumption of i.i.d. samples is not suitable for practical application to problems in which samples are time dependent. In this paper, we are mainly concerned with the empirical risk minimization (ERM) based learning process for time-dependent samples drawn from a continuous-time Markov chain. This learning process covers many kinds of practical applications, e.g., the prediction for a time series and the estimation of channel state information. Thus, it is significant to study its theoretical properties including the generalization bound, the asymptotic convergence, and the rate of convergence. It is noteworthy that, since samples are time dependent in this learning process, the concerns of this paper cannot (at least straightforwardly) be addressed by existing methods developed under the sample i.i.d. assumption. We first develop a deviation inequality for a sequence of time-dependent samples drawn from a continuous-time Markov chain and present a symmetrization inequality for such a sequence. By using the resultant deviation inequality and symmetrization inequality, we then obtain the generalization bounds of the ERM-based learning process for time-dependent samples drawn from a continuous-time Markov chain. Finally, based on the resultant generalization bounds, we analyze the asymptotic convergence and the rate of convergence of the learning process. Hand gesture recognition has been intensively applied in various human-computer interaction (HCI) systems. Different hand gesture recognition methods were developed based on particular features, e.g., gesture trajectories and acceleration signals. However, it has been noticed that the limitation of either features can lead to flaws of a HCI system. In this paper, to overcome the limitations but combine the merits of both features, we propose a novel feature fusion approach for 3D hand gesture recognition. In our approach, gesture trajectories are represented by the intersection numbers with randomly generated line segments on their 2D principal planes, acceleration signals are represented by the coefficients of discrete cosine transformation (DCT). Then, a hidden space shared by the two features is learned by using penalized maximum likelihood estimation (MLE). An iterative algorithm, composed of two steps per iteration, is derived to for this penalized MLE, in which the first step is to solve a standard least square problem and the second step is to solve a Sylvester equation. We tested our hand gesture recognition approach on different hand gesture sets. Results confirm the effectiveness of the feature fusion method. Manifold learning has been demonstrated as an effective way to represent intrinsic geometrical structure of samples. In this paper, a new manifold learning approach, named Local Coordinates Alignment (LCA), is developed based on the alignment technique. LCA first obtains local coordinates as representations of local neighborhood by preserving proximity relations on a patch, which is Euclidean. Then, these extracted local coordinates are aligned to yield the global embeddings. To solve the out of sample problem, linearization of LCA (LLCA) is proposed. In addition, in order to solve the non-Euclidean problem in real world data when building the locality, kernel techniques are utilized to represent similarity of the pairwise points on a local patch. Empirical studies on both synthetic data and face image sets show effectiveness of the developed approaches. Motion deblurring is a challenging problem in computer vision. Most previous blind deblurring approaches usually assume that the Point Spread Function (PSF) is spatially invariant. However, non-uniform motions exist ubiquitously and cannot be handled successfully. In this paper, we present an automatic method for object motion deblurring based on non-uniform motion information from video. First, the feature points of the object are tracked throughout a video sequence. Then, the object motion between frames is estimated and the circular blurring paths (i.e. PSFs) of each point are computed along the linear moving path in polar coordinates. Finally, an alpha matte of the blurred object is extracted to separate the foreground from the background, and an iterative Richardson-Lucy algorithm is carried out on the foreground using the obtained blurring paths. Experimental results show our proposed approach outperforms the state-of-the-art motion deblurring algorithms. Principal component analysis (PCA) computes a succinct data representation by converting the data to a few new variables while retaining maximum variation. However, the new variables are difficult to interpret, because each one is combined with all of the original input variables and has obscure semantics. Under the umbrella of Bayesian data analysis, this paper presents a new prior to explicitly regularize combinations of input variables. In particular, the prior penalizes pair-wise products of the coefficients of PCA and encourages a sparse model. Compared to the commonly used l1-regularizer, the proposed prior encourages the sparsity pattern in the resultant coefficients to be consistent with the intrinsic groups in the original input variables. Moreover, the proposed prior can be explained as recovering a robust estimation of the covariance matrix for PCA. The proposed model is suited for analyzing visual data, where it encourages the output variables to correspond to meaningful parts in the data. We demonstrate the characteristics and effectiveness of the proposed technique through experiments on both synthetic and real data. The analysis of gene expression data obtained from microarray experiments is important for discovering the biological process of genes. Biclustering algorithms have been proven to be able to group the genes with similar expression patterns under a number of experimental conditions. In this paper, we propose a new biclustering algorithm based on evolutionary learning. By converting the biclustering problem into a common clustering problem, the algorithm can be applied in a search space constructed by the conditions. To further reduce the size of the search space, we randomly separate the full conditions into a number of condition subsets (subspaces), each of which has a smaller number of conditions. The algorithm is applied to each subspace and is able to discover bicluster seeds within a limited computing time. Finally, an expanding and merging procedure is employed to combine the bicluster seeds into larger biclusters according to a homogeneity criterion. We test the performance of the proposed algorithm using synthetic and real microarray data sets. Compared with several previously developed biclustering algorithms, our algorithm demonstrates a significant improvement in discovering additive biclusters. A study of the development of visual reranking methods can facilitate an understanding of the field, offer a clearer view of what has been achieved, and help overcome emerging obstacles in this area. Pedestrian detection is one of the fundamental tasks of an intelligent transportation system. Differences in illumination, posture and point of view make pedestrian detection confront with great challenges. In this paper, we focus on the main defect in the existing methods: the interference of the non-person area. Firstly, we use mapping vectors to map the original feature matrix to the different mask spaces, then using a part-based structure, we implicitly formulate the model into a multiple-instance problem, and finally use a MIL-SVM to solve the problem. Based on the model, we design a system which can find pedestrians from pictures. We give detailed description on the model and the system in this paper. The experimental results on public data sets show that our method decreases the miss rate greatly. The neighbor-embedding (NE) algorithm for single-image super-resolution (SR) reconstruction assumes that the feature spaces of low-resolution (LR) and high-resolution (HR) patches are locally isometric. However, this is not true for SR because of one-to-many mappings between LR and HR patches. To overcome or at least to reduce the problem for NE-based SR reconstruction, we apply a joint learning technique to train two projection matrices simultaneously and to map the original LR and HR feature spaces onto a unified feature subspace. Subsequently, the k -nearest neighbor selection of the input LR image patches is conducted in the unified feature subspace to estimate the reconstruction weights. To handle a large number of samples, joint learning locally exploits a coupled constraint by linking the LR-HR counterparts together with the K-nearest grouping patch pairs. In order to refine further the initial SR estimate, we impose a global reconstruction constraint on the SR outcome based on the maximum a posteriori framework. Preliminary experiments suggest that the proposed algorithm outperforms NE-related baselines. View-based 3-D object retrieval and recognition has become popular in practice, e.g., in computer aided design. It is difficult to precisely estimate the distance between two objects represented by multiple views. Thus, current view-based 3-D object retrieval and recognition methods may not perform well. In this paper, we propose a hypergraph analysis approach to address this problem by avoiding the estimation of the distance between objects. In particular, we construct multiple hypergraphs for a set of 3-D objects based on their 2-D views. In these hypergraphs, each vertex is an object, and each edge is a cluster of views. Therefore, an edge connects multiple vertices. We define the weight of each edge based on the similarities between any two views within the cluster. Retrieval and recognition are performed based on the hypergraphs. Therefore, our method can explore the higher order relationship among objects and does not use the distance between objects. We conduct experiments on the National Taiwan University 3-D model dataset and the ETH 3-D object collection. Experimental results demonstrate the effectiveness of the proposed method by comparing with the state-of-the-art methods. The broadcast soccer video is usually recorded by one main camera, which is constantly gazing somewhere of playfield where a highlight event is happening. So the camera parameters and their variety have close relationship with semantic information of soccer video, and much interest has been caught in camera calibration for soccer video. The previous calibration methods either deal with goal scene, or have strict calibration conditions and high complexity. So, it does not properly handle the non-goal scene such as midfield or center-forward scene. In this paper, based on a new soccer field model, a field symbol extraction algorithm is proposed to extract the calibration information. Then a two-stage calibration approach is developed which can calibrate camera not only for goal scene but also for non-goal scene. The preliminary experimental results demonstrate its robustness and accuracy. Visual search reranking involves an optimization process that uses visual content to recover the ''genuine'' ranking list from the helpful but noisy one generated by textual search. This paper presents an evolutionary approach, called Adaptive Particle Swarm Optimization (APSO), for unsupervised visual search reranking. The proposed approach incorporates the visual consistency regularization and the ranking list distance. In addition, to address the problem that existing list distance fails to capture the genuine disagreement between two ranking lists, we propose a numerical ranking list distance. Furthermore, the parameters in APSO are self-tuned adaptively according to the fitness values of the particles to avoid being trapped in local optima. We conduct extensive experiments on automatic search task over TRECVID 2006-2007 benchmarks and show significant and consistent improvements over state-of-the-art works. Existing image retrieval systems suffer from a radical performance variance for different queries. The bad initial search results for \\\"difficult\\\" queries may greatly degrade the performance of their subsequent refinements, especially the refinement that utilizes the information mined from the search results, e.g., pseudo relevance feedback based reranking. In this paper, we tackle this problem by proposing a query difficulty guided image retrieval system, which selectively performs reranking according to the estimated query difficulty. To improve the performance of both reranking and difficulty estimation, we apply multiview embedding (ME) to images represented by multiple different features for integrating a joint subspace by preserving the neighborhood information in each feature space. However, existing ME approaches suffer from both \\\"out of sample\\\" and huge computational cost problems, and cannot be applied to online reranking or offline large-scale data processing for practical image retrieval systems. Therefore, we propose a linear multiview embedding algorithm which learns a linear transformation from a small set of data and can effectively infer the subspace features of new data. Empirical evaluations on both Oxford and 500K ImageNet datasets suggest the effectiveness of the proposed difficulty guided retrieval system with LME. In the 3D facial animation and synthesis community, input faces are usually required to be labeled by a set of landmarks for parameterization. Because of the variations in pose, expression and resolution, automatic 3D face landmark localization remains a challenge. In this paper, a novel landmark localization approach is presented. The approach is based on local coordinate coding (LCC) and consists of two stages. In the first stage, we perform nose detection, relying on the fact that the nose shape is usually invariant under the variations in the pose, expression, and resolution. Then, we use the iterative closest points algorithm to find a 3D affine transformation that aligns the input face to a reference face. In the second stage, we perform resampling to build correspondences between the input 3D face and the training faces. Then, an LCC-based localization algorithm is proposed to obtain the positions of the landmarks in the input face. Experimental results show that the proposed method is comparable to state of the art methods in terms of its robustness, flexibility, and accuracy. It was recently highlighted in a special issue of Nature [1] that the value of big data has yet to be effectively exploited for innovation, competition and productivity. To realize the full potential of big data, big learning algorithms need to be developed to keep pace with the continuous creation, storage and sharing of data. Least squares (LS) and least absolute deviation (LAD) have been successful regression tools used in business, government and society over the past few decades. However, these existing technologies are severely limited by noisy data because their breakdown points are both zero, i.e., they do not tolerate outliers. By appropriately setting the turning constant of Cauchy regression (CR), the maximum possible value (50%) of the breakdown point can be attained. CR therefore has the capability to learn a robust model from noisy big data. Although the theoretical analysis of the breakdown point for CR has been comprehensively investigated, we propose a new approach by interpreting the optimization of an objective function as a sample-weighted procedure. We therefore clearly show the differences of the robustness between LS, LAD and CR. We also study the statistical performance of CR. This study derives the generalization error bounds for CR by analyzing the covering number and Rademacher complexity of the hypothesis class, as well as showing how the scale parameter affects its performance. Gesture recognition plays an important role in human machine interactions (HMIs) for multimedia entertainment. In this paper, we present a dimension reduction based approach for dynamic real-time hand gesture recognition. The hand gestures are recorded as acceleration signals by using a handheld with a 3-axis accelerometer sensor installed, and represented by discrete cosine transform (DCT) coefficients. To recognize different hand gestures, we develop a new dimension reduction method, locally regularized sliced inverse regression (LR-SIR), to find an effective low dimensional subspace, in which different hand gestures are well separable, following which recognition can be performed by using simple and efficient classifiers, e.g., nearest mean, k-nearest-neighbor rule and support vector machine. LR-SIR is built upon the well-known sliced inverse regression (SIR), but overcomes its limitation that it ignores the local geometry of the data distribution. Besides, LR-SIR can be effectively and efficiently solved by eigen-decomposition. Finally, we apply the LR-SIR based gesture recognition to control our recently developed dance robot for multimedia entertainment. Thorough empirical studies on 'digits'-gesture recognition suggest the effectiveness of the new gesture recognition scheme for HMI. Considering that the Human Visual System (HVS) has different perceptual characteristics for different morphological components, a novel image quality metric is proposed by incorporating Morphological Component Analysis (MCA) and HVS, which is capable of assessing the image with different kinds of distortion. Firstly, reference and distorted images are decomposed into linearly combined texture and cartoon components by MCA respectively. Then these components are turned into perceptual features by Just Noticeable Difference (JND) which integrates masking features, luminance adaptation and Contrast Sensitive Function (CSF). Finally, the discrimination between reference and distorted images perceptual features is quantified using a pooling strategy before the final image quality is obtained. Experimental results demonstrate that the performance of the proposed prevails over some existing methods on LIVE database II. Existing image retrieval systems suffer from a performance variance for different queries. Severe performance variance may greatly degrade the effectiveness of the subsequent query-dependent ranking optimization algorithms, especially those that utilize the information mined from the initial search results. In this paper, we tackle this problem by proposing a query difficulty guided image retrieval system, which can predict the queries' ranking performance in terms of their difficulties and adaptively apply ranking optimization approaches. We estimate the query difficulty by comprehensively exploring the information residing in the query image, the retrieval results, and the target database. To handle the high-dimensional and multi-model image features in the large-scale image retrieval setting, we propose a linear multiple feature embedding algorithm which learns a linear transformation from a small set of data by integrating a joint subspace in which the neighborhood information is preserved. The transformation can be effectively and efficiently used to infer the subspace features of the newly observed data in the online setting. We prove the significance of query difficulty to image retrieval by applying it to guide the conduction of three retrieval refinement applications, i.e., reranking, federated search, and query suggestion. Thorough empirical studies on three datasets suggest the effectiveness and scalability of the proposed image query difficulty estimation algorithm, as well as the promising of the image difficulty guided retrieval system. With the explosive growth of the use of imagery, visual recognition plays an important role in many applications and attracts increasing research attention. Given several related tasks, single-task learning learns each task separately and ignores the relationships among these tasks. Different from single-task learning, multi-task learning can explore more information to learn all tasks jointly by using relationships among these tasks. In this paper, we propose a novel multi-task learning model based on the proximal support vector machine. The proximal support vector machine uses the large-margin idea as does the standard support vector machines but with looser constraints and much lower computational cost. Our multi-task proximal support vector machine inherits the merits of the proximal support vector machine and achieves better performance compared with other popular multi-task learning models. Experiments are conducted on several multi-task learning datasets, including two classification datasets and one regression dataset. All results demonstrate the effectiveness and efficiency of our proposed multi-task proximal support vector machine. HighlightsPropose highly efficient multi-task proximal support vector machine (MTPSVM).Develop a method to optimize the learning procedure of MTPSVM.Unbalanced MTPSVM is proposed to deal with the unbalanced sample problem.Propose proximal support vector regression (SVR) and multi-task proximal SVR.Extensive experiments demonstrate the effectiveness and efficiency of our MTPSVM. The well-known bag-of-features (BoF) model is widely utilized for large scale image retrieval. However, BoF model lacks the spatial information of visual words, which is informative for local features to build up meaningful visual patches. To compensate for the spatial information loss, in this paper, we propose a novel query expansion method called Spatial Co-occurrence Query Expansion (SCQE), by utilizing the spatial co-occurrence information of visual words mined from the database images to boost the retrieval performance. In offline phase, for each visual word in the vocabulary, we treat the visual words that are frequently co-occurred with it in the database images as neighbors, base on which a spatial co-occurrence graph is built. In online phase, a query image can be expanded with some spatial co-occurred but unseen visual words according to the spatial co-occurrence graph, and the retrieval performance can be improved by expanding these visual words appropriately. Experimental results demonstrate that, SCQE achieves promising improvements over the typical BoF baseline on two datasets comprising 5K and 505K images respectively. Tag-based social image search has attracted great interest and how to order the search results based on relevance level is a research problem. Visual content of images and tags have both been investigated. However, existing methods usually employ tags and visual content separately or sequentially to learn the image relevance. This paper proposes a tag-based image search with visual-text joint hypergraph learning. We simultaneously investigate the bag-of-words and bag-of-visual-words representations of images and accomplish the relevance estimation with a hypergraph learning approach. Each textual or visual word generates a hyperedge in the constructed hypergraph. We conduct experiments with a real-world data set and experimental results demonstrate the effectiveness of our approach. Glaucoma is a chronic eye disease that leads to vision loss. As it cannot be cured, detecting the disease in time is important. Current tests using intraocular pressure (IOP) are not sensitive enough for population based glaucoma screening. Optic nerve head assessment in retinal fundus images is both more promising and superior. This paper proposes optic disc and optic cup segmentation using superpixel classification for glaucoma screening. In optic disc segmentation, histograms, and center surround statistics are used to classify each superpixel as disc or non-disc. A self-assessment reliability score is computed to evaluate the quality of the automated optic disc segmentation. For optic cup segmentation, in addition to the histograms and center surround statistics, the location information is also included into the feature space to boost the performance. The proposed segmentation methods have been evaluated in a database of 650 images with optic disc and optic cup boundaries manually marked by trained professionals. Experimental results show an average overlapping error of 9.5% and 24.1% in optic disc and optic cup segmentation, respectively. The results also show an increase in overlapping error as the reliability score is reduced, which justifies the effectiveness of the self-assessment. The segmented optic disc and optic cup are then used to compute the cup to disc ratio for glaucoma screening. Our proposed method achieves areas under curve of 0.800 and 0.822 in two data sets, which is higher than other methods. The methods can be used for segmentation and glaucoma screening. The self-assessment will be used as an indicator of cases with large errors and enhance the clinical deployment of the automatic segmentation and screening. Auroral oval segmentation from ultraviolet imager images is of significance in the field of spatial physics. Compared with various existing image segmentation methods, level set is a promising auroral oval segmentation method with satisfactory precision. However, the traditional level set methods are time consuming, which is not suitable for the processing of large aurora image database. For this purpose, an improving level set method is proposed for fast auroral oval segmentation. The proposed algorithm combines four strategies to solve the four problems leading to the high-time complexity. The first two strategies, including our shape knowledge-based initial evolving curve and neighbor embedded level set formulation, can not only accelerate the segmentation process but also improve the segmentation accuracy. And then, the latter two strategies, including the universal lattice Boltzmann method and sparse field method, can further reduce the time cost with an unlimited time step and narrow band computation. Experimental results illustrate that the proposed algorithm achieves satisfactory performance for auroral oval segmentation within a very short processing time. Abstract Partial least squares (PLS) regression has achieved desirable performance for modeling the relationship between a set of dependent (response) variables with another set of independent (predictor) variables, especially when the sample size is small relative to the dimension of these variables. In each iteration, PLS finds two latent variables from a set of dependent and independent variables via maximizing the product of three factors: variances of the two latent variables as well as the square of the correlation between these two latent variables. In this paper, we derived the mathematical formulation of the relationship between mean square error (MSE) and these three factors. We find that MSE is not monotonous with the product of the three factors. However, the corresponding optimization problem is difficult to solve if we extract the optimal latent variables directly based on this relationship. To address these problems, a novel multilinear regression model-variance constrained partial least squares (VCPLS) is proposed. In the proposed VCPLS, we find the latent variables via maximizing the product of the variance of latent variable from dependent variables and the square of the correlation between the two latent variables, while constraining the variance of the latent variable from independent variables must be larger than a predetermined threshold. The corresponding optimization problem can be solved computational efficiently, and the latent variables extracted by VCPLS are near-optimal. Compared with classical PLS and it is variants, VCPLS can achieve lower prediction error in the sense of MSE. The experiments are conducted on three near-infrared spectroscopy (NIR) data sets. To demonstrate the applicability of our proposed VCPLS, we also conducted experiments on another data set, which has different characteristics from NIR data. Experimental results verified the superiority of our proposed VCPLS. Bag-of-visual-words (BOVW) based image representation has received intense attention in recent years and has improved content based image retrieval (CBIR) significantly. BOVW does not consider the spatial correlation between visual words in natural images and thus biases the generated visual words towards noise when the corresponding visual features are not stable. In this paper, we construct a visual word co-occurrence table by exploring visual word co-occurrence extracted from small affine-invariant regions in a large collection of natural images. Based on this visual word co-occurrence table, we first present a novel high-order predictor to accelerate the generation of neighboring visual words. A co-occurrence matrix is introduced to refine the similarity measure for image ranking. Like the inverse document frequency (idf), it down-weights the contribution of the words that are less discriminative because of frequent co-occurrence. We conduct experiments on Oxford and Paris Building datasets, in which the ImageNet dataset is used to implement a large scale evaluation. Thorough experimental results suggest that our method outperforms the state-of-the-art, especially when the vocabulary size is comparatively small. In addition, our method is not much more costly than the BOVW model. An auroral substorm is an important geophysical phenomenon that reflects the interaction between the solar wind and the Earth\\u2019s magnetosphere. Detecting substorms is of practical significance in order to prevent disruption to communication and global positioning systems. However, existing detection methods can be inaccurate or require time-consuming manual analysis and are therefore impractical for large-scale data sets. In this paper, we propose an automatic auroral substorm detection method based on a shape-constrained sparse and low-rank decomposition (SCSLD) framework. Our method automatically detects real substorm onsets in large-scale aurora sequences, which overcomes the limitations of manual detection. To reduce noise interference inherent in current SLD methods, we introduce a shape constraint to force the noise to be assigned to the low-rank part (stationary background), thus ensuring the accuracy of the sparse part (moving object) and improving the performance. Experiments conducted on aurora sequences in solar cycle 23 (1996\\u20132008) show that the proposed SCSLD method achieves good performance for motion analysis of aurora sequences. Moreover, the obtained results are highly consistent with manual analysis, suggesting that the proposed automatic method is useful and effective in practice. It is well known that robust lossless data embedding (RLDE) methods can be used to protect copyright of digital images when the intactness of host images is highly demanded and the unintentional attacks may be encountered in data communication. However, the existing RLDE methods cannot be applied satisfactorily to the practical scenarios due to different drawbacks, e.g., serious ''salt-and-pepper'' noise, low capacity and unreliable reversibility. In this paper, we propose an effective solution to RLDE by improving the histogram rotation (HR)-based embedding model. The proposed method is a content-adaptive reliable RLDE or CAR for short. It eliminates the ''salt-and-pepper'' noise in HR by the pixel adjustment mechanism. Therefore, reliable regions for embedding can be well constructed. Furthermore, we basically expect the watermark strengths to be adaptive to different image contents, and thus we have a chance to make an effective tradeoff between invisibility and robustness. The luminance masking together with the threshold strategy is duly adopted in the proposed RLDE method, so the just noticeable distortion thresholds of different local regions can be well utilized to control the watermark strengths. Experimental evidence on 300 test images including natural, medical and synthetic aperture radar (SAR) images demonstrates the effectiveness of the proposed data embedding method. Universal blind image quality assessment (IQA) metrics that can work for various distortions are of great importance for image processing systems, because neither ground truths are available nor the distortion types are aware all the time in practice. Existing state-of-the-art universal blind IQA algorithms are developed based on natural scene statistics (NSS). Although NSS-based metrics obtained promising performance, they have some limitations: 1) they use either the Gaussian scale mixture model or generalized Gaussian density to predict the nonGaussian marginal distribution of wavelet, Gabor, or discrete cosine transform coefficients. The prediction error makes the extracted features unable to reflect the change in nonGaussianity (NG) accurately. The existing algorithms use the joint statistical model and structural similarity to model the local dependency (LD). Although this LD essentially encodes the information redundancy in natural images, these models do not use information divergence to measure the LD. Although the exponential decay characteristic (EDC) represents the property of natural images that large/small wavelet coefficient magnitudes tend to be persistent across scales, which is highly correlated with image degradations, it has not been applied to the universal blind IQA metrics; and 2) all the universal blind IQA metrics use the same similarity measure for different features for learning the universal blind IQA metrics, though these features have different properties. To address the aforementioned problems, we propose to construct new universal blind quality indicators using all the three types of NSS, i.e., the NG, LD, and EDC, and incorporating the heterogeneous property of multiple kernel learning (MKL). By analyzing how different distortions affect these statistical properties, we present two universal blind quality assessment models, NSS global scheme and NSS two-step scheme. In the proposed metrics: 1) we exploit the NG of natural images using the original marginal distribution of wavelet coefficients; 2) we measure correlations between wavelet coefficients using mutual information defined in information theory; 3) we use features of EDC in universal blind image quality prediction directly; and 4) we introduce MKL to measure the similarity of different features using different kernels. Thorough experimental results on the Laboratory for Image and Video Engineering database II and the Tampere Image Database2008 demonstrate that both metrics are in remarkably high consistency with the human perception, and overwhelm representative universal blind algorithms as well as some standard full reference quality indexes for various types of distortions. Dimension reduction algorithms have attracted a lot of attentions in face recognition because they can select a subset of effective and efficient discriminative features in the face images. Most of dimension reduction algorithms can not well model both the intra-class geometry and interclass discrimination simultaneously. In this paper, we introduce the Discriminative Hessian Eigenmaps (DHE), a novel dimension reduction algorithm to address this problem. DHE will consider encoding the geometric and discriminative information in a local patch by improved Hessian Eigenmaps and margin maximization respectively. Empirical studies on public face database thoroughly demonstrate that DHE is superior to popular algorithms for dimension reduction, e.g., FLDA, LPP, MFA and DLA. In multimedia retrieval, multi-label annotation for image, text and video is challenging and attracts rapidly growing interests in past decades. The main crux of multi-label annotation lies on 1) how to reduce the model complexity when the label space expands exponentially with the increase of the number of labels; and 2) how to leverage the label correlations which have broadly believed useful for boosting annotation performance. In this paper, we propose \\\"labelsets anchored subspace ensemble (LASE)\\\" to solve both problems in an efficient scheme, whose training is a regularized matrix decomposition and prediction is an inference of group sparse representations. In order to shrink the label space, we firstly introduce \\\"label distilling\\\" extracting the frequent labelsets to replace the original labels. In the training stage, the data matrix is decomposed as the sum of several low-rank matrices and a sparse residual via a randomized optimization, where each low-rank part defines a feature subspace mapped by a labelset. A manifold regularization is applied to map the labelset geometry to the geometry of the obtained subspaces. In the prediction stage, the group sparse representation of a new sample on the subspace ensemble is estimated by group lasso. The selected subspaces indicate the labelsets that the sample should be annotated with. Experiments on several benchmark datasets of texts, images, web data and videos validate the appealing performance of LASE in multi-label annotation. 3-D facial expression generation, including synthesis and retargeting, has received intensive attentions in recent years, because it is important to produce realistic 3-D faces with specific expressions in modern film production and computer games. In this paper, we present joint sparse learning (JSL) to learn mapping functions and their respective inverses to model the relationship between the high-dimensional 3-D faces (of different expressions and identities) and their corresponding low-dimensional representations. Based on JSL, we can effectively and efficiently generate various expressions of a 3-D face by either synthesizing or retargeting. Furthermore, JSL is able to restore 3-D faces with holes by learning a mapping function between incomplete and intact data. Experimental results on a wide range of 3-D faces demonstrate the effectiveness of the proposed approach by comparing with representative ones in terms of quality, time cost, and robustness. Effective representation of image texture is important for an image-classification task. Statistical modelling in wavelet domains has been widely used to image texture representation. However, due to the intraclass complexity and interclass diversity of textures, it is hard to use a predefined probability distribution function to fit adaptively all wavelet subband coefficients of different textures. In this article, we propose a novel modelling approach, Heterogeneous and Incrementally Generated Histogram (HIGH), to indirectly model the wavelet coefficients by use of four local features in wavelet subbands. By concatenating all the HIGHs in all wavelet subbands of a texture, we can construct a nonnegative multiresolution vector (NMV) to represent a texture image. Considering the NMV\\u2019s high dimensionality and nonnegativity, we further propose a Hessian regularized discriminative nonnegative matrix factorization to compute a low-dimensional basis of the linear subspace of NMVs. Finally, we present a texture classification approach by projecting NMVs on the low-dimensional basis. Experimental results show that our proposed texture classification method outperforms seven representative approaches. Recently, we have witnessed a surge of interests of learning a low-dimensional subspace for scene classification. The existing methods do not perform well since they do not consider scenes' multiple features from different views in low-dimensional subspace construction. In this paper, we describe scene images by finding a group of features and explore their complementary characteristics. We consider the problem of multiview dimensionality reduction by learning a unified low-dimensional subspace to effectively fuse these features. The new proposed method takes both intraclass and interclass geometries into consideration, as a result the discriminability is effectively preserved because it takes into account neighboring samples which have different labels. Due to the semantic gap, the fusion of multiview features still cannot achieve excellent performance of scene classification in real applications. Therefore, a user labeling procedure is introduced in our approach. Initially, a query image is provided by the user, and a group of images are retrieved by a search engine. After that, users label some images in the retrieved set as relevant or irrelevant with the query. The must-links are constructed between the relevant images, and the cannot-links are built between the irrelevant images. Finally, an alternating optimization procedure is adopted to integrate the complementary nature of different views with the user labeling information, and develop a novel multiview dimensionality reduction method for scene classification. Experiments are conducted on the real-world datasets of natural scenes and indoor scenes, and the results demonstrate that the proposed method has the best performance in scene classification. In addition, the proposed method can be applied to other classification problems. The experimental results of shape classification on Caltech 256 suggest the effectiveness of our method. Highlights? We describe scene images through multimodal features. ? We explore the complementary characteristics of these features. ? We address multimodal dimensionality reduction by learning a unified low-dimensional subspace. ? We adopt alternating optimization to integrate the features with the user labeling information. ? We develop a novel multimodal dimensionality reduction method for scene classification. Various sparse-representation-based methods have been proposed to solve tracking problems, and most of them employ least squares (LSs) criteria to learn the sparse representation. In many tracking scenarios, traditional LS-based methods may not perform well owing to the presence of heavy-tailed noise. In this paper, we present a tracking approach using an approximate least absolute deviation (LAD)-based multitask multiview sparse learning method to enjoy robustness of LAD and take advantage of multiple types of visual features, such as intensity, color, and texture. The proposed method is integrated in a particle filter framework, where learning the sparse representation for each view of the single particle is regarded as an individual task. The underlying relationship between tasks across different views and different particles is jointly exploited in a unified robust multitask formulation based on LAD. In addition, to capture the frequently emerging outlier tasks, we decompose the representation matrix to two collaborative components that enable a more robust and accurate approximation. We show that the proposed formulation can be effectively approximated by Nesterov\\u2019s smoothing method and efficiently solved using the accelerated proximal gradient method. The presented tracker is implemented using four types of features and is tested on numerous synthetic sequences and real-world video sequences, including the CVPR2013 tracking benchmark and ALOV++ data set. Both the qualitative and quantitative results demonstrate the superior performance of the proposed approach compared with several state-of-the-art trackers. Stroke correspondence construction is a precondition for generating inbetween frames from a set of key frames. In our case, each stroke in a key frame is a vector represented as a Disk B-Spline Curve (DBSC) which is a flexible and compact vector format. However, it is not easy to construct correspondences between multiple DBSC strokes effectively because of the following points: (1) with the use of shape descriptors, the dimensionality of the feature space is high; (2) the number of strokes in different key frames is usually large and different from each other and (3) the length of corresponding strokes can be very different. The first point makes matching difficult. The other two points imply \\u2018many to many\\u2019 and \\u2018part to whole\\u2019 correspondences between strokes. To solve these problems, this paper presents a DBSC stroke correspondence construction approach, which introduces a manifold learning technique to the matching process. Moreover, in order to handle the mapping between unequal numbers of strokes with different lengths, a stroke reconstruction algorithm is developed to convert the \\u2018many to many\\u2019 and \\u2018part to whole\\u2019 stroke correspondences to \\u2018one to one\\u2019 compound stroke correspondence. Under the homoscedastic Gaussian assumption, it has been shown that Fisherpsilas linear discriminant analysis (FLDA) suffers from the class separation problem when the dimensionality of subspace selected by FLDA is strictly less than the class number minus 1, i.e., the projection to a subspace tends to merge close class pairs. A recent result shows that maximizing the geometric mean of Kullback-Leibler (KL) divergences of class pairs can significantly reduce this problem. In this paper, to further reduce the class separation problem, the harmonic mean is applied to replace the geometric mean for subspace selection. The new method is termed maximization of the harmonic mean of all pairs of symmetric KL divergences (MHMD). As MHMD is invariant to rotational transformations, an efficient optimization procedure can be conducted on the Grassmann manifold. Thorough empirical studies demonstrate the effective of harmonic mean in dealing with the class separation problem. Dimension reduction algorithms have attracted a lot of attentions in face recognition and human gait recognition because they can select a subset of effective and efficient discriminative features. In this paper, we apply the Discriminative Geometry Preserving Projections (DGPP), a new subspace learning algorithm to address these problems. DGPP models both the intraclass geometry and interclass discrimination. Meanwhile, DGPP will not meet the undersampled problem. Thoroughly empirical studies on YALE face database, UMIST face database, FERET face database and USF Human-ID gait database demonstrate that DGPP is superior the popular algorithms for dimension reduction, e.g., PCA, LDA, NPE and LPP. It has been widely acknowledged that learning- and reconstruction-based super-resolution (SR) methods are effective to generate a high-resolution (HR) image from a single low-resolution (LR) input. However, learning-based methods are prone to introduce unexpected details into resultant HR images. Although reconstruction-based methods do not generate obvious artifacts, they tend to blur fine details and end up with unnatural results. In this paper, we propose a new SR framework that seamlessly integrates learning- and reconstruction-based methods for single image SR to: 1) avoid unexpected artifacts introduced by learning-based SR and 2) restore the missing high-frequency details smoothed by reconstruction-based SR. This integrated framework learns a single dictionary from the LR input instead of from external images to hallucinate details, embeds nonlocal means filter in the reconstruction-based SR to enhance edges and suppress artifacts, and gradually magnifies the LR input to the desired high-quality SR result. We demonstrate both visually and quantitatively that the proposed framework produces better results than previous methods from the literature. In this paper, we present the manifold elastic net (MEN) for sparse variable selection. MEN combines merits of the manifold regularization and the elastic net regularization, so it considers both the nonlinear manifold structure of a dataset and the sparse property of the redundant data representation. Face based gender recognition has received much attention in the psychophysical and video surveillance literatures. Most of existing works apply the appearance based information for data representation. A face image with size 40 by 40 could be seen as a point in a linear space with 1600 dimensions. For gender recognition, we have two classes (male and female) in total, so it is essential to find a small number of variables for representation to generalize duly. MEN can duly find the intrinsic structure of a dataset for separating males from the females. Sufficient experimental results on FERET and UMIST datasets suggest that MEN is more effective in selecting discriminative variables for face based gender recognition compared to principal component analysis, sparse principal component analysis, and discriminative locality alignment. The human visual system (HVS) provides a suitable cue for image quality assessment (IQA). In this paper, we develop a novel reduced reference (RR) IQA scheme by incorporating the merits from the contourlet transform, contrast sensitivity function (CSF), and Weber's law of just noticeable difference (JND). In this scheme, the contourlet transform is utilized to decompose images and then extract features to mimic the multichannel structure of HVS. CSF is applied to weight coefficients obtained by the contourlet transform to simulate the appearance of images to observers by taking into account many of the nonlinearities inherent in HVS. JND is finally introduced to produce a noticeable variation in sensory experience. Thorough empirical studies are carried out upon the laboratory for image and video engineering database against the subjective mean opinion score and demonstrate that the proposed framework has good consistency with subjective perception values and the objective assessment results can well reflect the visual quality of images. Topic model, Latent Dirichlet Allocation (LDA), is an effective tool for statistical analysis of large collections of documents. In LDA, each document is modeled as a mixture of topics and the topic proportions are generated from the unimodal Dirichlet distribution prior. When a collection of documents are drawn from multiple classes, this unimodal prior is insufficient for data fitting. To solve this problem, we exploit the multimodal Dirichlet mixture prior, and propose the Dirichlet mixture allocation (DMA). We report experiments on the popular TDT2 Corpus demonstrating that DMA models a collection of documents more precisely than LDA when the documents are obtained from multiple classes. We propose an automatic approximation of the intrinsic manifold for general semi-supervised learning (SSL) problems. Unfortunately, it is not trivial to define an optimization function to obtain optimal hyperparameters. Usually, cross validation is applied, but it does not necessarily scale up. Other problems derive from the suboptimality incurred by discrete grid search and the overfitting. Therefore, we develop an ensemble manifold regularization (EMR) framework to approximate the intrinsic manifold by combining several initial guesses. Algorithmically, we designed EMR carefully so it 1) learns both the composite manifold and the semi-supervised learner jointly, 2) is fully automatic for learning the intrinsic manifold hyperparameters implicitly, 3) is conditionally optimal for intrinsic manifold approximation under a mild and reasonable assumption, and 4) is scalable for a large number of candidate manifold hyperparameters, from both time and space perspectives. Furthermore, we prove the convergence property of EMR to the deterministic matrix at rate root-n. Extensive experiments over both synthetic and real data sets demonstrate the effectiveness of the proposed framework. Active appearance model (AAM) is a powerful generative method for modeling deformable objects. The model decouples the shape and the texture variations of objects, which is followed by an efficient gradient-based model fitting method. Due to the flexible and simple framework, AAM has been widely applied in the fields of computer vision. However, difficulties are met when it is applied to various practical issues, which lead to a lot of prominent improvements to the model. Nevertheless, these difficulties and improvements have not been studied systematically. This motivates us to review the recent advances of AAM. This paper focuses on the improvements in the literature in turns of the problems suffered by AAM in practical applications. Therefore, these algorithms are summarized from three aspects, i.e., efficiency, discrimination, and robustness. Additionally, some applications and implementations of AAM are also enumerated. The main purpose of this paper is to serve as a guide for further research. In this paper, we present a non-negative patch alignment framework (NPAF) to unify popular non-negative matrix factorization (NMF) related dimension reduction algorithms. It offers a new viewpoint to better understand the common property of different NMF algorithms. Although multiplicative update rule (MUR) can solve NPAF and is easy to implement, it converges slowly. Thus, we propose a fast gradient descent (FGD) to overcome the aforementioned problem. FGD uses the Newton method to search the optimal step size, and thus converges faster than MUR. Experiments on synthetic and real-world datasets confirm the efficiency of FGD compared with MUR for optimizing NPAF. Based on NPAF, we develop non-negative discriminative locality alignment (NDLA). Experiments on face image and handwritten datasets suggest the effectiveness of NDLA in classification tasks and its robustness to image occlusions, compared with representative NMF-related dimension reduction algorithms. Combining multiple observation views has proven beneficial for tracking. In this paper, we cast tracking as a novel multi-task multi-view sparse learning problem and exploit the cues from multiple views including various types of visual features, such as intensity, color, and edge, where each feature observation can be sparsely represented by a linear combination of atoms from an adaptive feature dictionary. The proposed method is integrated in a particle filter framework where every view in each particle is regarded as an individual task. We jointly consider the underlying relationship between tasks across different views and different particles, and tackle it in a unified robust multi-task formulation. In addition, to capture the frequently emerging outlier tasks, we decompose the representation matrix to two collaborative components which enable a more robust and accurate approximation. We show that the proposed formulation can be efficiently solved using the Accelerated Proximal Gradient method with a small number of closed-form updates. The presented tracker is implemented using four types of features and is tested on numerous benchmark video sequences. Both the qualitative and quantitative results demonstrate the superior performance of the proposed approach compared to several state-of-the-art trackers. Support vector machines (SVMs) are invaluable tools for many practical applications in artificial intelligence, e.g., classification and event recognition. However, popular SVM solvers are not sufficiently efficient for applications with a great deal of samples as well as a large number of features. In this paper, thus, we present NESVM, a fast gradient SVM solver that can optimize various SVM models, e.g., classical SVM, linear programming SVM and least square SVM. Compared against SVM-Perf cite{SVM_Perf}cite{PerfML} (whose convergence rate in solving the dual SVM is upper bounded by $mathcal O(1/sqrt{k})$ where $k$ is the number of iterations) and Pegasos cite{Pegasos} (online SVM that converges at rate $mathcal O(1/k)$ for the primal SVM), NESVM achieves the optimal convergence rate at $mathcal O(1/k^{2})$ and a linear time complexity. In particular, NESVM smoothes the non-differentiable hinge loss and $ell_1$-norm in the primal SVM. Then the optimal gradient method without any line search is adopted to solve the optimization. In each iteration round, the current gradient and historical gradients are combined to determine the descent direction, while the Lipschitz constant determines the step size. Only two matrix-vector multiplications are required in each iteration round. Therefore, NESVM is more efficient than existing SVM solvers. In addition, NESVM is available for both linear and nonlinear kernels. We also propose ``homotopy NESVM'' to accelerate NESVM by dynamically decreasing the smooth parameter and using the continuation method. Our experiments on census income categorization, indoor/outdoor scene classification, event recognition and scene recognition suggest the efficiency and the effectiveness of NESVM. The MATLAB code of NESVM will be available on our website for further assessment. Editing faces in videos is a popular yet challenging task in computer vision and graphics that encompasses various applications, including facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation. Directly applying the existing warping methods to video face editing has the major problem of temporal incoherence in the synthesized videos, which cannot be addressed by simply employing face tracking techniques or manual interventions, as it is difficult to eliminate the subtly temporal incoherence of the facial feature point localizations in a video sequence. In this article, we propose a temporal-spatial-smooth warping (TSSW) method to achieve a high temporal coherence for video face editing. TSSW is based on two observations: (1) the control lattices are critical for generating warping surfaces and achieving the temporal coherence between consecutive video frames, and (2) the temporal coherence and spatial smoothness of the control lattices can be simultaneously and effectively preserved. Based upon these observations, we impose the temporal coherence constraint on the control lattices on two consecutive frames, as well as the spatial smoothness constraint on the control lattice on the current frame. TSSW calculates the control lattice (in either the horizontal or vertical direction) by updating the control lattice (in the corresponding direction) on its preceding frame, i.e., minimizing a novel energy function that unifies a data-driven term, a smoothness term, and feature point constraints. The contributions of this article are twofold: (1) we develop TSSW, which is robust to the subtly temporal incoherence of the facial feature point localizations and is effective to preserve the temporal coherence and spatial smoothness of the control lattices for editing faces in videos, and (2) we present a new unified video face editing framework that is capable for improving the performances of facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation. Distribution calibration plays an important role in cross-domain learning. However, existing distribution distance metrics are not geodesic; therefore, they cannot measure the intrinsic distance between two distributions. In this paper, we calibrate two distributions by using the geodesic distance in Riemannian symmetric space. Our method learns a latent subspace in the reproducing kernel Hilbert space, where the geodesic distance between the distribution of the source and the target domains is minimized. The corresponding geodesic distance is thus equivalent to the geodesic distance between two symmetric positive definite (SPD) matrices defined in the Riemannian symmetric space. These two SPD matrices parameterize the marginal distributions of the source and target domains in the latent subspace. We carefully design an evolutionary algorithm to find a local optimal solution that minimizes this geodesic distance. Empirical studies on face recognition, text categorization, and web image annotation suggest the effectiveness of the proposed scheme. General tensors can represent colour images more naturally than conventional features; however, the general tensors' stability properties are not reported and remain to be a key problem. In this paper, we use the tensor minimax probability (TMPM) to prove that the tensor representation is stable. The proof is based on the random subspace method through a large number of experiments. Blind image quality assessment (BIQA) is an important yet difficult task in image processing related applications. Existing algorithms for universal BIQA learn a mapping from features of an image to the corresponding subjective quality or divide the image into different distortions before mapping. Although these algorithms are promising, they face the following problems: 1) they require a large number of samples (pairs of distorted image and its subjective quality) to train a robust mapping; 2) they are sensitive to different datasets; and 3) they have to be retrained when new training samples are available. In this paper, we introduce a simple yet effective algorithm based upon the sparse representation of natural scene statistics (NSS) feature. It consists of three key steps: extracting NSS features in the wavelet domain, representing features via sparse coding, and weighting differential mean opinion scores by the sparse coding coefficients to obtain the final visual quality values. Thorough experiments on standard databases show that the proposed algorithm outperforms representative BIQA algorithms and some full-reference metrics. Human gait is a promising biometrics resource. In this paper, the information about gait is obtained from the motions of the different parts of the silhouette. The human silhouette is segmented into seven components, namely head, arm, trunk, thigh, front-leg, back-leg, and feet. The leg silhouettes for the front-leg and the back-leg are considered separately because, during walking, the left leg and the right leg are in front or at the back by turns. Each of the seven components and a number of combinations of the components are then studied with regard to two useful applications: human identification (ID) recognition and gender recognition. More than 500 different experiments on human ID and gender recognition are carried out under a wide range of circumstances. The effectiveness of the seven human gait components for ID and gender recognition is analyzed. In computer vision applications, Active Appearance Models (AAMs) is usually used to model the shape and the gray-level appearance of an object of interest using statistical methods, such as PCA. However, intensity values used in standard AAMs cannot provide enough information for image alignment. In this paper, we firstly propose to utilize Gabor filters to represent the image texture. The benefit of Gabor-based representation is that it can express local structures of an image. As a result, this representation can lead to more accurate matching when condition changes. Given the problem of the excessive storage and computational complexity of the Gabor, three different Gabor-based image representations are used in AAMs: (1) GaborD is the sum of Gabor filter responses over directions, (2) GaborS is the sum of Gabor filter responses over scales, and (3) GaborSD is the sum of Gabor filter responses over scales and directions. Through a large number of experiments, we show that the proposed Gabor representations lead to more accurate and robust matching between model and images. Mammographic mass segmentation plays an important role in computer-aided diagnosis systems. It is very challenging because masses are always of low contrast with ambiguous margins, connected with the normal tissues, and of various scales and complex shapes. To effectively detect true boundaries of mass regions, we propose a feature embedded vector-valued contour-based level set method with relaxed shape constraint. In particular, we initially use the contour-based level set method to obtain the initial boundaries on the smoothed mammogram as the shape constraint. To prevent the contour leaking and meanwhile preserve the radiative characteristics of specific malignant masses, afterward, we relax the obtained shape constraint by analyzing possible valid regions around the initial boundaries. The relaxed shape constraint is then used to design a novel stopping function for subsequent vector-valued level set method. Since texture maps, gradient maps, and the original intensity map can reflect different characteristics of the mammogram, we integrate them together to obtain more accurate segmentation by incorporating the new stopping function into the newly proposed feature embedded vector-valued contour-based level set method. The experimental results suggest that the proposed feature embedded vector-valued contour-based level set method with relaxed shape constraint can effectively find ambiguous margins of the mass regions. Comparing against existing active contours methods, the new scheme is more effective and robust in detecting complex masses. Until now, neighbor-embedding-based (NE) algorithms for super-resolution (SR) have carried out two independent processes to synthesize high-resolution (HR) image patches. In the first process, neighbor search is performed using the Euclidean distance metric, and in the second process, the optimal weights are determined by solving a constrained least squares problem. However, the separate processes are not optimal. In this paper, we propose a sparse neighbor selection scheme for SR reconstruction. We first predetermine a larger number of neighbors as potential candidates and develop an extended Robust-SL0 algorithm to simultaneously find the neighbors and to solve the reconstruction weights. Recognizing that the k-nearest neighbor (k-NN) for reconstruction should have similar local geometric structures based on clustering, we employ a local statistical feature, namely histograms of oriented gradients (HoG) of low-resolution (LR) image patches, to perform such clustering. By conveying local structural information of HoG in the synthesis stage, the k-NN of each LR input patch is adaptively chosen from their associated subset, which significantly improves the speed of synthesizing the HR image while preserving the quality of reconstruction. Experimental results suggest that the proposed method can achieve competitive SR quality compared with other state-of-the-art baselines. With the fast development of video semantic analysis, there has been increasing attention to the typical issue of the semantic analysis of soccer program. Based on the color feature analysis, this paper focuses on the video shot segmentation problem from the perspective of semantic analysis, i.e. the semantic shot segmentation. Most existing works segment and classify the shot by using the dominant color of the field in soccer video. In this paper, we extend the traditional dominant color to several semantic colors, and define the color ratio feature. Then, the support vector machine is used for shot classification. The experimental result shows that the color ratio feature is useful to improve the performance. Furthermore, considering the temporal variations in the semantic color due to environment, an adaptive semantic color extraction algorithm is proposed, and the influence of the number of semantic colors on classification precision is evaluated also. Subspace learning is the process of finding a proper feature subspace and then projecting high-dimensional data onto the learned low-dimensional subspace. The projection operation requires many floating-point multiplications and additions, which makes the projection process computationally expensive. To tackle this problem, this paper proposes two simple-but-effective fast subspace learning and image projection methods, fast Haar transform (FHT) based principal component analysis and FHT based spectral regression discriminant analysis. The advantages of these two methods result from employing both the FHT for subspace learning and the integral vector for feature extraction. Experimental results on three face databases demonstrated their effectiveness and efficiency. In this paper we present a new video object trajectory clustering algorithm1, which allows us to model and analyse the patterns of object behaviors based on the extracted features using tensor analysis. The proposed algorithm consists of three steps as follows: extraction of trajectory features by tensor analysis, non-parametric probabilistic mean shift clustering and clustering correction. The performance of the proposed algorithm is evaluated on standard data-sets and compared with classical techniques. In this paper, we extend the theory of the information bottleneck (IB) to learning from examples represented by multi-view features. We formulate the problem as one of encoding a communication system with multiple senders, each of which represents one view of the data. Based on the precise components filtered out from multiple information sources through a \\u201cbottleneck\\u201d, a margin maximization approach is then used to strengthen the discrimination of the encoder by improving the code distance within the frame of coding theory. The resulting algorithm therefore inherits all the merits of the IB principle and coding theory. It has two distinct advantages over existing algorithms, namely, that our method finds a tradeoff between the accuracy and complexity of the multi-view model, and that the encoded multi-view data retains sufficient discrimination for classification. We also derive the robustness and generalization error bound of the proposed algorithm, and reveal the specific properties of multi-view learning. First, the complementarity of multi-view features guarantees the robustness of the algorithm. Second, the consensus of multi-view features reduces the empirical Rademacher complexity of the objective function, enhances the accuracy of the solution, and improves the generalization error bound of the algorithm. The resulting objective function is solved efficiently using the alternating direction method. Experimental results on annotation, classification and recognition tasks demonstrate that the proposed algorithm is promising for practical applications. Principal component analysis (PCA) is a widely used model for dimensionality reduction. In this paper, we address the problem of determining the intrinsic dimensionality of a general type data population by selecting the number of principal components for a generalized PCA model. In particular, we propose a generalized Bayesian PCA model, which deals with general type data by employing exponential family distributions. Model selection is realized by empirical Bayesian inference of the model. We name the model as simple exponential family PCA (SePCA), since it embraces both the principal of using a simple model for data representation and the practice of using a simplified computational procedure for the inference. Our analysis shows that the empirical Bayesian inference in SePCA formally realizes an intuitive criterion for PCA model selection - a preserved principal component must sufficiently correlate to data variance that is uncorrelated to the other principal components. Experiments on synthetic and real data sets demonstrate effectiveness of SePCA and exemplify its characteristics for model selection. Correspondence construction of characters in key frames is the prerequisite for cartoon animations' automatic inbetweening and coloring. Since each frame of an animation consists of multiple layers, characters are complicated in terms of shape and structure. Therefore, existing shape matching algorithms, specifically designed for simple structures such as a single closed contour, cannot perform well on characters constructed by multiple contours. This paper proposes an automatic cartoon correspondence construction approach with iterative graph based transductive learning (Graph-TL) and distance metric learning (DML) estimation. In details, this new method defines correspondence construction as a many-to-many labeling problem, which assigns the points from one key frame into the points from another key frame. Then, to refine the correspondence construction, we adopt an iterative optimization scheme to alternatively carry out the Graph-TL and DML estimation. In addition, in this paper, we adopt the local shape descriptor for cartoon application, which can successfully achieve rotation and scale invariance in cartoon matching. Plenty of experimental results on our cartoon dataset, which is built upon industrial production suggest the effectiveness of the proposed methods for constructing correspondences of complicated characters. In this chapter, we will present three groups of dimension reduction algorithms for subspace based face recognition. Specifically, we present the general mean criteria and the max-min distance analysis (MMDA) algorithm; manifold learning algorithms, including the discriminative locality alignment (DLA) and manifold elastic net (MEN); and the transfer subspace learning framework. Experiments on face recognition are also provided. Feature-based image watermarking schemes, which aim to survive various geometric distortions, have attracted great attention in recent years. Existing schemes have shown robustness against rotation, scaling, and translation, but few are resistant to cropping, nonisotropic scaling, random bending attacks (RBAs), and affine transformations. Seo and Yoo present a geometrically invariant image watermarking based on affine covariant regions (ACRs) that provide a certain degree of robustness. To further enhance the robustness, we propose a new image watermarking scheme on the basis of Seo's work, which is insensitive to geometric distortions as well as common image processing operations. Our scheme is mainly composed of three components: 1) feature selection procedure based on graph theoretical clustering algorithm is applied to obtain a set of stable and nonoverlapped ACRs; 2) for each chosen ACR, local normalization, and orientation alignment are performed to generate a geometrically invariant region, which can obviously improve the robustness of the proposed watermarking scheme; and 3) in order to prevent the degradation in image quality caused by the normalization and inverse normalization, indirect inverse normalization is adopted to achieve a good compromise between the imperceptibility and robustness. Experiments are carried out on an image set of 100 images collected from Internet, and the preliminary results demonstrate that the developed method improves the performance over some representative image watermarking approaches in terms of robustness. Active appearance model (AAM) has been widely used for modeling the shape and the texture of deformable objects and matching new ones effectively. The traditional AAM consists of two parts, shape model and texture model. In the texture model, for the sake of simplicity, the image intensity is usually employed to represent the texture information. However, the intensity is easy to be interfered by the external environment change, e.g. illumination variations, which results in an unsatisfied model fitting. To this purpose, we present a new texture representation in AAM, which combines Gabor wavelet and Local Binary Patterns (LBP) operator. On the one hand, Gabor wavelet can encode multi-scale and multi-direction information of an image. On the other hand, LBP is able to efficiently encode local information and compress the redundancy in the Gabor filtered images. Since the new texture representation can express an object more sophisticatedly, it will improve the accuracy of the model fitting. The Experimental results on various datasets demonstrate the effectiveness of the proposed texture representation, which results in a more accurate and reliable matching between the model and new images. Prior to pattern recognition, feature selection is often used to identify relevant features and discard irrelevant ones for obtaining improved analysis results. In this paper, we aim to develop an unsupervised feature ranking algorithm that evaluates features using discovered local coherent patterns, which are known as biclusters. The biclusters (viewed as submatrices) are discovered from a data matrix. These submatrices are used for scoring relevant features from two aspects, i.e., the interdependence of features and the separability of instances. The features are thereby ranked with respect to their accumulated scores from the total discovered biclusters before the pattern classification. Experimental results show that this proposed method can yield comparable or even better performance in comparison with the well-known Fisher score, Laplacian score, and variance score using three UCI data sets, well improve the results of gene expression data analysis using gene ontology annotation, and finally demonstrate its advantage of unsupervised feature ranking for high-dimensional data. In computer vision, image datasets used for classification are naturally associated with multiple labels and comprised of multiple views, because each image may contain several objects (e.g., pedestrian, bicycle, and tree) and is properly characterized by multiple visual features (e.g., color, texture, and shape). Currently, available tools ignore either the label relationship or the view complementarily. Motivated by the success of the vector-valued function that constructs matrix-valued kernels to explore the multilabel structure in the output space, we introduce multiview vector-valued manifold regularization $({ m MV}^{3}{ m MR})$ to integrate multiple features. ${ m MV}^{3}{ m MR}$ exploits the complementary property of different features and discovers the intrinsic local geometry of the compact support shared by different features under the theme of manifold regularization. We conduct extensive experiments on two challenging, but popular, datasets, PASCAL VOC' 07 and MIR Flickr, and validate the effectiveness of the proposed ${ m MV}^{3}{ m MR}$ for image classification. Effectively modeling a collection of three-dimensional (3-D) faces is an important task in various applications, especially facial expression-driven ones, e.g., expression generation, retargeting, and synthesis. These 3-D faces naturally form a set of second-order tensors-one modality for identity and the other for expression. The number of these second-order tensors is three times of that of the vertices for 3-D face modeling. As for algorithms, Bayesian data modeling, which is a natural data analysis tool, has been widely applied with great success; however, it works only for vector data. Therefore, there is a gap between tensor-based representation and vector-based data analysis tools. Aiming at bridging this gap and generalizing conventional statistical tools over tensors, this paper proposes a decoupled probabilistic algorithm, which is named Bayesian tensor analysis (BTA). Theoretically, BTA can automatically and suitably determine dimensionality for different modalities of tensor data. With BTA, a collection of 3-D faces can be well modeled. Empirical studies on expression retargeting also justify the advantages of BTA. The Gaussian process latent variable model (GP-LVM) has been identified to be an effective probabilistic approach for dimensionality reduction because it can obtain a low-dimensional manifold of a data set in an unsupervised fashion. Consequently, the GP-LVM is insufficient for supervised learning tasks (e.g., classification and regression) because it ignores the class label information for dimensionality reduction. In this paper, a supervised GP-LVM is developed for supervised learning tasks, and the maximum a posteriori algorithm is introduced to estimate positions of all samples in the latent variable space. We present experimental evidences suggesting that the supervised GP-LVM is able to use the class label information effectively, and thus, it outperforms the GP-LVM and the discriminative extension of the GP-LVM consistently. The comparison with some supervised classification methods, such as Gaussian process classification and support vector machines, is also given to illustrate the advantage of the proposed method. Relevance feedback (RF) schemes based on support vector machines (SVMs) have been widely used in content-based image retrieval (CBIR). However, the performance of SVM-based RF approaches is often poor when the number of labeled feedback samples is small. This is mainly due to 1) the SVM classifier being unstable for small-size training sets because its optimal hyper plane is too sensitive to the training examples; and 2) the kernel method being ineffective because the feature dimension is much greater than the size of the training samples. In this paper, we develop a new machine learning technique, multitraining SVM (MTSVM), which combines the merits of the cotraining technique and a random sampling method in the feature space. Based on the proposed MTSVM algorithm, the above two problems can be mitigated. Experiments are carried out on a large image set of some 20 000 images, and the preliminary results demonstrate that the developed method consistently improves the performance over conventional SVM-based RFs in terms of precision and standard deviation, which are used to evaluate the effectiveness and robustness of a RF algorithm, respectively Imbalance oriented selection scheme was recently proposed to detect stable image points in weakly or sparsely textured images. The scheme chooses image points whose one-pixel-wide directional intensity variations can be clustered into two imbalanced classes as candidates. In this paper, we propose general imbalance decided by multi-pixel-wide directional intensity variations. We present a case study of general imbalanced points in road sign images, which demonstrates the good potential of general imbalanced points. In recent years, a variety of relevance feedback (RF) schemes have been developed to improve the performance of content-based image retrieval (CBIR). Given user feedback information, the key to a RF scheme is how to select a subset of image features to construct a suitable dissimilarity measure. Among various RF schemes, biased discriminant analysis (BDA) based RF is one of the most promising. It is based on the observation that all positive samples are alike, while in general each negative sample is negative in its own way. However, to use BDA, the small sample size (SSS) problem is a big challenge, as users tend to give a small number of feedback samples. To explore solutions to this issue, this paper proposes a direct kernel BDA (DKBDA), which is less sensitive to SSS. An incremental DKBDA (IDKBDA) is also developed to speed up the analysis. Experimental results are reported on a real-world image collection to demonstrate that the proposed methods outperform the traditional kernel BDA (KBDA) and the support vector machine (SVM) based RF algorithms Tensor analysis has been widely utilized in image-related machine learning applications, which has preferable performance over the vector-based approaches for its capability of holding the spatial structure information in some research field. The traditional tensor representation only includes the intensity values, which is sensitive to illumination variation. For this purpose, a weighted tensor subspace (WTS) is defined as object descriptor by combining the Retinex image with the original image. Then, an incremental learning algorithm is developed for WTS to adapt to the appearance change during the tracking. The proposed method could learn the lightness changing incrementally and get robust tracking performance under various luminance conditions. The experimental results illustrate the effectiveness of the proposed visual tracking Scheme. In this paper, we study the generalization bound for an empirical process of samples independently drawn from an infinitely divisible (ID) distribution, which is termed as the ID empirical process. In particular, based on a martingale method, we develop deviation inequalities for the sequence of random variables of an ID distribution. By applying the obtained deviation inequalities, we then show the generalization bound for ID empirical process based on the annealed VapnikChervonenkis (VC) entropy. Afterward, according to Sauer\\u2019s lemma, we get the generalization bound for ID empirical process based on the VC dimension. Finally, by using a resulted result bound, we analyze the asymptotic convergence of ID empirical process and show that the convergence rate of ID empiri This paper presents a new image segmentation method that applies an edge-based level set method in a relay fashion. The proposed method segments an image in a series of nested subregions that are automatically created by shrinking the stabilized curves in their previous subregions. The final result is obtained by combining all boundaries detected in these subregions. The proposed method has the following three advantages: 1) It can be automatically executed without human-computer interactions; 2) it applies the edge-based level set method with relay fashion to detect all boundaries; and 3) it automatically obtains a full segmentation without specifying the number of relays in advance. The comparison experiments illustrate that the proposed method performs better than the representative level set methods, and it can obtain similar or better results compared with other popular segmentation algorithms. Learning tasks such as classification and clustering usually perform better and cost less (time and space) on compressed representations than on the original data. Previous works mainly compress data via dimension reduction. In this paper, we propose \\u201cdouble shrinking\\u201d to compress image data on both dimensionality and cardinality via building either sparse low-dimensional representations or a sparse projection matrix for dimension reduction. We formulate a double shrinking model (DSM) as an l1 regularized variance maximization with constraint ||x||2=1, and develop a double shrinking algorithm (DSA) to optimize DSM. DSA is a path-following algorithm that can build the whole solution path of locally optimal solutions of different sparse levels. Each solution on the path is a \\u201cwarm start\\u201d for searching the next sparser one. In each iteration of DSA, the direction, the step size, and the Lagrangian multiplier are deduced from the Karush-Kuhn-Tucker conditions. The magnitudes of trivial variables are shrunk and the importances of critical variables are simultaneously augmented along the selected direction with the determined step length. Double shrinking can be applied to manifold learning and feature selections for better interpretation of features, and can be combined with classification and clustering to boost their performance. The experimental results suggest that double shrinking produces efficient and effective data compression. It is difficult to find the optimal sparse solution of a manifold learning based dimensionality reduction algorithm. The lasso or the elastic net penalized manifold learning based dimensionality reduction is not directly a lasso penalized least square problem and thus the least angle regression (LARS) (Efron et al., Ann Stat 32(2):407---499, 2004), one of the most popular algorithms in sparse learning, cannot be applied. Therefore, most current approaches take indirect ways or have strict settings, which can be inconvenient for applications. In this paper, we proposed the manifold elastic net or MEN for short. MEN incorporates the merits of both the manifold learning based dimensionality reduction and the sparse learning based dimensionality reduction. By using a series of equivalent transformations, we show MEN is equivalent to the lasso penalized least square problem and thus LARS is adopted to obtain the optimal sparse solution of MEN. In particular, MEN has the following advantages for subsequent classification: (1) the local geometry of samples is well preserved for low dimensional data representation, (2) both the margin maximization and the classification error minimization are considered for sparse projection calculation, (3) the projection matrix of MEN improves the parsimony in computation, (4) the elastic net penalty reduces the over-fitting problem, and (5) the projection matrix of MEN can be interpreted psychologically and physiologically. Experimental evidence on face recognition over various popular datasets suggests that MEN is superior to top level dimensionality reduction algorithms. Fast training and testing procedures are crucial in biometrics recognition research. Conventional algorithms, e.g., principal component analysis (PCA), fail to efficiently work on large-scale and high-resolution image data sets. By incorporating merits from both two-dimensional PCA (2DPCA)-based image decomposition and fast numerical calculations based on Haarlike bases, this technical correspondence first proposes binary 2DPCA (B-2DPCA). Empirical studies demonstrated the advantages of B-2DPCA compared with 2DPCA and binary PCA. With many potential multimedia applications, content-based image retrieval (CBIR) has recently gained more attention for image management and Web search. A wide variety of relevance feedback (RF) algorithms have been developed in recent years to improve the performance of CBIR systems. These RF algorithms capture user's preferences and bridge the semantic gap. However, there is still a big room to further the RF performance, because the popular RF algorithms ignore the manifold structure of image low-level visual features. In this paper, we propose the biased discriminative Euclidean embedding (BDEE) which parameterises samples in the original high-dimensional ambient space to discover the intrinsic coordinate of image low-level visual features. BDEE precisely models both the intraclass geometry and interclass discrimination and never meets the undersampled problem. To consider unlabelled samples, a manifold regularization-based item is introduced and combined with BDEE to form the semi-supervised BDEE, or semi-BDEE for short. To justify the effectiveness of the proposed BDEE and semi-BDEE, we compare them against the conventional RF algorithms and show a significant improvement in terms of accuracy and stability based on a subset of the Corel image gallery. In this paper, we propose a locality-constrained and sparsity-encouraged manifold fitting approach, aiming at capturing the locally sparse manifold structure into neighborhood graph construction by exploiting a principled optimization model. The proposed model formulates neighborhood graph construction as a sparse coding problem with the locality constraint, therefore achieving simultaneous neighbor selection and edge weight optimization. The core idea underlying our model is to perform a sparse manifold fitting task for each data point so that close-by points lying on the same local manifold are automatically chosen to connect and meanwhile the connection weights are acquired by simple geometric reconstruction. We term the novel neighborhood graph generated by our proposed optimization model M-Fitted Graph since such a graph stems from sparse manifold fitting. To evaluate the robustness and effectiveness of M-fitted graphs, we leverage graph-based semi-supervised learning as the testbed. Extensive experiments carried out on six benchmark datasets validate that the proposed M-fitted graph is superior to state-of-the-art neighborhood graphs in terms of classification accuracy using popular graph-based semi-supervised learning methods. In this paper, we propose a novel approach for palmprint identification, which contains two interesting components. Firstly, we propose the directional representation for appearance based approaches. The new representation is robust to drastic illumination changes and preserves important discriminative information for classification. We then generate virtual samples to enlarge the training set to compensate for matching errors caused by large rotations and translations. Based on these two strategies, the recognition performance of representative appearance based approaches can be improved significantly. Secondly, in order to improve the robustness of palmprint identification, we propose a fusion method combining proposed method and orientation based approaches e.g. Competitive Code, which can obtain very low Equal Error Rates. Subspace selection approaches are powerful tools in pattern classification and data visualization. One of the most important subspace approaches is the linear dimensionality reduction step in the Fisher's linear discriminant analysis (FLDA), which has been successfully employed in many fields such as biometrics, bioinformatics, and multimedia information management. However, the linear dimensionality reduction step in FLDA has a critical drawback: for a classification task with c classes, if the dimension of the projected subspace is strictly lower than c - 1, the projection to a subspace tends to merge those classes, which are close together in the original feature space. If separate classes are sampled from Gaussian distributions, all with identical covariance matrices, then the linear dimensionality reduction step in FLDA maximizes the mean value of the Kullback-Leibler (KL) divergences between different classes. Based on this viewpoint, the geometric mean for subspace selection is studied in this paper. Three criteria are analyzed: 1) maximization of the geometric mean of the KL divergences, 2) maximization of the geometric mean of the normalized KL divergences, and 3) the combination of 1 and 2. Preliminary experimental results based on synthetic data, UCI Machine Learning Repository, and handwriting digits show that the third criterion is a potential discriminative subspace selection method, which significantly reduces the class separation problem in comparing with the linear dimensionality reduction step in FLDA and its several representative extensions. Human gait is an important biometric feature. It can be perceived from a great distance and has recently attracted greater attention in video-surveillance-related applications, such as closed-circuit television. We explore gait recognition based on a matrix representation in this paper. First, binary silhouettes over one gait cycle are averaged. As a result, each gait video sequence, containing a number of gait cycles, is represented by a series of gray-level averaged images. Then, a matrix-based unsupervised algorithm, namely coupled subspace analysis (CSA), is employed as a preprocessing step to remove noise and retain the most representative information. Finally, a supervised algorithm, namely discriminant analysis with tensor representation, is applied to further improve classification ability. This matrix-based scheme demonstrates a much better gait recognition performance than state-of-the-art algorithms on the standard USF HumanID Gait database Nonnegative matrix factorization (NMF) decomposes a nonnegative dataset X into two low-rank nonnegative factor matrices, i.e., W and H, by minimizing either Kullback-Leibler (KL) divergence or Euclidean distance between X and WH. NMF has been widely used in pattern recognition, data mining and computer vision because the non-negativity constraints on both W and H usually yield intuitive parts-based representation. However, NMF suffers from two problems: 1) it ignores geometric structure of dataset, and 2) it does not explicitly guarantee parts-based representation on any datasets. In this paper, we propose an orthogonal nonnegative locally linear embedding (ONLLE) method to overcome aforementioned problems. ONLLE assumes that each example embeds in its nearest neighbors and keeps such relationship in the learned subspace to preserve geometric structure of a dataset. For the purpose of learning parts-based representation, ONLLE explicitly incorporates an orthogonality constraint on the learned basis to keep its spatial locality. To optimize ONLLE, we applied an efficient fast gradient descent (FGD) method on Stiefel manifold which accelerates the popular multiplicative update rule (MUR). The experimental results on real-world datasets show that FGD converges much faster than MUR. To evaluate the effectiveness of ONLLE, we conduct both face recognition and image clustering on real-world datasets by comparing with the representative NMF methods. With many potential industrial applications, content-based image retrieval (CBIR) has recently gained more attention for image management and web searching. As an important tool to capture users' preferences and thus to improve the performance of CBIR systems, a variety of relevance feedback (RF) schemes have been developed in recent years. One key issue in RF is: which features (or feature dimensions) can benefit this human-computer iteration procedure? In this paper, we make theoretical and practical comparisons between principal and complement components of image features in CBIR RF. Most of the previous RF approaches treat the positive and negative feedbacks equivalently although this assumption is not appropriate since the two groups of training feedbacks have very different properties. That is, all positive feedbacks share a homogeneous concept while negative feedbacks do not. We explore solutions to this important problem by proposing an orthogonal complement component analysis. Experimental results are reported on a real-world image collection to demonstrate that the proposed complement components method consistently outperforms the conventional principal components method in both linear and kernel spaces when users want to retrieve images with a homogeneous concept. Visual reranking has been widely deployed to refine the quality of conventional content-based image retrieval engines. The current trend lies in employing a crowd of retrieved results stemming from multiple feature modalities to boost the overall performance of visual reranking. However, a major challenge pertaining to current reranking methods is how to take full advantage of the complementary property of distinct feature modalities. Given a query image and one feature modality, a regular visual reranking framework treats the top-ranked images as pseudo positive instances which are inevitably noisy, difficult to reveal this complementary property, and thus lead to inferior ranking performance. This paper proposes a novel image reranking approach by introducing a Co-Regularized Multi-Graph Learning (Co-RMGL) framework, in which the intra-graph and inter-graph constraints are simultaneously imposed to encode affinities in a single graph and consistency across different graphs. Moreover, weakly supervised learning driven by image attributes is performed to denoise the pseudo-labeled instances, thereby highlighting the unique strength of individual feature modality. Meanwhile, such learning can yield a few anchors in graphs that vitally enable the alignment and fusion of multiple graphs. As a result, an edge weight matrix learned from the fused graph automatically gives the ordering to the initially retrieved results. We evaluate our approach on four benchmark image retrieval datasets, demonstrating a significant performance gain over the state-of-the-arts. The motion analysis of the human body is an important topic of research in computer vision devoted to detecting, tracking, and understanding people's physical behavior. This strong interest is driven by a wide spectrum of applications in various areas such as smart video surveillance. Most research in behavior (or gesture) representation focusses on view-dependent representation, and some research on view invariance considers only information from 3-D models, which is effective under considerable changes of viewpoint. This paper introduces a view-independent behavior-analysis framework based on decision fusion in which distance and view angle factors are analyzed. This is a first effort to tackle the problem of behaviors under significant changes in view angle, and a first corresponding video database is built. Recently, lossless data hiding has attracted increasing interests. As a reversible watermark scheme, the host media and hidden data should be recovered without distortion. A latest lossless data hiding technique based on image blocking and block classification has achieved good performance for image authentication. However, this method cannot always fully restore all the blocks of host images and watermarks. For this purpose, we propose an improved algorithm, which is characterized by two aspects. First, a block skipping scheme (BSS) is developed for the host blocks selection to embed watermark; secondly, the embedding level is modified by a novel parameter model to guarantee that the host blocks can be recovered without distortion as well as the embedded data. Extensive experiments conducted on standard grayscale images, medical images, and color images have demonstrated the effectiveness of the improved lossless data hiding scheme. In this paper, we unify C1 units and the locality preserving projections (LPP) into the conventional gist model for scene classification. For the improved gist model, we first utilize the C1 units, intensity channel and color channel of color image to represent the color image with the high dimensional feature, then we project high dimensional samples to a low dimensional subspace via LPP to preserve both the local geometry and the discriminate information, and finally, we apply the nearest neighbour rule with the Euclidean distance for classification. Experimental results based on the USC scene database not only demonstrate that the proposed gist improves the classification accuracy around 7% but also reduce the testing cost around 50 times in comparing with the original gist model proposed by Siagian and Itti in TPAMI 2007. Non-negative matrix factorization (NMF) approximates a non-negative matrix $X$ by a product of two non-negative low-rank factor matrices $W$ and $H$. NMF and its extensions minimize either the Kullback-Leibler divergence or the Euclidean distance between $X$ and $W^T H$ to model the Poisson noise or the Gaussian noise. In practice, when the noise distribution is heavy tailed, they cannot perform well. This paper presents Manhattan NMF (MahNMF) which minimizes the Manhattan distance between $X$ and $W^T H$ for modeling the heavy tailed Laplacian noise. Similar to sparse and low-rank matrix decompositions, MahNMF robustly estimates the low-rank part and the sparse part of a non-negative matrix and thus performs effectively when data are contaminated by outliers. We extend MahNMF for various practical applications by developing box-constrained MahNMF, manifold regularized MahNMF, group sparse MahNMF, elastic net inducing MahNMF, and symmetric MahNMF. The major contribution of this paper lies in two fast optimization algorithms for MahNMF and its extensions: the rank-one residual iteration (RRI) method and Nesterov's smoothing method. In particular, by approximating the residual matrix by the outer product of one row of W and one row of $H$ in MahNMF, we develop an RRI method to iteratively update each variable of $W$ and $H$ in a closed form solution. Although RRI is efficient for small scale MahNMF and some of its extensions, it is neither scalable to large scale matrices nor flexible enough to optimize all MahNMF extensions. Since the objective functions of MahNMF and its extensions are neither convex nor smooth, we apply Nesterov's smoothing method to recursively optimize one factor matrix with another matrix fixed. By setting the smoothing parameter inversely proportional to the iteration number, we improve the approximation accuracy iteratively for both MahNMF and its extensions. Nonnegative matrix factorization (NMF) has become a popular dimension-reduction method and has been widely applied to image processing and pattern recognition problems. However, conventional NMF learning methods require the entire dataset to reside in the memory and thus cannot be applied to large-scale or streaming datasets. In this paper, we propose an efficient online RSA-NMF algorithm (OR-NMF) that learns NMF in an incremental fashion and thus solves this problem. In particular, OR-NMF receives one sample or a chunk of samples per step and updates the bases via robust stochastic approximation. Benefitting from the smartly chosen learning rate and averaging technique, OR-NMF converges at the rate of in each update of the bases. Furthermore, we prove that OR-NMF almost surely converges to a local optimal solution by using the quasi-martingale. By using a buffering strategy, we keep both the time and space complexities of one step of the OR-NMF constant and make OR-NMF suitable for large-scale or streaming datasets. Preliminary experimental results on real-world datasets show that OR-NMF outperforms the existing online NMF (ONMF) algorithms in terms of efficiency. Experimental results of face recognition and image annotation on public datasets confirm the effectiveness of OR-NMF compared with the existing ONMF algorithms. The bag-of-visual-words (BoW) model is effective for representing images and videos in many computer vision problems, and achieves promising performance in image retrieval. Nevertheless, the level of retrieval efficiency in a large-scale database is not acceptable for practical usage. Considering that the relevant images in the database of a given query are more likely to be distinctive than ambiguous, this paper defines \\u201cdatabase saliency\\u201d as the distinctiveness score calculated for every image to measure its overall \\u201csaliency\\u201d in the database. By taking advantage of database saliency, we propose a saliency- inspired fast image retrieval scheme, S-sim, which significantly improves efficiency while retains state-of-the-art accuracy in image retrieval . There are two stages in S-sim: the bottom-up saliency mechanism computes the database saliency value of each image by hierarchically decomposing a posterior probability into local patches and visual words, the concurrent information of visual words is then bottom-up propagated to estimate the distinctiveness, and the top-down saliency mechanism discriminatively expands the query via a very low-dimensional linear SVM trained on the top-ranked images after initial search, ranking images are then sorted on their distances to the decision boundary as well as the database saliency values. We comprehensively evaluate S-sim on common retrieval benchmarks, e.g., Oxford and Paris datasets. Thorough experiments suggest that, because of the offline database saliency computation and online low-dimensional SVM, our approach significantly speeds up online retrieval and outperforms the state-of-the-art BoW-based image retrieval schemes. In this paper, we propose a superpixel classification based optic cup segmentation for glaucoma detection. In the proposed method, each optic disc image is first over-segmented into superpixels. Then mean intensities, center surround statistics and the location features are extracted from each superpixel to classify it as cup or non-cup. The proposed method has been evaluated in one database of 650 images with manual optic cup boundaries marked by trained professionals and one database of 1676 images with diagnostic outcome. Experimental results show average overlapping error around 26.0% compared with manual cup region and area under curve of the receiver operating characteristic curve in glaucoma detection at 0.811 and 0.813 in the two databases, much better than other methods. The method could be used for glaucoma screening. Dimension reduction has been widely used in real-world applications such as image retrieval and document classification. In many scenarios, different features (or multiview data) can be obtained, and how to duly utilize them is a challenge. It is not appropriate for the conventional concatenating strategy to arrange features of different views into a long vector. That is because each view has its specific statistical property and physical interpretation. Even worse, the performance of the concatenating strategy will deteriorate if some views are corrupted by noise. In this paper, we propose a multiview stochastic neighbor embedding (m-SNE) that systematically integrates heterogeneous features into a unified representation for subsequent processing based on a probabilistic framework. Compared with conventional strategies, our approach can automatically learn a combination coefficient for each view adapted to its contribution to the data embedding. This combination coefficient plays an important role in utilizing the complementary information in multiview data. Also, our algorithm for learning the combination coefficient converges at a rate of O(1/k2), which is the optimal rate for smooth problems. Experiments on synthetic and real data sets suggest the effectiveness and robustness of m-SNE for data visualization, image retrieval, object categorization, and scene recognition. Monocular multiple-object tracking is a fundamental yet under-addressed computer vision problem. In this paper, we propose a novel learning framework for tracking multiple objects by detection. First, instead of heuristically defining a tracking algorithm, we learn that a discriminative structure prediction model from labeled video data captures the interdependence of multiple influence factors. Given the joint targets state from the last time step and the observation at the current frame, the joint targets state at the current time step can then be inferred by maximizing the joint probability score. Second, our detection results benefit from tracking cues. The traditional detection algorithms need a nonmaximal suppression postprocessing to select a subset from the total detection responses as the final output and a large number of selection mistakes are induced, especially under a congested circumstance. Our method integrates both detection and tracking cues. This integration helps to decrease the postprocessing mistake risk and to improve performance in tracking. Finally, we formulate the entire model training into a convex optimization problem and estimate its parameters using the cutting plane optimization. Experiments show that our method performs effectively in a large variety of scenarios, including pedestrian tracking in crowd scenes and vehicle tracking in congested traffic. Recently, relevance feedback has been widely used to improve the performance of content-based image retrieval. How to select a subset of features from a large-scale feature pool and to construct a suitable dissimilarity measure are key steps in a relevance feedback system. Biased discriminant analysis (BDA) has been proposed to select features during relevance feedback iterations. However, to solve the BDA, we often encounter the matrix singular problem. Motivated by the direct method and null-space method successfully used in the Fisher linear discriminant analysis for face recognition, we generalize them into the Hilbert space for BDA. Because the direct method and the null-space method may lose some discriminant information, we propose a new full-space method to contain all discriminant information. We also generalize the full-space method into the Hilbert space. All the new methods are demonstrated to outperform the traditional kernel BDA based relevance feedback algorithms based on a statistical experiment in the Corel database with 17, 800 images Orthogonal neighborhood-preserving projection (ONPP) is a recently developed orthogonal linear algorithm for overcoming the out-of-sample problem existing in the well-known manifold learning algorithm, i.e., locally linear embedding. It has been shown that ONPP is a strong analyzer of high-dimensional data. However, when applied to classification problems in a supervised setting, ONPP only focuses on the intraclass geometrical information while ignores the interaction of samples from different classes. To enhance the performance of ONPP in classification, a new algorithm termed discriminative ONPP (DONPP) is proposed in this paper. DONPP 1) takes into account both intraclass and interclass geometries; 2) considers the neighborhood information of interclass relationships; and 3) follows the orthogonality property of ONPP. Furthermore, DONPP is extended to the semisupervised case, i.e., semisupervised DONPP (SDONPP). This uses unlabeled samples to improve the classification accuracy of the original DONPP. Empirical studies demonstrate the effectiveness of both DONPP and SDONPP. The inconsistency between textual features and visual contents can cause poor image search results. To solve this problem, click features, which are more reliable than textual information in justifying the relevance between a query and clicked images, are adopted in image ranking model. However, the existing ranking model cannot integrate visual features, which are efficient in refining the click-based search results. In this paper, we propose a novel ranking model based on the learning to rank framework. Visual features and click features are simultaneously utilized to obtain the ranking model. Specifically, the proposed approach is based on large margin structured output learning and the visual consistency is integrated with the click features through a hypergraph regularizer term. In accordance with the fast alternating linearization method, we design a novel algorithm to optimize the objective function. This algorithm alternately minimizes two different approximations of the original objective function by keeping one function unchanged and linearizing the other. We conduct experiments on a large-scale dataset collected from the Microsoft Bing image search engine, and the results demonstrate that the proposed learning to rank models based on visual features and user clicks outperforms state-of-the-art algorithms. We propose two new methods in the nonlinear kernel feature space for pixel clustering based on the traditional KMeans and Gaussian mixture model (GMM). Unlike the previous work on the kernel machines, we give out a new perspective on the new developed kernel machines. That is, kernel principle component analysis (KPCA) combined with the KMeans and the GMM are kernel KMeans (KKMeans) and kernel GMM (KGMM), respectively. In this paper, we prove the new perspective on KKMeans and give out a clear statement on the KGMM as well. Based on this new perspectives, we can implement the KKMeans and the KGMM conveniently. At the end of the paper, we utilize these new algorithms on the problem of the colour image segmentation. Based on a series of experimental results on Corel colour images, we find that the KKMeans and KGMM can outperform the traditional KMeans and GMM consistently, respectively. Example learning-based image super-resolution (SR) is recognized as an effective way to produce a high-resolution (HR) image with the help of an external training set. The effectiveness of learning-based SR methods, however, depends highly upon the consistency between the supporting training set and low-resolution (LR) images to be handled. To reduce the adverse effect brought by incompatible high-frequency details in the training set, we propose a single image SR approach by learning multiscale self-similarities from an LR image itself. The proposed SR approach is based upon an observation that small patches in natural images tend to redundantly repeat themselves many times both within the same scale and across different scales. To synthesize the missing details, we establish the HR-LR patch pairs using the initial LR input and its down-sampled version to capture the similarities across different scales and utilize the neighbor embedding algorithm to estimate the relationship between the LR and HR image pairs. To fully exploit the similarities across various scales inside the input LR image, we accumulate the previous resultant images as training examples for the subsequent reconstruction processes and adopt a gradual magnification scheme to upscale the LR input to the desired size step by step. In addition, to preserve sharper edges and suppress aliasing artifacts, we further apply the nonlocal means method to learn the similarity within the same scale and formulate a nonlocal prior regularization term to well pose SR estimation under a reconstruction-based SR framework. Experimental results demonstrate that the proposed method can produce compelling SR recovery both quantitatively and perceptually in comparison with other state-of-the-art baselines. Subspace selection is a powerful tool in data mining. An important subspace method is the Fisher-Rao linear discriminant analysis (LDA), which has been successfully applied in many fields such as biometrics, bioinformatics, and multimedia retrieval. However, LDA has a critical drawback: the projection to a subspace tends to merge those classes that are close together in the original feature space. If the separated classes are sampled from Gaussian distributions, all with identical covariance matrices, then LDA maximizes the mean value of the Kullback-Leibler (KL) divergences between the different classes. We generalize this point of view to obtain a framework for choosing a subspace by 1) generalizing the KL divergence to the Bregman divergence and 2) generalizing the arithmetic mean to a general mean. The framework is named the general averaged divergence analysis (GADA). Under this GADA framework, a geometric mean divergence analysis (GMDA) method based on the geometric mean is studied. A large number of experiments based on synthetic data show that our method significantly outperforms LDA and several representative LDA extensions. Reconstruction of a 3-D face model from a single 2-D face image is fundamentally important for face recognition and animation because the 3-D face model is invariant to changes of viewpoint, illumination, background clutter, and occlusions. Given a coupled training set that contains pairs of 2-D faces and the corresponding 3-D faces, we train a novel coupled radial basis function network (C-RBF) to recover the 3-D face model from a single 2-D face image. The C-RBF network explores: 1) the intrinsic representations of 3-D face models and those of 2-D face images; 2) mappings between a 3-D face model and its intrinsic representation; and 3) mappings between a 2-D face image and its intrinsic representation. Since a particular face can be reconstructed by its nearest neighbors, we can assume that the linear combination coefficients for a particular 2-D face image reconstruction are identical to those for the corresponding 3-D face model reconstruction. Therefore, we can reconstruct a 3-D face model by using a single 2-D face image based on the C-RBF network. Extensive experimental results on the BU3D database indicate the effectiveness of the proposed C-RBF network for recovering the 3-D face model from a single 2-D face image. In plenty of scenarios, data can be represented as vectors and then mathematically abstracted as points in a Euclidean space. Because a great number of machine learning and data mining applications need proximity measures over data, a simple and universal distance metric is desirable, and metric learning methods have been explored to produce sensible distance measures consistent with data relationship. However, most existing methods suffer from limited labeled data and expensive training. In this paper, we address these two issues through employing abundant unlabeled data and pursuing sparsity of metrics, resulting in a novel metric learning approach called semi-supervised sparse metric learning. Two important contributions of our approach are: 1) it propagates scarce prior affinities between data to the global scope and incorporates the full affinities into the metric learning; and 2) it uses an efficient alternating linearization method to directly optimize the sparse metric. Compared with conventional methods, ours can effectively take advantage of semi-supervision and automatically discover the sparse metric structure underlying input data patterns. We demonstrate the efficacy of the proposed approach with extensive experiments carried out on six datasets, obtaining clear performance gains over the state-of-the-arts. Imbalanced points are image points whose first-order intensity can be clustered into two imbalanced classes. An important property of imbalanced points is that they can be contiguous to each other. The property helps improve the localization accuracy of imbalanced points across imaging variations. Based on this local geometric coherency property, we propose a global-to-local scheme for imbalanced point matching. The proposed matching scheme first builds correspondence between components of coherent imbalanced points and then refines point correspondence within corresponding components. We test the global-to-local matching scheme, compared with several other well-known methods, on a set of groundtruth stereo images. Furthermore, we present a case study of the proposed scheme in face liveness detection. Our results show the promise of the global-to-local matching scheme. In remote-sensing image target recognition, the target or background object is usually transformed to a feature vector, such as a spectral feature vector. However, this kind of vector represents only one pixel of a remote-sensing image that considers the spectral information but ignores the spatial relationship of neighboring pixels (i.e., the local texture and structure). In this letter, we propose a new way to represent an image object as a multifeature tensor that encodes both the spectral and textural information (Gabor function) and then apply the support tensor machine for target recognition. A range of experiments demonstrates that the effectiveness of the proposed method can deliver a high and correct recognition rate with a small number of training samples. Human body motions, including gait information, are a promising biometrics resource. In this paper, the human silhouette is segmented into seven components for visual surveillance applications, namely, head, arm, body, thigh, front-leg, back-leg, and feet. The legs are classified as front-leg or back-leg because of the bipedal walking style: during walking, the left-leg and the right-leg are in front or at the back in turn. The motions of the individual components and of a number of combinations of components are then studied for gender recognition. For HumanID recognition under different cases, the performances of and underlying links amongst the seven human gait components are analyzed. Expressing data as linear functions of a small number of unknown variables is a useful approach employed by several classical data analysis methods, e.g., factor analysis, principal component analysis, or latent semantic indexing. These models represent the data using the product of two factors. In practice, one important concern is how to link the learned factors to relevant quantities in the context of the application. To this end, various specialized forms of the factors have been proposed to improve interpretability. Toward developing a unified view and clarifying the statistical significance of the specialized factors, we propose a Bayesian model family. We employ exponential family distributions to specify various types of factors, which provide a unified probabilistic formulation. A Gibbs sampling procedure is constructed as a general computation routine. We verify the model by experiments, in which the proposed model is shown to be effective in both emulating existing models and motivating new model designs for particular problem settings. Human gait is an effective biometric source for human identification and visual surveillance; therefore human gait recognition becomes to be a hot topic in recent research. However, the elapsed time problem, which is in its infancy, still receives poor performance. In this paper, we introduce a novel discriminant analysis method to improve the performance. The new model inherits the merits from the tensor rank one analysis, which handles the small samples size problem naturally, and the linear discriminant analysis, which is optimal for classification. Although 2 DLDA and DATR also benefit from these two methods, they cannot converge during the training procedure. This means they can be hardly utilized for practical applications. Based on a lot of experiments on elapsed time problem in human gait recognition, the new method is demonstrated to significantly outperform the existing appearance-based methods, such as the principle component analysis, the linear discriminant analysis, and the tensor rank one analysis Visual reranking has been widely deployed to refine the traditional text-based image retrieval. Its current trend is to combine the retrieval results from various visual features to boost reranking precision and scalability. And its prominent challenge is how to effectively exploit the complementary property of different features. Another significant issue raises from the noisy instances, from manual or automatic labels, which makes the exploration of such complementary property difficult. This paper proposes a novel image reranking by introducing a new Co-Regularized Multi- Graph Learning (Co-RMGL) framework, in which intra-graph and inter-graph constraints are integrated to simultaneously encode the similarity in a single graph and the consistency across multiple graphs. To deal with the noisy instances, weakly supervised learning via co-occurred visual attribute is utilized to select a set of graph anchors to guide multiple graphs alignment and fusion, and to filter out those pseudo labeling instances to highlight the strength of individual features. After that, a learned edge weighting matrix from a fused graph is used to reorder the retrieval results. We evaluate our approach on four popular image retrieval datasets and demonstrate a significant improvement over state-of-the-art methods. In machine learning, Gaussian process latent variable model (GP-LVM) has been extensively applied in the field of unsupervised dimensionality reduction. When some supervised information, e.g., pairwise constraints or labels of the data, is available, the traditional GP-LVM cannot directly utilize such supervised information to improve the performance of dimensionality reduction. In this case, it is necessary to modify the traditional GP-LVM to make it capable of handing the supervised or semi-supervised learning tasks. For this purpose, we propose a new semi-supervised GP-LVM framework under the pairwise constraints. Through transferring the pairwise constraints in the observed space to the latent space, the constrained priori information on the latent variables can be obtained. Under this constrained priori, the latent variables are optimized by the maximum a posteriori (MAP) algorithm. The effectiveness of the proposed algorithm is demonstrated with experiments on a variety of data sets. In this paper, we introduce a novel indexing scheme-query context tree (QUC-tree) to facilitate efficient query sensitive music search under different query contexts. Distinguished from the previous approaches, QUC-tree is a balanced multiway tree structure, where each level represents the data space at different dimensionality. Before the tree structure construction, principle component analysis (PCA) is applied for data analysis and transforming the raw composite features into a new feature space sorted by the importance of acoustic features. The PCA transformed data and reduced dimensions in the upper levels can alleviate suffering from dimensionality curse. To accurately mimic human perception, an extension called QUC +-tree is proposed, which further applies multivariate regression and EM based algorithm to estimate the weight of each individual feature. The comprehensive extensive experiments to evaluate the proposed structures against state-of-art techniques based on different datasets. The experimental results demonstrate the superiority of our technique. Heterogeneous image transformation (HIT) plays an important role in both law enforcements and digital entertainment. Some available popular transformation methods, like locally linear embedding based, usually generate images with lower definition and blurred details mainly due to two defects: (1) these approaches use a fixed number of nearest neighbors (NN) to model the transformation process, i.e., K-NN-based methods; (2) with overlapping areas averaged, the transformed image is approximately equivalent to be filtered by a low pass filter, which filters the high frequency or detail information. These drawbacks reduce the visual quality and the recognition rate across heterogeneous images. In order to overcome these two disadvantages, a two step framework is constructed based on sparse feature selection (SFS) and support vector regression (SVR). In the proposed model, SFS selects nearest neighbors adaptively based on sparse representation to implement an initial transformation, and subsequently the SVR model is applied to estimate the lost high frequency information or detail information. Finally, by linear superimposing these two parts, the ultimate transformed image is obtained. Extensive experiments on both sketch-photo database and near infrared-visible image database illustrates the effectiveness of the proposed heterogeneous image transformation method. Neighbor embedding algorithm has been widely used in example-based super-resolution reconstruction from a single frame, which makes the assumption that neighbor patches embedded are contained in a single manifold. However, it is not always true for complicated texture structure. In this paper, we believe that textures may be contained in multiple manifolds, corresponding to classes. Under this assumption, we present a novel example-based image super-resolution reconstruction algorithm with clustering and supervised neighbor embedding (CSNE). First, a class predictor for low-resolution (LR) patches is learnt by an unsupervised Gaussian mixture model. Then by utilizing class label information of each patch, a supervised neighbor embedding is used to estimate high-resolution (HR) patches corresponding to LR patches. The experimental results show that the proposed method can achieve a better recovery of LR comparing with other simple schemes using neighbor embedding. In reduced-reference (RR) image quality assessment (IQA), the visual quality of distorted images is evaluated with only partial information extracted from original images. In this paper, by considering the information of textures and directions during image distortion, we propose a new reduced- reference IQA metric to calculate the diversifications based on contourlet transform Experimental results illustrate that even with low data rate, the presented metric has still good consistency with the subjective perception. Multi-task learning (MTL) plays an important role in image analysis applications, e.g. image classification, face recognition and image annotation. That is because MTL can estimate the latent shared subspace to represent the common features given a set of images from different tasks. However, the geometry of the data probability distribution is always supported on an intrinsic image sub-manifold that is embedded in a high dimensional Euclidean space. Therefore, it is improper to directly apply MTL to multiclass image classification. In this paper, we propose a manifold regularized MTL (MRMTL) algorithm to discover the latent shared subspace by treating the high-dimensional image space as a sub-manifold embedded in an ambient space. We conduct experiments on the PASCAL VOC'07 dataset with 20 classes and the MIR dataset with 38 classes by comparing MRMTL with conventional MTL and several representative image classification algorithms. The results suggest that MRMTL can properly extract the common features for image representation and thus improve the generalization performance of the image classification models. Human tastes in art motivate the need for effective means to build a visual mosaic picture which is made up of many small tiles. In some previous work researchers have tried to translate pictures' styles.1 In this paper, a new approach to image processing in arts is presented in the domain of generating a series of artistic mosaic pictures. An arbitrary image is first translated into a mosaic-based one by dividing it into a number of sub-blocks each with the mean value of the pixels in it. Each of these small fragments can be assigned a new location within this image so that a new mosaic picture is generated. By this mean, images with similar color features can be used to create a series of mosaic pictures. The mosaic pictures consist of the same elements as each other, but might be extremely different from the semantic contents. A basic algorithm is presented, followed by some further improvements. Some preliminary experimental results are then given to show the impact of the proposed special techniques. Spectral analysis-based dimensionality reduction algorithms are important and have been popularly applied in data mining and computer vision applications. To date many algorithms have been developed, e.g., principal component analysis, locally linear embedding, Laplacian eigenmaps, and local tangent space alignment. All of these algorithms have been designed intuitively and pragmatically, i.e., on the basis of the experience and knowledge of experts for their own purposes. Therefore, it will be more informative to provide a systematic framework for understanding the common properties and intrinsic difference in different algorithms. In this paper, we propose such a framework, named \\\"patch alignment,rdquo which consists of two stages: part optimization and whole alignment. The framework reveals that (1) algorithms are intrinsically different in the patch optimization stage and (2) all algorithms share an almost identical whole alignment stage. As an application of this framework, we develop a new dimensionality reduction algorithm, termed discriminative locality alignment (DLA), by imposing discriminative information in the part optimization stage. DLA can (1) attack the distribution nonlinearity of measurements; (2) preserve the discriminative ability; and (3) avoid the small-sample-size problem. Thorough empirical studies demonstrate the effectiveness of DLA compared with representative dimensionality reduction algorithms. In this paper, we aim to propose an unsupervised feature ranking algorithm for evaluating features using discovered biclusters which are local patterns extracted from a data matrix. The biclusters can be expressed as sub-matrices which are used for scoring relevant features from two aspects, i.e. the interdependence of features and the separability of instances. The features are thereby ranked with respect to their accumulated scores from the total discovered biclusters before the pattern classification. Experimental results show that this proposed algorithm can yield comparable or even better performance in comparison with the well-known Fisher Score, Laplacian Score and Variance Score using several UCI data sets. Shilling attackers apply biased rating profiles to recommender systems for manipulating online product recommendations. Although many studies have been devoted to shilling attack detection, few of them can handle the hybrid shilling attacks that usually happen in practice, and the studies for real-life applications are rarely seen. Moreover, little attention has yet been paid to modeling both labeled and unlabeled user profiles, although there are often a few labeled but numerous unlabeled users available in practice. This paper presents a Hybrid Shilling Attack Detector, or HySAD for short, to tackle these problems. In particular, HySAD introduces MC-Relief to select effective detection metrics, and Semi-supervised Naive Bayes (SNB_lambda) to precisely separate Random-Filler model attackers and Average-Filler model attackers from normal users. Thorough experiments on MovieLens and Netflix datasets demonstrate the effectiveness of HySAD in detecting hybrid shilling attacks, and its robustness for various obfuscated strategies. A real-life case study on product reviews of Amazon.cn is also provided, which further demonstrates that HySAD can effectively improve the accuracy of a collaborative-filtering based recommender system, and provide interesting opportunities for in-depth analysis of attacker behaviors. These, in turn, justify the value of HySAD for real-world applications. A visual attention mechanism is believed to be responsible for the most informative spots in complex scenes. We proposed a novel biologically inspired attention model based on Cortex-like mechanisms and sparse representation. Biological Inspired model, HMAX model, is a feature extraction method and this method is motivated by a quantitative model of visual cortex. This biological inspired feature will be used to build the Saliency Criteria to measure the perspective fields. Saliency Criteria is obtained from Shannon's information entropy and sparse representation. We demonstrate that the proposed model achieves superior accuracy with the comparison to classical approach in static saliency map generation on real data of natural scenes and psychology stimuli patterns. Different kinds of high-dimensional visual features can be extracted from a single image. Images can thus be treated as multiple view data when taking each type of extracted high-dimensional visual feature as a particular understanding of images. In this paper, we propose a framework of sparse unsupervised dimensionality reduction for multiple view data. The goal of our framework is to find a low-dimensional optimal consensus representation from multiple heterogeneous features by multiview learning. In this framework, we first learn low-dimensional patterns individually from each view, considering the specific statistical property of each view. We construct a low-dimensional optimal consensus representation from those learned patterns, the goal of which is to leverage the complementary nature of the multiple views. We formulate the construction of the low-dimensional consensus representation to approximate the matrix of patterns by means of a low-dimensional consensus base matrix and a loading matrix. To select the most discriminative features for the spectral embedding of multiple views, we propose to add an l1-norm into the loading matrix's columns and impose orthogonal constraints on the base matrix. We develop a new alternating algorithm, i.e., spectral sparse multiview embedding, to efficiently obtain the solution. Each row of the loading matrix encodes structured information corresponding to multiple patterns. In order to gain flexibility in sharing information across subsets of the views, we impose a novel structured sparsity-inducing norm penalty on the loading matrix's rows. This penalty makes the loading coefficients adaptively load shared information across subsets of the learned patterns. We call this method structured sparse multiview dimensionality reduction. Experiments on a toy benchmark image data set and two real-world Web image data sets demonstrate the effectiveness of the proposed algorithms. Video-based facial expression recognition is a challenging problem in computer vision and human-computer interaction. To target this problem, texture features have been extracted and widely used, because they can capture image intensity changes raised by skin deformation. However, existing texture features encounter problems with albedo and lighting variations. To solve both problems, we propose a new texture feature called image ratio features. Compared with previously proposed texture features, e.g., high gradient component features, image ratio features are more robust to albedo and lighting variations. In addition, to further improve facial expression recognition accuracy based on image ratio features, we combine image ratio features with facial animation parameters (FAPs), which describe the geometric motions of facial feature points. The performance evaluation is based on the Carnegie Mellon University Cohn-Kanade database, our own database, and the Japanese Female Facial Expression database. Experimental results show that the proposed image ratio feature is more robust to albedo and lighting variations, and the combination of image ratio features and FAPs outperforms each feature alone. In addition, we study asymmetric facial expressions based on our own facial expression database and demonstrate the superior performance of our combined expression recognition system. The proportion of aurora to the field-of-view in temporal series of all-sky images is an important index to investigate the evolvement of aurora. To obtain such an index, a crucial phase is to segment the aurora from the background of sky. A new aurora segmentation approach, including a feature extraction method and the segmentation algorithm, is presented in this paper. The proposed feature extraction method, called adaptive local binary patterns (ALBP), selects the frequently occurred patterns to construct the main pattern set, which avoids using the same pattern set to describe different texture structures in traditional local binary patterns. According to the different morphologies and different semantics of aurora, the segmentation algorithm is designed into two parts, texture part segmentation based on ALBP features and patch part segmentation based on modified Otsu method. As it is simple and efficient, our implementation is suitable for large-scale datasets. The experiments exhibited the segmentation effect of the proposed method is satisfactory from human visual aspect and segmentation accuracy. Levy processes play an important role in the stochastic process theory. However, since samples are non-i.i.d., statistical learning re- sults based on the i.i.d. scenarios cannot be utilized to study the risk bounds for Levy processes. In this paper, we present risk bounds for non-i.i.d. samples drawn from Levy processes in the PAC-learning frame- work. In particular, by using a concentra- tion inequality for infinitely divisible distri- butions, we first prove that the function of risk error is Lipschitz continuous with a high probability, and then by using a specific con- centration inequality for Levy processes, we obtain the risk bounds for non-i.i.d. samples drawn from Levy processes without Gaus- sian components. Based on the resulted risk bounds, we analyze the factors that affect the convergence of the risk bounds and then prove the convergence. Biologically inspired feature (BIF) and its variations have been demonstrated to be effective and efficient for scene classification. It is unreasonable to measure the dissimilarity between two BIFs based on their Euclidean distance. This is because BIFs are extrinsically very high dimensional and intrinsically low dimensional, i.e., BIFs are sampled from a low-dimensional manifold and embedded in a high-dimensional space. Therefore, it is essential to find the intrinsic structure of a set of BIFs, obtain a suitable mapping to implement the dimensionality reduction, and measure the dissimilarity between two BIFs in the low-dimensional space based on their Euclidean distance. In this paper, we study the manifold constructed by a set of BIFs utilized for scene classification, form a new dimensionality reduction algorithm by preserving both the geometry of intra BIFs and the discriminative information inter BIFs termed Discriminative and Geometry Preserving Projections (DGPP), and construct a new framework for scene classification. In this framework, we represent an image based on a new BIF, which combines the intensity channel, the color channel, and the C1 unit of a color image; then we project the high-dimensional BIF to a low-dimensional space based on DGPP; and, finally, we conduct the classification based on the multiclass support vector machine (SVM). Thorough empirical studies based on the USC scene dataset demonstrate that the proposed framework improves the classification rates around 100% relatively and the training speed 60 times for different sites in comparing with previous gist proposed by Siagian and Itti in 2007. This paper presents a fast part-based subspace selection algorithm, termed the binary sparse nonnegative matrix factorization (B-SNMF). Both the training process and the testing process of B-SNMF are much faster than those of binary principal component analysis (B-PCA). Besides, B-SNMF is more robust to occlusions in images. Experimental results on face images demonstrate the effectiveness and the efficiency of the proposed B-SNMF. A person\\u2019s gait changes when he or she is carrying an object such as a bag, suitcase or rucksack. As a result, human identification and tracking are made more difficult because the averaged gait image is too simple to represent the carrying status. Therefore, in this paper we first introduce a set of Gabor based human gait appearance models, because Gabor functions are similar to the receptive field profiles in the mammalian cortical simple cells. The very high dimensionality of the feature space makes training difficult. In order to solve this problem we propose a general tensor discriminant analysis (GTDA), which seamlessly incorporates the object (Gabor based human gait appearance model) structure information as a natural constraint. GTDA differs from the previous tensor based discriminant analysis methods in that the training converges. Existing methods fail to converge in the training stage. This makes them unsuitable for practical tasks. Experiments are carried out on the USF baseline data set to recognize a human\\u2019s ID from the gait silhouette. The proposed Gabor gait incorporated with GTDA is demonstrated to significantly outperform the existing appearance-based methods. Relevance feedback schemes based on support vector machines (SVM) have been widely used in content-based image retrieval (CBIR). However, the performance of SVM-based relevance feedback is often poor when the number of labeled positive feedback samples is small. This is mainly due to three reasons: 1) an SVM classifier is unstable on a small-sized training set, 2) SVM's optimal hyperplane may be biased when the positive feedback samples are much less than the negative feedback samples, and 3) overfitting happens because the number of feature dimensions is much higher than the size of the training set. In this paper, we develop a mechanism to overcome these problems. To address the first two problems, we propose an asymmetric bagging-based SVM (AB-SVM). For the third problem, we combine the random subspace method and SVM for relevance feedback, which is named random subspace SVM (RS-SVM). Finally, by integrating AB-SVM and RS-SVM, an asymmetric bagging and random subspace SVM (ABRS-SVM) is built to solve these three problems and further improve the relevance feedback performance Eye detection plays an important role in many practical applications. This paper presents a novel two-step scheme for eye detection. The first step models an eye by a newly defined visual-context pattern (VCP), and the second step applies semisupervised boosting for precise detection. VCP describes both the space and appearance relations between an eye region (region of eye) and a reference region (region of reference). The context feature of a VCP is extracted by using the integral image. Aiming to reduce the human labeling efforts, we apply semisupervised boosting, which integrates the context feature and the Haar-like features for precise eye detection. Experimental results on several standard face data sets demonstrate that the proposed approach is effective, robust, and efficient. We finally show that this approach is ready for practical applications. Histogram-based lossless data embedding (LDE) has been recognized as an effective and efficient way for copyright protection of multimedia. Recently, a LDE method using the statistical quantity histogram has achieved good performance, which utilizes the similarity of the arithmetic average of difference histogram (AADH) to reduce the diversity of images and ensure the stable performance of LDE. However, this method is strongly dependent on some assumptions, which limits its applications in practice. In addition, the capacities of the images with the flat AADH, e.g., texture images, are a little bit low. For this purpose, we develop a novel framework for LDE by incorporating the merits from the generalized statistical quantity histogram (GSQH) and the histogram-based embedding. Algorithmically, we design the GSQH driven LDE framework carefully so that it: (1) utilizes the similarity and sparsity of GSQH to construct an efficient embedding carrier, leading to a general and stable framework; (2) is widely adaptable for different kinds of images, due to the usage of the divide-and-conquer strategy; (3) is scalable for different capacity requirements and avoids the capacity problems caused by the flat histogram distribution; (4) is conditionally robust against JPEG compression under a suitable scale factor; and (5) is secure for copyright protection because of the safe storage and transmission of side information. Thorough experiments over three kinds of images demonstrate the effectiveness of the proposed framework. Recovering a large low-rank matrix from highly corrupted, incomplete or sparse outlier overwhelmed observations is the crux of various intriguing statistical problems. We explore the power of \\u201cgreedy bilateral (GreB)\\u201d paradigm in reducing both time and sample complexities for solving these problems. GreB models a lowrank variable as a bilateral factorization, and updates the left and right factors in a mutually adaptive and greedy incremental manner. We detail how to model and solve low-rank approximation, matrix completion and robust PCA in GreB\\u2019s paradigm. On their MATLAB implementations, approximating a noisy 10 4 \\u00d7 10 4 matrix of rank 500 with SVD accuracy takes 6s; MovieLens10M matrix of size 69878 \\u00d7 10677 can be completed in 10s from 30% of 10 7 ratings with RMSE 0.86 on the rest 70%; the low-rank background and sparse moving outliers in a 120\\u00d7160 video of 500 frames are accurately separated in 1s. This brings 30 to 100 times acceleration in solving these popular statistical problems. Video retrieval and indexing research aims to efficiently and effectively manage very large video databases, e.g., CCTV records, which is a key component in video-based object and event analysis. In this paper, for the purpose of video retrieval, we propose a novel method to represent video data by developing an optical flow tensor (OFT) and incorporating hidden Markov models (HMMs). As video is content-sensitive and normally carries rich motion information of objects, optical flow field is first employed to estimate such motion. Then, a shot HMMs tree is built to model video clips in different levels in a database. Experimental results demonstrate that the newly developed method inherits advantages of both optical flow and HMMs in video representation. With the newly developed video representation, in video retrieval and indexing tasks, no need to exhaustively compare a query video shot with all video shot records in the database. Moreover, the novel representation method works well when linear discriminant analysis (LDA) is utilized to reduce the feature dimensionality and further speed up the retrieval procedure. The state-of-the-art metric-learning algorithms cannot perform well for domain adaptation settings, such as cross-domain face recognition, image annotation, etc., because labeled data in the source domain and unlabeled ones in the target domain are drawn from different, but related distributions. In this paper, we propose the domain adaptation metric learning (DAML), by introducing a data-dependent regularization to the conventional metric learning in the reproducing kernel Hilbert space (RKHS). This data-dependent regularization resolves the distribution difference by minimizing the empirical maximum mean discrepancy between source and target domain data in RKHS. Theoretically, by using the empirical Rademacher complexity, we prove risk bounds for the nearest neighbor classifier that uses the metric learned by DAML. Practically, learning the metric in RKHS does not scale up well. Fortunately, we can prove that learning DAML in RKHS is equivalent to learning DAML in the space spanned by principal components of the kernel principle component analysis (KPCA). Thus, we can apply KPCA to select most important principal components to significantly reduce the time cost of DAML. We perform extensive experiments over four well-known face recognition datasets and a large-scale Web image annotation dataset for the cross-domain face recognition and image annotation tasks under various settings, and the results demonstrate the effectiveness of DAML. In this paper, we study the manifold regularization for the Sliced Inverse Regression (SIR). The manifold regularization improves the standard SIR in two aspects: 1) it encodes the local geometry for SIR and 2) it enables SIR to deal with transductive and semi-supervised learning problems. We prove that the proposed graph Laplacian based regularization is convergent at rate root-n. The projection directions of the regularized SIR are optimized by using a conjugate gradient method on the Grassmann manifold. Experimental results support our theory. Images are usually represented by features from multiple views, e.g., color and texture. In image classification, the goal is to fuse all the multi-view features in a reasonable manner and achieve satisfactory classification performance. However, the features are often different in nature and it is nontrivial to fuse them. Particularly, some extracted features are redundant or noisy and are consequently not discriminative for classification. To alleviate these problems in an image classification context, we propose in this paper a novel multi-view embedding framework, termed as Grassmannian regularized structured multi-view embedding, or GrassReg for short. GrassReg transfers the graph Laplacian obtained from each view to a point on the Grassmann manifold and penalizes the disagreement between different views according to Grassmannian distance. Therefore, a view that is consistent with others is more important than a view that disagrees with others for learning a unified subspace for multi-view data representation. In addition, we impose the group sparsity penalty onto the low-dimensional embeddings obtained hence they can better explore the group structure of the intrinsic data distribution. Empirically, we compare GrassReg with representative multi-view algorithms and show the effectiveness of GrassReg on a number of multi-view image data sets. Low-rank and sparse structures have been profoundly studied in matrix completion and compressed sensing. In this paper, we develop \\\"Go Decomposition\\\" (GoDec) to efficiently and robustly estimate the low-rank part L and the sparse part S of a matrix X = L + S + G with noise G. GoDec alternatively assigns the low-rank approximation of X - S to L and the sparse approximation of X - L to S. The algorithm can be significantly accelerated by bilateral random projections (BRP). We also propose GoDec for matrix completion as an important variant. We prove that the objective value ||X - L - S||2F converges to a local minimum, while L and S linearly converge to local optimums. Theoretically, we analyze the influence of L, S and G to the asymptotic/convergence speeds in order to discover the robustness of GoDec. Empirical studies suggest the efficiency, robustness and effectiveness of GoDec comparing with representative matrix decomposition and completion tools, e.g., Robust PCA and OptSpace. In this paper, we present a robust approach of digital watermarking embedding and retrieval for digital images. This new approach works in special domain and it has two major steps: (1) to extract affine co-variant regions, and (2) to embed watermarks with a novel fuzzy inference mechanism. The advantage of this proposed approach is demonstrated by a large amount of experiments, which are mainly against different attacks, such as common image processing, rotation, scaling, cropping, and several other affine transformations. 3D facial expression recognition has great potential in human computer interaction and intelligent robot systems. In this paper, we propose a two-step approach which combines both the feature selection and the feature fusion techniques to choose more comprehensive and discriminative features for 3D facial expression recognition. In the feature selection stage, we utilize a novel normalized cut-based filter (NCBF) algorithm to select the high relevant and low redundant geometrically localized features (GLF) and surface curvature features (SCF), respectively. Then in the feature fusion stage, PCA is performed on the selected GLF and SCF in order to avoid the curse-of-dimensionality challenge. Finally, the processed GLF and SCF are fused together to capture the most discriminative information in 3D expressional faces. Experiments are carried out on the BU-3DFE database, and the proposed approach outperforms the conventional methods by providing more competitive results. In this paper, we present an efficient query-by-singing based musical retrieval system. We first combine multiple Support Vector Machines by classifier committee learning to segment the sentences from a song automatically. Many new methods in manipulating Mel-Frequency Cepstral Coefficient (MFCC) matrix are studied and compared for optimal feature selection. Experiments show that the 3rd coefficient is the most relevant to music comparison out of 13 coefficients and the proposed simplified MFCC feature is able to achieve a reasonable trade-off between accuracy and efficiency. To improve system efficiency, we re-organize the database by a new two-stage clustering scheme in both time space and feature space. We combine K-means algorithm and dynamic time wrapping similarity measurement for feature space clustering. We also propose a new method for model-selection of K-means algorithm. Experiments show that the proposed approach can achieve more than 30 percent increase in accuracy while speed up more than 16 times in average query time. Image search reranking methods usually fail to capture the user's intention when the query term is ambiguous. Therefore, reranking with user interactions, or active reranking, is highly demanded to effectively improve the search performance. The essential problem in active reranking is how to target the user's intention. To complete this goal, this paper presents a structural information based sample selection strategy to reduce the user's labeling efforts. Furthermore, to localize the user's intention in the visual feature space, a novel local-global discriminative dimension reduction algorithm is proposed. In this algorithm, a submanifold is learned by transferring the local geometry and the discriminative information from the labelled images to the whole (global) image database. Experiments on both synthetic datasets and a real Web image search dataset demonstrate the effectiveness of the proposed active reranking scheme, including both the structural information based active sample selection strategy and the local-global discriminative dimension reduction algorithm. Active Appearance Models (AAMs) are generative models which can describe deformable objects. However, the texture in basic AAMs is represented using intensity values. Despite its simplicity, this representation does not contain enough information for image matching. In this paper, we firstly propose to utilize Gabor filters to represent the image texture. The benefit of Gabor-based representation is that it can express local structures of an image. As a result, this representation can lead to more accurate matching when condition changes. Given the problem of the excessive storage and computational complexity of the Gabor, three different Gabor-based image representations are used in AAMs: (1) GaborD is the sum of Gabor filter responses over directions, (2) GaborS is the sum of Gabor filter responses over scales, and (3) GaborSD is the sum of Gabor filter responses over scales and directions. Through a large number of experiments, we show that the proposed Gabor representations lead to more accurate and reliable matching between model and images. Nonnegative matrix factorization (NMF) has become a popular data-representation method and has been widely used in image processing and pattern-recognition problems. This is because the learned bases can be interpreted as a natural parts-based representation of data and this interpretation is consistent with the psychological intuition of combining parts to form a whole. For practical classification tasks, however, NMF ignores both the local geometry of data and the discriminative information of different classes. In addition, existing research results show that the learned basis is unnecessarily parts-based because there is neither explicit nor implicit constraint to ensure the representation parts-based. In this paper, we introduce the manifold regularization and the margin maximization to NMF and obtain the manifold regularized discriminative NMF (MD-NMF) to overcome the aforementioned problems. The multiplicative update rule (MUR) can be applied to optimizing MD-NMF, but it converges slowly. In this paper, we propose a fast gradient descent (FGD) to optimize MD-NMF. FGD contains a Newton method that searches the optimal step length, and thus, FGD converges much faster than MUR. In addition, FGD includes MUR as a special case and can be applied to optimizing NMF and its variants. For a problem with 165 samples in R1600 , FGD converges in 28 s, while MUR requires 282 s. We also apply FGD in a variant of MD-NMF and experimental results confirm its efficiency. Experimental results on several face image datasets suggest the effectiveness of MD-NMF. In this paper, we study semisupervised linear dimensionality reduction. Beyond conventional supervised methods which merely consider labeled instances, the semisupervised scheme allows to leverage abundant and ample unlabeled instances into learning so as to achieve better generalization performance. Under semisupervised settings, our objective is to learn a smooth as well as discriminative subspace and linear dimensionality reduction is thus achieved by mapping all samples into the subspace. Specifically, we present the transductive component analysis (TCA) algorithm to generate such a subspace founded on a graph-theoretic framework. Considering TCA is nonorthogonal, we further present the orthogonal transductive component analysis (OTCA) algorithm to iteratively produce a series of orthogonal basis vectors. OTCA has better discriminating power than TCA. Experiments carried out on synthetic and real-world datasets by OTCA show a clear improvement over the results of representative dimensionality reduction algorithms. Mammographic mass detection is an important task for the early diagnosis of breast cancer. However, it is difficult to distinguish masses from normal regions because of their abundant morphological characteristics and ambiguous margins. To improve the mass detection performance, it is essential to effectively preprocess mammogram to preserve both the intensity distribution and morphological characteristics of regions. In this paper, morphological component analysis is first introduced to decompose a mammogram into a piecewise-smooth component and a texture component. The former is utilized in our detection scheme as it effectively suppresses both structural noises and effects of blood vessels. Then, we propose two novel concentric layer criteria to detect different types of suspicious regions in a mammogram. The combination is evaluated based on the Digital Database for Screening Mammography, where 100 malignant cases and 50 benign cases are utilized. The sensitivity of the proposed scheme is 99% in malignant, 88% in benign, and 95.3% in all types of cases. The results show that the proposed detection scheme achieves satisfactory detection performance and preferable compromises between sensitivity and false positive rates. Reduced-reference (RR) image quality assessment (IQA) metrics evaluate the quality of a distorted (or degraded) image by using some, not all, information of the original (reference) image. In this paper, we propose a novel RR IQA metric based on hybrid wavelets and directional filter banks (HWD). With HWD as a pre-processing stage, the newly proposed metric mainly focuses on subbands coefficients of the distorted and original images. It performs well under low data rate, because only a threshold and several proportion values are recorded from the original images and transmitted. Experiments are carried out upon well recognized data sets and the results demonstrate advantages of the metric compared with existing ones. Moreover, a separate set of experiments shows that this proposed metric has good consistency with human subjective perception. In this paper, a robust image watermarking approach is presented based on image local invariant features. The affine invariant point detector is used to extract feature regions of the given host image. Image normalization and dominant gradient orientation alignment are performed to these feature regions. Thereafter, circular watermark pattern are embedded in these image normalized patches. After the above procedures, full affine invariance can be achieved over these normalized patches. A large amount of experiments demonstrate that the proposed approach is robust against different attacks, such as common image processing, rotation, scaling, cropping, and several other geometric distortions. Vector data are normally used for probabilistic graphical models with Bayesian inference. However, tensor data, i.e., multidimensional arrays, are actually natural representations of a large amount of real data, in data mining, computer vision, and many other applications. Aiming at breaking the huge gap between vectors and tensors in conventional statistical tasks, e.g., automatic model selection, this paper proposes a decoupled probabilistic algorithm, named Bayesian tensor analysis (BTA). BTA automatically selects a suitable model for tensor data, as demonstrated by empirical studies. Relevance feedback (RF) is an important tool to improve the performance of content-based image retrieval system. Support vector machine (SVM) based RF is popular because it can generalize better than most other classifiers. However, directly using SVM in RF may not be appropriate, since SVM treats the positive and negative feedbacks equally. Given the different properties of positive samples and negative samples in RF, they should be treated differently. Considering this, we propose an orthogonal complement components analysis (OCCA) combined with SVM in this paper. We then generalize the OCCA to Hilbert space and define the kernel empirical OCCA (KEOCCA). Through experiments on a Corel photo database with 17,800 images, we demonstrate that the proposed method can significantly improve the performance of conventional SVM-based RF. We investigate the tracking of 2-D human poses in a video stream to determine the spatial configuration of body parts in each frame, but this is not a trivial task because people may wear different kinds of clothing and may move very quickly and unpredictably. The technology of pose estimation is typically applied, but it ignores the temporal context and cannot provide smooth, reliable tracking results. Therefore, we develop a tracking and estimation integrated model (TEIM) to fully exploit temporal information by integrating pose estimation with visual tracking. However, joint parsing of multiple articulated parts over time is difficult, because a full model with edges capturing all pairwise relationships within and between frames is loopy and intractable. In previous models, approximate inference was usually resorted to, but it cannot promise good results and the computational cost is large. We overcome these problems by exploring the idea of divide and conquer, which decomposes the full model into two much simpler tractable submodels. In addition, a novel two-step iteration strategy is proposed to efficiently conquer the joint parsing problem. Algorithmically, we design TEIM very carefully so that: 1) it enables pose estimation and visual tracking to compensate for each other to achieve desirable tracking results; 2) it is able to deal with the problem of tracking loss; and 3) it only needs past information and is capable of tracking online. Experiments are conducted on two public data sets in the wild with ground truth layout annotations, and the experimental results indicate the effectiveness of the proposed TEIM framework. It has been demonstrated by Serre et al. that the biologically inspired model (BIM) is effective for object recognition. It outperforms many state-of-the-art methods in challenging databases. However, BIM has the following three problems: a very heavy computational cost due to dense input, a disputable pooling operation in modeling relations of the visual cortex, and blind feature selection in a feed-forward framework. To solve these problems, we develop an enhanced BIM (EBIM), which removes uninformative input by imposing sparsity constraints, utilizes a novel local weighted pooling operation with stronger physiological motivations, and applies a feedback procedure that selects effective features for combination. Empirical studies on the CalTech5 database and CalTech101 database show that EBIM is more effective and efficient than BIM. We also apply EBIM to the MIT-CBCL street scene database to show it achieves comparable performance in comparison with the current best performance. Moreover, the new system can process images with resolution 128 times 128 at a rate of 50 frames per second and enhances the speed 20 times at least in comparison with BIM in common applications. Over the past years, research and applications of image quality assessment have attracted increasing attention. Basically, naked human eyes are the final receivers of an image and human visual system is able to extract structural information from the viewing field with high adaptation. Different frequency components of an image play different roles on image semantic contents, and the extraction of the visual information is also crucial in computerized image quality assessment systems. In this paper, an image quality assessment metrics is proposed based on wavelet structure and human perception. With the proposed metric, structural similarity is well expanded from pixel-wise to frequency field. Experimental results illustrate that the proposed metric gives good consistency with subjective assessment results of naked human eyes, i.e., it fits well the perception of human visual system. Crater detection from panchromatic images has its unique challenges when comparing to the traditional object detection tasks. Craters are numerous, have large range of sizes and textures, and they continuously merge into image backgrounds. Using traditional feature construction methods to describe craters cannot well embody the diversified characteristics of craters. On the other hand, we are gradually revealing the secret of object recognition in the primate's visual cortex. Biologically inspired features, designed to mimic the human cortex, have achieved great performance on object detection problem. Therefore, it is time to reconsider crater detection by using biologically inspired features. In this paper, we represent crater images by utilizing the C1 units, which correspond to complex cells in the visual cortex, and pool over the S1 units by using a maximum operation to reserve only the maximum response of each local area of the S1 units. The features generated from the C1 units have the hallmarks of size invariance and location invariance. We further extract a set of improved Haar features on each C1 map which contain gradient texture information. We apply this biologically inspired based Haar feature to crater detection. Because the feature construction process requires a set of biologically inspired transformations, these features are embedded in a high dimension space. We apply a subspace learning algorithm to find the intrinsic discriminative subspace for accurate classification. Experiments on Mars impact crater dataset show the superiority of the proposed method. In this paper, we propose a new method to recognize gestures of cartoon images with two practical applications, i.e., content-based cartoon image retrieval and interactive cartoon clip synthesis. Upon analyzing the unique properties of four types of features including global color histogram, local color histogram (LCH), edge feature (EF), and motion direction feature (MDF), we propose to employ different features for different purposes and in various phases. We use EF to define a graph and then refine its local structure by LCH. Based on this graph, we adopt a transductive learning algorithm to construct local patches for each cartoon image. A spectral method is then proposed to optimize the local structure of each patch and then align these patches globally. MDF is fused with EF and LCH and a cartoon gesture space is constructed for cartoon image gesture recognition. We apply the proposed method to content-based cartoon image retrieval and interactive cartoon clip synthesis. The experiments demonstrate the effectiveness of our method. With great potential for assisting radiological image interpretation and decision making, content-based image retrieval in the medical domain has become a hot topic in recent years. Many methods to enhance the performance of content-based medical image retrieval have been proposed, among which the relevance feedback (RF) scheme is one of the most promising. Given user feedback information, RF algorithms interactively learn a user\\u2019s preferences to bridge the \\u201csemantic gap\\u201d between low-level computerized visual features and high-level human semantic perception and thus improve retrieval performance. However, most existing RF algorithms perform in the original high-dimensional feature space and ignore the manifold structure of the low-level visual features of images. In this paper, we propose a new method, termed dual-force ISOMAP (DFISOMAP), for content-based medical image retrieval. Under the assumption that medical images lie on a low-dimensional manifold embedded in a high-dimensional ambient space, DFISOMAP operates in the following three stages. First, the geometric structure of positive examples in the learned low-dimensional embedding is preserved according to the isometric feature mapping (ISOMAP) criterion. To precisely model the geometric structure, a reconstruction error constraint is also added. Second, the average distance between positive and negative examples is maximized to separate them; this margin maximization acts as a force that pushes negative examples far away from positive examples. Finally, the similarity propagation technique is utilized to provide negative examples with another force that will pull them back into the negative sample set. We evaluate the proposed method on a subset of the IRMA medical image dataset with a RF-based medical image retrieval framework. Experimental results show that DFISOMAP outperforms popular approaches for content-based medical image retrieval in terms of accuracy and stability. Relevance feedback (RF) schemes based on support vector machine (SVM) have been widely used in content-based image retrieval. However, the performance of SVM based RF is often poor when the number of labeled positive feedback samples is small. This is mainly due to three reasons: (1) SVM classifier is unstable on small size training set; (2) SVM's optimal hyper-plane may be biased when the positive feedback samples are much less than the negative feedback samples; (3) overfitting due to that the feature dimension is much higher than the size of the training set. In this paper, we try to use random sampling techniques to overcome these problems. To address the first two problems, we propose an asymmetric bagging based SVM. For the third problem, we combine the random subspace method (RSM) and SVM for RF. Finally, by integrating bagging and RSM we solve all the three problems and further improve the RF performance. Reduced-reference (RR) image quality assessment (IQA) has been recognized as an effective and efficient way to predict the visual quality of distorted images. The current standard is the wavelet-domain natural image statistics model (WNISM), which applies the Kullback-Leibler divergence between the marginal distributions of wavelet coefficients of the reference and distorted images to measure the image distortion. However, WNISM fails to consider the statistical correlations of wavelet coefficients in different subbands and the visual response characteristics of the mammalian cortical simple cells. In addition, wavelet transforms are optimal greedy approximations to extract singularity structures, so they fail to explicitly extract the image geometric information, e.g., lines and curves. Finally, wavelet coefficients are dense for smooth image edge contours. In this paper, to target the aforementioned problems in IQA, we develop a novel framework for IQA to mimic the human visual system (HVS) by incorporating the merits from multiscale geometric analysis (MGA), contrast sensitivity function (CSF), and the Weber's law of just noticeable difference (JND). In the proposed framework, MGA is utilized to decompose images and then extract features to mimic the multichannel structure of HVS. Additionally, MGA offers a series of transforms including wavelet, curvelet, bandelet, contourlet, wavelet-based contourlet transform (WBCT), and hybrid wavelets and directional filter banks (HWD), and different transforms capture different types of image geometric information. CSF is applied to weight coefficients obtained by MGA to simulate the appearance of images to observers by taking into account many of the nonlinearities inherent in HVS. JND is finally introduced to produce a noticeable variation in sensory experience. Thorough empirical studies are carried out upon the LIVE database against subjective mean opinion score (MOS) and demonstrate that 1) the proposed framework has good consistency with subjective perception values and the objective assessment results can well reflect the visual quality of images, 2) different transforms in MGA under the new framework perform better than the standard WNISM and some of them even perform better than the standard full-reference IQA model, i.e., the mean structural similarity index, and 3) HWD performs best among all transforms in MGA under the framework. In image processing, cartoon character classification, retrieval, and synthesis are critical, so that cartoonists can effectively and efficiently make cartoons by reusing existing cartoon data. To successfully achieve these tasks, it is essential to extract visual features that comprehensively represent cartoon characters and to construct an accurate distance metric to precisely measure the dissimilarities between cartoon characters. In this paper, we introduce three visual features, color histogram, shape context, and skeleton, to characterize the color, shape, and action, respectively, of a cartoon character. These three features are complementary to each other, and each feature set is regarded as a single view. However, it is improper to concatenate these three features into a long vector, because they have different physical properties, and simply concatenating them into a high-dimensional feature vector will suffer from the so-called curse of dimensionality. Hence, we propose a semisupervised multiview distance metric learning (SSM-DML). SSM-DML learns the multiview distance metrics from multiple feature sets and from the labels of unlabeled cartoon characters simultaneously, under the umbrella of graph-based semisupervised learning. SSM-DML discovers complementary characteristics of different feature sets through an alternating optimization-based iterative algorithm. Therefore, SSM-DML can simultaneously accomplish cartoon character classification and dissimilarity measurement. On the basis of SSM-DML, we develop a novel system that composes the modules of multiview cartoon character classification, multiview graph-based cartoon synthesis, and multiview retrieval-based cartoon synthesis. Experimental evaluations based on the three modules suggest the effectiveness of SSM-DML in cartoon applications. Normalized graph Laplacian has been widely used in many practical machine learning algorithms, e.g., spectral clustering and semisupervised learning. However, all of them use the Euclidean distance to construct the graph Laplacian, which does not necessarily reflect the inherent distribution of the data. In this brief, we propose a method to directly optimize the normalized graph Laplacian by using pairwise constraints. The learned graph is consistent with equivalence and nonequivalence pairwise relationships, and thus it can better represent similarity between samples. Meanwhile, our approach, unlike metric learning, automatically determines the scale factor during the optimization. The learned normalized Laplacian matrix can be directly applied in spectral clustering and semisupervised learning algorithms. Comprehensive experiments demonstrate the effectiveness of the proposed approach. The biologically inspired model (BIM) proposed by Serre presents a promising solution to object categorization. It emulates the process of object recognition in primates' visual cortex by constructing a set of scale- and position-tolerant features whose properties are similar to those of the cells along the ventral stream of visual cortex. However, BIM has potential to be further improved in two aspects: mismatch by dense input and randomly feature selection due to the feedforward framework. To solve or alleviate these limitations, we develop an enhanced BIM (EBIM) in terms of the following two aspects: 1) removing uninformative inputs by imposing sparsity constraints, 2) apply a feedback loop to middle level feature selection. Each aspect is motivated by relevant psychophysical research findings. To show the effectiveness of the EBIM, we apply it to object categorization and conduct empirical studies on four computer vision data sets. Experimental results demonstrate that the EBIM outperforms the BIM and is comparable to state-of-the-art approaches in terms of accuracy. Moreover, the new system is about 20 times faster than the BIM. Traditional image representations are not suited to conventional classification methods such as the linear discriminant analysis (LDA) because of the undersample problem (USP): the dimensionality of the feature space is much higher than the number of training samples. Motivated by the successes of the two-dimensional LDA (2DLDA) for face recognition, we develop a general tensor discriminant analysis (GTDA) as a preprocessing step for LDA. The benefits of GTDA, compared with existing preprocessing methods such as the principal components analysis (PCA) and 2DLDA, include the following: 1) the USP is reduced in subsequent classification by, for example, LDA, 2) the discriminative information in the training tensors is preserved, and 3) GTDA provides stable recognition rates because the alternating projection optimization algorithm to obtain a solution of GTDA converges, whereas that of 2DLDA does not. We use human gait recognition to validate the proposed GTDA. The averaged gait images are utilized for gait representation. Given the popularity of Gabor-function-based image decompositions for image understanding and object recognition, we develop three different Gabor-function-based image representations: 1) GaborD is the sum of Gabor filter responses over directions, 2) GaborS is the sum of Gabor filter responses over scales, and 3) GaborSD is the sum of Gabor filter responses over scales and directions. The GaborD, GaborS, and GaborSD representations are applied to the problem of recognizing people from their averaged gait images. A large number of experiments were carried out to evaluate the effectiveness (recognition rate) of gait recognition based on first obtaining a Gabor, GaborD, GaborS, or GaborSD image representation, then using GDTA to extract features and, finally, using LDA for classification. The proposed methods achieved good performance for gait recognition based on image sequences from the University of South Florida (USF) HumanID Database. Experimental comparisons are made with nine state-of-the-art classification methods in gait recognition. We propose a new criterion for discriminative dimension reduction, max-min distance analysis (MMDA). Given a data set with C classes, represented by homoscedastic Gaussians, MMDA maximizes the minimum pairwise distance of these C classes in the selected low-dimensional subspace. Thus, unlike Fisher's linear discriminant analysis (FLDA) and other popular discriminative dimension reduction criteria, MMDA duly considers the separation of all class pairs. To deal with general case of data distribution, we also extend MMDA to kernel MMDA (KMMDA). Dimension reduction via MMDA/KMMDA leads to a nonsmooth max-min optimization problem with orthonormal constraints. We develop a sequential convex relaxation algorithm to solve it approximately. To evaluate the effectiveness of the proposed criterion and the corresponding algorithm, we conduct classification and data visualization experiments on both synthetic data and real data sets. Experimental results demonstrate the effectiveness of MMDA/KMMDA associated with the proposed optimization algorithm. Cartoons play important roles in many areas, but it requires a lot of labor to produce new cartoon clips. In this paper, we propose a gesture recognition method for cartoon character images with two applications, namely content-based cartoon image retrieval and cartoon clip synthesis. We first define Edge Features (EF) and Motion Direction Features (MDF) for cartoon character images. The features are classified into two different groups, namely intra-features and inter-features. An Unsupervised Bi-Distance Metric Learning (UBDML) algorithm is proposed to recognize the gestures of cartoon character images. Different from the previous research efforts on distance metric learning, UBDML learns the optimal distance metric from the heterogeneous distance metrics derived from intra-features and inter-features. Content-based cartoon character image retrieval and cartoon clip synthesis can be carried out based on the distance metric learned by UBDML. Experiments show that the cartoon character image retrieval has a high precision and that the cartoon clip synthesis can be carried out efficiently. A fast subspace analysis and feature extraction algorithm is proposed which is based on fast Haar transform and integral vector. In rapid object detection and conventional binary subspace learning, Haar-like functions have been frequently used but true Haar functions are seldom employed. In this paper we have shown that true Haar functions can be successfully used to accelerate subspace analysis and feature extraction. Both the training and testing speed of the proposed method is higher than conventional algorithms. Experimental results on face database demonstrated its effectiveness. The work presented here solves the multi-sensor centralized fusion problem in the linear Gaussian model without the measurement noise variance. We generalize the variational Bayesian approximation based adaptive Kalman filter (VB_AKF) from the single sensor filtering to a multi-sensor fusion system, and propose two new centralized fusion algorithms, i.e., VB_AKF-based augmented centralized fusion algorithm and VB_AKF-based sequential centralized fusion algorithm, to deal with the case that the measurement noise variance is unknown. The simulation results show the effectiveness of the proposed algorithms. Image quality is a key characteristic in image processing,10,11 image retrieval,12,13 and biometrics.14 In this paper, a novel reduced-reference image quality assessment method is proposed based on wavelet transform. By simulating the human visual system, we take the variance of the visual sensitive coefficients into account to measure a distorted image. The computational complexity of the proposed method is much lower compared with some existing methods. Experimental results demonstrate its advantages in terms of correlation coefficient, outlier ratio, transmitted information, and CPU cost. Moreover, it is also illustrated that the proposed method has a good accordance with human subjective perception. Both commercial and scientific applications often need to transform color images into gray-scale images, e.g., to reduce the publication cost in printing color images or to help color blind people see visual cues of color images. However, conventional color to gray algorithms are not ready for practical applications because they encounter the following problems: 1) Visual cues are not well defined so it is unclear how to preserve important cues in the transformed gray-scale images; 2) some algorithms have extremely high time cost for computation; and 3) some require human-computer interactions to have a reasonable transformation. To solve or at least reduce these problems, we propose a new algorithm based on a probabilistic graphical model with the assumption that the image is defined over a Markov random field. Thus, color to gray procedure can be regarded as a labeling process to preserve the newly well--defined visual cues of a color image in the transformed gray-scale image. Visual cues are measurements that can be extracted from a color image by a perceiver. They indicate the state of some properties of the image that the perceiver is interested in perceiving. Different people may perceive different cues from the same color image and three cues are defined in this paper, namely, color spatial consistency, image structure information, and color channel perception priority. We cast color to gray as a visual cue preservation procedure based on a probabilistic graphical model and optimize the model based on an integral minimization problem. We apply the new algorithm to both natural color images and artificial pictures, and demonstrate that the proposed approach outperforms representative conventional algorithms in terms of effectiveness and efficiency. In addition, it requires no human-computer interactions. In recent years, cross-domain learning algorithms have attracted much attention to solve labeled data insufficient problem. However, these cross-domain learning algorithms cannot be applied for subspace learning, which plays a key role in multimedia, e. g., web image annotation. This paper envisions the cross-domain discriminative subspace learning and provides an effective solution to cross-domain subspace learning. In particular, we propose the cross-domain discriminative Hessian Eigenmaps or CDHE for short. CDHE connects the training and the testing samples by minimizing the quadratic distance between the distribution of the training samples and that of the testing samples. Therefore, a common subspace for data representation can be preserved. We basically expect the discriminative information to separate the concepts in the training set can be shared to separate the concepts in the testing set as well and thus we have a chance to address above cross-domain problem duly. The margin maximization is duly adopted in CDHE so the discriminative information for separating different classes can be well preserved. Finally, CDHE encodes the local geometry of each training class in the local tangent space which is locally isometric to the data manifold and thus can locally preserve the intra-class local geometry. Experimental evidence on real world image datasets demonstrates the effectiveness of CDHE for cross-domain web image annotation. Multiframe super-resolution (SR) reconstruction aims to produce a high-resolution (HR) image using a set of low-resolution (LR) images. In the process of reconstruction, fuzzy registration usually plays a critical role. It mainly focuses on the correlation between pixels of the candidate and the reference images to reconstruct each pixel by averaging all its neighboring pixels. Therefore, the fuzzy-registration-based SR performs well and has been widely applied in practice. However, if some objects appear or disappear among LR images or different angle rotations exist among them, the correlation between corresponding pixels becomes weak. Thus, it will be difficult to use LR images effectively in the process of SR reconstruction. Moreover, if the LR images are noised, the reconstruction quality will be affected seriously. To address or at least reduce these problems, this paper presents a novel SR method based on the Zernike moment, to make the most of possible details in each LR image for high-quality SR reconstruction. Experimental results show that the proposed method outperforms existing methods in terms of robustness and visual effects. Sketch-photo synthesis is one of the important research issues of heterogeneous image transformation. Some available popular synthesis methods, like locally linear embedding (LLE), usually generate sketches or photos with lower definition and blurred details, which reduces the visual quality and the recognition rate across the heterogeneous images. In order to improve the quality of the synthesized images, a multi-dictionary sparse representation based face sketch-photo synthesis model is constructed. In the proposed model, LLE is used to estimate an initial sketch or photo, while the multi-dictionary sparse representation model is applied to generate the high frequency and detail information. Finally, by linear superimposing, the enhanced face sketch or photo can be obtained. Experimental results show that sketches and photos synthesized by the proposed method have higher definition and much richer detail information resulting in a higher face recognition rate between sketches and photos. Reconstruction- and example-based super-resolution (SR) methods are promising for restoring a high-resolution (HR) image from low-resolution (LR) image(s). Under large magnification, reconstruction-based methods usually fail to hallucinate visual details while example-based methods sometimes introduce unexpected details. Given a generic LR image, to reconstruct a photo-realistic SR image and to suppress artifacts in the reconstructed SR image, we introduce a multi-scale dictionary to a novel SR method that simultaneously integrates local and non-local priors. The local prior suppresses artifacts by using steering kernel regression to predict the target pixel from a small local area. The non-local prior enriches visual details by taking a weighted average of a large neighborhood as an estimate of the target pixel. Essentially, these two priors are complementary to each other. Experimental results demonstrate that the proposed method can produce high quality SR recovery both quantitatively and perceptually. From data mining to computer vision, from visual surveillance to biometrics research, from biomedical imaging to bioinformatics, and from multimedia retrieval to information management, a large amount of data are naturally represented by multidimensional arrays, i.e., tensors. However, conventional probabilistic graphical models with probabilistic inference only model data in vectorformat, although they are very important in many statistical problems, e.g., model selection. Is it possible to construct multilinear probabilistic graphical models for tensorformat data to conduct probabilistic inference, e.g., model selection? This paper provides a positive answer based on the proposed decoupled probabilistic model by developing the probabilistic tensor analysis(PTA), which selects suitable model for tensorformat data modeling based on Akaike information criterion(AIC) and Bayesian information criterion(BIC). Empirical studies demonstrate that PTA associated with AIC and BIC selects correct number of models. In the original paper named above (ibid., vol. 20, no. 9, pp. 2627-2635, Sep 2011), Table II incorrectly appeared as a duplicate of Table III. Instead, Table II should have correctly appeared as shown here. Past decades, numerous spectral analysis based algorithms have been proposed for dimensionality reduction, which plays an important role in machine learning and artificial intelligence. However, most of these existing algorithms are developed intuitively and pragmatically, i.e., on the base of the experience and knowledge of experts for their own purposes. Therefore, it will be more informative to provide some a systematic framework for understanding the common properties and intrinsic differences in the algorithms. In this paper, we propose such a framework, i.e., ldquopatch alignmentrdquo, which consists of two stages: part optimization and whole alignment. With the proposed framework, various algorithms including the conventional linear algorithms and the manifold learning algorithms are reformulated into a unified form, which gives us some new understandings on these algorithms. The regularization principals [31] lead approximation schemes to deal with various learning problems, e.g., the regularization of the norm in a reproducing kernel Hilbert space for the ill-posed problem. In this paper, we present a family of subspace learning algorithms based on a new form of regularization, which transfers the knowledge gained in training samples to testing samples. In particular, the new regularization minimizes the Bregman divergence between the distribution of training samples and that of testing samples in the selected subspace, so it boosts the performance when training and testing samples are not independent and identically distributed. To test the effectiveness of the proposed regularization, we introduce it to popular subspace learning algorithms, e.g., principal components analysis (PCA) for cross-domain face modeling; and Fisher's linear discriminant analysis (FLDA), locality preserving projections (LPP), marginal Fisher's analysis (MFA), and discriminative locality alignment (DLA) for cross-domain face recognition and text categorization. Finally, we present experimental evidence on both face image data sets and text data sets, suggesting that the proposed Bregman divergence-based regularization is effective to deal with cross-domain learning problems. This paper presents a new region-based unified tensor level set model for image segmentation. This model introduces a three-order tensor to comprehensively depict features of pixels, e.g., gray value and the local geometrical features, such as orientation and gradient, and then, by defining a weighted distance, we generalized the representative region-based level set method from scalar to tensor. The proposed model has four main advantages compared with the traditional representative method as follows. First, involving the Gaussian filter bank, the model is robust against noise, particularly the salt- and pepper-type noise. Second, considering the local geometrical features, e.g., orientation and gradient, the model pays more attention to boundaries and makes the evolving curve stop more easily at the boundary location. Third, due to the unified tensor pixel representation representing the pixels, the model segments images more accurately and naturally. Fourth, based on a weighted distance definition, the model possesses the capacity to cope with data varying from scalar to vector, then to high-order tensor. We apply the proposed method to synthetic, medical, and natural images, and the result suggests that the proposed method is superior to the available representative region-based level set method. Mass boundary segmentation plays an important role in computer aided diagnosis (CAD) system. Since the shape and boundary are crucial discriminant features in CAD, the active contour methods are more competitive in mass segmentation. However, the general active contour methods are not so effective for some cases, because most masses possess very blurry margin that easily induce the contour leaking. To the end, this paper presents an improved geometric active contour for mass segmentation. It firstly introduces the morphological concentric layer model for automatically initializing. Then an embedded level set is used to extract the adaptive shape constraints. For refining the boundary, a new shape constraint function and stopping function are designed for the enhanced geometric active contour method. The proposed method is tested on real mammograms containing masses, and the results suggest that the proposed method could effectively restrain the contour leaking and get better segmented results than general active contour methods. Conventional linear subspace learning methods like principal component analysis (PCA), linear discriminant analysis (LDA) derive subspaces from the whole data set. These approaches have limitations in the sense that they are linear while the data distribution we are trying to model is typically nonlinear. Moreover, these algorithms fail to incorporate local variations of the intrinsic sample distribution manifold. Therefore, these algorithms are ineffective when applied on large scale datasets. Kernel versions of these approaches can alleviate the problem to certain degree but face a serious computational challenge when data set is large, where the computing involves Eigen/QP problems of size N \\u00d7 N. When N is large, kernel versions are not computationally practical. To tackle the aforementioned problems and improve recognition/searching performance, especially on large scale image datasets, we propose a novel local subspace indexing model for image search termed Subspace Indexing Model on Grassmann Manifold (SIM-GM). SIM-GM partitions the global space into local patches with a hierarchical structure; the global model is, therefore, approximated by piece-wise linear local subspace models. By further applying the Grassmann manifold distance, SIM-GM is able to organize localized models into a hierarchy of indexed structure, and allow fast query selection of the optimal ones for classification. Our proposed SIM-GM enjoys a number of merits: 1) it is able to deal with a large number of training samples efficiently; 2) it is a query-driven approach, i.e., it is able to return an effective local space model, so the recognition performance could be significantly improved; 3) it is a common framework, which can incorporate many learning algorithms. Theoretical analysis and extensive experimental results confirm the validity of this model. Objective: Glaucoma is an irreversible chronic eye disease that leads to vision loss. As it can be slowed down through treatment, detecting the disease in time is important. However, many patients are unaware of the disease because it progresses slowly without easily noticeable symptoms. Currently, there is no effective method for low-cost population-based glaucoma detection or screening. Recent studies have shown that automated optic nerve head assessment from 2-D retinal fundus images is promising for low-cost glaucoma screening. In this paper, we propose a method for cup to disc ratio (CDR) assessment using 2-D retinal fundus images. Methods: In the proposed method, the optic disc is first segmented and reconstructed using a novel sparse dissimilarity-constrained coding (SDC) approach which considers both the dissimilarity constraint and the sparsity constraint from a set of reference discs with known CDRs. Subsequently, the reconstruction coefficients from the SDC are used to compute the CDR for the testing disc. Results: The proposed method has been tested for CDR assessment in a database of 650 images with CDRs manually measured by trained professionals previously. Experimental results show an average CDR error of 0.064 and correlation coefficient of 0.67 compared with the manual CDRs, better than the state-of-the-art methods. Our proposed method has also been tested for glaucoma screening. The method achieves areas under curve of 0.83 and 0.88 on datasets of 650 and 1676 images, respectively, outperforming other methods. Conclusion: The proposed method achieves good accuracy for glaucoma detection. Significance: The method has a great potential to be used for large-scale population-based glaucoma screening. Inspired by human visual cognition mechanism, this paper first presents a scene classification method based on an improved standard model feature. Compared with state-of-the-art efforts in scene classification, the newly proposed method is more robust, more selective , and of lower complexity. These advantages are demonstrated by two sets of experiments on both our own database and standard public ones. Furthermore, occlusion and disorder problems in scene classification in video surveillance are also first studied in this paper. Given several tasks, multi-task learning (MTL) learns multiple tasks jointly by exploring the interdependence between them. The basic assumption in MTL is that those tasks are indeed related. Existing MTL methods model the task relatedness/ interdependence in two different ways, either common parameter-sharing or common featuresharing across tasks. In this paper, we propose a novel multi-task learning method to jointly learn shared parameters and shared feature representation. Our objective is to learn a set of common features with which the tasks are related as closely as possible, therefore common parameters shared across tasks can be optimally learned. We present a detailed deviation of our multi-task learning method and propose an alternating algorithm to solve the non-convex optimization problem. We further present a theoretical bound which directly demonstrates that the proposed multi-task learning method can successfully model the relatedness via joint common parameter- and common feature-learning. Extensive experiments are conducted on several real world multi-task learning datasets. All results demonstrate the effectiveness of our multitask model and feature joint learning method. Event detection is one of the most fundamental components for various kinds of domain applications of video information system. In recent years, it has gained a considerable interest of practitioners and academics from different areas. While detecting video event has been the subject of extensive research efforts recently, much less existing approach has considered multimodal information and related efficiency issues. In this paper, we use a subspace selection technique to achieve fast and accurate video event detection using a subspace selection technique. The approach is capable of discriminating different classes and preserving the intramodal geometry of samples within an identical class. With the method, feature vectors presenting different kind of multi data can be easily projected from different identities and modalities onto a unified subspace, on which recognition process can be performed. Furthermore, the training stage is carried out once and we have a unified transformation matrix to project different modalities. Unlike existing multimodal detection systems, the new system works well when some modalities are not available. Experimental results based on soccer video and TRECVID news video collections demonstrate the effectiveness, efficiency and robustness of the proposed MMP for individual recognition tasks in comparison to the existing approaches. Correspondence construction of objects in key frames is the precondition for inbetweening and coloring in 2-D computer-assisted animation production. Since each frame of an animation consists of multiple layers, objects are complex in terms of shape and structure. Therefore, existing shape-matching algorithms specifically designed for simple structures such as a single closed contour cannot perform well on objects constructed by multiple contours with an open shape. This paper introduces a semisupervised patch alignment framework for complex object correspondence construction. In particular, the new framework constructs local patches for each point on an object and aligns these patches in a new feature space, in which correspondences between objects can be detected by the subsequent clustering. For local patch construction, pairwise constraints, which indicate the corresponding points (must link) or unfitting points (cannot link), are introduced by users to improve the performance of correspondence construction. This kind of input is convenient for animation software users via user-friendly interfaces. A dozen of experimental results on our cartoon data set that is built on industrial production suggest the effectiveness of the proposed framework for constructing correspondences of complex objects. As an extension of our framework, additional shape retrieval experiments on MPEG-7 data set show that its performance is comparable with that of a prominent algorithm published in T-PAMI 2009. In computer vision and multimedia search, it is common to use multiple features from different views to represent an object. For example, to well characterize a natural scene image, it is essential to find a set of visual features to represent its color, texture, and shape information and encode each feature into a vector. Therefore, we have a set of vectors in different spaces to represent the image. Conventional spectral-embedding algorithms cannot deal with such datum directly, so we have to concatenate these vectors together as a new vector. This concatenation is not physically meaningful because each feature has a specific statistical property. Therefore, we develop a new spectral-embedding algorithm, namely, multiview spectral embedding (MSE), which can encode different features in different ways, to achieve a physically meaningful embedding. In particular, MSE finds a low-dimensional embedding wherein the distribution of each view is sufficiently smooth, and MSE explores the complementary property of different views. Because there is no closed-form solution for MSE, we derive an alternating optimization-based iterative algorithm to obtain the low-dimensional embedding. Empirical evaluations based on the applications of image retrieval, video annotation, and document clustering demonstrate the effectiveness of the proposed approach. The capability of inferring colours from the texture (grayscale contents) of an image is useful in many application areas, when the imaging device/environment is limited. Traditional manual or limited automatic colour assignment involves intensive human effort. In this paper, we have developed a user-friendly colourisation technique, where the algorithm learns the relation between textures and colours in a user-provided example image and applies the relation to predict the colours in the target image. The key contribution of the proposed technique is trifold. First, we have explicitly built a linear model for the texture-colour relation. Second, we have considered the global non-linear structure of the data distribution by applying the linear model locally; and the local area is determined automatically by sparsity constraints. Third, we have introduced semantic information to further improve the colourisation. Examples demonstrate the effectiveness of the proposed techniques. Moreover, we have conducted a subjective study, where user experience supports the superiority of our method over existing techniques. Single trial electroencephalogram (EEG) classification is essential in developing brain-computer interfaces (BCIs). However, popular classification algorithms, e.g., common spatial patterns (CSP), usually highly depend on the prior neurophysiologic knowledge for noise removing, although this knowledge is not always known in practical applications. In this paper, a novel tensor-based scheme is proposed for single trial EEG classification, which performs well without the prior neurophysiologic knowledge. In this scheme, EEG signals are represented in the spatial-spectral-temporal domain by the wavelet transform, the multilinear discriminative subspace is reserved by the general tensor discriminant analysis (GTDA), redundant indiscriminative patterns are removed by Fisher score, and the classification is conducted by the support vector machine (SVM). Applications to three datasets confirm the effectiveness and the robustness of the proposed tensor scheme in analyzing EEG signals, especially in the case of lacking prior neurophysiologic knowledge. In this paper, we propose a new low-rank tensor model based on the circulant algebra, namely, twist tensor nuclear norm or t-TNN for short. The twist tensor denotes a 3-way tensor representation to laterally store 2D data slices in order. On one hand, t-TNN convexly relaxes the tensor multi-rank of the twist tensor in the Fourier domain, which allows an efficient computation using FFT. On the other, t-TNN is equal to the nuclear norm of block circulant matricization of the twist tensor in the original domain, which extends the traditional matrix nuclear norm in a block circulant way. We test the t-TNN model on a video completion application that aims to fill missing values and the experiment results validate its effectiveness, especially when dealing with video recorded by a non-stationary panning camera. The block circulant matricization of the twist tensor can be transformed into a circulant block representation with nuclear norm invariance. This representation, after transformation, exploits the horizontal translation relationship between the frames in a video, and endows the t-TNN model with a more powerful ability to reconstruct panning videos than the existing state-of-the-art low-rank models. In this paper, we introduce an efficient tensor to vector projection algorithm for human gait feature representation and recognition. The proposed approach is based on the multi-dimensional or tensor signal processing technology, which finds a low-dimensional tensor subspace of original input gait sequence tensors while most of the data variation has been well captured. In order to further enhance the class separability and avoid the potential overfitting, we adopt a discriminative locality preserving projection with sparse regularization to transform the refined tensor data to the final vector feature representation for subsequent recognition. Numerous experiments are carried out to evaluate the effectiveness of the proposed sparse and discriminative tensor to vector projection algorithm, and the proposed method achieves good performance for human gait recognition using the sequences from the University of South Florida (USF) HumanID Database. HighlightsThis paper proposes a tensor to vector projection for human gait representation.A novel discriminative locality preserving projection is adopted in the algorithm.A sparse regularization is involved to avoid the potential overfitting. In this paper, we present a robust approach to digital watermarking embedding and retrieval for digital images. The affine-invariant point detector is used to extract feature regions of the given host image. Image normalization and dominant gradient orientation alignment are applied to these feature regions to achieve scaling and rotation invariance. Thereafter, watermarks are embedded in these feature regions. The advantage of this proposed approach is demonstrated by a large amount of experiments, which can against different attacks, such as common image processing, rotation, scaling, cropping, and several other affine transformations. User interaction is an effective way to handle the semantic gap problem in image annotation. To minimize user effort in the interactions, many active learning methods were proposed. These methods treat the semantic concepts individually or correlatively. However, they still neglect the key motivation of user feedback: to tackle the semantic gap. The size of the semantic gap of each concept is an important factor that affects the performance of user feedback. User should pay more efforts to the concepts with large semantic gaps, and vice versa. In this paper, we propose a semantic-gap-oriented active learning method, which incorporates the semantic gap measure into the information-minimization-based sample selection strategy. The basic learning model used in the active learning framework is an extended multilabel version of the sparse-graph-based semisupervised learning method that incorporates the semantic correlation. Extensive experiments conducted on two benchmark image data sets demonstrated the importance of bringing the semantic gap measure into the active learning process. This paper investigates how to blindly evaluate the visual quality of an image by learning rules from linguistic descriptions. Extensive psychological evidence shows that humans prefer to conduct evaluations qualitatively rather than numerically. The qualitative evaluations are then converted into the numerical scores to fairly benchmark objective image quality assessment (IQA) metrics. Recently, lots of learning-based IQA models are proposed by analyzing the mapping from the images to numerical ratings. However, the learnt mapping can hardly be accurate enough because some information has been lost in such an irreversible conversion from the linguistic descriptions to numerical scores. In this paper, we propose a blind IQA model, which learns qualitative evaluations directly and outputs numerical scores for general utilization and fair comparison. Images are represented by natural scene statistics features. A discriminative deep model is trained to classify the features into five grades, corresponding to five explicit mental concepts, i.e., excellent, good, fair, poor, and bad. A newly designed quality pooling is then applied to convert the qualitative labels into scores. The classification framework is not only much more natural than the regression-based models, but also robust to the small sample size problem. Thorough experiments are conducted on popular databases to verify the model\\u2019s effectiveness, efficiency, and robustness. Recently, relevance feedback (RF) in content-based image retrieval (CBIR) has been implemented as an online binary classifier to separate the positive samples from the negative samples, where both sets of samples are labeled by the user. In many applications, it is reasonable to assume that all the positive samples are alike and thus that the region of the feature space occupied by the positive samples can be described by a single hypersurface. However, for the negative samples, previous RF methods either treat each one of the negative samples as an isolated point or assume the whole negative set can be described by a single convex hypersurface. In this paper, we argue that these treatments of the negative samples are not sound. Our belief is all positive samples are included in a set and the negative samples split into a small number of subsets, each one of which has a simple distribution. Therefore, we first cluster the negative samples into several groups; for each such negative group, we build a marginal convex machine (MCM) subclassifier between it and the single positive group which results in a series of subclassifiers. These subclassifiers are then incorporated into a biased MCM (BMCM) for RF. Experiments were carried out to prove the advantages of BMCM-based RF over previous methods for RF A challenging problem of multi-label learning is that both the label space and the model complexity will grow rapidly with the increase in the number of labels, and thus makes the available training samples insufficient for training a proper model. In this paper, we eliminate this problem by learning a mapping of each label in the feature space as a robust subspace, and formulating the prediction as finding the group sparse representation of a given instance on the subspace ensemble. We term this approach as \\u201cmulti-label subspace ensemble (MSE)\\u201d. In the training stage, the data matrix is decomposed as the sum of several low-rank matrices and a sparse residual via a randomized optimization, where each low-rank part defines a subspace mapped by a label. In the prediction stage, the group sparse representation on the subspace ensemble is estimated by group lasso. Experiments on several benchmark datasets demonstrate the appealing performance of MSE. The features used in many image analysis-based applications are frequently of very high dimension. Feature extraction offers several advantages in high-dimensional cases, and many recent studies have used multi-task feature extraction approaches, which often outperform single-task feature extraction approaches. However, most of these methods are limited in that they only consider data represented by a single type of feature, even though features usually represent images from multiple modalities. We, therefore, propose a novel large margin multi-modal multi-task feature extraction (LM3FE) framework for handling multi-modal features for image classification. In particular, LM3FE simultaneously learns the feature extraction matrix for each modality and the modality combination coefficients. In this way, LM3FE not only handles correlated and noisy features, but also utilizes the complementarity of different modalities to further help reduce feature redundancy in each modality. The large margin principle employed also helps to extract strongly predictive features, so that they are more suitable for prediction (e.g., classification). An alternating algorithm is developed for problem optimization, and each subproblem can be efficiently solved. Experiments on two challenging real-world image data sets demonstrate the effectiveness and superiority of the proposed method. We propose a new method for fine-grained object recognition that employs part-level annotations and deep convolutional neural networks (CNNs) in a unified framework. Although both schemes have been widely used to boost recognition performance, due to the difficulty in acquiring detailed part annotations, strongly supervised fine-grained datasets are usually too small to keep pace with the rapid evolution of CNN architectures. In this paper, we solve this problem by exploiting inexhaustible web data. The proposed method improves classification accuracy in two ways: more discriminative CNN feature representations are generated using a training set augmented by collecting a large number of part patches from weakly supervised web images, and more robust object classifiers are learned using a multi-instance learning algorithm jointly on the strong and weak datasets. Despite its simplicity, the proposed method delivers a remarkable performance improvement on the CUB200-2011 dataset compared to baseline part-based R-CNN methods, and achieves the highest accuracy on this dataset even in the absence of test image annotations. Image retrieval plays an increasingly important role in our daily lives. There are many factors which affect the quality of image search results, including chosen search algorithms, ranking functions, and indexing features. Applying different settings for these factors generates search result lists with varying levels of quality. However, no setting can always perform optimally for all queries. Therefore, given a set of search result lists generated by different settings, it is crucial to automatically determine which result list is the best in order to present it to users. This paper aims to solve this problem and makes four main innovations. First, a preference learning model is proposed to quantitatively study and formulate the best image search result list identification problem. Second, a set of valuable preference learning related features is proposed by exploring the visual characters of returned images. Third, a query-dependent preference learning model is further designed for building a more precise and query-specific model. Fourth, the proposed approach has been tested on a variety of applications including reranking ability assessment, optimal search engine selection, and synonymous query suggestion. Extensive experimental results on three image search datasets demonstrate the effectiveness and promising potential of the proposed method. In the context of fine-grained visual categorization, the ability to interpret models as human-understandable visual manuals is sometimes as important as achieving high classification accuracy. In this paper, we propose a novel Part-Stacked CNN architecture that explicitly explains the finegrained recognition process by modeling subtle differences from object parts. Based on manually-labeled strong part annotations, the proposed architecture consists of a fully convolutional network to locate multiple object parts and a two-stream classification network that encodes object-level and part-level cues simultaneously. By adopting a set of sharing strategies between the computation of multiple object parts, the proposed architecture is very efficient running at 20 frames/sec during inference. Experimental results on the CUB-200-2011 dataset reveal the effectiveness of the proposed architecture, from multiple perspectives of classification accuracy, model interpretability, and efficiency. Being able to provide interpretable recognition results in realtime, the proposed method is believed to be effective in practical applications. Localizing facial landmarks is a fundamental step in facial image analysis. However, the problem continues to be challenging due to the large variability in expression, illumination, pose, and the existence of occlusions in real-world face images. In this paper, we present a dual sparse constrained cascade regression model for robust face alignment. Instead of using the least-squares method during the training process of regressors, sparse constraint is introduced to select robust features and compress the size of the model. Moreover, sparse shape constraint is incorporated between each cascade regression, and the explicit shape constraints are able to suppress the ambiguity in local features. To improve the model\\u2019s adaptation to large pose variation, face pose is estimated by five fiducial landmarks located by deep convolutional neuron network, which is used to adaptively design the cascade regression model. To the best of our best knowledge, this is the first attempt to fuse explicit shape constraint (sparse shape constraint) and implicit context information (sparse feature selection) for robust face alignment in the framework of cascade regression. Extensive experiments on nine challenging wild data sets demonstrate the advantages of the proposed method over the state-of-the-art methods. Regarded as two independent tasks, both face identification and facial expression recognition perform poorly given small size training sets. To address this problem, we propose a multi-task facial inference model (MT-FIM) for simultaneous face identification and facial expression recognition. In particular, face identification and facial expression recognition are learnt simultaneously by extracting and utilizing appropriate shared information across them in the framework of multi-task learning, in which the shared information refers to the parameter controlling the sparsity. MT-FIM simultaneously minimizes the within-class scatter and maximizes the distance between different classes to enable the robust performance of each individual task. We conduct comprehensive experiments on three face image databases. The experimental results show that our algorithm outperforms the state-of-the-art algorithms. Optimization-based filtering smoothes an image by minimizing a fidelity function and simultaneously preserves edges by exploiting a sparse norm penalty over gradients. It has obtained promising performance in practical problems, such as detail manipulation, HDR compression and deblurring, and thus has received increasing attentions in fields of graphics, computer vision and image processing. This paper derives a new type of image filter called sparse norm filter (SNF) from optimization-based filtering. SNF has a very simple form, introduces a general class of filtering techniques, and explains several classic filters as special implementations of SNF, e.g. the averaging filter and the median filter. It has advantages of being halo free, easy to implement, and low time and memory costs (comparable to those of the bilateral filter). Thus, it is more generic than a smoothing operator and can better adapt to different tasks. We validate the proposed SNF by a wide variety of applications including edge-preserving smoothing, outlier tolerant filtering, detail manipulation, HDR compression, non-blind deconvolution, image segmentation, and colorization. In low-rank & sparse matrix decomposition, the entries of the sparse part are often assumed to be i.i.d. sampled from a random distribution. But the structure of sparse part, as the central interest of many problems, has been rarely studied. One motivating problem is tracking multiple sparse object flows (motions) in video. We introduce \\\"shifted subspaces tracking (SST)\\\" to segment the motions and recover their trajectories by exploring the low-rank property of background and the shifted subspace property of each motion. SST is composed of two steps, background modeling and flow tracking. In step 1, we propose \\\"semi-soft GoDec\\\" to separate all the motions from the low-rank background L as a sparse outlier S. Its soft-thresholding in updating S significantly speeds up GoDec and facilitates the parameter tuning. In step 2, we update X as S obtained in step 1 and develop \\\"SST algorithm\\\" further decomposing X as X = \\u03a3i=1k L(i)o\\u03c4(i)+ S+G, wherein L(i) is a low-rank matrix storing the ith flow after transformation \\u03c4(i). SST algorithm solves k sub-problems in sequel by alternating minimization, each of which recovers one L(i) and its \\u03c4(i) by randomized method. Sparsity of L(i) and between-frame affinity are leveraged to save computations. We justify the effectiveness of SST on surveillance video sequences. Blur in facial images significantly impedes the efficiency of recognition approaches. However, most existing blind deconvolution methods cannot generate satisfactory results due to their dependence on strong edges, which are sufficient in natural images but not in facial images. In this paper, we represent point spread functions (PSFs) by the linear combination of a set of pre-defined orthogonal PSFs, and similarly, an estimated intrinsic (EI) sharp face image is represented by the linear combination of a set of pre-defined orthogonal face images. In doing so, PSF and EI estimation is simplified to discovering two sets of linear combination coefficients, which are simultaneously found by our proposed coupled learning algorithm. To make our method robust to different types of blurry face images, we generate several candidate PSFs and EIs for a test image, and then, a non-blind deconvolution method is adopted to generate more EIs by those candidate PSFs. Finally, we deploy a blind image quality assessment metric to automatically select the optimal EI. Thorough experiments on the facial recognition technology database, extended Yale face database B, CMU pose, illumination, and expression (PIE) database, and face recognition grand challenge database version 2.0 demonstrate that the proposed approach effectively restores intrinsic sharp face images and, consequently, improves the performance of face recognition. Automatic face alignment is a fundamental step in facial image analysis. However, this problem continues to be challenging due to the large variability of expression, illumination, occlusion, pose, and detection drift in the real-world face images. In this paper, we present a multi-view, multi-scale and multi-component cascade shape regression (M3CSR) model for robust face alignment. Firstly, face view is estimated according to the deformable facial parts for learning view specified CSR, which can decrease the shape variance, alleviate the drift of face detection and accelerate shape convergence. Secondly, multi-scale HoG features are used as the shape-index features to incorporate local structure information implicitly, and a multi-scale optimization strategy is adopted to avoid trapping in local optimum. Finally, a component-based shape refinement process is developed to further improve the performance of face alignment. Extensive experiments on the IBUG dataset and the 300-W challenge dataset demonstrate the superiority of the proposed method over the state-of-the-art methods. We investigate how face detection affects face alignment.We improve the CSR model by multi-view, multi-scale and multi-component strategies.We obtain impressive results on the IBUG and 300-W challenge datasets. The k-dimensional coding schemes refer to a collection of methods that attempt to represent data using a set of representative k-dimensional vectors and include nonnegative matrix factorization, dictionary learning, sparse coding, k-means clustering, and vector quantization as special cases. Previous generalization bounds for the reconstruction error of the k-dimensional coding schemes are mainly dimensionality-independent. A major advantage of these bounds is that they can be used to analyze the generalization error when data are mapped into an infinite-or high-dimensional feature space. However, many applications use finite-dimensional data features. Can we obtain dimensionality-dependent generalization bounds for k-dimensional coding schemes that are tighter than dimensionality-independent bounds when data are in a finite-dimensional feature space? Yes. In this letter, we address this problem and derive a dimensionality-dependent generalization bound for k-dimensional coding schemes by bounding the covering number of the loss function class induced by the reconstruction error. The bound is of order , where m is the dimension of features, k is the number of the columns in the linear implementation of coding schemes, and n is the size of sample, when n is finite and when n is infinite. We show that our bound can be tighter than previous results because it avoids inducing the worst-case upper bound on k of the loss function. The proposed generalization bound is also applied to some specific coding schemes to demonstrate that the dimensionality-dependent bound is an indispensable complement to the dimensionality-independent generalization bounds. In this paper, we present a simple but effective method for multi-label classification (MLC), termed Correlated Logistic Models (Corrlog), which extends multiple Independent Logistic Regressions (ILRs) by modeling the pairwise correlation between labels. Algorithmically, we propose an efficient method for learning parameters of Corrlog, which is based on regularized maximum pseudolikelihood estimation and has a linear computational complexity with respect to the number of labels. Theoretically, we show that Corrlog enjoys a satisfying generalization bound which is independent of the number of labels. The effectiveness of Corrlog on modeling label correlations is illustrated by a toy example, and further experiments on real data show that Corrlog achieves competitive performance compared with popular MLC algorithms. In multi-label learning, an example is represented by a descriptive feature associated with several labels. Simply considering labels as independent or correlated is crude; it would be beneficial to define and exploit the causality between multiple labels. For example, an image label 'lake' implies the label 'water', but not vice versa. Since the original features are a disorderly mixture of the properties originating from different labels, it is intuitive to factorize these raw features to clearly represent each individual label and its causality relationship. Following the large-margin principle, we propose an effective approach to discover the causal features of multiple labels, thus revealing the causality between labels from the perspective of feature. We show theoretically that the proposed approach is a tight approximation of the empirical multi-label classification error, and the causality revealed strengthens the consistency of the algorithm. Extensive experimentations using synthetic and real-world data demonstrate that the proposed algorithm effectively discovers label causality, generates causal features, and improves multi-label learning. Inferring potential links is a fundamental problem in social networks. In the link recommendation problem, the aim is to suggest a list of potential people to each user, ordered by the preferences of the user. Although various approaches have been developed to solve this problem, the difficulty of producing a ranking list with high precision at the top -- the most important consideration for real world applications -- remains largely an open problem. In this work, we propose two top-k link recommendation algorithms which focus on optimizing the top ranked links. For this purpose, we define a cost-sensitive ranking loss which penalizes the mistakes at the top of a ranked list more than the mistakes at the bottom. In particular, we propose a log loss, derive its surrogate, and formulate a top-k link recommendation model by optimizing this surrogate loss function based upon latent features. Moreover, we extend this top-k link recommendation model by incorporating both the latent features and explicit features of the network. Finally, an efficient learning scheme to learn the model parameters is provided. We conduct empirical studies based upon four real world datasets, i.e., Wikipedia, CondMat, Epinions, and MovieLens 1M, of which the largest network contains more than 70 thousand nodes and over one million links. Our experiments demonstrate that the proposed algorithms outperform several state-of-the-art methods. Photo enhancement refers to the process of increasing the aesthetic appeal of a photo, such as changing the photo aspect ratio and spatial recomposition. It is a widely used technique in the printing industry, graphic design, and cinematography. In this paper, we propose a unified and socially aware photo enhancement framework which can leverage the experience of photographers with various aesthetic topics (e.g., portrait and landscape). We focus on photos from the image hosting site Flickr, which has 87 million users and to which more than 3.5 million photos are uploaded daily. First, a tagwise regularized topic model is proposed to describe the aesthetic topic of each Flickr user, and coherent and interpretable topics are discovered by leveraging both the visual features and tags of photos. Next, a graph is constructed to describe the similarities in aesthetic topics between the users. Noticeably, densely connected users have similar aesthetic topics, which are categorized into different communities by a dense subgraph mining algorithm. Finally, a probabilistic model is exploited to enhance the aesthetic attractiveness of a test photo by leveraging the photographic experiences of Flickr users from the corresponding communities of that photo. Paired-comparison-based user studies show that our method performs competitively on photo retargeting and recomposition. Moreover, our approach accurately detects aesthetic communities in a photo set crawled from nearly 100000 Flickr users. Recognizing complex human actions is very challenging, since training a robust learning model requires a large amount of labeled data, which is difficult to acquire. Considering that each complex action is composed of a sequence of simple actions which can be easily obtained from existing data sets, this paper presents a simple to complex action transfer learning model (SCA-TLM) for complex human action recognition. SCA-TLM improves the performance of complex action recognition by leveraging the abundant labeled simple actions. In particular, it optimizes the weight parameters, enabling the complex actions to be learned to be reconstructed by simple actions. The optimal reconstruct coefficients are acquired by minimizing the objective function, and the target weight parameters are then represented as a combination of source weight parameters. The main advantage of the proposed SCA-TLM compared with existing approaches is that we exploit simple actions to recognize complex actions instead of only using complex actions as training samples. To validate the proposed SCA-TLM, we conduct extensive experiments on two well-known complex action data sets: 1) Olympic Sports data set and 2) UCF50 data set. The results show the effectiveness of the proposed SCA-TLM for complex action recognition. In this paper, we present multinomial latent logistic regression (MLLR), a new learning paradigm that introduces latent variables to logistic regression. By inheriting the advantages of logistic regression, MLLR is efficiently optimized using the second-order derivatives and provides effective probabilistic analysis on output predictions. MLLR is particularly effective in weakly supervised settings where the latent variable has an exponential number of possible values. The effectiveness of MLLR is demonstrated on four different image understanding applications, including a new challenging architectural style classification task. Furthermore, we show that MLLR can be generalized to general structured output prediction, and in doing so, we provide a thorough investigation of the connections and differences between MLLR and existing related algorithms, including latent structural SVMs and hidden conditional random fields. Recent years have witnessed the success of binary hashing techniques in approximate nearest neighbor search. In practice, multiple hash tables are usually built using hashing to cover more desired results in the hit buckets of each table. However, rare work studies the unified approach to constructing multiple informative hash tables using any type of hashing algorithms. Meanwhile, for multiple table search, it also lacks of a generic query-adaptive and fine-grained ranking scheme that can alleviate the binary quantization loss suffered in the standard hashing techniques. To solve the above problems, in this paper, we first regard the table construction as a selection problem over a set of candidate hash functions. With the graph representation of the function set, we propose an efficient solution that sequentially applies normalized dominant set to finding the most informative and independent hash functions for each table. To further reduce the redundancy between tables, we explore the reciprocal hash tables in a boosting manner, where the hash function graph is updated with high weights emphasized on the misclassified neighbor pairs of previous hash tables. To refine the ranking of the retrieved buckets within a certain Hamming radius from the query, we propose a query-adaptive bitwise weighting scheme to enable fine-grained bucket ranking in each hash table, exploiting the discriminative power of its hash functions and their complement for nearest neighbor search. Moreover, we integrate such scheme into the multiple table search using a fast, yet reciprocal table lookup algorithm within the adaptive weighted Hamming radius. In this paper, both the construction method and the query-adaptive search method are general and compatible with different types of hashing algorithms using different feature spaces and/or parameter settings. Our extensive experiments on several large-scale benchmarks demonstrate that the proposed techniques can significantly outperform both the naive construction methods and the state-of-the-art hashing algorithms. Localizing objects of interest in images when provided with only image-level labels is a challenging visual recognition task. Previous efforts have required carefully designed features and have difficulty in handling images with cluttered backgrounds. Up-scaling to large datasets also poses a challenge to applying these methods to real applications. In this paper, we propose an efficient and effective learning framework called MILinear, which is able to learn an object localization model from large-scale data without using bounding box annotations. We integrate rich general prior knowledge into a learning model using a large pre-trained convolutional network. Moreover, to reduce ambiguity in positive images, we present a bag-splitting algorithm that iteratively generates new negative bags from positive ones. We evaluate the proposed approach on the challenging Pascal VOC 2007 dataset, and our method outperforms other state-of-the-art methods by a large margin; some results are even comparable to fully supervised models trained with bounding box annotations. To further demonstrate scalability, we also present detection results on the ILSVRC 2013 detection dataset, and our method outperforms supervised deformable part-based model without using box annotations. Single image haze removal is a challenging ill-posed problem. Existing methods use various constraints/priors to get plausible dehazing solutions. The key to achieve haze removal is to estimate a medium transmission map for an input hazy image. In this paper, we propose a trainable end-to-end system called DehazeNet, for medium transmission estimation. DehazeNet takes a hazy image as input, and outputs its medium transmission map that is subsequently used to recover a haze-free image via atmospheric scattering model. DehazeNet adopts convolutional neural network-based deep architecture, whose layers are specially designed to embody the established assumptions/priors in image dehazing. Specifically, the layers of Maxout units are used for feature extraction, which can generate almost all haze-relevant features. We also propose a novel nonlinear activation function in DehazeNet, called bilateral rectified linear unit, which is able to improve the quality of recovered haze-free image. We establish connections between the components of the proposed DehazeNet and those used in existing methods. Experiments on benchmark images show that DehazeNet achieves superior performance over existing methods, yet keeps efficient and easy to use. Visual tracking is challenging due to image variations caused by various factors, such as object deformation, scale change, illumination change, and occlusion. Given the superior tracking performance of human visual system (HVS), an ideal design of biologically inspired model is expected to improve computer visual tracking. This is, however, a difficult task due to the incomplete understanding of neurons\\u2019 working mechanism in the HVS. This paper aims to address this challenge based on the analysis of visual cognitive mechanism of the ventral stream in the visual cortex, which simulates shallow neurons (S1 units and C1 units) to extract low-level biologically inspired features for the target appearance and imitates an advanced learning mechanism (S2 units and C2 units) to combine generative and discriminative models for target location. In addition, fast Gabor approximation and fast Fourier transform are adopted for real-time learning and detection in this framework. Extensive experiments on large-scale benchmark data sets show that the proposed biologically inspired tracker performs favorably against the state-of-the-art methods in terms of efficiency, accuracy, and robustness. The acceleration technique in particular ensures that biologically inspired tracker maintains a speed of approximately 45 frames/s. Recent studies have demonstrated the advantages of fusing information from multiple views for various machine learning applications. However, most existing approaches assumed the shared component common to all views and ignored the private components of individual views, which thereby restricts the learning performance. In this paper, we propose a new multi-view, low-rank, and sparse matrix decomposition scheme to seamlessly integrate diverse yet complementary information stemming from multiple views. Unlike previous approaches, our approach decomposes an input data matrix concatenated from multiple views as the sum of low-rank, sparse, and noisy parts. Then a unified optimization framework is established, where the low-rankness and group-structured sparsity constraints are imposed to simultaneously capture the shared and private components in both instance and view levels. A proven optimization algorithm is developed to solve the optimization, yielding the learned augmented representation which is used as features for classification tasks. Extensive experiments conducted on six benchmark image datasets show that our approach enjoys superior performance over the state-of-the-art approaches. In this paper, we study the problem of learning a metric and propose a loss function based metric learning framework, in which the metric is estimated by minimizing an empirical risk over a training set. With mild conditions on the instance distribution and the used loss function, we prove that the empirical risk converges to its expected counterpart at rate of root-n. In addition, with the assumption that the best metric that minimizes the expected risk is bounded, we prove that the learned metric is consistent. Two example algorithms are presented by using the proposed loss function based metric learning framework, each of which uses a log loss function and a smoothed hinge loss function, respectively. Experimental results suggest the effectiveness of the proposed algorithms. Exploiting the information from multiple views can improve clustering accuracy. However, most existing multi-view clustering algorithms are nonconvex and are thus prone to becoming stuck into bad local minima, especially when there are outliers and missing data. To overcome this problem, we present a new multi-view self-paced learning (MSPL) algorithm for clustering, that learns the multi-view model by not only progressing from 'easy' to 'complex' examples, but also from 'easy' to 'complex' views. Instead of binarily separating the examples or views into 'easy' and 'complex', we design a novel probabilistic smoothed weighting scheme. Employing multiple views for clustering and defining complexity across both examples and views are shown theoretically to be beneficial to optimal clustering. Experimental results on toy and real-world data demonstrate the efficacy of the proposed algorithm. Aging face recognition refers to matching the same person\\u2019s faces across different ages, e.g., matching a person\\u2019s older face to his (or her) younger one, which has many important practical applications, such as finding missing children. The major challenge of this task is that facial appearance is subject to significant change during the aging process. In this paper, we propose to solve the problem with a hierarchical model based on two-level learning. At the first level, effective features are learned from low-level microstructures, based on our new feature descriptor called local pattern selection (LPS). The proposed LPS descriptor greedily selects low-level discriminant patterns in a way, such that intra-user dissimilarity is minimized. At the second level, higher level visual information is further refined based on the output from the first level. To evaluate the performance of our new method, we conduct extensive experiments on the MORPH data set (the largest face aging data set available in the public domain), which show a significant improvement in accuracy over the state-of-the-art methods. : Recent advances in object detection have led to the development of segmentation by detection approaches that integrate top-down geometric priors for multiclass object segmentation. A key yet under-addressed issue in utilizing top-down cues for the problem of multiclass object segmentation by detection is efficiently generating robust and accurate geometric priors. In this paper, we propose a random geometric prior forest scheme to obtain object-adaptive geometric priors efficiently and robustly. In the scheme, a testing object first searches for training neighbors with similar geometries using the random geometric prior forest, and then the geometry of the testing object is reconstructed by linearly combining the geometries of its neighbors. Our scheme enjoys several favorable properties when compared with conventional methods. First, it is robust and very fast because its inference does not suffer from bad initializations, poor local minimums or complex optimization. Second, the figure/ground geometries of training samples are utilized in a multitask manner. Third, our scheme is object-adaptive but does not require the labeling of parts or poselets, and thus, it is quite easy to implement. To demonstrate the effectiveness of the proposed scheme, we integrate the obtained top-down geometric priors with conventional bottom-up color cues in the frame of graph cut. The proposed random geometric prior forest achieves the best segmentation results of all of the methods tested on VOC2010/2012 and is 90 times faster than the current state-of-the-art method. Manifold learning is a powerful tool for solving nonlinear dimension reduction problems. By assuming that the high-dimensional data usually lie on a low-dimensional manifold, many algorithms have been proposed. However, most algorithms simply adopt the traditional graph Laplacian to encode the data locality, so the discriminative ability is limited and the embedding results are not always suitable for the subsequent classification. Instead, this paper deploys the signed graph Laplacian and proposes Signed Laplacian Embedding (SLE) for supervised dimension reduction. By exploring the label information, SLE comprehensively transfers the discrimination carried by the original data to the embedded low-dimensional space. Without perturbing the discrimination structure, SLE also retains the locality. Theoretically, we prove the immersion property by computing the rank of projection, and relate SLE to existing algorithms in the frame of patch alignment. Thorough empirical studies on synthetic and real datasets demonstrate the effectiveness of SLE. Multi-label image classification is of significant interest due to its major role in real-world web image analysis applications such as large-scale image retrieval and browsing. Recently, matrix completion (MC) has been developed to deal with multi-label classification tasks. MC has distinct advantages, such as robustness to missing entries in the feature and label spaces and a natural ability to handle multi-label problems. However, current MC-based multi-label image classification methods only consider data represented by a single-view feature, therefore, do not precisely characterize images that contain several semantic concepts. An intuitive way to utilize multiple features taken from different views is to concatenate the different features into a long vector; however, this concatenation is prone to over-fitting and leads to high time complexity in MC-based image classification. Therefore, we present a novel multi-view learning model for MC-based image classification, called low-rank multi-view matrix completion (lrMMC), which first seeks a low-dimensional common representation of all views by utilizing the proposed low-rank multi-view learning (lrMVL) algorithm. In lrMVL, the common subspace is constrained to be low rank so that it is suitable for MC. In addition, combination weights are learned to explore complementarity between different views. An efficient solver based on fixed-point continuation (FPC) is developed for optimization, and the learned low-rank representation is then incorporated into MC-based image classification. Extensive experimentation on the challenging PASCAL VOC' 07 dataset demonstrates the superiority of lr-MMC compared to other multi-label image classification approaches. Heterogeneous face recognition, also known as cross-modality face recognition or intermodality face recognition, refers to matching two face images from alternative image modalities. Since face images from different image modalities of the same person are associated with the same face object, there should be mutual components that reflect those intrinsic face characteristics that are invariant to the image modalities. Motivated by this rationality, we propose a novel approach called Mutual Component Analysis (MCA) to infer the mutual components for robust heterogeneous face recognition. In the MCA approach, a generative model is first proposed to model the process of generating face images in different modalities, and then an Expectation Maximization (EM) algorithm is designed to iteratively learn the model parameters. The learned generative model is able to infer the mutual components (which we call the hidden factor, where hidden means the factor is unreachable and invisible, and can only be inferred from observations) that are associated with the person\\u2019s identity, thus enabling fast and effective matching for cross-modality face recognition. To enhance recognition performance, we propose an MCA-based multiclassifier framework using multiple local features. Experimental results show that our new approach significantly outperforms the state-of-the-art results on two typical application scenarios: sketch-to-photo and infrared-to-visible face recognition. Hyperspectral unmixing is a hot topic in signal and image processing. A set of high-dimensional data matrices can be decomposed into two sets of non-negative low-dimensional matrices by Non-negative matrix factorization (NMF). However, the algorithm has many local solutions because of the non-convexity of the objective function. Some algorithms solve this problem by adding auxiliary constraints, such as sparse. The sparse NMF has a good performance but the result is unstable and sensitive to noise. Using the structural information for the unmixing approaches can make the decomposition stable. Someone used a clustering based on Euclidean distance to guide the decomposition and obtain good performance. The Euclidean distance is just used to measure the straight line distance of two points. However, the ground objects usually obey certain statistical distribution. It's difficult to measure the difference between the statistical distributions comprehensively by Euclidean distance. Kullback-Leibler divergence (KL divergence) is a better metric. In this paper, we propose a new approach named KL divergence constrained NMF which measures the statistical distribution difference using KL divergence instead of the Euclidean distance. It can improve the accuracy of structured information by using the KL divergence in the algorithm. Experimental results based on synthetic and real hyperspectral data show the superiority of the proposed algorithm with respect to other state-of-the-art algorithms. Many graph-based semi-supervised learning methods for large datasets have been proposed to cope with the rapidly increasing size of data, such as Anchor Graph Regularization (AGR). This model builds a regularization framework by exploring the underlying structure of the whole dataset with both datapoints and anchors. Nevertheless, AGR still has limitations in its two components: (1) in anchor graph construction, the estimation of the local weights between each datapoint and its neighboring anchors could be biased and relatively slow; and (2) in anchor graph regularization, the adjacency matrix that estimates the relationship between datapoints, is not sufficiently effective. In this paper, we develop an Efficient Anchor Graph Regularization (EAGR) by tackling these issues. First, we propose a fast local anchor embedding method, which reformulates the optimization of local weights and obtains an analytical solution. We show that this method better reconstructs datapoints with anchors and speeds up the optimizing process. Second, we propose a new adjacency matrix among anchors by considering the commonly linked datapoints, which leads to a more effective normalized graph Laplacian over anchors. We show that, with the novel local weight estimation and normalized graph Laplacian, EAGR is able to achieve better classification accuracy with much less computational costs. Experimental results on several publicly available datasets demonstrate the effectiveness of our approach. Human activity analysis in videos has increasingly attracted attention in computer vision research with the massive number of videos now accessible online. Although many recognition algorithms have been reported recently, activity representation is challenging. Recently, manifold regularized sparse coding has obtained promising performance in action recognition, because it simultaneously learns the sparse representation and preserves the manifold structure. In this paper, we propose a generalized version of Laplacian regularized sparse coding for human activity recognition called $p$ -Laplacian regularized sparse coding (pLSC). The proposed method exploits $p$ -Laplacian regularization to preserve the local geometry. The $p$ -Laplacian is a nonlinear generalization of standard graph Laplacian and has tighter isoperimetric inequality. As a result, pLSC provides superior theoretical evidence than standard Laplacian regularized sparse coding with a proper $p$ . We also provide a fast iterative shrinkage-thresholding algorithm for the optimization of pLSC. Finally, we input the sparse codes learned by the pLSC algorithm into support vector machines and conduct extensive experiments on the unstructured social activity attribute dataset and human motion database (HMDB51) for human activity recognition. The experimental results demonstrate that the proposed pLSC algorithm outperforms the manifold regularized sparse coding algorithms including the standard Laplacian regularized sparse coding algorithm with a proper $p$ . Deep neural networks have been applied to image restoration to achieve the top-level performance. From a neuroscience perspective, the layerwise abstraction of knowledge in a deep neural network can, to some extent, reveal the mechanisms of how visual cues are processed in human brain. A pivotal property of human brain is that similar visual cues can stimulate the same neuron to induce similar neurological signals. However, conventional neural networks do not consider this property, and the resulting models are, as a result, unstable regarding their internal propagation. In this paper, we develop the (stacked) non-local auto-encoder, which exploits self-similar information in natural images for stability. We propose that similar inputs should induce similar network propagation. This is achieved by constraining the difference between the hidden representations of non-local similar image blocks during training. By applying the proposed model to image restoration, we then develop a collaborative stabilization step to further rectify forward propagation. To obtain a reliable deep model, we employ several strategies to simplify training and improve testing. Extensive image restoration experiments, including image denoising and super-resolution, demonstrate the effectiveness of the proposed method. Hyperspectral images (HSIs) are inevitably corrupted by mixture noise during their acquisition process, in which various kinds of noise, e.g., Gaussian noise, impulse noise, dead lines, and stripes, may exist concurrently. In this paper, mixture noise removal is well illustrated by the task of recovering the low-rank and sparse components of a given matrix, which is constructed by stacking vectorized HSI patches from all the bands at the same position. Instead of applying a traditional nuclear norm, a nonconvex low-rank regularizer, i.e., weighted Schatten p -norm (WSN), is introduced to not only give better approximation to the original low-rank assumption but also to consider the importance of different rank components. The resulted nonconvex low-rank matrix approximation (LRMA) model falls into the applicable scope of an augmented Lagrangian method, and its WSN minimization subproblem can be efficiently solved by generalized iterated shrinkage algorithm. Moreover, the proposed model is integrated into an iterative regularization schema to produce final results, leading to a completed HSI restoration framework. Extensive experimental testing on simulated and real data shows, both qualitatively and quantitatively, that the proposed method has achieved highly competent objective performance compared with several state-of-the-art HSI restoration methods. It is challenging to handle a large volume of labels in multi-label learning. However, existing approaches explicitly or implicitly assume that all the labels in the learning process are given, which could be easily violated in changing environments. In this paper, we define and study streaming label learning (SLL), i.e., labels are arrived on the fly, to model newly arrived labels with the help of the knowledge learned from past labels. The core of SLL is to explore and exploit the relationships between new labels and past labels and then inherit the relationship into hypotheses of labels to boost the performance of new classifiers. In specific, we use the label self-representation to model the label relationship, and SLL will be divided into two steps: a regression problem and a empirical risk minimization (ERM) problem. Both problems are simple and can be efficiently solved. We further show that SLL can generate a tighter generalization error bound for new labels than the general ERM framework with trace norm or Frobenius norm regularization. Finally, we implement extensive experiments on various benchmark datasets to validate the new setting. And results show that SLL can effectively handle the constantly emerging new labels and provides excellent classification performance. Nowadays, many consumer videos are captured by portable devices such as iPhone. Different from constrained videos that are produced by professionals, e.g., those for broadcast, summarizing multiple handheld videos from a same scenery is a challenging task. This is because: 1) these videos have dramatic semantic and style variances, making it difficult to extract the representative key frames; 2) the handheld videos are with different degrees of shakiness, but existing summarization techniques cannot alleviate this problem adaptively; and 3) it is difficult to develop a quality model that evaluates a video summary, due to the subjectiveness of video quality assessment. To solve these problems, we propose perceptual multiattribute optimization which jointly refines multiple perceptual attributes (i.e., video aesthetics, coherence, and stability) in a multivideo summarization process. In particular, a weakly supervised learning framework is designed to discover the semantically important regions in each frame. Then, a few key frames are selected based on their contributions to cover the multivideo semantics. Thereafter, a probabilistic model is proposed to dynamically fit the key frames into an aesthetically pleasing video summary, wherein its frames are stabilized adaptively. Experiments on consumer videos taken from sceneries throughout the world demonstrate the descriptiveness, aesthetics, coherence, and stability of the generated summary. Principal component analysis (PCA) is widely applied in various areas, one of the typical applications is in face. Many versions of PCA have been developed for face recognition. However, most of these approaches are sensitive to grossly corrupted entries in a 2D matrix representing a face image. In this paper, we try to reduce the influence of grosses like variations in lighting, facial expressions and occlusions to improve the robustness of PCA. In order to achieve this goal, we present a simple but effective unsupervised preprocessing method, two-dimensional whitening reconstruction (TWR), which includes two stages: 1) A whitening process on a 2D face image matrix rather than a concatenated 1D vector; 2) 2D face image matrix reconstruction. TWR reduces the pixel redundancy of the internal image, meanwhile maintains important intrinsic features. In this way, negative effects introduced by gross-like variations are greatly reduced. Furthermore, the face image with TWR preprocessing could be approximate to a Gaussian signal, on which PCA is more effective. Experiments on benchmark face databases demonstrate that the proposed method could significantly improve the robustness of PCA methods on classification and clustering, especially for the faces with severe illumination changes. Band selection plays an important role in hyperspectral image processing, which can reduce subsequent computation and storage requirement. There are two problems that are rarely investigated for band selection. First, some low-discriminating bands need to be manually removed by experts, which is time consuming and expensive; second, how to automatically determine the number of selected bands is not well investigated, though this is an indispensable step in practical applications. In this paper, we propose an automatic band selection (ABS)\\u00a0method to solve these problems. First, we exploit spatial structure to determine the discriminating power of each band, these bands with little structure information will be discarded; then, a powerful classifier is used for clustering, which can automatically find the underlying number of clusters. Experiments based on three real hyperspectral datasets demonstrate the effectiveness of our method. Semi-supervised image classification aims to classify a large quantity of unlabeled images by typically harnessing scarce labeled images. Existing semi-supervised methods often suffer from inadequate classification accuracy when encountering difficult yet critical images, such as outliers, because they treat all unlabeled images equally and conduct classifications in an imperfectly ordered sequence. In this paper, we employ the curriculum learning methodology by investigating the difficulty of classifying every unlabeled image. The reliability and the discriminability of these unlabeled images are particularly investigated for evaluating their difficulty. As a result, an optimized image sequence is generated during the iterative propagations, and the unlabeled images are logically classified from simple to difficult. Furthermore, since images are usually characterized by multiple visual feature descriptors, we associate each kind of features with a teacher, and design a multi-modal curriculum learning (MMCL) strategy to integrate the information from different feature modalities. In each propagation, each teacher analyzes the difficulties of the currently unlabeled images from its own modality viewpoint. A consensus is subsequently reached among all the teachers, determining the currently simplest images (i.e., a curriculum), which are to be reliably classified by the multi-modal learner. This well-organized propagation process leveraging multiple teachers and one learner enables our MMCL to outperform five state-of-the-art methods on eight popular image data sets. Tail labels in the multi-label learning problem undermine the low-rank assumption. Nevertheless, this problem has rarely been investigated. In addition to using the low-rank structure to depict label correlations, this paper explores and exploits an additional sparse component to handle tail labels behaving as outliers, in order to make the classical low-rank principle in multi-label learning valid. The divide-and-conquer optimization technique is employed to increase the scalability of the proposed algorithm while theoretically guaranteeing its performance. A theoretical analysis of the generalizability of the proposed algorithm suggests that it can be improved by the low-rank and sparse decomposition given tail labels. Experimental results on real-world data demonstrate the significance of investigating tail labels and the effectiveness of the proposed algorithm. Extracting low-rank and sparse structures from matrices has been extensively studied in machine learning, compressed sensing, and conventional signal processing, and has been widely applied to recommendation systems, image reconstruction, visual analytics, and brain signal processing. Manhattan nonnegative matrix factorization (MahNMF) is an extension of the conventional NMF, which models the heavy-tailed Laplacian noise by minimizing the Manhattan distance between a nonnegative matrix $X$ and the product of two nonnegative low-rank factor matrices. Fast algorithms have been developed to restore the low-rank and sparse structures of $X$ in the MahNMF. In this paper, we study the statistical performance of the MahNMF in the frame of the statistical learning theory. We decompose the expected reconstruction error of the MahNMF into the estimation error and the approximation error. The estimation error is bounded by the generalization error bounds of the MahNMF, while the approximation error is analyzed using the asymptotic results of the minimum distortion of vector quantization. The generalization error bound is valuable for determining the size of the training sample needed to guarantee a desirable upper bound for the defect between the expected and empirical reconstruction errors. Statistical performance analysis shows how the reduced dimensionality affects the estimation and approximation errors. Our framework can also be used for analyzing the performance of the NMF. In multi-object tracking, it is critical to explore the data associations by exploiting the temporal information from a sequence of frames rather than the information from the adjacent two frames. Since straightforwardly obtaining data associations from multi-frames is an NP-hard multi-dimensional assignment (MDA) problem, most existing methods solve this MDA problem by either developing complicated approximate algorithms, or simplifying MDA as a 2D assignment problem based upon the information extracted only from adjacent frames. In this paper, we show that the relation between associations of two observations is the equivalence relation in the data association problem, based on the spatial\\u2013temporal constraint that the trajectories of different objects must be disjoint. Therefore, the MDA problem can be equivalently divided into independent subproblems by equivalence partitioning. In contrast to existing works for solving the MDA problem, we develop a connected component model (CCM) by exploiting the constraints of the data association and the equivalence relation on the constraints. Based upon CCM, we can efficiently obtain the global solution of the MDA problem for multi-object tracking by optimizing a sequence of independent data association subproblems. Experiments on challenging public data sets demonstrate that our algorithm outperforms the state-of-the-art approaches. Multiple distortion assessment is a big challenge in image quality assessment (IQA). In this letter, a no reference IQA model for multiply-distorted images is proposed. The features, which are sensitive to each distortion type even in the presence of other distortions, are first selected from three kinds of NSS features. An improved Bag-of-Words (BoW) model is then applied to encode the selected features. Lastly, a simple yet effective linear combination is used to map the image features to the quality score. The combination weights are obtained through lasso regression. A series of experiments show that the feature selection strategy and the improved BoW model are effective in improving the accuracy of quality prediction for multiple distortion IQA. Compared with other algorithms, the proposed method delivers the best result for multiple distortion IQA. In this paper, we propose a dual diversified dynamical Gaussian process latent variable model ( $\\text{D}^{3}$ GPLVM) to tackle the video repairing issue. For preservation purposes, videos have to be conserved on media. However, storing on media, such as films and hard disks, can suffer from unexpected data loss, for instance, physical damage. So repairing of missing or damaged pixels is essential for better video maintenance. Most methods seek to fill in missing holes by synthesizing similar textures from local patches (the neighboring pixels), consecutive frames, or the whole video. However, these can introduce incorrect contexts, especially when the missing hole or number of damaged frames is large. Furthermore, simple texture synthesis can introduce artifacts in undamaged and recovered areas. To address aforementioned problems, we introduce two diversity encouraging priors to both of inducing points and latent variables for considering the variety in existing videos. In $\\text{D}^{3}$ GPLVM, the inducing points constitute a smaller subset of observed data, while latent variables are a low-dimensional representation of observed data. Since they have a strong correlation with the observed data, it is essential that both of them can capture distinct aspects of and fully represent the observed data. The dual diversity encouraging priors ensure that the trained inducing points and latent variables are more diverse and resistant for context-aware and artifacts-free-based video repairing. The defined objective function in our proposed model is initially not analytically tractable and must be solved by variational inference. Finally, experimental testing results illustrate the robustness and effectiveness of our method for damaged video repairing. In this paper, we present correlated logistic (CorrLog) model for multilabel image classification. CorrLog extends conventional logistic regression model into multilabel cases, via explicitly modeling the pairwise correlation between labels. In addition, we propose to learn the model parameters of CorrLog with elastic net regularization, which helps exploit the sparsity in feature selection and label correlations and thus further boost the performance of multilabel classification. CorrLog can be efficiently learned, though approximately, by regularized maximum pseudo likelihood estimation, and it enjoys a satisfying generalization bound that is independent of the number of labels. CorrLog performs competitively for multilabel image classification on benchmark data sets MULAN scene, MIT outdoor scene, PASCAL VOC 2007, and PASCAL VOC 2012, compared with the state-of-the-art multilabel classification algorithms. Example learning-based image super-resolution techniques estimate a high-resolution image from a low-resolution input image by relying on high- and low-resolution image pairs. An important issue for these techniques is how to model the relationship between high- and low-resolution image patches: most existing complex models either generalize hard to diverse natural images or require a lot of time for model training, while simple models have limited representation capability. In this paper, we propose a simple, effective, robust, and fast (SERF) image super-resolver for image super-resolution. The proposed super-resolver is based on a series of linear least squares functions, namely, cascaded linear regression. It has few parameters to control the model and is thus able to robustly adapt to different image data sets and experimental settings. The linear least square functions lead to closed form solutions and therefore achieve computationally efficient implementations. To effectively decrease these gaps, we group image patches into clusters via k-means algorithm and learn a linear regressor for each cluster at each iteration. The cascaded learning process gradually decreases the gap of high-frequency detail between the estimated high-resolution image patch and the ground truth image patch and simultaneously obtains the linear regression parameters. Experimental results show that the proposed method achieves superior performance with lower time consumption than the state-of-the-art methods. Face sketch\\u2013photo synthesis plays an important role in law enforcement and digital entertainment. Most of the existing methods only use pixel intensities as the feature. Since face images can be described using features from multiple aspects, this paper presents a novel multiple representations-based face sketch\\u2013photo-synthesis method that adaptively combines multiple representations to represent an image patch. In particular, it combines multiple features from face images processed using multiple filters and deploys Markov networks to exploit the interacting relationships between the neighboring image patches. The proposed framework could be solved using an alternating optimization strategy and it normally converges in only five outer iterations in the experiments. Our experimental results on the Chinese University of Hong Kong (CUHK) face sketch database, celebrity photos, CUHK Face Sketch FERET Database, IIIT-D Viewed Sketch Database, and forensic sketches demonstrate the effectiveness of our method for face sketch\\u2013photo synthesis. In addition, cross-database and database-dependent style-synthesis evaluations demonstrate the generalizability of this novel method and suggest promising solutions for face identification in forensic science. Hashing has become an increasingly popular technique for fast nearest neighbor search. Despite its successful progress in classic pointto-point search, there are few studies regarding point-to-hyperplane search, which has strong practical capabilities of scaling up applications like active learning with SVMs. Existing hyperplane hashing methods enable the fast search based on randomly generated hash codes, but still suffer from a low collision probability and thus usually require long codes for a satisfying performance. To overcome this problem, this paper proposes a multilinear hyperplane hashing that generates a hash bit using multiple linear projections. Our theoretical analysis shows that with an even number of random linear projections, the multilinear hash function possesses strong locality sensitivity to hyperplane queries. To leverage its sensitivity to the angle distance, we further introduce an angular quantization based learning framework for compact multilinear hashing, which considerably boosts the search performance with less hash bits. Experiments with applications to large-scale (up to one million) active learning on two datasets demonstrate the overall superiority of the proposed approach. Domain adaptation arises in supervised learning when the training (source domain) and test (target domain) data have different distributions. Let X and Y denote the features and target, respectively, previous work on domain adaptation mainly considers the covariate shift situation where the distribution of the features P(X) changes across domains while the conditional distribution P(Y|X) stays the same. To reduce domain discrepancy, recent methods try to find invariant components \\u03c4 (X) that have similar P(\\u03c4 (X)) on different domains by explicitly minimizing a distribution discrepancy measure. However, it is not clear if P(Y|\\u03c4 (X)) in different domains is also similar when P(Y|X) changes. Furthermore, transferable components do not necessarily have to be invariant. If the change in some components is identifiable, we can make use of such components for prediction in the target domain. In this paper, we focus on the case where P(X|Y) and P(Y) both change in a causal system in which Y is the cause for X. Under appropriate assumptions, we aim to extract conditional transferable components whose conditional distribution P(\\u03c4 (X)|Y) is invariant after proper location-scale (LS) transformations, and identify how P(Y) changes between domains simultaneously. We provide theoretical analysis and empirical evaluation on both synthetic and real-world data to show the effectiveness of our method. Multi-label image classification aims to predict multiple labels for a single image which contains diverse content. By utilizing label correlations, various techniques have been developed to improve classification performance. However, current existing methods either neglect image features when exploiting label correlations or lack the ability to learn image-dependent conditional label structures. In this paper, we develop conditional graphical Lasso (CGL) to handle these challenges. CGL provides a unified Bayesian framework for structure and parameter learning conditioned on image features. We formulate the multi-label prediction as CGL inference problem, which is solved by a mean field variational approach. Meanwhile, CGL learning is efficient due to a tailored proximal gradient procedure by applying the maximum a posterior (MAP) methodology. CGL performs competitively for multi-label image classification on benchmark datasets MULAN scene, PASCAL VOC 2007 and PASCAL VOC 2012, compared with the state-of-the-art multi-label classification algorithms. For regression-based single-image super-resolution (SR) problem, the key is to establish a mapping relation between high-resolution (HR) and low-resolution (LR) image patches for obtaining a visually pleasing quality image. Most existing approaches typically solve it by dividing the model into several single-output regression problems, which obviously ignores the circumstance that a pixel within an HR patch affects other spatially adjacent pixels during the training process, and thus tends to generate serious ringing artifacts in resultant HR image as well as increase computational burden. To alleviate these problems, we propose to use structured output regression machine (SORM) to simultaneously model the inherent spatial relations between the HR and LR patches, which is propitious to preserve sharp edges. In addition, to further improve the quality of reconstructed HR images, a nonlocal (NL) self-similarity prior in natural images is introduced to formulate as a regularization term to further enhance the SORM-based SR results. To offer a computation-effective SORM method, we use a relative small nonsupport vector samples to establish the accurate regression model and an accelerating algorithm for NL self-similarity calculation. Extensive SR experiments on various images indicate that the proposed method can achieve more promising performance than the other state-of-the-art SR methods in terms of both visual quality and computational cost. Occlusion boundaries contain rich perceptual information about the underlying scene structure. They also provide important cues in many visual perception tasks such as scene understanding, object recognition, and segmentation. In this paper, we improve occlusion boundary detection via enhanced exploration of contextual information (e.g., local structural boundary patterns, observations from surrounding regions, and temporal context), and in doing so develop a novel approach based on convolutional neural networks (CNNs) and conditional random fields (CRFs). Experimental results demonstrate that our detector significantly outperforms the state-of-the-art (e.g., improving the F-measure from 0.62 to 0.71 on the commonly used CMU benchmark). Last but not least, we empirically assess the roles of several important components of the proposed detector, so as to validate the rationale behind this approach. Recent years have witnessed the success of deep neural networks in dealing with a plenty of practical problems. The invention of effective training techniques largely contributes to this success. The so-called \\\"Dropout\\\" training scheme is one of the most powerful tool to reduce over-fitting. From the statistic point of view, Dropout works by implicitly imposing an L2 regularizer on the weights. In this paper, we present a new training scheme: Shakeout. Instead of randomly discarding units as Dropout does at the training stage, our method randomly chooses to enhance or inverse the contributions of each unit to the next layer. We show that our scheme leads to a combination of L1 regularization and L2 regularization imposed on the weights, which has been proved effective by the Elastic Net models in practice. We have empirically evaluated the Shakeout scheme and demonstrated that sparse network weights are obtained via Shakeout training. Our classification experiments on real-life image datasets MNIST and CIFAR-10 show that Shakeout deals with over-fitting effectively. In this paper, we tackle the problem of RGB-D semantic segmentation of indoor images. We take advantage of deconvolutional networks which can predict pixel-wise class labels, and develop a new structure for deconvolution of multiple modalities. We propose a novel feature transformation network to bridge the convolutional networks and deconvolutional networks. In the feature transformation network, we correlate the two modalities by discovering common features between them, as well as characterize each modality by discovering modality specific features. With the common features, we not only closely correlate the two modalities, but also allow them to borrow features from each other to enhance the representation of shared information. With specific features, we capture the visual patterns that are only visible in one modality. The proposed network achieves competitive segmentation accuracy on NYU depth dataset V1 and V2. Hash-based nearest neighbor search has become attractive in many applications. However, the quantization in hashing usually degenerates the discriminative power when using Hamming distance ranking. Besides, for large-scale visual search, existing hashing methods cannot directly support the efficient search over the data with multiple sources, and while the literature has shown that adaptively incorporating complementary information from diverse sources or views can significantly boost the search performance. To address the problems, this paper proposes a novel and generic approach to building multiple hash tables with multiple views and generating fine-grained ranking results at bitwise and tablewise levels. For each hash table, a query-adaptive bitwise weighting is introduced to alleviate the quantization loss by simultaneously exploiting the quality of hash functions and their complement for nearest neighbor search. From the tablewise aspect, multiple hash tables are built for different data views as a joint index, over which a query-specific rank fusion is proposed to rerank all results from the bitwise ranking by diffusing in a graph. Comprehensive experiments on image search over three well-known benchmarks show that the proposed method achieves up to 17.11% and 20.28% performance gains on single and multiple table search over the state-of-the-art methods. Spatial information has been widely used for hyperspectral image classification, which can dramatically improve the classification accuracy. Though band selection is an important pre-processing step for hyperspectral image processing, spatial information has not been well exploited in this field. In this article, we will exploit the spatial information for band selection. This article mainly includes two parts: algorithm design, and algorithm evaluation. In the first part, we propose an efficient band selection method by using the spatial structure information and spectral information. In the second part, we advocate the use of the local spatial filtering and the spectral-spatial classifier for evaluating the performance of band selection algorithms instead of the traditional pixel-wise classifiers. Comprehensive experiments over diverse publicly available benchmark data sets reveal some interesting results. Hyperspectral images provide great potential for target detection, however, new challenges are also introduced for hyperspectral target detection, resulting that hyperspectral target detection should be treated as a new problem and modeled differently. Many classical detectors are proposed based on the linear mixing model and the sparsity model. However, the former type of model cannot deal well with spectral variability in limited endmembers, and the latter type of model usually treats the target detection as a simple classification problem and pays less attention to the low target probability. In this case, can we find an efficient way to utilize both the high-dimension features behind hyperspectral images and the limited target information to extract small targets? This paper proposes a novel sparsity-based detector named the hybrid sparsity and statistics detector (HSSD) for target detection in hyperspectral imagery, which can effectively deal with the above two problems. The proposed algorithm designs a hypothesis-specific dictionary based on the prior hypotheses for the test pixel, which can avoid the imbalanced number of training samples for a class-specific dictionary. Then, a purification process is employed for the background training samples in order to construct an effective competition between the two hypotheses. Next, a sparse representation-based binary hypothesis model merged with additive Gaussian noise is proposed to represent the image. Finally, a generalized likelihood ratio test is performed to obtain a more robust detection decision than the reconstruction residual-based detection methods. Extensive experimental results with three hyperspectral data sets confirm that the proposed HSSD algorithm clearly outperforms the state-of-the-art target detectors. With the rapid development of remote sensing technology, huge quantities of high resolution remote sensing images are available now. Understanding these images in semantic level is of great significance. Hence, a deep multimodal neural network model for semantic understanding of the high resolution remote sensing images is proposed in this paper, which uses both visual and textual information of the high resolution remote sensing images to generate natural sentences describing the given images. In the proposed model, the convolution neural network is utilized to extract the image feature, which is then combined with the text descriptions of the images by RNN or LSTMs. And in the experiments, two new remote sensing image-captions datasets are built at first. Then different kinds of CNNs with RNN or LSTMs are combined to find which is the best combination for caption generation. The experiments results prove that the proposed method achieves good performances in semantic understanding of high resolution remote sensing images. Dynamic textures (DTs) that represent moving scenes such as flames, smoke, and waves, exhibit fixed dynamics within a period of time and have been successfully modeled using linear dynamic systems (LDS). In this paper, we show that the widely used LDS model can be approximated using a principal component regression (PCR) model with the main advantage of simplicity. Furthermore, to capture the nonlinearity of training frames, we extend traditional PCR to its kernelized version and introduce kernel principal component regression (KPCR) to model and synthesize DTs. To ensure algorithm stability, we remove the standard state model and directly apply the quantized kernel least mean squares algorithm from signal processing domain to approximate the performance achieved with KPCR. We term this improvement kernel adaptive dynamic texture synthesis (KADTS), which also has the benefits of computational and memory efficiency. These advantages make KADTS ideally suited for real-world applications, since the majority of electronic devices, including cell phones and laptops, suffer from limited memory and real-time constraints. We demonstrate, via both theoretical and experimental analyses, the connections between DT synthesis using KPCR and KADTS with a regularization network theory. We also show the superiority of our proposed algorithms for DT synthesis compared with other dynamic system-based benchmarks. MATLAB code is available from our project homepage http://bmal.hust.edu.cn/project/dts.html . Multi-label learning is a challenging problem in computer vision field. In this paper, we propose a novel active learning approach to reduce the annotation costs greatly for multi-label classification. State-of-the-art active learning methods either annotate all the relevant samples without diagnosing discriminative information in the labels or annotate only limited discriminative samples manually, that has weak immunity for the outlier labels. To overcome these problems, we propose a multi-label active learning method based on Maximum Correntropy Criterion (MCC) by merging uncertainty and representativeness. We use the the labels of labeled data and the prediction labels of unknown data to enhance the uncertainty and representativeness measurement by merging strategy, and use the MCC to alleviate the influence of outlier labels for discriminative labeling. Experiments on several challenging benchmark multi-label datasets show the superior performance of our proposed method to the state-of-the-art methods. Hashing or binary code learning has been recognized to accomplish efficient near neighbor search, and has thus attracted broad interests in recent retrieval, vision, and learning studies. One main challenge of learning to hash arises from the involvement of discrete variables in binary code optimization. While the widely used continuous relaxation may achieve high learning efficiency, the pursued codes are typically less effective due to accumulated quantization error. In this paper, we propose a novel binary code optimization method, dubbed discrete proximal linearized minimization (DPLM) , which directly handles the discrete constraints during the learning process. Specifically, the discrete (thus nonsmooth nonconvex) problem is reformulated as minimizing the sum of a smooth loss term with a nonsmooth indicator function. The obtained problem is then efficiently solved by an iterative procedure with each iteration admitting an analytical discrete solution, which is thus shown to converge very fast. In addition, the proposed method supports a large family of empirical loss functions, which is particularly instantiated in this paper by both a supervised and an unsupervised hashing losses, together with the bits uncorrelation and balance constraints. In particular, the proposed DPLM with a supervised $ell _{2}$ loss encodes the whole NUS-WIDE database into 64-b binary codes within 10 s on a standard desktop computer. The proposed approach is extensively evaluated on several large-scale data sets and the generated binary codes are shown to achieve very promising results on both retrieval and classification tasks. The increasing number of 3D objects in various applications has increased the requirement for effective and efficient 3D object retrieval methods, which attracted extensive research efforts in recent years. Existing works mainly focus on how to extract features and conduct object matching. With the increasing applications, 3D objects come from different areas. In such circumstances, how to conduct object retrieval becomes more important. To address this issue, we propose a multi-view object retrieval method using multi-scale topic models in this paper. In our method, multiple views are first extracted from each object, and then the dense visual features are extracted to represent each view. To represent the 3D object, multi-scale topic models are employed to extract the hidden relationship among these features with respect to varied topic numbers in the topic model. In this way, each object can be represented by a set of bag of topics. To compare the objects, we first conduct topic clustering for the basic topics from two data sets, and then generate the common topic dictionary for new representation. Then, the two objects can be aligned to the same common feature space for comparison. To evaluate the performance of the proposed method, experiments are conducted on two data sets. The 3D object retrieval experimental results and comparison with existing methods demonstrate the effectiveness of the proposed method. A cascade classifier has turned out to be effective in sliding-window based real-time object detection. In a cascade classifier, node learning is the key process, which includes feature selection and classifier design. Previous algorithms fail to effectively tackle the asymmetry and intersection problems existing in cascade classification, thereby limiting the performance of object detection. In this paper, we improve current feature selection algorithm by addressing both asymmetry and intersection problems. We formulate asymmetric feature selection as a submodular function maximization problem. We then propose a new algorithm SAFS with formal performance guarantee to solve this problem. We use face detection as a case study and perform experiments on two real-world face detection datasets. The experimental results demonstrate that our algorithm SAFS outperforms the state-of-art feature selection algorithms in cascade object detection, such as FFS and LACBoost. Convolutional neural networks (CNNs) have achieved state-of-the-art results on many visual recognition tasks. However, current CNN models still exhibit a poor ability to be invariant to spatial transformations of images. Intuitively, with sufficient layers and parameters, hierarchical combinations of convolution (matrix multiplication and non-linear activation) and pooling operations should be able to learn a robust mapping from transformed input images to transform-invariant representations. In this paper, we propose randomly transforming (rotation, scale, and translation) feature maps of CNNs during the training stage. This prevents complex dependencies of specific rotation, scale, and translation levels of training images in CNN models. Rather, each convolutional kernel learns to detect a feature that is generally helpful for producing the transform-invariant answer given the combinatorially large variety of transform levels of its input feature maps. In this way, we do not require any extra training supervision or modification to the optimization process and training images. We show that random transformation provides significant improvements of CNNs on many benchmark tasks, including small-scale image recognition, large-scale image recognition, and image retrieval. No reference image quality assessment (NR-IQA) is to evaluate image quality blindly without the ground truth. Most of the emerging NR-IQA algorithms are only effective for some specific distortion. Universal metrics that can work for various categories of distortions have hardly been explored, and the algorithms available are not fully adequate in performance. In this paper, we study the local dependency (LD) characteristic of natural images, and propose two universal NR-IQA metrics: LD global scheme (LD-GS) and LD two-step scheme (LD-TS). We claim that the local dependency characteristic among wavelet coefficients is disturbed by various distortion processes, and the disturbances are strongly correlated to image qualities. Experimental results on LIVE database II demonstrate that both the proposed metrics are highly consistent with the human perception and outpace the state-of-the-art NR-IQA indexes and some full reference quality indicators for diverse distortions and across the entire database. Video dehazing has a wide range of real-time applications, but the challenges mainly come from spatio-temporal coherence and computational efficiency. In this paper, a spatio-temporal optimization framework for real-time video dehazing is proposed, which reduces blocking and flickering artifacts and achieves high-quality enhanced results. We build a Markov Random Field (MRF) with an Intensity Value Prior (IVP) to handle spatial consistency and temporal coherence. By maximizing the MRF likelihood function, the proposed framework estimates the haze concentration and preserves the information optimally. Moreover, to facilitate real-time applications, integral image technique is approximated to reduce the main computational burden. Experimental results demonstrate that the proposed framework is effectively to remove haze and flickering artifacts, and sufficiently fast for real-time applications. Deep convolutional neural networks (CNNs) are successfully used in a number of applications. However, their storage and computational requirements have largely prevented their widespread use on mobile devices. Here we present an effective CNN compression approach in the frequency domain, which focuses not only on smaller weights but on all the weights and their underlying connections. By treating convolutional filters as images, we decompose their representations in the frequency domain as common parts (i.e., cluster centers) shared by other similar filters and their individual private parts (i.e., individual residuals). A large number of low-energy frequency coefficients in both parts can be discarded to produce high compression without significantly compromising accuracy. We relax the computational burden of convolution operations in CNNs by linearly combining the convolution responses of discrete cosine transform (DCT) bases. The compression and speed-up ratios of the proposed algorithm are thoroughly analyzed and evaluated on benchmark image datasets to demonstrate its superiority over state-of-the-art methods. The human visual system excels at detecting local blur of visual images, but the underlying mechanism is not well understood. Traditional views of blur such as reduction in energy at high frequencies and loss of phase coherence at localized features have fundamental limitations. For example, they cannot well discriminate flat regions from blurred ones. Here we propose that high-level semantic information is critical in successfully identifying local blur. Therefore, we resort to deep neural networks that are proficient at learning high-level features and propose the first end-to-end local blur mapping algorithm based on a fully convolutional network. By analyzing various architectures with different depths and design philosophies, we empirically show that high-level features of deeper layers play a more important role than low-level features of shallower layers in resolving challenging ambiguities for this task. We test the proposed method on a standard blur detection benchmark and demonstrate that it significantly advances the state-of-the-art (ODS F-score of 0.853). Furthermore, we explore the use of the generated blur maps in three applications, including blur region segmentation, blur degree estimation, and blur magnification. This report presents results from the Video Person Recognition Evaluation held in conjunction with the 8th IEEE International Conference on Biometrics: Theory, Applications, and Systems (BTAS). Two experiments required algorithms to recognize people in videos from the Point-and-Shoot Face Recognition Challenge Problem (PaSC). The first consisted of videos from a tripod mounted high quality video camera. The second contained videos acquired from 5 different handheld video cameras. There were 1,401 videos in each experiment of 265 subjects. The subjects, the scenes, and the actions carried out by the people are the same in both experiments. An additional experiment required algorithms to recognize people in videos from the Video Database of Moving Faces and People (VDMFP). There were 958 videos in this experiment of 297 subjects. Four groups from around the world participated in the evaluation. The top verification rate for PaSC from this evaluation is 0.98 at a false accept rate of 0.01 \\u2014 a remarkable advancement in performance from the competition held at FG 2015. Multi-label propagation aims to transmit the multi-label information from labeled examples to unlabeled examples based on a weighted graph. Existing methods ignore the specific propagation difficulty of different unlabeled examples and conduct the propagation in an imperfect sequence, leading to the error-prone classification of some difficult examples with uncertain labels. To address this problem, this paper associates each possible label with a \\\"teacher\\\", and proposes a \\\"Multi-Label Teaching-to-Learn and Learning-to-Teach\\\" (ML-TLLT) algorithm, so that the entire propagation process is guided by the teachers and manipulated from simple examples to more difficult ones. In the teaching-to-learn step, the teachers select the simplest examples for the current propagation by investigating both the definitiveness of each possible label of the unlabeled examples, and the dependencies between labels revealed by the labeled examples. In the learning-to-teach step, the teachers reversely learn from the learner's feedback to properly select the simplest examples for the next propagation. Thorough empirical studies show that due to the optimized propagation sequence designed by the teachers, ML-TLLT yields generally better performance than seven state-of-the-art methods on the typical multi-label benchmark datasets. Video-based facial expression recognition (FER) has recently received increased attention as a result of its widespread application. Many kinds of features have been proposed to represent different properties of facial expressions in videos. However the dimensionality of these features is usually high. In addition, due to the complexity of the information available in video sequences, using only one type of feature is often inadequate. How to effectively reduce the dimensionality and combine multi-view features thus becomes a challenging problem. In this paper, motivated by the recent success in exclusive feature selection, we first introduce exclusive group LASSO (EG-LASSO) to unsupervised dimension reduction (UDR). This leads to the proposed exclusive UDR (EUDR) framework, which allows arbitrary sparse structures on the feature space. To properly combine multiple kinds of features, we further extend EUDR to multi-view EUDR (MEUDR), where the structured sparsity is enforced at both intra- and interview levels. In addition, combination weights are learned for all views to allow them to contribute differently to the final consensus presentation. A reliable solution is then obtained. Experiments on two challenging video-based FER datasets demonstrate the effectiveness of the proposed method. Online multi-object tracking (MOT) is challenging: frame-by-frame matching of detection hypotheses to the correct trackers can be difficult. The Hungarian algorithm is the most commonly used online MOT data association method due to its rapid assignment; however, the Hungarian algorithm simply considers associations based on an affinity model. For crowded scenarios, frequently occurring interactions between objects complicate associations, and affinity-based methods usually fail in these scenarios. Here we introduce quadratic pseudo-Boolean optimization (QPBO) to an online MOT model to analyze frequent interactions. Specifically, we formulate two useful interaction types as pairwise potentials in QPBO, a design that benefits our model by exploiting informative interactions and allowing our online tracker to handle complex scenes. The auxiliary interactions result in a non-submodular QPBO, so we accelerate our online tracker by solving the model with a graph cut combined with a simple heuristic method. This combination achieves a reasonable local optimum and, importantly, implements the tracker efficiently. Extensive experiments on publicly available datasets from both static and moving cameras demonstrate the superiority of our method. Traditional metric learning methods usually make decisions based on a fixed threshold, which may result in a suboptimal metric when the inter-class and inner-class variations are complex. To address this issue, in this paper we propose an effective metric learning method by exploiting privileged information to relax the fixed threshold under the empirical risk minimization framework. Privileged information describes useful high-level semantic information that is only available during training. Our goal is to improve the performance by incorporating privileged information to design a locally adaptive decision function. We jointly learn two distance metrics by minimizing the empirical loss penalizing the difference between the distance in the original space and that in the privileged space. The distance in the privileged space functions as a locally adaptive decision threshold, which can guide the decision making like a teacher. We optimize the objective function using the Accelerated Proximal Gradient approach to obtain a global optimum solution. Experiment results show that by leveraging privileged information, our proposed method can achieve satisfactory performance. Deep convolutional networks have achieved successful performance in data mining field. However, training large networks still remains a challenge, as the training data may be insufficient and the model can easily get overfitted. Hence the training process is usually combined with a model regularization. Typical regularizers include weight decay, Dropout, etc. In this paper, we propose a novel regularizer, named Structured Decorrelation Constraint (SDC), which is applied to the activations of the hidden layers to prevent overfitting and achieve better generalization. SDC impels the network to learn structured representations by grouping the hidden units and encouraging the units within the same group to have strong connections during the training procedure. Meanwhile, it forces the units in different groups to learn non-redundant representations by minimizing the cross-covariance between them. Compared with Dropout, SDC reduces the co-adaptions between the hidden units in an explicit way. Besides, we propose a novel approach called Reg-Conv that can help SDC to regularize the complex convolutional layers. Experiments on extensive datasets show that SDC significantly reduces overfitting and yields very meaningful improvements on classification performance (on CIFAR-10 6.22% accuracy promotion and on CIFAR-100 9.63% promotion). Low-rank structure have been profoundly studied in data mining and machine learning. In this paper, we show a dense matrix $X$'s low-rank approximation can be rapidly built from its left and right random projections $Y_1=XA_1$ and $Y_2=X^TA_2$, or bilateral random projection (BRP). We then show power scheme can further improve the precision. The deterministic, average and deviation bounds of the proposed method and its power scheme modification are proved theoretically. The effectiveness and the efficiency of BRP based low-rank approximation is empirically verified on both artificial and real datasets. We analyze the local Rademacher complexity of empirical risk minimization (ERM)-based multi-label learning algorithms, and in doing so propose a new algorithm for multi-label learning. Rather than using the trace norm to regularize the multi-label predictor, we instead minimize the tail sum of the singular values of the predictor in multi-label learning. Benefiting from the use of the local Rademacher complexity, our algorithm, therefore, has a sharper generalization error bound and a faster convergence rate. Compared to methods that minimize over all singular values, concentrating on the tail singular values results in better recovery of the low-rank structure of the multi-label predictor, which plays an import role in exploiting label correlations. We propose a new conditional singular value thresholding algorithm to solve the resulting objective function. Empirical studies on real-world datasets validate our theoretical results and demonstrate the effectiveness of the proposed algorithm. The $k$-dimensional coding schemes refer to a collection of methods that attempt to represent data using a set of representative $k$-dimensional vectors, and include non-negative matrix factorization, dictionary learning, sparse coding, $k$-means clustering and vector quantization as special cases. Previous generalization bounds for the reconstruction error of the $k$-dimensional coding schemes are mainly dimensionality independent. A major advantage of these bounds is that they can be used to analyze the generalization error when data is mapped into an infinite- or high-dimensional feature space. However, many applications use finite-dimensional data features. Can we obtain dimensionality-dependent generalization bounds for $k$-dimensional coding schemes that are tighter than dimensionality-independent bounds when data is in a finite-dimensional feature space? The answer is positive. In this paper, we address this problem and derive a dimensionality-dependent generalization bound for $k$-dimensional coding schemes by bounding the covering number of the loss function class induced by the reconstruction error. The bound is of order $mathcal{O}left(left(mkln(mkn)/n ight)^{lambda_n} ight)$, where $m$ is the dimension of features, $k$ is the number of the columns in the linear implementation of coding schemes, $n$ is the size of sample, $lambda_n>0.5$ when $n$ is finite and $lambda_n=0.5$ when $n$ is infinite. We show that our bound can be tighter than previous results, because it avoids inducing the worst-case upper bound on $k$ of the loss function and converges faster. The proposed generalization bound is also applied to some specific coding schemes to demonstrate that the dimensionality-dependent bound is an indispensable complement to these dimensionality-independent generalization bounds. Sparse coding represents a signal sparsely by using an overcomplete dictionary, and obtains promising performance in practical computer vision applications, especially for signal restoration tasks such as image denoising and image inpainting. In recent years, many discriminative sparse coding algorithms have been developed for classification problems, but they cannot naturally handle visual data represented by multiview features. In addition, existing sparse coding algorithms use graph Laplacian to model the local geometry of the data distribution. It has been identified that Laplacian regularization biases the solution towards a constant function which possibly leads to poor extrapolating power. In this paper, we present multiview Hessian discriminative sparse coding (mHDSC) which seamlessly integrates Hessian regularization with discriminative sparse coding for multiview learning problems. In particular, mHDSC exploits Hessian regularization to steer the solution which varies smoothly along geodesics in the manifold, and treats the label information as an additional view of feature for incorporating the discriminative power for image annotation. We conduct extensive experiments on PASCAL VOC'07 dataset and demonstrate the effectiveness of mHDSC for image annotation. Canonical correlation analysis (CCA) has proven an effective tool for two-view dimension reduction due to its profound theoretical foundation and success in practical applications. In respect of multi-view learning, however, it is limited by its capability of only handling data represented by two-view features, while in many real-world applications, the number of views is frequently many more. Although the ad hoc way of simultaneously exploring all possible pairs of features can numerically deal with multi-view data, it ignores the high order statistics (correlation information) which can only be discovered by simultaneously exploring all features. Therefore, in this work, we develop tensor CCA (TCCA) which straightforwardly yet naturally generalizes CCA to handle the data of an arbitrary number of views by analyzing the covariance tensor of the different views. TCCA aims to directly maximize the canonical correlation of multiple (more than two) views. Crucially, we prove that the multi-view canonical correlation maximization problem is equivalent to finding the best rank-1 approximation of the data covariance tensor, which can be solved efficiently using the well-known alternating least squares (ALS) algorithm. As a consequence, the high order correlation information contained in the different views is explored and thus a more reliable common subspace shared by all features can be obtained. In addition, a non-linear extension of TCCA is presented. Experiments on various challenge tasks, including large scale biometric structure prediction, internet advertisement classification and web image annotation, demonstrate the effectiveness of the proposed method. Support vector machines (SVMs) are invaluable tools for many practical applications in artificial intelligence, e.g., classification and event recognition. However, popular SVM solvers are not sufficiently efficient for applications with a great deal of samples as well as a large number of features. In this paper, thus, we present NESVM, a fast gradient SVM solver that can optimize various SVM models, e.g., classical SVM, linear programming SVM and least square SVM. Compared against SVM-Perf cite{SVM_Perf}cite{PerfML} (its convergence rate in solving the dual SVM is upper bounded by $mathcal O(1/sqrt{k})$, wherein $k$ is the number of iterations.) and Pegasos cite{Pegasos} (online SVM that converges at rate $mathcal O(1/k)$ for the primal SVM), NESVM achieves the optimal convergence rate at $mathcal O(1/k^{2})$ and a linear time complexity. In particular, NESVM smoothes the non-differentiable hinge loss and $ell_1$-norm in the primal SVM. Then the optimal gradient method without any line search is adopted to solve the optimization. In each iteration round, the current gradient and historical gradients are combined to determine the descent direction, while the Lipschitz constant determines the step size. Only two matrix-vector multiplications are required in each iteration round. Therefore, NESVM is more efficient than existing SVM solvers. In addition, NESVM is available for both linear and nonlinear kernels. We also propose \\\"homotopy NESVM\\\" to accelerate NESVM by dynamically decreasing the smooth parameter and using the continuation method. Our experiments on census income categorization, indoor/outdoor scene classification, event recognition and scene recognition suggest the efficiency and the effectiveness of NESVM. The MATLAB code of NESVM will be available on our website for further assessment. The capacity to recognize faces under varied poses is a fundamental human ability that presents a unique challenge for computer vision systems. Compared to frontal face recognition, which has been intensively studied and has gradually matured in the past few decades, pose-invariant face recognition (PIFR) remains a largely unsolved problem. However, PIFR is crucial to realizing the full potential of face recognition for real-world applications, since face recognition is intrinsically a passive biometric technology for recognizing uncooperative subjects. In this paper, we discuss the inherent difficulties in PIFR and present a comprehensive review of established techniques. Existing PIFR methods can be grouped into four categories, i.e., pose-robust feature extraction approaches, multi-view subspace learning approaches, face synthesis approaches, and hybrid approaches. The motivations, strategies, pros/cons, and performance of representative approaches are described and compared. Moreover, promising directions for future research are discussed. This paper presents a comprehensive survey of facial feature point detection with the assistance of abundant manually labeled images. Facial feature point detection favors many applications such as face recognition, animation, tracking, hallucination, expression analysis and 3D face modeling. Existing methods can be categorized into the following four groups: constrained local model (CLM)-based, active appearance model (AAM)-based, regression-based, and other methods. CLM-based methods consist of a shape model and a number of local experts, each of which is utilized to detect a facial feature point. AAM-based methods fit a shape model to an image by minimizing texture synthesis errors. Regression-based methods directly learn a mapping function from facial image appearance to facial feature points. Besides the above three major categories of methods, there are also minor categories of methods which we classify into other methods: graphical model-based methods, joint face alignment methods, independent facial feature point detectors, and deep learning-based methods. Though significant progress has been made, facial feature point detection is limited in its success by wild and real-world conditions: variations across poses, expressions, illuminations, and occlusions. A comparative illustration and analysis of representative methods provide us a holistic understanding and deep insight into facial feature point detection, which also motivates us to explore promising future directions. Inspired by the theory of Leitners learning box from the field of psychology, we propose DropSample, a new method for training deep convolutional neural networks (DCNNs), and apply it to large-scale online handwritten Chinese character recognition (HCCR). According to the principle of DropSample, each training sample is associated with a quota function that is dynamically adjusted on the basis of the classification confidence given by the DCNN softmax output. After a learning iteration, samples with low confidence will have a higher probability of being selected as training data in the next iteration; in contrast, well-trained and well-recognized samples with very high confidence will have a lower probability of being involved in the next training iteration and can be gradually eliminated. As a result, the learning process becomes more efficient as it progresses. Furthermore, we investigate the use of domain-specific knowledge to enhance the performance of DCNN by adding a domain knowledge layer before the traditional CNN. By adopting DropSample together with different types of domain-specific knowledge, the accuracy of HCCR can be improved efficiently. Experiments on the CASIA-OLHDWB 1.0, CASIA-OLHWDB 1.1, and ICDAR 2013 online HCCR competition datasets yield outstanding recognition rates of 97.33%, 97.06%, and 97.51% respectively, all of which are significantly better than the previous best results reported in the literature. In this paper, we study the risk bounds for samples independently drawn from an infinitely divisible (ID) distribution. In particular, based on a martingale method, we develop two deviation inequalities for a sequence of random variables of an ID distribution with zero Gaussian component. By applying the deviation inequalities, we obtain the risk bounds based on the covering number for the ID distribution. Finally, we analyze the asymptotic convergence of the risk bound derived from one of the two deviation inequalities and show that the convergence rate of the bound is faster than the result for the generic i.i.d. empirical process (Mendelson, 2003).\",\n",
            "  \"150772663\": \"In this work we discuss the reliability of the coupling among three-dimensional (3D) and one-dimensional (1D) models, that describe blood flowing into the circulatory tree. In particular, we study the physical consistency of the 1D model with respect to the 3D one. To this aim, we introduce a general criterion based on energy balance for the proper choice of coupling conditions between models. We also propose a way to include in the 1D model the effect of the external tissue surrounding the vessel and we discuss its importance whenever this effect is considered in the 3D model. Finally, we propose several numerical results in real human carotids, studying different configurations for the 1D model and highlighting the best one in view of the physical consistency. Mathematical modeling of human physiopathology is a tremendously ambitious task. It encompasses the modeling of most diverse compartments such as the cardiovascular, respiratory, skeletal and nervous systems, as well as the mechanical and biochemical interaction between blood flow and arterial walls, or electrocardiac processes and the electric conduction into biological tissues. Mathematical models can be set up to simulate both vasculogenesis (the aggregation and organisation of endothelial cells dispersed in a given environment) and angiogenesis (the formation of new vessels sprouting from an existing vessel) that are relevant to the formation of vascular networks, and in particular to the description of tumor growth. The integration of models aimed at simulating the cooperation and interrelation of different systems is an even more difficult task. It calls for the set up of, for instance, interaction models for the integrated cardio-vascular system and the interplay between central circulation and peripheral compartments, models for the mid-long range cardiovascular adjustments to pathological conditions (e.g. to account for surgical interventions, congenital malformations, or tumor growth), models for the integration among circulation, tissue perfusion, biochemical and thermal regulation, models for parameter identification and sensitivity analysis to parameter changes or data uncertainty \\u2013 and many others. The heart is a complex system in itself, where electrical phenomena are functionally related with the wall deformation. In its turn, electrical activity is related with heart physiology. It involves nonlinear reaction-diffusion processes and provides the activation stimulus to the heart dynamics and eventually the blood ventricular flow that drives the haemodynamics of the whole circulatory system. In fact, the influence is reciprocal, since the circulatory system in turns affects the heart dynamics and may induce an overload depending upon the individual physiopathologies ( for instance the presence of a stenotic artery or a vascular prosthesis).Virtually, all the fields of mathematics have a role to play in this context. Geometry and approximation theory provide the tools for handling clinical data acquired by tomography or magnetic resonance, identifying meaningful geometrical patterns and producing three-dimensional geometrical models stemming from the original patients data. Mathematical analysis, flow and solid dynamics, stochastic analysis are used to set up the differential models and predict uncertainty. Numerical analysis and high performance computing are needed to numerically solve the complex differential models. Finally, methods from stochastic and statistical analysis are exploited for the modeling and interpretation of space-time patterns. Indeed, the complexity of the problems at hand often stimulates the use of innovative mathematical techniques that are able, for instance, to accurately catch those processes that occur at multiple scales in time and space (like cellular and systemic effects), and that are governed by heterogeneous physical laws. The numerical simulation of free-surface flows around sailing boats is a complex topic that addresses multiple mathematical tasks: the correct study of the flow field around a rigid hull, the numerical simulation of the hull dynamics, the deformation of the sails and appendages under transient external conditions like gusts of wind or wave patterns and, overall, the coupling among all these components. In this paper, we present some recent advances that have been achieved in different research topics related to yacht design and performance prediction. In particular, we describe the numerical algorithms that have been developped in the framework of open-source libraries for the simulation of free-surface hydrodynamics and boat dynamics, as well as for the analysis of the fluid-structure interaction between wind and sails. Moreover, an algorithm for shape optimization, based on the solution of the adjoint problem and combined with the Free Form Deformation (FFD) method for the shape parameterization and mesh motion, is presented and discussed. Theoretical and methodological aspects are described, and the first preliminary results are reported. In this note, we propose Steklov-Poincare iterative algorithms (mutuated from the analogy with heterogeneous domain decomposition) to solve fluidstructure interaction problems. Although our framework is very general, the driving application is concerned with the interaction of blood flow and vessel wall in large arteries. Publisher Summary The development of mathematical models, algorithms and numerical simulation tools for the investigation of the human cardiovascular system has received a great impulse in the past years. This chapter addresses the problem of developing models for the numerical simulation of the human circulatory system. It particularly focuses on the problem of hemodynamics in large human arteries. There are several important aspects, which require the use of sophisticated mathematical and numerical tools, such as the reconstruction of geometries from medical data; the transport of biochemicals in blood and vessel wall tissue; the heart dynamics; and blood rheology. Besides, the need of validating the models calls for development of accurate in-vivo measurement techniques. The number and complexity of the mathematical, numerical and technological problems involved makes the development of tools for accurate, reliable and efficient simulations of the human cardiovascular system one of the challenges of the next decades. The solution of inverse problems in cardiovascular mathematics is computationally expensive. In this paper we apply a domain parametrization technique to reduce both the geometrical and computational complexity of the forward problem, and replace the finite element solution of the incompressible Navier-Stokes equations by a computationally less expensive reduced basis approximation. This greatly reduces the cost of simulating the forward problem. We then consider the solution of inverse problems in both the deterministic sense, by solving a least-squares problem, and in the statistical sense, by using a Bayesian framework for quantifying uncertainty. Two inverse problems arising in haemodynamics modelling are considered: (i) a simplified fluid-structure interaction model problem in a portion of a stenosed artery for quantifying the risk of atherosclerosis by identifying the material parameters of the arterial wall based on pressure measurements; (ii) a simplified femoral bypass graft model for robust shape design under uncertain residual flow in the main arterial branch identified from pressure measurements. Summary  The goal of this work is the development and numerical implementation of a mathematical model describing the functioning of heart valves. To couple the pulsatile blood flow with a highly deformable thin structure (the valve's leaflets), a resistive Eulerian surfaces framework is adopted. A lumped-parameter model helps to couple the movement of the leaflets with the blood dynamics. A reduced circulation model describes the systemic hemodynamics and provides a physiological pressure profile at the downstream boundary of the valve. The resulting model is relatively simple to describe for a healthy valve and pathological heart valve functioning while featuring an affordable computational burden. Efficient time and spatial discretizations are considered and implemented. We address in detail the main features of the proposed method, and we report several numerical experiments for both two-dimensional and three-dimensional cases with the aim of illustrating its accuracy. Copyright \\u00a9 2015 John Wiley & Sons, Ltd. In this work we develop an adaptive and reduced computational algorithm based on dimension-adaptive sparse grid approximation and reduced basis methods for solving high-dimensional uncertainty quantification (UQ) problems. In order to tackle the computational challenge of \\\"curse of dimensionality\\\" commonly faced by these problems, we employ a dimension-adaptive tensor-product algorithm 16 and propose a verified version to enable effective removal of the stagnation phenomenon besides automatically detecting the importance and interaction of different dimensions. To reduce the heavy computational cost of UQ problems modelled by partial differential equations (PDE), we adopt a weighted reduced basis method 7 and develop an adaptive greedy algorithm in combination with the previous verified algorithm for efficient construction of an accurate reduced basis approximation. The efficiency and accuracy of the proposed algorithm are demonstrated by several numerical experiments. We investigate a one-dimensional model of blood flow in human arteries. In particular, we consider the case when an abrupt variation of mechanical characteristic of artery is caused by the presence of a vascular prosthesis. The derivations of the model and of a numerical scheme adopted for its solution are detailed. Numerical experiments show the effectiveness of the model. In these notes we consider systems of hyperbolic equations and their reformulation in the framework of multi-domain partition of the computational domain. In this work a reduced-order computational framework for the study of haemodynamics in three-dimensional patient-specific configurations of coronary artery bypass grafts dealing with a wide range of scenarios is proposed. We combine several efficient algorithms to face at the same time both the geometrical complexity involved in the description of the vascular network and the huge computational cost entailed by time dependent patient-specific flow simulations. Medical imaging procedures allow to reconstruct patient-specific configurations from clinical data. A centerlines-based parametrization is proposed to efficiently handle geometrical variations. POD-Galerkin reduced-order models are employed to cut down large computational costs. This computational framework allows to characterize blood flows for different physical and geometrical variations relevant in the clinical practice, such as stenosis factors and anastomosis variations, in a rapid and reliable way. Several numerical results are discussed, highlighting the computational performance of the proposed framework, as well as its capability to carry out sensitivity analysis studies, so far out of reach. In particular, a reduced-order simulation takes only a few minutes to run, resulting in computational savings of 99 % of CPU time with respect to the full-order discretization. Moreover, the error between full-order and reduced-order solutions is also studied, and it is numerically found to be less than 1 % for reduced-order solutions obtained with just O(100) online degrees of freedom. In this paper we further develop an approach previously introduced in [Lassila and Rozza, C.M.A.M.E 2010] for shape optimization that combines a suitable low-dimensional parametrization of the geometry (yielding a geometrical reduction) with reduced basis methods (yielding a reduction of computational complexity). More precisely, free-form deformation techniques are considered for the geometry description and its parametrization, while reduced basis methods are used upon a finite element discretization to solve systems of parametrized partial differential equations. This allows an efficient flow field computation and cost functional evaluation during the iterative optimization procedure, resulting in effective computational savings with respect to usual shape optimization strategies. This approach is very general and can be applied to a broad variety of problems. In this paper we apply it to find the optimal shape of aorto-coronaric bypass anastomoses based on vorticity minimization in the down-field region. Blood flows in the coronary arteries are modelled using Stokes equations; afterwards, results have been verified in feedback using Navier-Stokes equations In this paper we propose a new model reduction technique aimed at real-time blood flow simulations on a given family of geometrical shapes of arterial vessels. Our approach is based on the combination of a low-dimensional shape parametrization of the computational domain and the reduced basis method to solve the associated parametrized flow equations. We propose a preliminary analysis carried on a set of arterial vessel geometries, described by means of a radial basis functions parametrization. In order to account for patient-specific arterial configurations, we reconstruct the latter by solving a suitable parameter identification problem. Real-time simulation of blood flows are thus performed on each reconstructed parametrized geometry, by means of the reduced basis method. We focus on a family of parametrized carotid artery bifurcations, by modelling blood flows using Navier-Stokes equations and measuring distributed outputs such as viscous energy dissipation or vorticity. The latter are indexes that might be correlated with the assessment of pathological risks. The approach advocated here can be applied to a broad variety of (different) flow problems related with geometry/shape variation, for instance related with shape sensitivity analysis, parametric exploration, and shape design. European funding under framework 7 (FP7) for the virtual physiological human (VPH) project has been in place now for nearly 2 years. The VPH network of excellence (NoE) is helping in the development of common standards, open-source software, freely accessible data and model repositories, and various training and dissemination activities for the project. It is also helping to coordinate the many clinically targeted projects that have been funded under the FP7 calls. An initial vision for the VPH was defined by framework 6 strategy for a European physiome (STEP) project in 2006. It is now time to assess the accomplishments of the last 2 years and update the STEP vision for the VPH. We consider the biomedical science, healthcare and information and communications technology challenges facing the project and we propose the VPH Institute as a means of sustaining the vision of VPH beyond the time frame of the NoE. We analyze the approximation properties of some interpolation operators and some L2-orthogonal projection operators related to systems of polynomials which are orthonormal with respect to a weight function o(x1, . . ., Xd), d > 1. The error estimates for the Legendre system and the Chebyshev system of the first kind are given in the norms of the Sobolev spaces H'. These results are useful in the numerical analysis of the approximation of partial differential equations by spectral methods. 0. Introduction. Spectral methods are a classical and largely used technique to solve differential equations, both theoretically and numerically. During the years they have gained new popularity in automatic computations for a wide class of physical problems (for instance in the fields of fluid and gas dynamics), due to the use of the Fast Fourier Transform algorithm. These methods appear to be competitive with finite difference and finite element methods and they must be decisively preferred to the last ones whenever the solution is highly regular and the geometric dimension of the domain becomes large. Moreover, by these methods it is possible to control easily the solution (filtering) of those numerical problems affected by oscillation and instability phenomena. The use of spectral and pseudo-spectral methods in computations in many fields of engineering has been matched by deeper theoretical studies; let us recall here the pioneering works by Orszag [25], [26], Kreiss and Oliger [14] and the monograph by Gottlieb and Orszag [13]. The theoretical results of such works are mainly concerned with the study of the stability of approximation of parabolic and hyperbolic equations; the solution is assumed to be infinitely differentiable, so that by an analysis of the Fourier coefficients an infinite order of convergence can be achieved. More recently (see Pasciak [27], Canuto and Quarteroni [10], [11], Maday and Quarteroni [20], [211, [22], Mercier [23]), the spectral methods have been studied by the variational techniques typical of functional analysis, to point out the dependence of the approximation error (for instance in the L2-norm, or in the energy norm) on the regularity of the solution of continuous problems and on the discretization parameter (the dimension of the space in which the approximate solution is sought). Indeed, often the solution is not infinitely differentiable; on the other hand, sometimes even if the solution is smooth, its derivatives may have very Received August 9, 1980; revised June 12, 1981. 1980 Mathematics Subject Classification. Primary 41A25; Secondary 41A 10, 41A05. ? 1982 American Mathematical Society 0025-571 8/82/0000-0470/$06.00 (67 This content downloaded from 207.46.13.111 on Tue, 09 Aug 2016 06:29:39 UTC All use subject to http://about.jstor.org/terms 68 C. CANUTO AND A. QUARTERONI large norms which affect negatively the rate of convergence (for instance in problems with boundary layers). Both spectral and pseudo-spectral methods are essentially Ritz-Galerkin methods (combined with some integration formulae in the pseudo-spectral case). It is well known that when Galerkin methods are used the distance between the exact and the discrete solution (approximation error) is bounded by the distance between the exact solution and its orthogonal projection upon the subspace (projection error), or by the distance between the exact solution and its interpolated polynomial at some suitable points (interpolation error). This upper bound is often realistic, in the sense that the asymptotic behavior of the approximation error is not better than the one of the projection (or even the interpolation) error. Even more, in some cases the approximate solution coincides with the projection of the true solution upon the subspace (for instance when linear problems with constant coefficients are approximated by spectral methods). This motivates the interest in evaluating the projection and the interpolation errors in differently weighted Sobolev norms. So we must face a situation different from the one of the classical approximation theory where the properties of approximation of orthogonal function systems, polynomial and trigonometric, are studied in the LP-norms, and mostly in the maximum norm (see, e.g., Butzer and Berens [6], Butzer and Nessel [7], Nikol'skiT [24], Sansone [291, Szego [30], Triebel [31], Zygmund [32]; see also Bube [5]). Approximation results in Sobolev norms for the trigonometric system have been obtained by Kreiss and Oliger [15]. In this paper we consider the systems of Legendre orthogonal polynomials, and of Chebyshev orthogonal polynomials of the first kind in dimension d > 1. The reason for this interest must be sought in the applications to spectral approximations of boundary value problems. Indeed, if the boundary conditions are not periodic, Legendre approximation seems to be the easiest to be investigated (the weight w is equal to 1). On the other hand, the Chebyshev approximation is the most effective for practical computations since it allows the use of the Fast Fourier Transform algorithm. The techniques used to obtain our results are based on the representation of a function in the terms of a series of orthogonal polynomials, on the use of the so-called inverse inequality, and finally on the operator interpolation theory in Banach spaces. For the theory of interpolation we refer for instance to Calderon [8], Lions [17], Lions and Peetre [19], Peetre [28]; a recent survey is given, e.g., by Bergh and Lofstrom [4]. An outline of the paper is as follows. In Section 1 some approximation results for the trigonometric system are recalled; the presentation of the results to the interpolation is made in the spirit of what will be its application to Chebyshev polynomials. In Section 2 we consider the La-projection operator upon the space of polynomials of degree at most N in any variable (w denotes the Chebyshev or Legendre weight). In Section 3 a general interpolation operator, built up starting by integration formulas which are not necessarily the same in different spatial dimensions, is considered, and its approximation properties are studied. In [22] Maday and Quarteroni use the results of Section 2 to study the approximation properties of some projection operators in higher order Sobolev norms. Recently, an interesting method which lies inbetween finite elements and This content downloaded from 207.46.13.111 on Tue, 09 Aug 2016 06:29:39 UTC All use subject to http://about.jstor.org/terms ORTHOGONAL POLYNOMIALS IN SOBOLEV SPACES 69 spectral methods has been investigated from the theoretical point of view by Babuska, Szabo and Katz [3]. In particular they obtain approximation properties of polynomials in the norms of the usual Sobolev spaces. Acknowledgements. Some of the results of this paper were announced in [9]; we thank Professor J. L. Lions for the presentation to the C. R. Acad. Sci. of Paris. We also wish to express our gratitude to Professors F. Brezzi and P. A. Raviart for helpful suggestions and continuous encouragement. Notations. Throughout this paper we shall use the following notations: I will be an open bounded interval c R, whose variable is denoted by x; Q the product Id C Rd (d integer > 1) whose variable is denoted by x = (x(.')I_ d; for a multi-integer k E Zd, we set ikV = jd X I'12 and IkloK = m x 1, Dj = a/ax@). The symbol X'J=p (q eventually + oo) will denote the summation over all integral k such that p 0 in U. Set L2(Q) = ({: Q -C I 0 is measurable and ( 0, set Hs ( () = C E L(Q) I 1111ksI, < +?}, where /d 2 11I412I= kENd f DI L/)4 D w dx. This work deals with the modelling of traffic flows in complex networks, spanning two-dimensional regions whose size ( macroscale ) is much greater than the characteristic size of the network arcs ( microscale). A typical example is the modelling of traffic flow in large urbanized areas with diameter of hundreds of kilometers, where standard models of traffic flows on networks resolving all the streets are computationally too expensive. Starting from a stochastic lattice gas model with simple constitutive laws, we derive a distributed two-dimensional model of traffic flow, in the form of a non-linear diffusion-advection equation for the particle density. The equation is formally equivalent to a (non-linear) Darcy's filtration law. In particular, it contains two parameters that can be seen as the porosity and the permeability tensor of the network. We provide suitable algorithms to extract these parameters starting from the geometry of the network and a given microscale model of traffic flow (for instance based on cellular automata). Finally, we compare the fully microscopic simulation with the finite element solution of our upscaled model in realistic cases, showing that our model is able to capture the large-scale feature of the flow. In this work we discuss the reliability of the coupling among three-dimensional (3D) and one-dimensional (1D) models that describe blood flowing into the circulatory tree. In particular, we study the physical consistency of the 1D model with respect to the 3D one. To this aim, we introduce a general criterion based on energy balance for the proper choice of coupling conditions between models. We also propose a way to include in the 1D model the effect of the external tissue surrounding the vessel and we discuss its importance whenever this effect is considered in the 3D model. Finally, we propose several numerical results in real human carotids, studying different configurations for the 1D model and highlighting the best one in view of the physical consistency. In this paper we review some classical algorithms for fluid\\u2013structure interaction problems and we propose an alternative viewpoint mutuated from the domain decomposition theory. This approach yields preconditioned Richardson iterations on the Steklov\\u2013Poincare nonlinear equation at the fluid\\u2013structure interface. We characterize the capacitance matrix associated with domain decompositions by spectral collocation methods for the solution of elliptic problems. Richardson and conjugate gradient iterations with preconditioners wel suited for computation in a parallel environment are discussed. The spectral properties of the preconditioned capacitance matrix are analyzed, also for the case of substructures with internal vertices. We give error estimates for both Legendre and Chebyshev spectral approximations of the steady state Burgers' problem $$ - u_{xx} + lambda (uu_x - f) = 0 in I = ( - 1,1); u( - 1) = u(1) = 0. $$ To do that we prove some abstract approximation properties of orthogonal projection operators in some weighted Sobolev spacesH ? s (I) for both Legendre and Chebyshev weights. Abstract The problem of preconditioning the pseudospectral Chebyshev approximation of an elliptic operator is considered. The numerical sensitivity to variations of the coefficients of the operator are investigated for two classes of preconditioning matrices: one arising from finite differences, the other from finite elements. The preconditioned system is solved by a conjugate gradient type method, and by a DuFort-Frankel method with dynamical parameters. The methods are compared on some test problems with the Richardson method [13] and with the minimal residual Richardson method [21]. During the past two decades, the lattice Boltzmann method (LBM) has been increasingly acknowledged as a valuable alternative to classical numerical techniques (e.g. finite elements, finite volumes, etc.) in fluid dynamics. A distinguishing feature of LBM is undoubtedly its highly parallelizable data structure. In this work we present a general parallel LBM framework for graphic processing units (GPUs) characterized by a high degree of modularity and memory efficiency, still preserving very good computational performance. After recalling the essential programming principles of the CUDA C language for GPUs, the details of the implementation will be provided. The data structure here presented takes into account the intrinsic properties of the Gauss-Hermite quadrature rules (on which the LBM is based) to guarantee a unique and flexible framework for two- and three-dimensional problems. In addition, a careful implementation of a memory efficient formulation of the LBM algorithm has allowed to limit the high memory consumption that typically affects this computational method. Numerical examples in two and three dimensions illustrate the reliability and the performance of the code. Several computational challenges arise when evaluating the failure probability of a given system in the context of risk prediction or reliability analysis. When the dimension of the uncertainties becomes high, well established direct numerical methods can not be employed because of the \\\"curse-of-dimensionality\\\". Many surrogate models have been proposed with the aim of reducing computational effort. However, most of them fail in computing an accurate failure probability when the limit state surface defined by the failure event in the probability space lacks smoothness. In addition, for a stochastic system modeled by partial differential equations (PDEs) with random input, only a limited number of the underlying PDEs (order of a few tens) are affordable to solve in practice due to the considerable computational effort, therefore preventing the application of many numerical methods especially for high dimensional random inputs. In this work we develop hybrid and goal-oriented adaptive reduced basis methods to tackle these challenges by accurately and efficiently computing the failure probability of a stochastic PDE. The curse-of-dimensionality is significantly alleviated by reduced basis approximation whose bases are constructed by goal-oriented adaptation. Moreover, an accurate evaluation of the failure probability for PDE system with solution of low regularity in probability space is guaranteed by the certified a posteriori error bound for the output approximation error. At the end of this paper we suitably extend our proposed method to deal with more general PDE models. Finally we perform several numerical experiments to illustrate its computational accuracy and efficiency. (C) 2013 Elsevier B.V. All rights reserved. Abstract This paper describes a strategy for the parallelization of a finite element code for the numerical simulation of shallow water flow. The numerical scheme adopted for the discretization of the equations in the scalar algorithm is briefly described, with emphasis on the aspects concerning its porting to a parallel architecture. The parallelization strategy is of the domain decomposition type: the implicit computational kernel of the scheme, a Poisson problem, is solved by an additive Schwarz preconditioning technique within conjugate gradient iterations. Both the theoretical and the implementation aspects of the domain decomposition method are described as applied in the present context. Finally, some computational examples are shown and discussed. When modelling the cardiovascular system, the effect of the vessel wall on the blood flow has great relevance. Arterial vessels are complex living tissues and three-dimensional specific models have been proposed to represent their behaviour. The numerical simulation of the 3D-3D Fluid-Structure Interaction (FSI) coupled problem has high computational costs in terms of required time and memory storage. Even if many possible solutions have been explored to speed up the resolution of such problem, we are far from having a 3D-3D FSI model that can be solved quickly. In 3D-3D FSI models two of the main sources of complexity are represented by the domain motion and the coupling between the fluid and the structural part. Nevertheless, in many cases, we are interested in the blood flow dynamics in compliant vessels, whereas the displacement of the domain is small and the structure dynamics is less relevant. In these situations, techniques to reduce the complexity of the problem can be used. One consists in using transpiration conditions for the fluid model as surrogate for the wall displacement, thus allowing problem's solution on a fixed domain. Another strategy consists in modelling the arterial wall as a thin membrane under specific assumptions (Figueroa et al., 2006, Nobile and Vergara, 2008) instead of using a more realistic (but more computationally intensive) 3D elastodynamic model. Using this strategy the dynamics of the vessel motion is embedded in the equation for the blood flow. Combining the transpiration conditions with the membrane model assumption, we obtain an attractive formulation, in fact, instead of solving two different models on two moving physical domains, we solve only a Navier-Stokes system in a fixed fluid domain where the structure model is integrated as a generalized Robin condition. In this paper, we present a general formulation in the boundary conditions which is independent of the time discretization scheme choice and on the stress-strain constitutive relation adopted for the vessel wall structure. Our aim is, first, to write a formulation of a reduced order model with zero order transpiration conditions for a generic time discretization scheme, then to compare a 3D-3D FSI model and a reduced FSI one in two realistic patient-specific cases: a femoropopliteal bypass and an aorta. In particular, we are interested in comparing the wall shear stresses, in fact this quantity can be used as a risk factor for some pathologies such as atherosclerosis or thrombogenesis. More in general we want to assess the accuracy and the computational convenience to use simpler formulations based on reduced order models. In particular, we show that, in the case of small displacements, using a 3D-3D FSI linear elastic model or the correspondent reduced order one yields many similar results. To properly describe the electrical activity of the left ventricle, it is necessary to model the Purkinje fibers, responsible for the fast and coordinate ventricular activation, and their interaction with the muscular propagation. The aim of this work is to propose a methodology for the generation of a patient-specific Purkinje network driven by clinical measurements of the activation times related to pathological propagations. In this case, one needs to consider a strongly coupled problem between the network and the muscle, where the feedback from the latter to the former cannot be neglected as in a normal propagation. We apply the proposed strategy to data acquired on three subjects, one of them suffering from muscular conduction problems owing to a scar and the other two with a muscular pre-excitation syndrome (Wolff\\u2013Parkinson\\u2013White). To assess the accuracy of the proposed method, we compare the results obtained by using the patient-specific Purkinje network generated by our strategy with the ones obtained by using a non-patient-specific network. The results show that the mean absolute errors in the activation time is reduced for all the cases, highlighting the importance of including a patient-specific Purkinje network in computational models. Copyright \\u00a9 2014 John Wiley & Sons, Ltd. The set of solutions of a parameter-dependent linear partial differential equation with smooth coefficients typically forms a compact manifold in a Hilbert space. In this paper we review the generalized reduced basis method as a fast computational tool for the uniform approximation of the solution manifold. As in many other fields of applied sciences, mathematical and numerical models are about to play an increasing role in biology and medicine. More noticeably, the mathematical and numerical investigation of the blood circulatory system, although energetically pursued since few years only, is destined to represent one of the major mathematical challenges of the next decades. In this paper we present a numerical tool to simulate dynamics of stratified sedimentary basins, i.e. depressions on the Earth's surface filled by sediments. The basins are usually complicated by crustal deformations and faulting of the sediments. The balance equations, the non-Newtonian rheology of the sediments, and the depth-porosity compaction laws describe here a model of basin evolution. We propose numerical schemes for the basin boundary movement and for the fault tracking. In addition, a time splitting algorithm is employed to reduce the original model into some simpler mathematical problems. The numerical stability and the other features of the developed methodology are shown using simple test cases and some realistic configurations of sedimentary basins. European funding under Framework 7 (FP7) for the virtual physiological human (VPH) project has been in place now for 5 years. The VPH Network of Excellence (NoE) has been set up to help develop common standards, open source software, freely accessible data and model repositories, and various training and dissemination activities for the project. It is also working to coordinate the many clinically targeted projects that have been funded under the FP7 calls. An initial vision for the VPH was defined by the FP6 STEP project in 2006. In 2010, we wrote an assessment of the accomplishments of the first two years of the VPH in which we considered the biomedical science, healthcare and information and communications technology challenges facing the project (Hunter et al. 2010 Phil. Trans. R. Soc. A 368, 2595-2614 (doi:10.1098/rsta.2010.0048)). We proposed that a not-for-profit professional umbrella organization, the VPH Institute, should be established as a means of sustaining the VPH vision beyond the time-frame of the NoE. Here, we update and extend this assessment and in particular address the following issues raised in response to Hunter et al.: (i) a vision for the VPH updated in the light of progress made so far, (ii) biomedical science and healthcare challenges that the VPH initiative can address while also providing innovation opportunities for the European industry, and (iii) external changes needed in regulatory policy and business models to realize the full potential that the VPH has to offer to industry, clinics and society generally. \\u00a9 2013 The Author(s) Published by the Royal Society. All rights reserved. Simulating arterial trees in the cardiovascular system can be made by the help of different models, depending on the outputs of interest and the desired degree of accuracy. In particular, one-dimensional fluid-structure interaction models for arteries are very effective in reproducing the physiological pressure wave propagation and in providing quantities like pressure and velocity, averaged on the cross section of the arterial lumen. In locations where one-dimensional models cannot capture the complete flow dynamics, e.g., in presence of stenoses and aneurysms, three-dimensional coupled fluid-structure interaction models are necessary to evaluate, for instance, critical factors responsible for pathologies which are associated to hemodynamics. In this work we formalize and investigate the geometrical multiscale problem, where heterogeneous fluid-structure interaction models for arteries are implicitly coupled. We introduce new coupling algorithms, describe their implementation and investigate on simple geometries the numerical reflections that occur at the interface between the heterogeneous models. We also simulate on a supercomputer a three-dimensional abdominal aorta under physiological conditions, coupled with up to six one-dimensional models representing the surrounding arterial branches. Finally, we compare CPU times and number of coupling iterations for different algorithms and time discretizations. We introduce a differential system based on the coupling of the (Navier) Stokes equations and the Darcy equation for the modelling of the interaction between surface and subsurface flows. We formulate the problem as an interface problem and analyze the associated Steklov-Poincare operator. We then propose a way of solving the coupled problem iteratively, based on a suitable splitting of the interface conditions, allowing the solution of two subproblems at each step. This chapter reviews techniques of model reduction of fluid dynamics systems. Fluid systems are known to be difficult to reduce efficiently due to several reasons. First of all, they exhibit strong nonlinearities \\u2014 which are mainly related either to nonlinear convection terms and/or some geometric variability \\u2014 that often cannot be treated by simple linearization. Additional difficulties arise when attempting model reduction of unsteady flows, especially when long-term transient behavior needs to be accurately predicted using reduced order models and more complex features, such as turbulence or multiphysics phenomena, have to be taken into consideration. We first discuss some general principles that apply to many parametric model order reduction problems, then we apply them on steady and unsteady viscous flows modelled by the incompressible Navier-Stokes equations. We address questions of inf-sup stability, certification through error estimation, computational issues and \\u2014 in the unsteady case \\u2014 long-time stability of the reduced model. Moreover, we provide an extensive list of literature references. We consider the numerical solution of second order Partial Differential Equations (PDEs) on lower dimensional manifolds, specifically on surfaces in three dimensional spaces. For the spatial approximation, we consider Isogeometric Analysis which facilitates the encapsulation of the exact geometrical description of the manifold in the analysis when this is represented by B-splines or NURBS. Our analysis addresses linear, nonlinear, time dependent, and eigenvalues problems involving the Laplace\\u2013Beltrami operator on surfaces. Moreover, we propose a priori error estimates under h-refinement in the general case of second order PDEs on the lower dimensional manifolds. We highlight the accuracy and efficiency of Isogeometric Analysis with respect to the exactness of the geometrical representations of the surfaces. The increasing computational load required by most applications and the limits in hardware performances affecting scientific computing contributed in the last decades to the development of parallel software and architectures. In fluid-structure interaction (FSI) for haemodynamic applications, parallelization and scalability are key issues (see [L. Formaggia, A. Quarteroni, and A. Veneziani, eds., Cardiovascular Mathematics: Modeling and Simulation of the Circulatory System, Modeling, Simulation and Applications 1, Springer, Milan, 2009]). In this work we introduce a class of parallel preconditioners for the FSI problem obtained by exploiting the block-structure of the linear system. We stress the possibility of extending the approach to a general linear system with a block-structure, then we provide a bound in the condition number of the preconditioned system in terms of the conditioning of the preconditioned diagonal blocks, and finally we show that the construction and evaluation of the devised preconditioner is modular. The preconditioners are tested on a benchmark three-dimensional (3D) geometry discretized in both a coarse and a fine mesh, as well as on two physiological aorta geometries. The simulations that we have performed show an advantage in using the block preconditioners introduced and confirm our theoretical results. The main goal of this work is to devise robust iterative strategies to partition the solution of the Navier-Stokes equations in a three-dimensional (3D) domain, into non overlapping 3D subdomains, which communicate through the exchange of averaged/integrated quantities across the interfaces. The novel aspect of the present approach is that at coupling boundaries the conservation of flow rates and of the associated dual variables is implicitly imposed, entailing a weak physical coupling. For the solution of the non-linear interface problem two strategies are compared: relaxed fixed point and Newton iterations. The algorithm is tested in several configurations for problems ranging from academic ones to some related to the computational hemodynamics field, which involve more than two components at each coupling interface. In some cases it is shown that relaxed fixed point methods are not convergent, whereas the Newton method leads in all tested cases to convergent schemes. One of the appealing aspects of the strategy proposed here is the flexibility in the setting of boundary conditions at the interfaces, where no hierarchy is established a priori (unlike Gauss-Seidel methods). The usefulness of this methodology is also discussed in the context of dimensionally-heterogeneous coupling and preconditioning for domain decomposition methods. This review paper addresses the so called geometric multiscale approach for the numerical simulation of blood flow problems, from its origin (that we can collocate in the second half of '90s) to our days. By this approach the blood fluid-dynamics in the whole circulatory system is described mathematically by means of heterogeneous problems featuring different degree of detail and different geometric dimension that interact together through appropriate interface coupling conditions. Our review starts with the introduction of the stand-alone problems, namely the 3D fluid-structure interaction problem, its reduced representation by means of 1D models, and the so-called lumped parameters (aka 0D) models, where only the dependence on time survives. We then address specific methods for stand-alone 3D models when the available boundary data are not enough to ensure the mathematical well posedness. These so-called \\\"defective problems\\\" naturally arise in practical applications of clinical relevance but also because of the interface coupling of heterogeneous problems that are generated by the geometric multiscale process. We also describe specific issues related to the boundary treatment of reduced models, particularly relevant to the geometric multiscale coupling. Next, we detail the most popular numerical algorithms for the solution of the coupled problems. Finally, we review some of the most representative works-from different research groups-which addressed the geometric multiscale approach in the past years. A proper treatment of the different scales relevant to the hemodynamics and their interplay is essential for the accuracy of numerical simulations and eventually for their clinical impact. This paper aims at providing a state-of-the-art picture of these topics, where the gap between theory and practice demands rigorous mathematical models to be reliably filled. (C) 2016 Elsevier B.V. All rights reserved.\",\n",
            "  \"2135591858\": \"Rationale and Objectives The aims of this study were to develop and validate an automated method to segment the renal cortex on contrast-enhanced abdominal computed tomographic images from kidney donors and to track cortex volume change after donation. Materials and Methods A three-dimensional fully automated renal cortex segmentation method was developed and validated on 37 arterial phase computed tomographic data sets (27 patients, 10 of whom underwent two computed tomographic scans before and after nephrectomy) using leave-one-out strategy. Two expert interpreters manually segmented the cortex slice by slice, and linear regression analysis and Bland-Altman plots were used to compare automated and manual segmentation. The true-positive and false-positive volume fractions were also calculated to evaluate the accuracy of the proposed method. Cortex volume changes in 10 subjects were also calculated. Results The linear regression analysis results showed that the automated and manual segmentation methods had strong correlations, with Pearson's correlations of 0.9529, 0.9309, 0.9283, and 0.9124 between intraobserver variation, interobserver variation, automated and user 1, and automated and user 2, respectively ( P P t test). The overall true-positive and false-positive volume fractions for cortex segmentation were 90.15 \\u00b1 3.11% and 0.85 \\u00b1 0.05%. With the proposed automated method, the time for cortex segmentation was reduced from 20 minutes for manual segmentation to 2 minutes. Conclusions The proposed method was accurate and efficient and can replace the current subjective and time-consuming manual procedure. The computer measurement confirms the volume of renal cortex increases after kidney donation. Automatic localization of objects is one of great important steps in object recognition and analysis, such as segmentation, registration in many medical applications. In this paper, an automated method is proposed to recognize renal cortex on contrast-enhanced abdominal CT images. The proposed method is based on a strategic combination of the Generalized Hough Transform and Active Appearance Model. It consists of two main phases: training and localization. In the training phase, we train the mean shape models of renal cortex by using Active Appearance Model and compute Generalized Hough Transform parameters. In the localization phase, a modified Generalized Hough Transform algorithm is advanced to estimate potential center of gravity for improving the conventional Active Appearance Model matching method, and then a two-pass Active Appearance Model matching method is proposed based on Generalized Hough Transform. The Active Appearance Models and Generalized Hough Transform parameters were trained with 20 CT angiography datasets, and then the proposed method was tested on a clinical data set of 17 CT angiography datasets. The experimental results show that: 1 an overall cortex localization accuracy is 0.9920\\u00b10.0038, average distance is 11.00\\u00b19.34 pixels. 2 The proposed method is highly efficient such that the overall localization can be finalized within 1.2075\\u00b10.3738 seconds for each 2D slice. In this paper, we propose a novel approach to solve the renal cortex segmentation problem, which has rarely been studied. In this study, the renal cortex segmentation problem is handled as a multiple-surfaces extraction problem, which is solved using the optimal surface search method. We propose a novel graph construction scheme in the optimal surface search to better accommodate multiple surfaces. Different surface sub-graphs are constructed according to their properties, and inter-surface relationships are also modeled in the graph. The proposed method was tested on 17 clinical CT datasets. The true positive volume fraction (TPVF) and false positive volume fraction (FPVF) are 74.10% and 0.08%, respectively. The experimental results demonstrate the effectiveness of the proposed method. The enhancement of the low quality fingerprint is a difficult and challenge task. This paper proposes an efficient algorithm based on anisotropic filtering to enhance the low quality fingerprint. In our algorithm, an orientation filed estimation with feedback method was proposed to compute the accurate fingerprint orientation. The gradient-based approach was firstly used to compute the coarse orientation. Then the reliability of orientation was computed from the gradient image. If the reliability of the estimated orientation is less than pre-specified threshold, the orientation will be corrected by the mixed orientation model. And an anisotropic filtering was used to enhance the fingerprint, with the advantages of its efficient ridge enhancement and its robustness against noise in the fingerprint image. The proposed algorithm has been evaluated on the databases of Fingerprint verification competition (FVC2004). Experimental results confirm that the proposed algorithm is effective and robust for the enhancement of the low quality fingerprint. This paper presents a prototype design and implementation of secured mobile phones based on embedded fingerprint recognition systems. One is a front-end fingerprint capture sub-system and the other is a back-end fingerprint recognition system based on smart phones. The fingerprint capture sub-system is an external module which contains two parts: an ARM-Core processor LPC2106 and an Atmel Finger Sensor AT77C101B. The LPC2106 processor controls the AT77C101B sensor to capture the fingerprint image. In the fingerprint recognition system, a new fingerprint verification algorithm was implemented on internal hardwares. The performance of the proposed system, with 4.16% equal error rate (EER) was examined on Atmel fingerprints database. The average computation time on a 13 MHz CPU S1C33 (by Epson) is about 5.0 sec. Coping with non-linear distortions in fingerprint matching is a real challenging task. This paper proposed a novel method, fuzzy features match (FFM), to match the deformed fingerprints. The fingerprint was represented by the fuzzy features: local triangle features set. The similarity between fuzzy features is used to character the similarity between fingerprints. First, a fuzzy similarity measure for two triangles was introduced. Second, the result is extended to construct a similarity vector which includes the triangle-level similarities for all triangles in two fingerprints. Accordingly, a similarity vector pair is defined to illustrate the similarities between two fingerprints. Finally, the FFM measure maps a similarity vector pair to a scalar quantity, within the real interval [0, 1], which quantifies the overall image to image similarity. To validate the method, fingerprints of FVC2004 were evaluated with the proposed algorithm. Experimental results show that FFM is a reliable and effective algorithm for fingerprint matching with non-liner distortions. This paper investigates, using prior shape models and the concept of ball scale (b-scale), ways of automatically recognizing objects in 3D images without performing elaborate searches or optimization. That is, the goal is to place the model in a single shot close to the right pose (position, orientation, and scale) in a given image so that the model boundaries fall in the close vicinity of object boundaries in the image. This is achieved via the following set of key ideas: (a) A semi-automatic way of constructing a multi-object shape model assembly. (b) A novel strategy of encoding, via b-scale, the pose relationship between objects in the training images and their intensity patterns captured in b-scale images. (c) A hierarchical mechanism of positioning the model, in a one-shot way, in a given image from a knowledge of the learnt pose relationship and the b-scale image of the given image to be segmented. The evaluation results on a set of 20 routine clinical abdominal female and male CT data sets indicate the following: (1) Incorporating a large number of objects improves the recognition accuracy dramatically. (2) The recognition algorithm can be thought as a hierarchical framework such that quick replacement of the model assembly is defined as coarse recognition and delineation itself is known as finest recognition. (3) Scale yields useful information about the relationship between the model assembly and any given image such that the recognition results in a placement of the model close to the actual pose without doing any elaborate searches or optimization. (4) Effective object recognition can make delineation most accurate. How to cope with non-linear distortions in the matching algorithm is a real challenge. In this paper, we proposed a novel fingerprint matching algorithm based on the local topologic structure and a novel method to compute the similarity between two fingerprints. The algorithm firstly aligns the template fingerprint and the input fingerprint. Then local topologic structure matching was introduced to improve the robustness of global alignment. Finally a novel method was introduced to compute the similarity between the template fingerprint and the input fingerprint. The proposed algorithm has been participated in Fingerprint verification competition (FVC2004). The performance was ranked 3rd position in open category in FVC2004. The current procedure of renal cortex segmentation is subjective and tedious. This investigation is to develop and validate an automated method to segment renal cortex on contrast-enhanced abdominal CT images. The proposed framework consists of four parts: first, an active appearance model (AAM) is built using a set of training images; second, the AAM is refined by live wire (LW) method to initialize the shape and location of the kidney; third, an iterative graph cut-oriented active appearance model (IGC-OAAM) method is applied to segment the kidney; Finally, the identified kidney contour is used as shape constraints for renal cortex segmentation which is also based on IGC-OAAM. The proposed method was validated on a clinical data set of 27 CT angiography images. The experimental results show that: (1) an overall cortex segmentation accuracy with overlap error \\u226412.7%, volume difference \\u2264 3.9%, average distance \\u2264 1.5 mm, root mean square (RMS) distance \\u2264 2.8 mm and maximal distance \\u2264 19.5 mm could be achieved. (2) The proposed method is highly efficient such that the overall segmentation can be finalized within 2 minutes. With the increasing volume of sensitive and private information stored in the mobile phone, the security issue of mobile phone becomes an important field to investigate. This paper proposes a fingerprint authentication system for mobile phone security application. A prototype of our system is developed from the platform of BIRD E868 mobile phone with external fingerprint capture module. It is composed of two parts. One is the front-end fingerprint capture sub-system, and the other is back-end fingerprint recognition system. A thermal sweep fingerprint sensor is used in the fingerprint capture sub-system to fit the limitations of size, cost, and power consumption. In the fingerprint recognition sub-system, an optimized algorithm is developed from the one participated in the FVC2004. The performance of the proposed system is evaluated on the database built by the thermal sweep fingerprint sensor. With the advancement of mobile technology, mobile phones can store significant amount of sensitive and private information. The security issue of mobile phones becomes an important field to investigate. This paper proposes a prototype of fingerprint authentication mobile phone based on sweep sensor MBF310. The prototype is composed of the front-end fingerprint capture sub-system and the back-end fingerprint recognition system. A sweep fingerprint sensor MBF310 is used to fit the request of the mobile phone in the field of the size, cost, and power consumption. The performance of the proposed prototype is evaluated on the database built by the sweep fingerprint sensor. The EER is 4.23%, and the average match time of the prototype is about 4.5 seconds. This paper introduces a novel method based on the elasticity analysis of the finger skin to discriminate fake fingers from real ones. We match the fingerprints before and after special distortion and gained their corresponding minutiae pairs as landmarks. The thin-plate spline (TPS) model is used to globally describe the finger distortion. For an input finger, we compute the bending energy vector by the TPS model and calculate the similarity of the bending energy vector to the bending energy fuzzy feature set. The similarity score is in the range [0, 1], indicating how much the current finger is similar to the real finger. The method realizes fake finger detection based on the normal steps of fingerprint processing without special hardware, so it is easily implemented and efficient. The experimental results on a database of real and fake fingers show that the performance of the method is available. An automated method is proposed to segment and quantify the volume of cystoid macular edema (CME) for the abnormal retina with macular hole (MH) in 3D OCT images. The proposed framework consists of three parts: (1) preprocessing, which includes denoising, intraretinal layers segmentation and flattening, MH and vessel silhouettes exclusion; (2) coarse segmentation, in which an AdaBoost classifier is used to get the seeds and constrained regions for Graph Cut; (3) fine segmentation, in which a graph cut algorithm is used to get the refine segmentation result. The proposed method was evaluated in 3D OCT images from 18 typical patients with CMEs and MH. The true positive volume fraction (TPVF), false positive volume fraction (FPVF) and accuracy rate (ACC) for CME volume segmentation are 84.6%, 1.7% and 99.7%, respectively. It is a difficult and challenge task to extract the accurate orientation field for the low quality fingerprints. This paper proposed a robust orientation estimation with feedback algorithm to get the accurate fingerprint orientation. First, the fingerprint image is segmented into recoverable and unrecoverable regions. The following orientation field estimation and orientation correction algorithms were only processed in the recoverable regions. Second, the coarse orientation image is estimated from the input fingerprint image using the gradient-based approach. Then we computed the reliability of orientation from the gradient image. If the reliability of the estimated orientation is less than pre-specified threshold, the orientation will be corrected by the proposed mixed orientation model. The proposed algorithm has been evaluated on the databases of FVC2004. Experimental results confirm that the proposed algorithm is a reliable and effective algorithm for the extraction orientation field of the low quality fingerprints. Purpose To assess the correlation and agreement between the Topcon built-in algorithm and our graph-based algorithm in measuring the total and regional macular thickness for normal and glaucoma subjects.   Methods A total of 228 normal eyes and 93 glaucomatous eyes were enrolled in our study. All patients underwent comprehensive ophthalmic examination and Topcon 3D-OCT 2000 scan. One eye was randomly selected for each subject. The thickness of each layer and the total and regional macular thickness on an Early Treatment of Diabetic Retinopathy Study (ETDRS) chart were measured using the Topcon algorithm and our three-dimensional graph-based algorithm. Correlation and agreement analyses between these two algorithms were performed.   Results Our graph search algorithm exhibited a strong correlation with Topcon algorithm. The macular GCC thickness values for normal and glaucoma subjects ranged from 0.86 to 0.91 and from 0.78 to 0.90, and the regional macular thickness values ranged from 0.79 to 0.96 and 0.70 to 0.95, respectively. Small differences were observed between the Topcon algorithm and our graph-based algorithm. The span of 95% limits of agreement of macular GCC thickness was less than 28 \\u03bcm in both normal and glaucoma subjects, respectively. These limits of total and regional macular thickness were 15.5 \\u03bcm and 23.1 \\u03bcm for normal subjects and 29.1 \\u03bcm and 46.4 \\u03bcm for glaucoma subjects, respectively.   Conclusion Our graph-based algorithm exhibited a high degree of agreement with the Topcon algorithm with respect to thickness measurements in normal and glaucoma subjects. Moreover, our graph-based algorithm can segment the retina into more layers than the Topcon algorithm does. Accurate lung tumor delineation plays an important role in radiotherapy treatment planning. Since the lung tumor has poor boundary in positron emission tomography (PET) images and low contrast in computed tomography (CT) images, segmentation of tumor in the PET and CT images is a challenging task. In this paper, we effectively integrate the two modalities by making fully use of the superior contrast of PET images and superior spatial resolution of CT images. Random walk and graph cut method is integrated to solve the segmentation problem, in which random walk is utilized as an initialization tool to provide object seeds for graph cut segmentation on the PET and CT images. The co-segmentation problem is formulated as an energy minimization problem which is solved by max-flow/min-cut method. A graph, including two sub-graphs and a special link, is constructed, in which one sub-graph is for the PET and another is for CT, and the special link encodes a context term which penalizes the difference of the tumor segmentation on the two modalities. To fully utilize the characteristics of PET and CT images, a novel energy representation is devised. For the PET, a downhill cost and a 3D derivative cost are proposed. For the CT, a shape penalty cost is integrated into the energy function which helps to constrain the tumor region during the segmentation. We validate our algorithm on a data set which consists of 18 PET-CT images. The experimental results indicate that the proposed method is superior to the graph cut method solely using the PET or CT is more accurate compared with the random walk method, random walk co-segmentation method, and non-improved graph cut method. Pigment epithelium detachment (PED) is an important clinical manifestation of multiple chorio-retinal disease processes, which can cause the loss of central vision. A 3-D method is proposed to automatically segment serous PED in SD-OCT images. The proposed method consists of five steps: first, a curvature anisotropic diffusion filter is applied to remove speckle noise. Second, the graph search method is applied for abnormal retinal layer segmentation associated with retinal pigment epithelium (RPE) deformation. During this process, Bruch\\u2019s membrane, which doesn\\u2019t show in the SD-OCT images, is estimated with the convex hull algorithm. Third, the foreground and background seeds are automatically obtained from retinal layer segmentation result. Fourth, the serous PED is segmented based on the graph cut method. Finally, a post-processing step is applied to remove false positive regions based on mathematical morphology. The proposed method was tested on 20 SD-OCT volumes from 20 patients diagnosed with serous PED. The average true positive volume fraction (TPVF), false positive volume fraction (FPVF), dice similarity coefficient (DSC) and positive predictive value (PPV) are 97.19%, 0.03%, 96.34% and 95.59%, respectively. Linear regression analysis shows a strong correlation (r = 0.975) comparing the segmented PED volumes with the ground truth labeled by an ophthalmology expert. The proposed method can provide clinicians with accurate quantitative information, including shape, size and position of the PED regions, which can assist diagnose and treatment. Empirical mode decomposition (EMD) is an adaptive method for nonlinear, non-stationary signal analysis. However, the upper and lower envelopes fitted by cubic spline interpolation (CSI) may often occur overshoots. In this paper, a new envelope fitting method based on the flattest constrained interpolation is proposed. The proposed method effectively integrates the difference between extremes into the cost function, and applies a chaos particle swarm optimization method to optimize the derivatives of the interpolation nodes. The proposed method was tested on three different types of data: ascertain signal, random signals and real electrocardiogram signals. The experimental results show that: (1) The proposed flattest envelope effectively solves the overshoots caused by CSI method and the artificial bends caused by piecewise parabola interpolation (PPI) method. (2) The index of orthogonality of the intrinsic mode functions (IMFs) based on the proposed method is 0.04054, 0.02222\\u00b10.01468 and 0.04013\\u00b10.03953 for the ascertain signal, random signals and electrocardiogram signals, respectively, which is lower than the CSI method and the PPI method, and means the IMFs are more orthogonal. (3) The index of energy conversation of the IMFs based on the proposed method is 0.96193, 0.93501\\u00b10.03290 and 0.93041\\u00b10.00429 for the ascertain signal, random signals and electrocardiogram signals, respectively, which is closer to 1 than the other two methods and indicates the total energy deviation amongst the components is smaller. (4) The comparisons of the Hilbert spectrums show that the proposed method overcomes the mode mixing problems very well, and make the instantaneous frequency more physically meaningful. Purpose: This paper studies the feasibility of developing an automatic anatomy segmentation (AAS) system in clinical radiology and demonstrates its operation on clinical 3D images.Methods: The AAS system, the authors are developing consists of two main parts: object recognition and object delineation. As for recognition, a hierarchical 3D scale-based multiobject method is used for the multiobject recognition task, which incorporates intensity weighted ball-scale (b-scale) information into the active shape model (ASM). For object delineation, an iterative graph-cut-ASM (IGCASM) algorithm is proposed, which effectively combines the rich statistical shape information embodied in ASM with the globally optimal delineation capability of the GC method. The presented IGCASM algorithm is a 3D generalization of the 2D GC-ASM method that they proposed previously in Chen et al. [Proc. SPIE, 7259, 72590C1\\u201372590C-8 (2009)]. The proposed methods are tested on two datasets comprised of images obtained from 20 patients (10 male and 10 female) of clinical abdominal CT scans, and 11 foot magnetic resonance imaging (MRI) scans. The test is for four organs (liver, left and right kidneys, and spleen) segmentation, five foot bones (calcaneus, tibia, cuboid, talus, and navicular). The recognition and delineation accuracies were evaluated separately. The recognition accuracy was evaluated in terms of translation, rotation, and scale (size) error. The delineation accuracy was evaluated in terms of true and false positive volume fractions (TPVF, FPVF). The efficiency of the delineation method was also evaluated on an Intel Pentium IV PC with a 3.4 GHZ CPU machine.Results: The recognition accuracies in terms of translation, rotation, and scale error over all organs are about 8 mm, 10\\u00b0 and 0.03, and over all foot bones are about 3.5709 mm, 0.35\\u00b0 and 0.025, respectively. The accuracy of delineation over all organs for all subjects as expressed in TPVF and FPVF is 93.01% and 0.22%, and all foot bones for all subjects are 93.75% and 0.28%, respectively. While the delineations for the four organs can be accomplished quite rapidly with average of 78 s, the delineations for the five foot bones can be accomplished with average of 70 s.Conclusions: The experimental results showed the feasibility and efficacy of the proposed automatic anatomy segmentation system: (a) the incorporation of shape priors into the GC framework is feasible in 3D as demonstrated previously for 2D images; (b) our results in 3D confirm the accuracy behavior observed in 2D. The hybrid strategy IGCASM seems to be more robust and accurate than ASM and GC individually; and (c) delineations within body regions and foot bones of clinical importance can be accomplished quite rapidly within 1.5 min. ABSTRACT Recognition of anatomical structures is an important step in model based medical image segmentation. Itprovides pose estimation of objects and information about \\u008dwhere\\u017d roughly the objects are in the image anddistinguishing them from other object-like entities. In, 1 we presented a general method of model-based multi-object recognition to assist in segmentation (delineation) tasks. It exploits the pose relationship that can beencoded, via the concept of ball scale (b-scale), betw een the binary training object s and their associated greyimages. The goal was to place the model, in a single shot, close to the right pose (position, orientation, andscale) in a given image so that the model boundaries fall in the close vicinity of object boundaries in the image.Unlike position and scale parameters, we observe that orientation parameters require more attention whenestimating the pose of the model as even small die rences in orientation parameters can lead to inappropriaterecognition. Motivated from the non-Euclidean nature of the pose information, we propose in this paper the useof non-Euclidean metrics to estimate orientation of the anatomical structures for more accurate recognition andsegmentation. We statistically analyze and evaluate the following metrics for orientation estimation: Euclidean,Log-Euclidean, Root-Euclidean, Procrustes Size-and-Shape, and mean Hermitian metrics. The results show thatmean Hermitian and Cholesky decomposition metrics provide more accurate orientation estimates than otherEuclidean and non-Euclidean metrics.Keywords: Anatomy Segmentation, Object Recognition, Herm itian Matrices, Non-Euclid ean metrics, MultipleObject Recognition An automated method is reported for segmenting 3-D fluid-associated abnormalities in the retina, so-called symptomatic exudate-associated derangements (SEAD), from 3-D OCT retinal images of subjects suffering from exudative age-related macular degeneration. In the first stage of a two-stage approach, retinal layers are segmented, candidate SEAD regions identified, and the retinal OCT image is flattened using a candidate-SEAD aware approach. In the second stage, a probability constrained combined graph search-graph cut method refines the candidate SEADs by integrating the candidate volumes into the graph cut cost function as probability constraints. The proposed method was evaluated on 15 spectral domain OCT images from 15 subjects undergoing intravitreal anti-VEGF injection treatment. Leave-one-out evaluation resulted in a true positive volume fraction (TPVF), false positive volume fraction (FPVF) and relative volume difference ratio (RVDR) of 86.5%, 1.7%, and 12.8%, respectively. The new graph cut-graph search method significantly outperformed both the traditional graph cut and traditional graph search approaches (p <; 0.01, p <; 0.04) and has the potential to improve clinical management of patients with choroidal neovascularization due to exudative age-related macular degeneration. Cardiac magnetic resonance imaging (CMRI) has been well validated and allows quantification of myocardial fibrosis in comparison to overall mass of the myocardium. Unfortunately, CMRI is relatively expensive and is contraindicated in patients with intracardiac devices. Cardiac CT (CCT) is widely available and has been validated for detection of scar and myocardial stress/rest perfusion. In this paper, we sought to evaluate the potential of low dose CCT for the measurement of myocardial whole heart extracellular volume (ECV) fraction. A novel framework was proposed for CCT whole heart ECV estimation, which consists of three main steps. First, a shape constrained graph cut (GC) method was proposed for myocardium and blood pool segmentation for post-contrast image. Second, the symmetric Demons deformable registrations method was applied to register pre-contrast to post-contrast images. Finally, the whole heart ECV value was computed. The proposed method was tested on 7 clinical low dose CCT datasets with pre-contrast and post-contrast images. The preliminary results demonstrated the feasibility and efficiency of the proposed method. Cardiac CT (CCT) is widely available and has been validated for the detection of focal myocardial scar using a delayed enhancement technique in this paper. CCT, however, has not been previously evaluated for quantification of diffuse myocardial fibrosis. In our investigation, we sought to evaluate the potential of low-dose CCT for the measurement of myocardial whole heart extracellular volume (ECV) fraction. ECV is altered under conditions of increased myocardial fibrosis. A framework consisting of three main steps was proposed for CCT whole heart ECV estimation. First, a shape-constrained graph cut (GC) method was proposed for myocardium and blood pool segmentation on postcontrast image. Second, the symmetric demons deformable registration method was applied to register precontrast to postcontrast images. So the correspondences between the voxels from precontrast to postcontrast images were established. Finally, the whole heart ECV value was computed. The proposed method was tested on 20 clinical low-dose CCT datasets with precontrast and postcontrast images. The preliminary results demonstrated the feasibility and efficiency of the proposed method. Fingerprint recognition with identical twins is a challenging task due to the closest genetics-based relationship existing in the identical twins. Several pioneers have analyzed the similarity between twins' fingerprints. In this work we continue to investigate the topic of the similarity of identical twin fingerprints. Our study was tested based on a large identical twin fingerprint database that contains 83 twin pairs, 4 fingers per individual and six impressions per finger: 3984 (83*2*4*6) images. Compared to the previous work, our contributions are summarized as follows: (1) Two state-of-the-art fingerprint identification methods: P071 and VeriFinger 6.1 were used, rather than one fingerprint identification method in previous studies. (2) Six impressions per finger were captured, rather than just one impression, which makes the genuine distribution of matching scores more realistic. (3) A larger sample (83 pairs) was collected. (4) A novel statistical analysis, which aims at showing the probability distribution of the fingerprint types for the corresponding fingers of identical twins which have same fingerprint type, has been conducted. (5) A novel analysis, which aims at showing which finger from identical twins has higher probability of having same fingerprint type, has been conducted. Our results showed that: (a) A state-of-the-art automatic fingerprint verification system can distinguish identical twins without drastic degradation in performance. (b) The chance that the fingerprints have the same type from identical twins is 0.7440, comparing to 0.3215 from non-identical twins. (c) For the corresponding fingers of identical twins which have same fingerprint type, the probability distribution of five major fingerprint types is similar to the probability distribution for all the fingers' fingerprint type. (d) For each of four fingers of identical twins, the probability of having same fingerprint type is similar. In the above-named article (ibid., vol. 31, no. 10, pp. 1849-1860, Oct. 2012), the author name Jian Tian should have been Jie Tian. In this paper, we present an automatic renal cortex segmentation approach using the implicit shape registration and novel multiple surfaces graph search. The proposed approach is based on a hierarchy system. First, the whole kidney is roughly initialized using an implicit shape registration method, with the shapes embedded in the space of Euclidean distance functions. Second, the outer and inner surfaces of renal cortex are extracted utilizing multiple surfaces graph searching, which is extended to allow for varying sampling distances and physical constraints to better separate the renal cortex and renal column. Third, a renal cortex refining procedure is applied to detect and reduce incorrect segmentation pixels around the renal pelvis, further improving the segmentation accuracy. The method was evaluated on 17 clinical computed tomography scans using the leave-one-out strategy with five metrics: Dice similarity coefficient (DSC), volumetric overlap error (OE), signed relative volume difference (SVD), average symmetric surface distance (Davg), and average symmetric rms surface distance (Drms). The experimental results of DSC, OE, SVD, Davg, and Drms were 90.50%\\u00b11.19%, 4.38% \\u00b13.93%, 2.37% \\u00b11.72%, 0.14 mm \\u00b10.09 mm , and 0.80 mm \\u00b10.64 mm, respectively. The results showed the feasibility, efficiency, and robustness of the proposed method. It is desirable to predict the tumor growth rate so that appropriate treatment can be planned in the early stage. Previously, we proposed a finite-element-method (FEM)-based 3-D kidney tumor growth prediction system using longitudinal images. A reaction-diffusion model was applied as the tumor growth model. In this paper, we not only improve the tumor growth model by coupling the reaction-diffusion model with a biomechanical model, but also take the surrounding tissues into account. Different diffusion and biomechanical properties are applied for different tissue types. An FEM is employed to simulate the coupled tumor growth model. Model parameters are estimated by optimizing an objective function of overlap accuracy using a hybrid optimization parallel search package. The proposed method was tested with kidney CT images of eight tumors from five patients with seven time points. The experimental results showed that the performance of the proposed method improved greatly compared to our previous work. Automated retinal layer segmentation of optical coherence tomography (OCT) images has been successful for normal eyes but becomes challenging for eyes with retinal diseases if the retinal morphology experiences critical changes. We propose a method to automatically segment the retinal layers in 3-D OCT data with serous retinal pigment epithelial detachments (PED), which is a prominent feature of many chorioretinal disease processes. The proposed framework consists of the following steps: fast denoising and B-scan alignment, multi-resolution graph search based surface detection, PED region detection and surface correction above the PED region. The proposed technique was evaluated on a dataset with OCT images from 20 subjects diagnosed with PED. The experimental results showed the following. 1) The overall mean unsigned border positioning error for layer segmentation is $7.87pm 3.36 ~mu{ m m}$ , and is comparable to the mean inter-observer variability ( $7.81pm 2.56 ~mu { m m}$ ). 2) The true positive volume fraction (TPVF), false positive volume fraction (FPVF) and positive predicative value (PPV) for PED volume segmentation are 87.1%, 0.37%, and 81.2%, respectively. 3) The average running time is 220 s for OCT data of 512 $,\\times ,$ 64 $,\\times,$ 480 voxels. A method is proposed for unsupervised 3D (volume) segmentation of registered multichannel medical images. To this end, multichannel image is treated as 4D tensor represented by a multilinear mixture model, i.e. the image is modeled as weighted linear combination of 3D intensity distributions of organs (tissues) present in the image. Interpretation of this model suggests that 3D segmentation of organs (tissues) can be implemented through sparseness constrained factorization of the nonnegative matrix obtained by mode-4 unfolding of the 4D image tensor. Sparseness constraint implies that only one organ (tissue) is dominantly present at each pixel or voxel element. The method is preliminary validated, in term of Dice's coefficient, on extraction of brain tumor from synthetic multispectral magnetic resonance image obtained from the TumorSim database. We introduce a novel computational framework to enable automated identification of texture and shape features of lesions on 18 F-FDG-PET images through a graph-based image segmentation method. The proposed framework predicts future morphological changes of lesions with high accuracy. The presented methodology has several benefits over conventional qualitative and semi-quantitative methods, due to its fully quantitative nature and high accuracy in each step of (i) detection, (ii) segmentation, and (iii) feature extraction. To evaluate our proposed computational framework, thirty patients received 2 18 F-FDG-PET scans (60 scans total), at two different time points. Metastatic papillary renal cell carcinoma, cerebellar hemongioblastoma, non-small cell lung cancer, neurofibroma, lymphomatoid granulomatosis, lung neoplasm, neuroendocrine tumor, soft tissue thoracic mass, nonnecrotizing granulomatous inflammation, renal cell carcinoma with papillary and cystic features, diffuse large B-cell lymphoma, metastatic alveolar soft part sarcoma, and small cell lung cancer were included in this analysis. The radiotracer accumulation in patients\\u2019 scans was automatically detected and segmented by the proposed segmentation algorithm. Delineated regions were used to extract shape and textural features, with the proposed adaptive feature extraction framework, as well as standardized uptake values (SUV) of uptake regions, to conduct a broad quantitative analysis. Evaluation of segmentation results indicates that our proposed segmentation algorithm has a mean dice similarity coefficient of 85.7561.75%. We found that 28 of 68 extracted imaging features were correlated well with SUVmax (p,0.05), and some of the textural features (such as entropy and maximum probability) were superior in predicting morphological changes of radiotracer uptake regions longitudinally, compared to single intensity feature such as SUVmax. We also found that integrating textural features with SUV measurements significantly improves the prediction accuracy of morphological changes (Spearman correlation coefficient=0.8715, p,2e-16). Our aim was to quantify the degree of pulmonary inflammation associated with centrilobular em physema using fluoro-18-2-fluoro-2-deoxy-D-glucose positron emission tomography ( 18 F-FDG-PET) and diagnostic unenhanced computed tomography (CT) based image segmentation and partial volume correction. Forty-nine subjects, with variable amounts of centrilobular emphysema, who had prior diagnostic unenhanced chest CT and either 18 F-FDG-PET or 18 F-FDG-PET/CT were selected. Lung parenchymal volume (L) (in cm 3 ) excluding large and small pulmonary vessels, emphysema volume (E) (in cm 3 ) based on a -910HU threshold, fraction of lung emphysema (F=E/L), and mean attenuation (HU) of non-emphysematous lung parenchyma (A) were calculated from CT images using the image analysis software 3DVIEWNIX. Lung uncorrected maximum SUV (USUVmax) was measured manually from PET images on a dedicated workstation. A first level of partial volume cor rection (PVC) of lung SUVmax to account for presence and degree of macroscopic emphysematous air space was calculated as CSUVmax=USUVmax/(1-F). A second level of PVC of non-emphysematous lung SUVmax to account for the mixture of air and lung parenchyma at the microscopic level was then estimated as CCSUVmax=CSUVmax/(A+1000/1040), assuming that air is -1000HU in attenuation and gasless lung parenchyma is 40HU in attenuation. The correlation of F with USUVmax, CSUVmax, CCSUVmax, % change between CSUVmax and USUVmax (%UC), and % change between CCSUVmax and USUVmax (%UCC) were then tested. The results showed that USUVmax was not significantly correlated with F (r=-.0973, P=0.34). CSUVmax (r=0.4660, P<0.0001) and CCSUVmax were significantly positively correlated with F (r=0.5479, P<0.0001), as were %UC (r=0.9383, P<0.0001) and %UCC (r=0.9369, P<0.0001). In conclusion, the degree of pulmonary inflammation increases with em physema severity based on 18 F-FDG-PET or 18 F-FDG-PET/CT assessment, but is only detectable when 18 F-FDG uptake is corrected for the partial volume effect based on data provided from diagnostic chest CT images. These results support the notion that pulmonary inflammation plays an important role in the pathophysiology of emphysema. This novel image analysis approach has great potential for practical, accurate, and precise combined structural-functional PET quantification of pulmonary inflammation in patients with emphysema or other pulmonary conditions, although further valida tion and refinement will be required. It is important to predict the tumor growth so that appropriate treatment can be planned in the early stage. In this letter, we propose a finite-element method (FEM)-based 3-D tumor growth prediction system using longitudinal kidney tumor images. To the best of our knowledge, this is the first kidney tumor growth prediction system. The kidney tissues are classified into three types: renal cortex, renal medulla, and renal pelvis. The reaction-diffusion model is applied as the tumor growth model. Different diffusion properties are considered in the model: the diffusion for renal medulla is considered as anisotropic, while those of renal cortex and renal pelvis are considered as isotropic. The FEM is employed to solve the diffusion model. The model parameters are estimated by the optimization of an objective function of overlap accuracy using a hybrid optimization parallel search package. The proposed method was tested on two longitudinal studies with seven time points on five tumors. The average true positive volume fraction and false positive volume fraction on all tumors is 91.4% and 4.0%, respectively. The experimental results showed the feasibility and efficacy of the proposed method. We present a novel method for the joint segmentation of anatomical and functional images. Our proposed methodology unifies the domains of anatomical and functional images, represents them in a product lattice, and performs simultaneous delineation of regions based on random walk image segmentation. Furthermore, we also propose a simple yet effective object/background seed localization method to make the proposed segmentation process fully automatic. Our study uses PET, PET-CT, MRI-PET, and fused MRI-PET-CT scans (77 studies in all) from 56 patients who had various lesions in different body regions. We validated the effectiveness of the proposed method on different PET phantoms as well as on clinical images with respect to the ground truth segmentation provided by clinicians. Experimental results indicate that the presented method is superior to threshold and Bayesian methods commonly used in PET image segmentation, is more accurate and robust compared to the other PET-CT segmentation methods recently published in the literature, and also it is general in the sense of simultaneously segmenting multiple scans in real-time with high accuracy needed in routine clinical use. In this paper, we propose a novel method based on a strategic combination of the active appearance model (AAM), live wire (LW), and graph cuts (GCs) for abdominal 3-D organ segmentation. The proposed method consists of three main parts: model building, object recognition, and delineation. In the model building part, we construct the AAM and train the LW cost function and GC parameters. In the recognition part, a novel algorithm is proposed for improving the conventional AAM matching method, which effectively combines the AAM and LW methods, resulting in the oriented AAM (OAAM). A multiobject strategy is utilized to help in object initialization. We employ a pseudo-3-D initialization strategy and segment the organs slice by slice via a multiobject OAAM method. For the object delineation part, a 3-D shape-constrained GC method is proposed. The object shape generated from the initialization step is integrated into the GC cost computation, and an iterative GC-OAAM method is used for object delineation. The proposed method was tested in segmenting the liver, kidneys, and spleen on a clinical CT data set and also on the MICCAI 2007 Grand Challenge liver data set. The results show the following: 1) The overall segmentation accuracy of true positive volume fraction TPVF >; 94.3% and false positive volume fraction FPVF <; 0.2% can be achieved; 2) the initializa- tion performance can be improved by combining the AAM and LW; 3) the multiobject strategy greatly facilitates initialization; 4) compared with the traditional 3-D AAM method, the pseudo-3-D OAAM method achieves comparable performance while running 12 times faster; and 5) the performance of the proposed method is comparable to state-of-the-art liver segmentation algorithm. The executable version of the 3-D shape-constrained GC method with a user interface can be downloaded from http://xinjianchen.word- press.com/research/. Coping with nonlinear distortions in fingerprint matching is a challenging task. This paper proposes a novel algorithm, normalized fuzzy similarity measure (NFSM), to deal with the nonlinear distortions. The proposed algorithm has two main steps. First, the template and input fingerprints were aligned. In this process, the local topological structure matching was introduced to improve the robustness of global alignment. Second, the method NFSM was introduced to compute the similarity between the template and input fingerprints. The proposed algorithm was evaluated on fingerprints databases of FVC2004. Experimental results confirm that NFSM is a reliable and effective algorithm for fingerprint matching with nonliner distortions. The algorithm gives considerably higher matching scores compared to conventional matching algorithms for the deformed fingerprints. An algorithm for the segmentation of fingerprints and a criterion for evaluating the block feature are presented. The segmentation uses three block features: the block clusters degree, the block mean information, and the block variance. An optimal linear classifier has been trained for the classification per block and the criteria of minimal number of misclassified samples are used. Morphology has been applied as post processing to reduce the number of classification errors. The algorithm is tested on FVC2002 database, only 2.45% of the blocks are misclassified, while the postprocessing further reduces this ratio. Experiments have shown that the proposed segmentation method performs very well in rejecting false fingerprint features from the noisy background. Coping with nonlinear distortions in fingerprint matching is a challenging task. This paper proposes a novel method, a fuzzy feature match (FFM) based on a local triangle feature set to match the deformed fingerprints. The fingerprint is represented by the fuzzy feature set: the local triangle feature set. The similarity between the fuzzy feature set is used to characterize the similarity between fingerprints. A fuzzy similarity measure for two triangles is introduced and extended to construct a similarity vector including the triangle-level similarities for all triangles in two fingerprints. Accordingly, a similarity vector pair is defined to illustrate the similarities between two fingerprints. The FFM method maps the similarity vector pair to a normalized value which quantifies the overall image to image similarity. The proposed algorithm has been evaluated with NIST 24 and FVC2004 fingerprint databases. Experimental results confirm that the proposed FFM based on the local triangle feature set is a reliable and effective algorithm for fingerprint matching with nonlinear distortions. Image segmentation methods may be classified into two categories: purely image based and model based. Each of these two classes has its own advantages and disadvantages. In this paper, we propose a novel synergistic combination of the image based graph-cut (GC) method with the model based ASM method to arrive at the GC-ASM method for medical image segmentation. A multi-object GC cost function is proposed which effectively integrates the ASM shape information into the GC framework. The proposed method consists of two phases: model building and segmentation. In the model building phase, the ASM model is built and the parameters of the GC are estimated. The segmentation phase consists of two main steps: initialization (recognition) and delineation. For initialization, an automatic method is proposed which estimates the pose (translation, orientation, and scale) of the model, and obtains a rough segmentation result which also provides the shape information for the GC method. For delineation, an iterative GC-ASM algorithm is proposed which performs finer delineation based on the initialization results. The proposed methods are implemented to operate on 2D images and evaluated on clinical chest CT, abdominal CT, and foot MRI data sets. The results show the following: (a) An overall delineation accuracy of TPVF>96%, FPVF<0.6% can be achieved via GC-ASM for different objects, modalities, and body regions. (b) GC-ASM improves over ASM in its accuracy and precision to search region. (c) GC-ASM requires far fewer landmarks (about 1/3 of ASM) than ASM. (d) GC-ASM achieves full automation in the segmentation step compared to GC which requires seed specification and improves on the accuracy of GC. (e) One disadvantage of GC-ASM is its increased computational expense owing to the iterative nature of the algorithm. PURPOSE. Disruption of external limiting membrane (ELM) integrity on spectral-domain optical coherence tomography (SD-OCT) is associated with lower visual acuity outcomes in patients suffering from diabetic macular edema (DME). However, no automated methods to detect ELM and/or determine its integrity from SD-OCT exist. METHODS. Sixteen subjects diagnosed with clinically significant DME (CSME) were included and underwent macula-centered SD-OCT (512 3 19 3 496 voxels). Sixteen subjects without retinal thickening and normal acuity were also scanned (200 3 200 3 1024 voxels). Automated quantification of ELM disruption was achieved as follows. First, 11 surfaces were automatically segmented using our standard 3-D graph-search approach, and the subvolume between surface 6 and 11 containing the ELM region was flattened based on the segmented retinal pigment epithelium (RPE) layer. A second, edge-based graph-search surface-detection method segmented the ELM region in close proximity \\u2018\\u2018above\\u2019\\u2019 the RPE, and each ELM A-scan was classified as disrupted or nondisrupted based on six texture features in the vicinity of the ELM surface. The vessel silhouettes were considered in the disruption classification process to avoid false detections of ELM disruption. RESULTS. In subjects with CSME, large areas of disrupted ELM were present. In normal subjects, ELM was largely intact. The mean and 95% confidence interval (CI) of the detected disruption area volume for normal and CSME subjects were meannormal \\u00bc 0.00087 mm 3 and CInormal \\u00bc (0.00074, 0.00100), and meanCSME \\u00bc 0.00461 mm 3 and CICSME \\u00bc (0.00347, 0.00576) mm 3 , respectively. Optical coherence tomography (OCT) images are usually degraded by significant speckle noise, which will strongly hamper their quantitative analysis. However, speckle noise reduction in OCT images is particularly challenging becauseof thedifficultyindifferentiating betweennoise and theinformationcomponents of the speckle pattern. To address this problem, the spiking cortical model (SCM)-based nonlocal means method is presented. The proposed method explores self-similarities of OCT images based on rotation-invariant features of image patches extracted by SCM and then restores the speckled images by averaging the similar patches. This method can provide sufficient speckle reduction while preserving image details very well due to its effectiveness in finding reliable similar patches under high speckle noise contamination. When applied to the retinal OCT image, this method provides signal-to-noise ratio improvements of >16 dB with a small 5.4% loss of similarity. \\u00a9 2014 Society Liver segmentation is still a challenging task in medical image processing area due to the complexity of the liver\\u2019s anatomy, low contrast with adjacent organs, and presence of pathologies. This investigation was used to develop and validate an automated method to segment livers in CT images. The proposed framework consists of three steps: 1) preprocessing; 2) initialization; and 3) segmentation. In the first step, a statistical shape model is constructed based on the principal component analysis and the input image is smoothed using curvature anisotropic diffusion filtering. In the second step, the mean shape model is moved using thresholding and Euclidean distance transformation to obtain a coarse position in a test image, and then the initial mesh is locally and iteratively deformed to the coarse boundary, which is constrained to stay close to a subspace of shapes describing the anatomical variability. Finally, in order to accurately detect the liver surface, deformable graph cut was proposed, which effectively integrates the properties and inter-relationship of the input images and initialized surface. The proposed method was evaluated on 50 CT scan images, which are publicly available in two databases Sliver07 and 3Dircadb. The experimental results showed that the proposed method was effective and accurate for detection of the liver surface. Large distortion may be introduced by non-orthogonal finger pressure and 3D-2D mapping during the process of fingerprint capturing. Furthermore, large variations in resolution and geometric distortion may exist among the fingerprint images acquired from different types of sensors. This distortion greatly challenges the traditional minutiae-based fingerprint matching algorithms. In this paper, we propose a novel ant colony optimization algorithm to establish minutiae correspondences in large-distorted fingerprints. First, minutiae similarity is measured by local features, and an assignment graph is constructed by local search. Then, the minutiae correspondences are established by a pseudo-greedy rule and local propagation, and the pheromone matrix is updated by the local and global update rules. Finally, the minutiae correspondences that maximize the matching score are selected as the matching result. To compensate resolution difference of fingerprint images captured from disparate sensors, a common resolution method is adopted. The proposed method is tested on FVC2004 DB1 and a FINGERPASS cross-matching database established by our lab. The experimental results demonstrate that the proposed algorithm can effectively improve the performance of large-distorted fingerprint matching, especially for those fingerprint images acquired from different modes of acquisition. Purpose: This paper studies the feasibility of developing an automatic anatomy recognition (AAR) system in clinical radiology and demonstrates its operation on clinical 2D images.  Methods: The anatomy recognition method described here consists of two main components: (a) multiobject generalization of OASM and (b) object recognition strategies. The OASM algorithm is generalized to multiple objects by including a model for each object and assigning a cost structure specific to each object in the spirit of live wire. The delineation of multiobject boundaries is done in MOASM via a three level dynamic programming algorithm, wherein the first level is at pixel level which aims to find optimal oriented boundary segments between successive landmarks, the second level is at landmark level which aims to find optimal location for the landmarks, and the third level is at the object level which aims to find optimal arrangement of object boundaries over all objects. The object recognition strategy attempts to find that pose vector (consisting of translation, rotation, and scale component) for the multiobject model that yields the smallest total boundary cost for all objects. The delineation and recognition accuracies were evaluated separately utilizing routine clinical chest CT, abdominal CT, and foot MRI data sets. The delineation accuracy was evaluated in terms of true and false positive volume fractions (TPVF and FPVF). The recognition accuracy was assessed (1) in terms of the size of the space of the pose vectors for the model assembly that yielded high delineation accuracy, (2) as a function of the number of objects and objects\\u2019 distribution and size in the model, (3) in terms of the interdependence between delineation and recognition, and (4) in terms of the closeness of the optimum recognition result to the global optimum.  Results: When multiple objects are included in the model, the delineation accuracy in terms of TPVF can be improved to 97%\\u201398% with a low FPVF of 0.1%\\u20130.2%. Typically, a recognition accuracy of \\u226590% yielded a TPVF \\u226595% and FPVF \\u22640.5%. Over the three data sets and over all tested objects, in 97% of the cases, the optimal solutions found by the proposed method constituted the true global optimum.  Conclusions: The experimental results showed the feasibility and efficacy of the proposed automatic anatomy recognition system. Increasing the number of objects in the model can significantly improve both recognition and delineation accuracy. More spread out arrangement of objects in the model can lead to improved recognition and delineation accuracy. Including larger objects in the model also improved recognition and delineation. The proposed method almost always finds globally optimum solutions. Segmentation of anatomical structures from medical images is a challenging problem, which depends on the accurate recognition (localization) of anatomical structures prior to delineation. This study generalizes anatomy segmentation problem via attacking two major challenges: 1) automatically locating anatomical structures without doing search or optimization, and 2) automatically delineating the anatomical structures based on the located model assembly. For 1), we propose intensity weighted ball-scale object extraction concept to build a hierarchical transfer function from image space to object (shape) space such that anatomical structures in 3-D medical images can be recognized without the need to perform search or optimization. For 2), we integrate the graph-cut (GC) segmentation algorithm with prior shape model. This integrated segmentation framework is evaluated on clinical 3-D images consisting of a set of 20 abdominal CT scans. In addition, we use a set of 11 foot MR images to test the generalizability of our method to the different imaging modalities as well as robustness and accuracy of the proposed methodology. Since MR image intensities do not possess a tissue specific numeric meaning, we also explore the effects of intensity nonstandardness on anatomical object recognition. Experimental results indicate that: 1) effective recognition can make the delineation more accurate; 2) incorporating a large number of anatomical structures via a model assembly in the shape model improves the recognition and delineation accuracy dramatically; 3) ball-scale yields useful information about the relationship between the objects and the image; 4) intensity variation among scenes in an ensemble degrades object recognition performance. Traditional minutiae-based matching algorithms are challenged by the probability that minutiae from different regions of different fingers may not be well matched, and hence lead to erroneous matching results. In this paper we introduce a novel feature called minutia handedness to deal with this problem. First, reference points are detected and additional checking conditions are added to ensure that genuine and accurate reference points can be found. Second, minutia handedness is defined for each minutia according to the bending degree of its associated ridges or the position of the reference points. There are three types of minutiae handedness: right-handed, left-handed and non-handed. Finally, the matching rules between different types of minutiae handedness are set up. The proposed method is tested on eight data sets of FVC2002 (2002) and FVC2004 (2004). The experimental results indicate that the performance of a convectional fingerprint recognition algorithm can be improved by incorporating minutia handedness with a small increment of template size. Pigment epithelium detachment (PED) is an important clinical manifestation of multiple chorioretinal diseases, which can cause loss of central vision. In this paper, an automated framework is proposed to segment serous PED in SD-OCT images. The proposed framework consists of four main steps: first, a multi-scale graph search method is applied to segment abnormal retinal layers; second, an effective AdaBoost method is applied to refine the initial segmented regions based on 62 extracted features; third, a shape-constrained graph cut method is applied to segment serous PED, in which the foreground and background seeds are obtained automatically; finally, an adaptive structure elements based morphology method is applied to remove false positive segmented regions. The proposed framework was tested on 25 SD-OCT volumes from 25 patients diagnosed with serous PED. The average true positive volume fraction (TPVF), false positive volume fraction (FPVF), dice similarity coefficient (DSC) and positive predictive value (PPV) are 90.08%, 0.22%, 91.20% and 92.62%, respectively. The proposed framework can provide clinicians with accurate quantitative information, including shape, size and position of the PED region, which can assist clinical diagnosis and treatment. In this paper, a fully automatic method is proposed to segment the lung tumor in clinical 3D PET-CT images. The proposed method effectively combines PET and CT information to make full use of the high contrast of PET images and superior spatial resolution of CT images. Our approach consists of three main parts: (1) initial segmentation, in which spines are removed in CT images and initial connected regions achieved by thresholding based segmentation in PET images; (2) coarse segmentation, in which monotonic downhill function is applied to rule out structures which have similar standardized uptake values (SUV) to the lung tumor but do not satisfy a monotonic property in PET images; (3) fine segmentation, random forests method is applied to accurately segment the lung tumor by extracting effective features from PET and CT images simultaneously. We validated our algorithm on a dataset which consists of 24 3D PET-CT images from different patients with non-small cell lung cancer (NSCLC). The average TPVF, FPVF and accuracy rate (ACC) were 83.65%, 0.05% and 99.93%, respectively. The correlation analysis shows our segmented lung tumor volumes has strong correlation ( average 0.985) with the ground truth 1 and ground truth 2 labeled by a clinical expert. In this paper, a fully automatic method is proposed to segment the kidney into multiple components: renal cortex, renal column, renal medulla and renal pelvis, in clinical 3D CT abdominal images. The proposed fast automatic segmentation method of kidney consists of two main parts: localization of renal cortex and segmentation of kidney components. In the localization of renal cortex phase, a method which fully combines 3D Generalized Hough Transform (GHT) and 3D Active Appearance Models (AAM) is applied to localize the renal cortex. In the segmentation of kidney components phase, a modified Random Forests (RF) method is proposed to segment the kidney into four components based on the result from localization phase. During the implementation, a multithreading technology is applied to speed up the segmentation process. The proposed method was evaluated on a clinical abdomen CT data set, including 37 contrast-enhanced volume data using leave-one-out strategy. The overall true-positive volume fraction and false-positive volume fraction were 93.15%, 0.37% for renal cortex segmentation; 83.09%, 0.97% for renal column segmentation; 81.92%, 0.55% for renal medulla segmentation; and 80.28%, 0.30% for renal pelvis segmentation, respectively. The average computational time of segmenting kidney into four components took 20 seconds. Speckle artifacts can strongly hamper quantitative analysis of optical coherence tomography (OCT), which is necessary to provide assessment of ocular disorders associated with vision loss. Here, we introduce a method for speckle reduction, which leverages from low-rank + sparsity decomposition (LRpSD) of the logarithm of intensity OCT images. In particular, we combine nonconvex regularization-based low-rank approximation of an original OCT image with a sparsity term that incorporates the speckle. State-of-the-art methods for LRpSD require a priori knowledge of a rank and approximate it with nuclear norm, which is not an accurate rank indicator. As opposed to that, the proposed method provides more accurate approximation of a rank through the use of nonconvex regularization that induces sparse approximation of singular values. Furthermore, a rank value is not required to be known a priori. This, in turn, yields an automatic and computationally more efficient method for speckle reduction, which yields the OCT image with improved contrast-to-noise ratio, contrast and edge fidelity. The source code will be available at www.mipav.net/English/research/research.html.\",\n",
            "  \"2318895841\": \"We study the problem of secure key establishment. We critically examine the security models of Bellare and Rogaway (1993) and Canetti and Krawczyk (2001) in the computational complexity approach, as these models are central in the understanding of the provable security paradigm. We show that the partnership definition used in the three-party key distribution (3PKD) protocol of Bellare and Rogaway (1995) is flawed, which invalidates the proof for the 3PKD protocol. We present an improved protocol with a new proof of security. We identify several variants of the key sharing requirement (i.e., two entities who have completed matching sessions, partners, are required to accept the same session key). We then present a brief discussion about the key sharing requirement. We identify several variants of the Bellare and Rogaway (1993) model. We present a comparative study of the relative strengths of security notions between the several variants of the Bellare-Rogaway model and the Canetti-Krawczyk model. In our comparative study, we reveal a drawback in the Bellare, Pointcheval, and Rogaway (2000) model with the protocol of Abdalla and Pointcheval (2005) as a case study.  We prove a revised protocol of Boyd (1996) secure in the Bellare-Rogaway model. We then extend the model in order to allow more realistic adversary capabilities by incorporating the notion of resetting the long-term compromised key of some entity. This allows us to detect a known weakness of the protocol that cannot be captured in the original model. We also present an alternative protocol that is efficient in both messages and rounds. We prove the protocol secure in the extended model. We point out previously unknown flaws in several published protocols and a message authenticator of Bellare, Canetti, and Krawczyk (1998) by refuting claimed proofs of security. We also point out corresponding flaws in their existing proofs. We propose fixes to these protocols and their proofs. In some cases, we present new protocols with full proofs of security. We examine the role of session key construction in key establishment protocols, and demonstrate that a small change to the way that session keys are constructed can have significant benefits. Protocols that were proven secure in a restricted Bellare-Rogaway model can then be proven secure in the full model. We present a brief discussion on ways to construct session keys in key establishment protocols and also prove the protocol of Chen and Kudla (2003) secure in a less restrictive Bellare-Rogaway model. To complement the computational complexity approach, we provide a formal specification and machine analysis of the Bellare-Pointcheval-Rogaway model using an automated model checker, Simple Homomorphism Verification Tool (SHVT). We demonstrate that structural flaws in protocols can be revealed using our framework. We reveal previously unknown flaws in the unpublished preproceedings version of the protocol due to Jakobsson and Pointcheval (2001) and several published protocols with only heuristic security arguments. We conclude this thesis with a listing of some open problems that were encountered in the study. In the past decade, there has been an increasing reliance on electronic means of transferring funds for personal and business purposes. One recent development has been the emergence of plastic cards with the capacity to store value electronically, which can be used for a range of retail transactions. With the advent of comprehensive anti-money laundering laws throughout the developed world, criminals are turning to alternative ways of moving funds across borders to circumvent reporting and detection systems. One identified risk is the misuse of prepaid stored value cards to keep the proceeds of crime and move them across borders without alerting law enforcement and financial intelligence units. This paper describes the nature of these risks and considers whether existing regulatory measures are adequate to address them. Judy Putt General Manager, Research The use of electronic transactions has increased considerably in recent years. In Australia, the volume and value of cheque transactions in paper-based clearing systems fell from an average of 2.7 million per day in 2001 to 2.1 million in 2005, and from an average of $8.3b per day in 2001 to $6.3b in 2005 (APCA 2005). A correspondingly large increase in electronic banking has also been observed. This is hardly surprising, as the financial incentive to do business electronically in today's highly competitive market is significant, with the cost of an online transaction often being a fraction of a non-electronic transaction (De Young 2001). Similarly, online retail spending has increased considerably with total sales in the United States in 2007 exceeding US$1 0Ob (Ames 2007). One of the more popular electronic payment systems is prepaid stored value cards (SVCs), such as gift cards issued by retail stores. The overall market for gift cards is projected to grow to nearly $88 billion in 2008, with the fastest growth occurring in corporate purchases of gift cards for employees and customers, and in \\\"open\\\" gift cards - like the American Express Gift Card that can be redeemed at multiple merchants. . . . Corporate purchases will rise 72% from 2005 to 2008, growing from [US]$9 billion to [US]$1 5.5 billion. Open gift card sales are expected to almost quadruple from 2005 to 2008, growing from [US]$1 .3 billion to [US]$5 billion, according to Mercator (American Express 2006). This extensive use of SVCs, coupled with the convergence of financial services and electronic payment technologies, has created new opportunities for money laundering. This paper examines the nature of the risks and how they can best be addressed. Prepaid stored value cards Stored value cards are cards with data encoded in either a magnetic strip or a computer chip that are preloaded with a fixed amount of electronic currency or value. This can be redeemed or transferred to individuals and/or merchants in a manner that is similar to spending physical currency. The players in a typical SVC program include: * program managers - owners of prepaid SVC programs who establish relationships with payment processing facilities (e.g. banks and payment networks) and distributors, and establish pooled account(s) at banks * payment processing facilities - are responsible for payment transactions for prepaid SVC programs, and they track and distribute funds in pooled accounts. Program managers may also choose to function as their own payment processors * banks - may also function as program managers and/or distributors, and are responsible for maintaining pooled accounts, settling payments and issuing branded prepaid SVCs (open-system cards) * the payments network - the link' between payment processing facilities, and the retailer and automated teller machine (ATM), for authorisation of payment transactions * a distributor (e.g. banks and nonfinancial institutions) - responsible for selling prepaid SVCs. The market for SVCs has increased considerably over the years, particularly in terms of its availability and size. \\u2026 As critical systems are increasingly dependent on software and are connected to the Internet, insider threats will be of ongoing concern. For example, corrupt insiders could deliberately introduce malicious software into the organisation\\u2019s system to surreptitiously gain control, and launch online attacks via and against compromised systems. In this paper, we present a method that an insider can use to facilitate data exfiltration from an air-gapped system without using any modified hardware. The method presented here uses inaudible sounds transmitted from the target machine\\u2019s speakers, which can transfer data to a nearby computer equipped with a microphone. We demonstrate how inaudible communication bridge air-gapped systems without any additional hardware. Our system is low-risk for an insider as it only requires one-off access to a system, and can be erased leaving little-to-no trace once it is no longer required. Finally we provide some recommendations for organisations to avoid similar data exfiltration techniques. As sensor nodes are deployed in an open and hostile environment, they are vulnerable to various attacks. It is of critical importance to be able to revoke compromised nodes in order to ensure the confidentiality of data traversing in the network. In this work, we propose a novel key revocation scheme which is a hybrid of centralized and distributed methods. The design of our scheme is based on Chan et al. (2005) but eliminates the requirement of prior knowledge. It mainly consists of a voting procedure among nodes and a global revocation by the base station. We also modify existing distributed revocation properties in Chan et al. (2005)\\u2019s protocol and extend them to key revocation properties of any hybrid schemes based on the voting process. Event logs provide an audit trail that records user events and activities on a computer and are a potential source of evidence in digital forensic investigations. This paper presents a Windows event forensic process (WinEFP) for analyzing Windows operating system event log files. The WinEFP covers a number of relevant events that are encountered in Windows forensics. As such, it provides practitioners with guidance on the use of Windows event logs in digital forensic investigations. Malicious cyber activities are no longer a matter of if but of when, and in our increasingly interconnected world, threats to our national sovereignty can come from unexpected sources and directions \\u2013 a 360-degree globalised challenge. Cyber security is no longer the preserve of any single country because of the trans-border nature of malicious cyber activities and an increasingly connected and sophisticated technological and user bases. The principle of territoriality, arguably, the \\u2018bedrock\\u2019 of criminal jurisdiction is central to the reason why trans-border malicious cyber activities are difficult to prosecute. At the very least, geographically based concepts of sovereignty must be \\u2018squared\\u2019 with the nature of open networks, possibly necessitating the development of a new law of cyber space to address the appearance of a lacuna in the law. In this paper, we seek to analyse the question \\u201cWhether there should there be a new body of law for cyber space?\\u201d by examining first the potentially broad application of jurisdictional criminal law, particularly the principle of extra-territorial jurisdiction, then if this hurdle is crossed, we then touch on what body of law should apply. We raise the notion that the conception of a new law of cyberspace and a forum for prosecution, is not impossible, but now is imperative. A range of court cases and forensic investigations have involved thumbnail pictures contained within operating system files, such as thumbcache and thumbs.db. In many of these cases, the thumbnail image has been the evidence presented to a court. Further analysis may locate additional information relating to thumbnail pictures, such as being able to link a thumbnail to a picture file on storage media, or locating information relating to the original file used to create the thumbnail, such as the full path and original file name. Using real-world law enforcement and test data, we demonstrate the application of our proposed operational methodology to conduct analysis of thumbcache files. We also propose a reporting and visualisation methodology to present the evidence to investigators, legal counsel, and court, which then forms the basis of our software prototype. Insider threat cases which involve pictures of intellectual property can potentially benefit from our proposed method. STorage as a Service (STaaS) cloud services have been adopted by both individuals and businesses as a dominant technology worldwide. Similar to other technologies, this widely accepted service can be misused by criminals. Investigating cloud platforms is becoming a standard component of contemporary digital investigation cases. Hence, digital forensic investigators need to have a working knowledge of the potential evidence that might be stored on cloud services. In this chapter, we conducted a number of experiments to locate data remnants of users' activities when utilizing the Ubuntu One cloud service. We undertook experiments based on common activities performed by users on cloud platforms including downloading, uploading, viewing, and deleting files. We then examined the resulting digital artifacts on a range of client devices, namely, Windows 8.1, Apple Mac OS X, and Apple iOS. Our examination extracted a variety of potentially evidential items ranging from Ubuntu One databases and log files on persistent storage to remnants of user activities in device memory and network traffic. Adversary model for iOS app DRM protection for video contents.Circumvent or bypass DRM control for iOS apps.Vulnerabilities in popular Video-on-Demand (VOD) apps.Need for hardening of iOS device security to avoid device jailbreaking. Due to the increasing use of mobile devices and apps to view copyright protected content (e.g. movies) on the go, digital rights management (DRM) protections have primarily been used to protect the digital intellectual property and control their distribution and usage on mobile devices. Unsurprisingly, attackers have sought to circumvent or bypass DRM control in order to obtain unauthorised access to copyrighted content. Given the ongoing and rapidly changing nature of mobile device technologies, it is essential for DRM protection designer to have an in-depth understanding of an attacker's capabilities and the potential attack vectors (e.g. vulnerabilities that can be exploited to bypass DRM protection). In this paper, we propose an adversary model that formalizes the real world capabilities of a DRM attacker targeting Apple iOS devices. We then demonstrate its utility using four Video-on-Demand (VoD) apps, one live TV app, and a security DRM protection module. To avoid similar structural mistakes in future designs, we outline two recommendations. As our dependence on cloud services increase, so does the risks associated with attacks on these services. To improve our defense against such attacks, we need to better understand their characteristics. A taxonomy is useful in this regard because it classifies and categorizes the different aspects of cloud attacks and appropriate countermeasures. Using on this classification, we can then assess and manage the risks. Based on a literature review of existing attack taxonomies published between January 2003 and April 2014 and attacks against cloud services, this chapter proposes a conceptual taxonomy of cloud attacks and risk mitigation strategies. To demonstrate how this taxonomy can be used in risk assessment, an example attack scenario is presented. The adoption of enterprise social network (ESN) for greater employee engagement and knowledge sharing practices within organisations is proliferating. However, ESN investments have thus far not resulted in expected gains in organisational benefits due to underutilisation by employees. Limited understanding of the implications of ESN use leads to a paucity of recommendations for effective use within an organisation. This research-in-progress paper seeks to determine the factors influencing the use of ESN among employees in a large Australian utility organisation, with the aim of contributing to a practical understanding of the key success factors of the use of this new workplace social platform. Our preliminary findings indicated that the employees\\u2019 ESN behaviour tends to be influenced by socio-technical factors, including technological (i.e. platform and content quality), organisational (i.e. top management support and ESN facilitating conditions), social (i.e. critical mass and communication climate), individual (i.e. perceived benefits, knowledge self-efficacy and time commitment) and task (i.e. task characteristics) factors. This paper concludes that a successful implementation of ESN in an organisation involves the nexus between these five factors and provides several recommendations about how ESN use can be enhanced. Mobile health applications (or mHealth apps, as they are commonly known) are increasingly popular with both individual end users and user groups such as physicians. Due to their ability to access, store and transmit personally identifiable and sensitive information (e.g. geolocation information and personal details), they are potentially an important source of evidentiary materials in digital investigations. In this paper, we examine 40 popular Android mHealth apps. Based on our findings, we propose a taxonomy incorporating artefacts of forensic interest to facilitate the timely collection and analysis of evidentiary materials from mobile devices involving the use of such apps. Artefacts of forensic interest recovered include user details and email addresses, chronology of user locations and food habits. We are also able to recover user credentials (e.g. user password and four-digit app login PIN number), locate user profile pictures and identify timestamp associated with the location of a user. We revisit Shin et al.'s leakage-resilient password-based authenticated key establishment protocol (LR-AKEP) and the security model used to prove the security of LR-AKEP. By refining the Leak oracle in the security model, we show that LR-AKE (1) can, in fact, achieve a stronger notion of leakage-resilience than initially claimed and (2) also achieve an additional feature of traceability, not previously mentioned. Dating apps for mobile devices, one popular GeoSocial app category, are growing increasingly popular. These apps encourage the sharing of more personal information than conventional social media apps, including continuous location data. However, recent high profile incidents have highlighted the privacy risks inherent in using these apps. In this paper, we present a case study utilizing forensic techniques on nine popular proximity-based dating apps in order to determine the types of data that can be recovered from user devices. We recover a number of data types from these apps that raise concerns about user privacy. For example, we determine that chat messages could be recovered in at least half of the apps examined and, in some cases, the details of any users that had been discovered nearby could also be extracted. Android devices continue to grow in popularity and capability meaning the need for a forensically sound evidence collection methodology for these devices also increases. This chapter proposes a methodology for evidence collection and analysis for Android devices that is, as far as practical, device agnostic. Android devices may contain a significant amount of evidential data that could be essential to a forensic practitioner in their investigations. However, the retrieval of this data requires that the practitioner understand and utilize techniques to analyze information collected from the device. The major contribution of this research is an in-depth evidence collection and analysis methodology for forensic practitioners. The modern business world has overseen a massive expansion in global technological capacity. This expansion has allowed us to take great strides in electronic commerce and international communication. There are downsides, however. The new technologies have opened up a vast array of avenues for criminal activity. The new technologies also carry with them intrusive capabilities, and these, too, will require policies and laws that hold accountable those who abuse them. Legislators and policymakers the world over must remain abreast of current developments, being constantly mindful of the difficulties that will challenge any society that keenly embraces new technological capacity without putting in place appropriate regulatory mechanisms and legal regimes. The following overview reviews these themes in the context of cloud technology. Cyber security is an area of strategic and policy interest to governments and enterprises globally, which results in an increase in the demand for cyber security professionals. However, there is a lack of education based on sound theories, standards and practices. In this paper, we adapted the Situational Crime Prevention Theory and the NICE National Cybersecurity Workforce Framework in the design and delivery of our courses, particularly in the Cyber Security Exercise (CSE) which forms an integral part of the courses. The CSE is an attack/defence environment where students are grouped and given a virtual machine with which to host a number of services (e.g. HTTP(S), FTP and SSH) for access by other groups. The CSE is designed to mirror real-world environments where the students\\u2019 skills will be applied. An overview of the CSE architecture was also provided for readers interested in replicating the exercise in their institutions. Based on student assessment and feedback, we found that our approach was useful in transferring theoretical knowledge to practical skills suitable for the cyber security workforce. Mobile devices are fast becoming critical information management tools often storing a range of personal and corporate confidential data often synced from online and cloud based storage services. Mobile device operating system designers are increasing the security available to users, not only from traditional security risk vectors, but also to protect their privacy from the various apps (with potential malicious intent) installed on their device. In this paper, we developed a process for enforcing file system permissions on Android external storage (with minimal modifications to the operating system). Our process makes use of the application sandboxing supported on this platform to restrict parts of the external file system to a particular app or multiple apps holding a particular permission. We present an Android File system Permissions (AFP) prototype demonstrating the applicability of this work and demonstrate its utility using the ownCloud app for Android. We then highlight a number of limitations with the current permission enforcement capabilities for external storage on the platform. Information and communications technologies (ICT) are fundamental to modern society and open the door to increased productivity, faster communication capabilities, and immeasurable convenience. The increased dependence on ICT and the pervasive interconnectivity of our ICT infrastructure also change the way criminals conduct their activities. There is a need for policing to adapt enforcement and investigation strategies in order to account for these changes. This chapter looks at challenges faced by law enforcement agencies in examining digital evidence for modern communication technologies. Three potential research questions are identified in the areas of physical memory forensics, forensic investigation in the cloud environment, and alternatively non prosecution-based approaches to low-end/less serious high volume data cyber criminal cases. A three-party password-based authenticated key exchange (PAKE) protocol allows two clients registered with a trusted server to generate a common cryptographic key from their individual passwords shared only with the server. A key requirement for three-party PAKE protocols is to prevent an adversary from mounting a dictionary attack. This requirement must be met even when the adversary is a malicious (registered) client who can set up normal protocol sessions with other clients. This work revisits three existing three-party PAKE protocols, namely, Guo et al.\\u2019s (2008) protocol, Huang\\u2019s (2009) protocol, and Lee and Hwang\\u2019s (2010) protocol, and demonstrates that these protocols are not secure against offline and/or (undetectable) online dictionary attacks in the presence of a malicious client. The offline dictionary attack we present against Guo et al.\\u2019s protocol also applies to other similar protocols including Lee and Hwang\\u2019s protocol. We conclude with some suggestions on how to design a three-party PAKE protocol that is resistant against dictionary attacks. The ubiquity of the smartphone means that there is a high probability that a suspect under investigation uses one. Because of society's growing dependence on smartphones, such devices will likely contain a wealth of incriminating data such as emails, text messages, phone logs, and sensitive documents. Furthermore, with the prevalence of cloud-based storage, it is also possible that the evidential data that forensic investigators seek would not be located directly on the device. Thus, there is a need for a forensically sound and specialized methodology to access this remote data, which might be critical in a forensic investigation. This column describes one such methodology. A 2015 Gartner report noted that data processing technologies haven't kept pace with the significant increase in the volume of digital healthcare data, and an integrated and trustworthy healthcare analytics solution can facilitate more effective decision making in patient care and risk management, improving quality of life, optimizing performance of services, and so on. The challenge is how to ensure data confidentiality and integrity when storing such data but still make it highly available, process it to extract actionable information for decision makers, including medical professionals, and share it with collaborators, while preserving the privacy of individual patients and giving them the full control of their data at all times. This challenge calls for a trustworthy big data processing platform. We revisit the password-based group key exchange protocol due to Lee et al. (2004), which carries a claimed proof of security in the Bresson et al. model under the intractability of the Decisional Diffie-Hellman problem (DDH) and Computational Diffie-Hellman (CDH) problem. We reveal a previously unpublished flaw in the protocol and its proof, whereby we demonstrate that the protocol violates the definition of security in the model. To provide a better insight into the protocol and proof failures, we present a fixed protocol. We hope our analysis will enable similar mistakes to be avoided in the future. We also revisit protocol 4 of Song and Kim (2000), and reveal a previously unpublished flaw in the protocol (i.e., a reflection attack).  The views and opinions expressed in this paper do not necessarily reflect those of the Commonwealth Government, the Minister for Justice and Customs, or the Australian Institute of Criminology. Research was performed while the author was with the Information Security Institute / Queensland University of Technology. Using the evidence collection and analysis methodology for Android devices proposed by Martini, Do and Choo, we examined and analyzed seven popular Android cloud-based apps. Firstly, we analyzed each app in order to see what information could be obtained from their private app storage and SD card directories. We collated the information and used it to aid our investigation of each app database files and AccountManager data. To complete our understanding of the forensic artefacts stored by apps we analyzed, we performed further analysis on the apps to determine if the user authentication credentials could be collected for each app based on the information gained in the initial analysis stages. The contributions of this research include a detailed description of artefacts, which are of general forensic interest, for each app analyzed. We observe that the definitions of security in the computational complexity proof models of Bellare & Rogaway (1993) and Canetti & Krawczyk (2001) require two partners in the presence of a malicious adversary to accept the same session key, which we term a key sharing requirement. We then revisit the Bellare\\u2013Rogaway three-party key distribution (3PKD) protocol and the Jeong\\u2013Katz\\u2013Lee two-party authenticated key exchange protocol $mathcal{TS}2$, which carry claimed proofs of security in the Canetti & Krawczyk (2001) model and the Bellare & Rogaway (1993) model respectively. We reveal previously unpublished flaws in these protocols where we demonstrate that both protocols fail to satisfy the definition of security in the respective models. We present a new 3PKD protocol as an improvement with a proof of security in the Canetti & Krawczyk (2001) model and a simple fix to the specification of protocol $mathcal{TS}2$. We also identify several variants of the key sharing requirement and present a brief discussion. Malicious cyber activities are no longer a matter of if but of when, and in our increasingly interconnected world, threats to our national sovereignty can come from unexpected sources and directions\\u2014a 360\\u00b0globalised challenge. Cyber threats are increasingly important and strategically relevant in both developed and developing countries. Cyber security is one of the highest priority items on the global policy and national security agendas, and an increasingly challenging policy area for governments. Our thesis is that cyber security is no longer the preserve of any single country, entity, (industry) sector or disciplinary field because of the nature and extent of an increasingly connected and sophisticated technological and user bases. There is, therefore, a need to bring together perspectives and approaches from different disciplines and countries, and investigate what we can do singularly and collaboratively to secure our cyberspace and future. This essay proposes a conceptual framework that allows theories from different disciplines and different strategies, techniques and best practices to be \\u201cplugged-and-played\\u201d when studying/understanding and responding to malicious cyber activities. Three potential research topics are also identified to seek to provide more evidence to support the proposed framework. Cybercrime is essentially a transnational crime with a \\u2018modus operandi\\u2019 that exploits inter-state differences in the capacity to respond to such crime, and appears in the same company as such powerful global concerns as civil war, genocide and poverty (United Nations 2004). This transnational character provides cybercriminals, especially organized crime (OC), with the agility to avoid counter-measures even when implemented by capable actors (Brenner 2002, 2006; Council of Europe 2004). Although some have questioned the existence of organized criminal activities in cyberspace, several studies have noted the potential and actual synergy between organized crime and cyberspace in recent years. This paper outlines some of the issues and problems in respect to the role of OC in cybercrime. A modified version of the Bellare and Rogaway (1993) adversarial model is encoded using Asynchronous Product Automata (APA). A model checker tool, Simple Homomorphism Verification Tool (SHVT), is then used to perform state-space analysis on the Automata in the setting of planning problem. The three-party identity-based secret public key protocol (3P-ID-SPK) protocol of Lim and Paterson (2006), which claims to provide explicit key authentication, is used as a case study. We then refute its heuristic security argument by revealing a previously unpublished flaw in the protocol using SHVT. We then show how our approach can automatically repair the protocol. This is, to the best of our knowledge, the first work that integrates an adversarial model from the computational complexity paradigm with an automated tool from the computer security paradigm to analyse protocols in an artificial intelligence problem setting - planning problem - and, more importantly, to repair protocols.  The views and opinions expressed in this paper are those of the author and do not reflect those of the Australian Government and Australian Institute of Criminology. Research was performed while the author was with the Intelligent Systems Laboratory / University of Western Sydney. With the increasing popularity of cloud services and their potential to either be the target or the tool in a cybercrime activity, organizational cloud services users need to ensure that they are able to collect evidential data should they be involved in litigation or a criminal investigation. In this paper, we seek to contribute to a better understanding of the technical issues and processes regarding collection of evidential data in the cloud computing environment. Using VMware vCloud as a case study in this paper, we describe the various artefacts available in the cloud environment and identify several forensic preservation considerations for forensics practitioners. We then propose a six-step process for the remote programmatic collection of evidential data to ensure as few changes as possible are made as part of evidence collection and that no potential evidence is missed. The six-step process is implemented in a proof of concept application to demonstrate utility of the process. ABSTRACTOrganizations are increasingly implementing enterprise social networks (ESNs) for improved communication and collaboration, as well as enhanced knowledge sharing and innovation among employees. However, the paradoxical relationship between ESN implementation and the promised benefits has been attributed to employees\\u2019 underutilization. Our research focuses on factors influencing employees\\u2019 decision to use ESN in their work role and draws on case studies of two multinational professional service firms (PSFs) based in Australia. Qualitative data were collected during ten semi-structured interviews with employees from both organizations, to determine their perceptions of ESN usage and capture the factors that influence their use behavior. The findings illustrate that the likelihood of ESN use is significantly influenced by technological, organizational, social and individual factors. A successful ESN use within an organization involves the nexus between these four factors and recommendations are made,... A smart-card-based user authentication scheme for wireless sensor networks (hereafter referred to as a SCA-WSN scheme) is designed to ensure that only users who possess both a smart card and the corresponding password are allowed to gain access to sensor data and their transmissions. Despite many research efforts in recent years, it remains a challenging task to design an efficient SCA-WSN scheme that achieves user anonymity. The majority of published SCA-WSN schemes use only lightweight cryptographic techniques (rather than public-key cryptographic techniques) for the sake of efficiency, and have been demonstrated to suffer from the inability to provide user anonymity. Some schemes employ elliptic curve cryptography for better security but require sensors with strict resource constraints to perform computationally expensive scalar-point multiplications; despite the increased computational requirements, these schemes do not provide user anonymity. In this paper, we present a new SCA-WSN scheme that not only achieves user anonymity but also is efficient in terms of the computation loads for sensors. Our scheme employs elliptic curve cryptography but restricts its use only to anonymous user-to-gateway authentication, thereby allowing sensors to perform only lightweight cryptographic operations. Our scheme also enjoys provable security in a formal model extended from the widely accepted Bellare-Pointcheval-Rogaway (2000) model to capture the user anonymity property and various SCA-WSN specific attacks (e.g., stolen smart card attacks, node capture attacks, privileged insider attacks, and stolen verifier attacks). Cloud storage is an emerging challenge to digital forensic examiners. The services are increasingly used by consumers, business, and government, and can potentially store large amounts of data. The retrieval of digital evidence from cloud storage services (particularly from offshore providers) can be a challenge in a digital forensic investigation, due to virtualisation, lack of knowledge on location of digital evidence, privacy issues, and legal or jurisdictional boundaries. Google Drive is a popular service, providing users a cost-effective, and in some cases free, ability to access, store, collaborate, and disseminate data. Using Google Drive as a case study, artefacts were identified that are likely to remain after the use of cloud storage, in the context of the experiments, on a computer hard drive and Apple iPhone3G, and the potential access point(s) for digital forensics examiners to secure evidence. Digital evidence can be stored in cloud storage services, such as Google Drive.Identification of potential data storage is a challenge to forensic examiners.Google Drive was examined in relation to data remnants on a PC and an iPhone.Investigation points include directory listings, prefetch, link and registry files. Distributed filesystems provide a cost-effective means of storing high-volume, velocity and variety information in cloud computing, big data and other contemporary systems. These technologies have the potential to be exploited for illegal purposes, which highlights the need for digital forensic investigations. However, there have been few papers published in the area of distributed filesystem forensics. In this paper, we aim to address this gap in knowledge. Using our previously published cloud forensic framework as the underlying basis, we conduct an in-depth forensic experiment on XtreemFS, a Contrail EU-funded project, as a case study for distributed filesystem forensics. We discuss the technical and process issues regarding collection of evidential data from distributed filesystems, particularly when used in cloud computing environments. A number of digital forensic artefacts are also discussed. We then propose a process for the collection of evidential data from distributed filesystems. Purpose \\u2013 The purpose of this paper is to examine the feasibility and desirability of using the Microsoft Passport service for client authentication and authorization. It aims to present the business benefits of using Microsoft Passport, an overview (logical) description of the work involved in adopting Microsoft Passport into a business site, and a functional description of how Microsoft Passport works in conceptual terms.Design/methodology/approach \\u2013 An analysis of the level of business adoption of passport is then presented.Findings \\u2013 The paper concludes that business adoption of Microsoft Passport appears to be losing ground due to lack of trust, control, and privacy; and the proliferation of other identity management paradigms. This issue report is prepared from the perspective of a business analyst and is pitched in terminology suitable for a business audience.Practical implications \\u2013 This work provides organisations investigating the feasibility/desirability of using the Microsoft Passport, a funda... We present the first provably-secure three-party password-only authenticated key exchange (PAKE) protocol that can run in only two communication rounds. Our protocol is generic in the sense that it can be constructed from any two-party PAKE protocol. The protocol is proven secure in a variant of the widely-accepted model of Bellare, Pointcheval and Rogaway (2000) without any idealized assumptions on the cryptographic primitives used. We also investigate the security of the two-round, three-party PAKE protocol of Wang, Hu and Li (2010) and demonstrate that this protocol cannot achieve implicit key authentication in the presence of an active adversary. Broadcast authentication, a fundamental feature in wireless sensor networks (WSNs), enables users to broadcast to multiple sensor nodes in an authenticated way. Symmetric-key-based muTESLA-like schemes are a popular solution due to their energy efficiency, but most schemes are not scalable (e.g. in terms of the number of senders). On the other hand, several schemes based on public key cryptography (PKC) are proposed to secure the multi-user broadcast in WSNs. However, the computation and communication overheads in the underlying PKC infrastructure restrict its implementation in resource-constrained WSNs. This paper presents an efficient scheme, which supports multi-sender broadcast authentication and allows dynamic addition of new senders. We delay disclosing the keys of the master key chain generated by the base station to authenticate the commitment of each sender's key chain. Considering that senders may be captured and compromised by adversaries in a hostile environment, our scheme also provides a mechanism to revoke the compromised senders' broadcast authentication capability. Protocols for password-only authenticated key exchange (PAKE) in the three-party setting allow two clients registered with the same authentication server to derive a common secret key from their individual password shared with the server. Existing three-party PAKE protocols were proven secure under the assumption of the existence of random oracles or in a model that does not consider insider attacks. Therefore, these protocols may turn out to be insecure when the random oracle is instantiated with a particular hash function or an insider attack is mounted against the partner client. The contribution of this paper is to present the first three-party PAKE protocol whose security is proven without any idealized assumptions in a model that captures insider attacks. The proof model we use is a variant of the indistinguishability-based model of Bellare, Pointcheval, and Rogaway (2000), which is one of the most widely accepted models for security analysis of password-based key exchange protocols. We demonstrated that our protocol achieves not only the typical indistinguishability-based security of session keys but also the password security against undetectable online dictionary attacks. Cloud storage services such as the popular Microsoft(C) SkyDrive(C) provide both organisational and individual users a cost-effective, and in some cases free, way of accessing, storing and disseminating data. The identification of digital evidence relating to cloud storage services can, however, be a challenge in a digital forensic investigation. Using SkyDrive as a case study, we identified the types of terrestrial artefacts that are likely to remain on a client's machine (in the context of our experiments; computer hard drive and iPhone), and where the access point(s) for digital forensics examiners are, that will allow them to undertake steps to secure evidence in a timely fashion. Modern mobile devices have security capabilities built into the native operating system, which are generally designed to ensure the security of personal or corporate data stored on the device, both at rest and in transit. In recent times, there has been interest from researchers and governments in securing as well as exfiltrating data stored on such devices (e.g. the high profile PRISM program involving the US Government). In this paper, we propose an adversary model for Android covert data exfiltration, and demonstrate how it can be used to construct a mobile data exfiltration technique (MDET) to covertly exfiltrate data from Android devices. Two proof-of-concepts were implemented to demonstrate the feasibility of exfiltrating data via SMS and inaudible audio transmission using standard mobile devices. Display Omitted Adversary model for Android covert data exfiltration.Mobile data exfiltration technique (MDET).Inaudible data exfiltration. Although the Yahalom protocol, proposed by Burrows, Abadi, and Needham in 1990, is one of the most prominent key establishment protocols analysed by researchers from the computer security community (using automated proof tools), a simplified version of the protocol is only recently proven secure by Backes and Pfitzmann [(2006) On the Cryptographic Key Secrecy of the Strengthened Yahalom Protocol. Proc. IFIP SEC 2006] in their cryptographic library framework. We present a protocol for key establishment that is closely based on the Yahalom protocol. We then present a security proof in the Bellare, M. and Rogaway, P. [(1993a). Entity Authentication and Key Distribution. Proc. of CRYPTO 1993, Santa Barbara, CA, August 22\\u201326, LNCS, Vol. 773, pp. 110\\u2013125. Springer-Verlag, Berlin] model and the random oracle model. We also observe that no partnering mechanism is specified within the Yahalom protocol. We then present a brief discussion on the role and the possible construct of session identifiers (SIDs) as a form of partnering mechanism, which allows the right session key to be identified in concurrent protocol executions. We then recommend that SIDs should be included within protocol specification rather than consider SIDs as artefacts in protocol proof. The majority of existing escrowable identity-based key agreement protocols only provide partial forward secrecy. Such protocols are, arguably, not suitable for many real-word applications, as the latter tends to require a stronger sense of forward secrecy\\u2014perfect forward secrecy. In this paper, we propose an efficient perfect forward-secure identity-based key agreement protocol in the escrow mode. We prove the security of our protocol in the random oracle model, assuming the intractability of the Gap Bilinear Diffie-Hellman (GBDH) problem. A major challenge to digital forensic analysis is the ongoing growth in the volume of data seized and presented for analysis. This is a result of the continuing development of storage technology, including increased storage capacity in consumer devices and cloud storage services, and an increase in the number of devices seized per case. Consequently, this has led to increasing backlogs of evidence awaiting analysis, often many months to years, affecting even the largest digital forensic laboratories. Over the preceding years, there has been a variety of research undertaken in relation to the volume challenge. Solutions posed range from data mining, data reduction, increased processing power, distributed processing, artificial intelligence, and other innovative methods. This paper surveys the published research and the proposed solutions. It is concluded that there remains a need for further research with a focus on real world applicability of a method or methods to address the digital forensic data volume challenge. Incident handling strategy is one key strategy to mitigate risks to the confidentiality, integrity and availability (CIA) of organisation assets, as well as minimising loss (e.g. financial, reputational and legal) particularly as organisations move to the cloud. In this paper, we surveyed existing incident handling and digital forensic literature with the aims of contributing to the knowledge gap(s) in handling incidents in the cloud environment. 139 English language publications between January 2009 and May 2014 were located by searching various sources including the websites of standard bodies (e.g. National Institute of Standards and Technology) and academic databases (e.g. Google Scholar, IEEEXplore, ACM Digital Library, Springer and ScienceDirect). We then propose a conceptual cloud incident handling model that brings together incident handling, digital forensic and the Capability Maturity Model for Services to more effectively handle incidents for organisations using the cloud. A discussion of open research issues concludes this survey. Display Omitted Survey of incident handling strategy and standards.Cloud security incident handling strategy.The role of digital forensics in incident handling.A conceptual cloud incident handling model.Research trends and future research directions. We revisit the identity-based (ID-based) key agreement protocol due to Ryu et al. The protocol is highly efficient and suitable for real-world applications despite offering no resilience against key-compromise impersonation (K-CI). We show that the protocol is also insecure against reflection attacks. We propose a slight modification to the protocol and prove its security in a widely accepted model. The storage as a service (StaaS) cloud computing architecture is showing significant growth as users adopt the capability to store data in the cloud environment across a range of devices. Cloud (storage) forensics has recently emerged as a salient area of inquiry. Using a widely used open source cloud StaaS application - ownCloud - as a case study, we document a series of digital forensic experiments with the aim of providing forensic researchers and practitioners with an in-depth understanding of the artefacts required to undertake cloud storage forensics. Our experiments focus upon client and server artefacts, which are categories of potential evidential data specified before commencement of the experiments. A number of digital forensic artefacts are found as part of these experiments and are used to support the selection of artefact categories and provide a technical summary to practitioners of artefact types. Finally we provide some general guidelines for future forensic analysis on open source StaaS products and recommendations for future work. The timely acquisition and preservation of data from cloud storage can be an issue for law enforcement agencies and other digital forensic practitioners. In a jurisdiction which has legal provisions to collect data available to a computer or device, the process may involve accessing an account to collect the data. Using three popular public cloud storage providers (Dropbox, Google Drive, and Microsoft SkyDrive) as case studies, this research explores the process of collecting data from a cloud storage account using a browser and also downloading files using client software. We then compare these with the original files and undertake analysis of the resulting data. We determined that there were no changes to the contents of files during the process of upload, storage, and download to the three cloud storage services. The timestamps of the files were also examined in relation to the files downloaded via a browser and via client software. It was observed that some of the timestamp information remained the same throughout the process of uploading, storing and downloading files. Timestamp information may be a crucial aspect of an investigation, prosecution, or civil action, and therefore it is important to record the information available, and to understand the circumstances relating to a timestamp on a file. Three categories of organised groups that exploit advances in information and communications technologies (ICT) to infringe legal and regulatory controls: (1) traditional organised criminal groups which make use of ICT to enhance their terrestrial criminal activities; (2) organised cybercriminal groups which operate exclusively online; and (3) organised groups of ideologically and politically motivated individuals who make use of ICT to facilitate their criminal conduct are described in this article. The need for law enforcement to have in-depth knowledge of computer forensic principles, guidelines, procedures, tools, and techniques, as well as anti-forensic tools and techniques will become more pronounced with the increased likelihood of digital content being a source of disputes or forming part of underlying evidence to support or refute a dispute in judicial proceedings. There is also a need for new strategies of response and further research on analysing organised criminal activities in cyberspace. Cyber threats are becoming more sophisticated with the blending of once distinct types of attack into more damaging forms. Increased variety and volume of attacks is inevitable given the desire of financially and criminally-motivated actors to obtain personal and confidential information, as highlighted in this paper. We describe how the Routine Activity Theory can be applied to mitigate these risks by reducing the opportunities for cyber crime to occur, making cyber crime more difficult to commit and by increasing the risks of detection and punishment associated with committing cyber crime. Potential research questions are also identified. Purpose \\u2013 The purpose of this paper is to consider the risks posed by politically exposed persons (PEPs) and explain the money laundering risk when entering into financial transactions and business relationships with PEPs. Risk mitigation by regulated entities and corruption prevention strategies are also outlined. To minimise money\\u2010laundering risks associated with PEPs, legislation will need to adapt to deal with threats that organized criminals and terrorists seek to exploit. Future directions for research in relation to PEPs are also identified.Design/methodology/approach \\u2013 An analysis of how regulated entities can reduce their risk of money laundering when entering into financial transactions and business relationships with PEPs is presented.Findings \\u2013 It was found that there is a need to harmonise legally enforceable obligations targeting PEPs. PEP monitoring, arguably, should be extended to individuals holding prominent public functions in their own jurisdictions, individuals exercising functions no... While a number of protocols for password-only authenticated key exchange (PAKE) in the 3-party setting have been proposed, it still remains a challenging task to prove the security of a 3-party PAKE protocol against insider dictionary attacks. To the best of our knowledge, there is no 3-party PAKE protocol that carries a formal proof, or even definition, of security against insider dictionary attacks. In this paper, we present the first 3-party PAKE protocol proven secure against both online and offline dictionary attacks as well as insider and outsider dictionary attacks. Our construct can be viewed as a protocol compiler that transforms any 2-party PAKE protocol into a 3-party PAKE protocol with 2 additional rounds of communication. We also present a simple and intuitive approach of formally modelling dictionary attacks in the password-only 3-party setting, which significantly reduces the complexity of proving the security of 3-party PAKE protocols against dictionary attacks. In addition, we investigate the security of the well-known 3-party PAKE protocol, called GPAKE, due to Abdalla et al. (2005, 2006), and demonstrate that the security of GPAKE against online dictionary attacks depends heavily on the composition of its two building blocks, namely a 2-party PAKE protocol and a 3-party key distribution protocol. Authenticated key exchange protocols are of fundamental importance in securing communications and are now extensively deployed for use in various real-world network applications. In this work, we reveal major previously unpublished security vulnerabilities in the password-based authenticated three-party key exchange protocol according to Lee and Hwang (2010): (1) the Lee-Hwang protocol is susceptible to a man-in-the-middle attack and thus fails to achieve implicit key authentication; (2) the protocol cannot protect clients\\u2019 passwords against an offline dictionary attack; and (3) the indistinguishability-based security of the protocol can be easily broken even in the presence of a passive adversary. We also propose an improved password-based authenticated three-party key exchange protocol that addresses the security vulnerabilities identified in the Lee-Hwang protocol. National information infrastructure (NII), vital to the nation's security and economic stability, comprises both physical and electronic infrastructures. Information and communications technologies (ICT) form the backbone of many aspects of the NII and reliance on ICT has created many new risks. Cyberthreats are becoming more sophisticated with the blending of once distinct types of attack into more damaging forms. This paper examines the technology-related risks associated with the NII and provides examples of existing incidents and areas in which new threats might emerge. To be able to mitigate these risks, it remains crucial to understand infrastructure interdependencies and to establish public-private partnerships to ensure that weaknesses in systems are not able to be exploited. Cloud storage services are popular with both individuals and businesses as they offer cost-effective, large capacity storage and multi-functional services on a wide range of devices such as personal computers (PCs), Mac computers, and smart mobile devices (e.g. iPhones). However, cloud services have also been known to be exploited by criminals, and digital forensics in the cloud remains a challenge, partly due to the diverse range of cloud services and devices that can be used to access such services. Using SugarSync (a popular cloud storage service) as a case study, research was undertaken to determine the types and nature of volatile and non-volatile data that can be recovered from Windows 8, Mac OS X 10.9, Android 4 and iOS 7 devices when a user has carried out different activities such as upload and download of files and folders. We then document the various digital artefacts that could be recovered from the respective devices. As cloud computing becomes more prevalent, there is a growing need for forensic investigations involving cloud technologies. The field of cloud forensics seeks to address the challenges to digital forensics presented by cloud technologies. This article reviews current research in the field of cloud forensics, with a focus on \\\"forensics in the cloud\\\"--that is, cloud computing as an evidence source for forensic investigations. Purpose - Opportunities for malicious cyber activities have expanded with the globalisation and advancements in information and communication technology (ICT). Such activities will increasingly affect the security of businesses with online presence and/or connected to the Internet. Although the real estate sector is a potential attack vector for and target of malicious cyber activities, it is an understudied industry. This paper aims to contribute to a better understanding of the information security threats, awareness, and risk management standards currently employed by the real estate sector in South Australia.Design/methodology/approach - Current study comprises both quantitative and qualitative methodologies, which include 20 survey questionnaires and 20 face-to-face interviews conducted in South Australia. Findings - There is a lack of understanding about the true magnitude of malicious cyber activities and its impact on the real estate sector, as illustrated in our findings of 40 real estate organisations in South Australia. Our findings and the escalating complexities of the online environment underscore the need for regular ongoing training programs for basic online security (including new cybercrime trends) and the promotion of a culture of information security (e.g. when using smart mobile devices to store and access sensitive data) among staff. Such initiatives will enable staff employed in the (South Australian) real estate sector to maintain current knowledge of the latest. Originality/value - This is the first academic study focusing on the real estate organisations in South Australia. Our findings will contribute to the evidence on the information security threats faced by the sector as well as in develop sector-specific information security risk management guidelines. Despite the importance of proofs in assuring protocol implementers about the security properties of key establishment protocols, many protocol designers fail to provide any proof of security. Flaws detected long after the publication and/or implementation of protocols will erode the credibility of key establishment protocols. We revisit recent work of Choo, Boyd, Hitchcock, Maitland where they utilize the Bellare, Pointcheval, Rogaway (Authenticated key exchange secure against dictionary attacks, in: B. Preneel (Ed.), Advances in Cryptology - Eurocrypt 2000, Springer-Verlag, LNCS 1807/2000, pp. 139-155, 2000) computational complexity proof model in a machine specification and analysis (using an automated model checker - SHVT) for provably secure key establishment protocol analysis. We then examine several key establishment protocols without proofs of security, namely: protocols due to J.-K. Jan, Y.-H. Chen (A new efficient MAKEP for wireless communications, in: 18th International Conference on Advanced Information Networking and Applications - AINA 2004, IEEE Computer Society, pp. 347-350, 2004), W.-H. Yang, J.-C. Shen, S.-P. Shieh (Designing authentication protocols against guessing attacks. Technical Report 2(3), Institute of Information & Computing Machinery, Taiwan, 1999. http://www.iicm.org.tw/communication/c2_3/page07.doc), Y.-S. Kim, E.-N. Huh, J. Hwang, B.-W. Lee (An efficient key agreement protocol for secure authentication, in: A. Lagana, M.L. Gavrilova, V. Kumar, Y. Mun, C.J.K. Tan, O. Gervasi (Eds.), International Conference On Computational Science And Its Applications - ICCSA 2004, Springer-Verlag, LNCS 3043/2004, pp. 746-754, 2004), C.-L. Lin, H.-M. Sun, T. Hwang. (Three-party encrypted key exchange: attacks and a solution, in: A CM SIGOPS Operating Systems Review, pp. 12-20, 2000), and H.-T. Yeh, H.-M. Sun (Simple authenticated key agreement protocol resistant to password guessing attacks, in: A CM SIGOPS Operating Systems Review, 36(4), pp. 14-22, 2002). Using these protocols as case studies, we demonstrate previously unpublished flaws in these protocols. We may speculate that such errors could have been found by protocol designers if proofs of security were to be constructed, and hope this work will encourage future protocol designers to provide proofs of security. Advances in information and communication technologies (ICT) have had, and will continue to have, wide-ranging influences on how the banking and finance industry operates. Making payments and transmitting money electronically or online are increasingly popular. An increased dependence on global electronic payment systems and the ability to move large amounts of money expeditiously across different jurisdiction, however, expose both payment processing companies and consumers to an evolving spectrum of threats such as fraud and money laundering. This article considers ways in which prepaid cards can be exploited by organised criminals and terrorists to launder their illicit proceeds of crime, and to transfer money anonymously and instantaneously over the globe for use by terrorist organisations. Risks identified in this article include recruiting card mules (e.g. international students) to purchase prepaid cards, and the mailing or shipping of prepaid cards out of the country without regulators being aware. To minimise risks of abuse by organised criminals and terrorists, legislation will need to adapt to deal with threats that organised criminals and terrorists seek to exploit. Future directions for research in relation to prepaid cards are also identified. Mobile Voice over Internet Protocol (mVoIP) applications have gained increasing popularity in the last few years, with millions of users communicating using such applications (e.g. Skype). Similar to other forms of Internet and telecommunications, mVoIP communications are vulnerable to both lawful and unauthorized interceptions. Encryption is a common way of ensuring the privacy of mVoIP users. To the best of our knowledge, there has been no academic study to determine whether mVoIP applications provide encrypted communications. In this paper, we examine Skype and nine other popular mVoIP applications for Android mobile devices, and analyze the intercepted communications to determine whether the captured voice and text communications are encrypted (or not). The results indicate that most of the applications encrypt text communications. However, voice communications may not be encrypted in six of the ten applications examined. Mobile devices are fast becoming critical information management tools often storing a range of personal and corporate confidential data often synced from online and cloud based storage services. Mobile device operating system designers are increasing the security available to users, not only from traditional security risk vectors, but also to protect their privacy from the various apps (with potential malicious intent) installed on their device. In this paper, we developed a process for enforcing file system permissions on Android external storage (with minimal modifications to the operating system). Our process makes use of the application sandboxing supported on this platform to restrict parts of the external file system to a particular app or multiple apps holding a particular permission. We present an Android File system Permissions (AFP) prototype demonstrating the applicability of this work and demonstrate its utility using the own Cloud app for Android. We then highlight a number of limitations with the current permission enforcement capabilities for external storage on the platform. With the increasing popularity of cloud services and their potential to either be the target or the tool in a cybercrime activity, organizational cloud services users need to ensure that they are able to collect evidential data should they be involved in litigation or a criminal investigation. In this paper, we seek to contribute to a better understanding of the technical issues and processes regarding collection of evidential data in the cloud computing environment. Using VMware vCloud as a case study in this paper, we describe the various artefacts available in the cloud environment and identify several forensic preservation considerations for forensics practitioners. We then propose a six-step process for the remote programmatic collection of evidential data to ensure as few changes as possible are made as part of evidence collection and that no potential evidence is missed. The six-step process is implemented in a proof of concept application to demonstrate utility of the process. This article considers how information and communications technologies (ICT) can be used by organised crime groups to infringe legal and regulatory controls. Three categories of groups are identified: traditional organised criminal groups which make use of ICT to enhance their terrestrial criminal activities; organised cybercriminal groups which operate exclusively online; and organised groups of ideologically and politically motivated individuals who make use of ICT to facilitate their criminal conduct. The activities of each group are then assessed in relation to five areas of risk: the use of online payment systems, online auctions, online gaming, social networking sites and blogs. It is concluded that the distinction between traditional organised crime groups and the other two groups\\u2014cybercriminal groups and ideologically/politically motivated cyber groups\\u2014is converging, with financially-motivated attacks becoming more targeted. Legislation will need to adapt to deal with new technological developments and threats that organised criminals seek to exploit. Given the relatively new phenomenon of online child exploitation \\u2013 an important area of criminological and policy concern \\u2013 it is difficult to obtain long term trend data on reported convictions. Existing legislative and prosecution-based approaches, while important, are unlikely to be adequate. This analysis of the legislative and prosecution-based responses in five Commonwealth countries (Australia, Canada, New Zealand, South Africa and United Kingdom), highlights the need for clear national and international definitions as the lack of consistency in the international legislative environment creates opportunities for sexual exploitation of children. To provide a multi-faceted and non-prosecutorial approach to address online child exploitation including the commercial exploitation of children, this paper identifies five potential research questions. Smart mobile devices are a potential attack vector for cyber criminal activities. Two hundred and fifty smart mobile device owners from the University of South Australia were surveyed. Not surprisingly, it was found that smart mobile device users in the survey generally underestimated the value that their collective identities have to criminals and how these can be sold. For example, participants who reported jail-breaking/rooting their devices were also more likely to exhibit risky behaviour (e.g. downloading and installing applications from unknown providers), and the participants generally had no idea of the value of their collective identities to criminals which can be sold to the highest bidder. In general, the participants did not understand the risks and may not have perceived cyber crime to be a real threat. Findings from the survey and the escalating complexities of the end-user mobile and online environment underscore the need for regular ongoing training programs for basic online security and t... As with most popular consumer technologies, criminals can exploit vulnerabilities in mobile devices and operating systems or mobile apps to target mobile device and app users. Because of their capability to store vast amounts of user data, cloud storage apps are a potential and attractive target for criminals. Fortunately, the private sector has enormous incentives for contributing to mobile device/app and cloud security. In this paper, we conducted a series of face-to-face interviews with 17 participants from 11 SA government entities, with the aim of validating whether existing processes and strategic direction were suf\\ufb01cient to satisfactorily achieve the implementation of an ISMS and classi\\ufb01cation of data for the respective SA government entities. Based on our interviews and review of ISMS associated reviews conducted within other Australian State and Territory jurisdictions, we identify key areas that the SA Government may need to consider as part of the progressive roll-out of the other phases of ISMF version 3 implementation up and to June 2017. Cloud storage has been identified as an emerging challenge to digital forensic researchers and practitioners in a range of literature. There are various types of cloud storage services with each type having a potentially different use in criminal activity. One area of difficulty is the identification, acquisition, and preservation of evidential data when disparate services can be utilised by criminals. Not knowing if a cloud service is being used, or which cloud service, can potentially impede an investigation. It would take additional time to contact all service providers to determine if data is being stored within their cloud service. Using Dropbox(TM) as a case study, research was undertaken to determine the data remnants on a Windows 7 computer and an Apple iPhone 3G when a user undertakes a variety of methods to store, upload, and access data in the cloud. By determining the data remnants on client devices, we contribute to a better understanding of the types of terrestrial artifacts that are likely to remain for digital forensics practitioners and examiners. Potential information sources identified during the research include client software files, prefetch files, link files, network traffic capture, and memory captures, with many data remnants available subsequent to the use of Dropbox by a user. Purpose \\u2013 The purpose of this paper is to show how modelling can be used to provide an easy\\u2010to\\u2010follow, visual representation of the important characteristics and aspects of money laundering behaviours extracted from real\\u2010world money laundering and terrorism financing typologies.Design/methodology/approach \\u2013 In total, 184 typologies were obtained from a number of anti\\u2010money laundering and counter\\u2010terrorism financing (AML/CTF) bodies to determine the common patterns and themes present in the cases involved. Financial flows, transactions and interactions between entities were extracted from each of the typologies and modelled using the Unified Modelling Language (UML) features within Microsoft Visio.Findings \\u2013 The paper demonstrates how complex transactional flows and interactions between the different entities involved in a money laundering and terrorism financing case can be shown in an easy\\u2010to\\u2010follow graphical representation, allowing practitioners to more easily and quickly extract the relevant informati... Traditionally, the financial sector is often seen as the gatekeepers of the Anti-Money Laundering/Counter Terrorism Financing (AML/CFT) regime. In recent years, new payment methods, particularly stored value prepaid cards and mobile money transfer systems, are increasingly been seen as a widely accepted payment method. However, they have also been highlighted as potential money laundering and terrorism financing instruments. This paper aims to provide an improved understanding of the money laundering and terrorism financing risk environment and hopefully, new payment method providers are better placed to manage new and emerging threats. A review of the compliance levels in 65 mutual evaluation (and follow-up) reports published by FATF in English between 1st of January 2010 and 31st of December 2012 suggests that there are still compliance issues in areas that might afford exploitative opportunities for transnational crime and terrorist networks - after all, global standards are only as strong as their weakest link. This can have detrimental effects on a country's national security through increasing risks of money laundering and financing of terrorism (e.g. due to regulatory arbitrage), and wastage due to the implementation of inappropriate regulatory measures. We conclude with a three-pronged evidence-based AML/CTF approach, with the aim of helping governments and key stakeholders to improve knowledge of the nature and dimensions to the problem, and of suitable risk management and mitigation strategies that would enable scarce resources in fighting money laundering and terrorism financing threats to be more effectively allocated and, hence, make the most impact. Traditionally, the financial sector is often seen as the gatekeepers of the Anti-money Laundering/Counter Terrorism Financing regime. Increasingly, governments and law enforcement agencies have recognised the importance of designated non-financial businesses and professions in the fight against money laundering and terrorism financing activities. A review of Financial Action Task Force (FATF) mutual evaluation (and follow-up) reports for 15 jurisdictions reveals that a significant number of jurisdictions were assessed to be either non-compliant or partially compliant with the FATF Recommendations 12, 16, 20, 24 and 25. The analysis of the review indicated that there are still compliance issues in areas that might afford exploitative opportunities for criminals and organised crime groups. Most jurisdictions, for example, were deemed to be non-compliant in relation to there being no supervisory regime for real estate agents, dealers in precious metals and stones, accountants, trust and company service providers and no comprehensive monitoring for lawyers. This could have the undesired result of regulatory arbitrage where criminals and organised crime groups take advantage of a regulatory difference between two or more jurisdictions to facilitate their money laundering activities. Several potential research topics are also identified to seek to provide more evidence to support the policy process. IOS devices generally allow users to synch their images (pictures) and video files using iTunes between Apple products (e.g. an iPhone and a Mac Book Pro). Recovering deleted images, particularly in a forensically sound manner, from iOS devices can be an expensive and challenging exercise (due to the hierarchical encrypted file system, etc). In this paper, we propose an operational technique that allows digital forensic practitioners to recover deleted image files by referring to iOS journaling file system. Using an iPhone as a case study, we then conduct a forensic analysis to validate our proposed technique. As businesses' dependence on cloud services increases, these services are vulnerable to security incidents, data breaches, and other malicious activities. To mitigate risks to the confidentiality, integrity, and availability of assets, as well as to minimize losses to cloud service providers and users, particularly organizational users, the attack risk elements must be identified, classified, quantified, and prioritized. This column presents a conceptual cloud attack and risk assessment taxonomy. Given the threat of security breaches, to both cloud service providers and organizational cloud service users, cloud security and privacy are growing public policy concerns as well a salient area of inquiry for researchers. To ensure organizational competitiveness, cloud service providers and organizational cloud service users must make detailed preparations to act against cyberthreats before they occur, and to recover from malicious cyberactivities when such threats succeed. A cloud security risk-management strategy should be a dynamic document that's regularly reviewed by stakeholders, and should include policies and objectives that align with the organization's needs. We examine the role of session key construction in provably-secure key establishment protocols. We revisit an ID-based key establishment protocol due to Chen & Kudla (2003) and an ID-based protocol 2P-IDAKA due to McCullagh & Barreto (2005). Both protocols carry proofs of security in a weaker variant of the Bellare & Rogaway (1993) model where the adversary is not allowed to make any Reveal query. We advocate the importance of such a (Reveal) query as it captures the known-key security requirement. We then demonstrate that a small change to the way that session keys are constructed in both protocols results in these protocols being secure without restricting the adversary from asking the Reveal queries in most situations. We point out some errors in the existing proof for protocol 2P-IDAKA, and provide proof sketches for the improved Chen & Kudla\\u2019s protocol. We conclude with a brief discussion on ways to construct session keys in key establishment protocols. New node deployment is inevitable in a wireless sensor network because nodes in the network may be lost, exhausted, or destroyed. To secure the new node deployment process, Kim and Lee (2009) proposed an enhanced novel access control protocol (ENACP) using the elliptic curve cryptography and the hash chain. We identified an inherent flaw in their design and demonstrated that ENACP is vulnerable to a new node masquerading attack and a legal node masquerading attack, in violation of their security claims. We hope that by identifying this design flaw, similar structural mistakes can be avoided in future designs. Due to the capability of mobile applications (or apps, as they are commonly known) to access sensitive data and personally identifiable information (PII) such as medical history and electronic health transactions, they present a genuine security and privacy threat to their users. In this paper, we propose a generic process to identify vulnerabilities and design weaknesses in apps for iOS devices. We validate our process with a widely used Australian Government Healthcare app and revealed previously unknown / unpublished vulnerability that consequently exposes the user's sensitive data and PII stored on the device. We then propose several recommendations with the hope that similar structural mistakes can be avoided in future app design. With increasing popularity of smart mobile devices such as iOS devices, security and privacy concerns have emerged as a salient area of inquiry. A relatively under-studied area is anti-mobile forensics to prevent or inhibit forensic investigations. In this paper, we propose a \\\"Concealment\\\" technique to enhance the security of non-protected (Class D) data that is at rest on iOS devices, as well as a \\\"Deletion\\\" technique to reinforce data deletion from iOS devices. We also demonstrate how our \\\"Insertion\\\" technique can be used to insert data into iOS devices surreptitiously that would be hard to pick up in a forensic investigation. Ensuring a secure cloud system (and ecosystem) is a highly specialized and interdisciplinary field. It requires a deep understanding of the underlying technical, social, public policy, regulatory, and legal and law enforcement aspects, as well as intimate knowledge of temporal trends (historical, recent, and emerging). Although security, privacy, public policy, legal, and forensic challenges associated with cloud computing have attracted academic attention-particularly the issues relating to data sovereignty and confidentiality and to the inadequacy of our existing legislative and regulatory frameworks to protect data from prying eyes-research on the topic is still in its infancy. Cloud storage is an emerging challenge to digital forensic examiners. The services are increasingly used by consumers, business, and government, and can potentially store large amounts of data. The retrieval of digital evidence from cloud storage services (particularly from offshore providers) can be a challenge in a digital forensic investigation, due to virtualisation, lack of knowledge on location of digital evidence, privacy issues, and legal or jurisdictional boundaries. Google Drive is a popular service, providing users a cost-effective, and in some cases free, ability to access, store, collaborate, and disseminate data. Using Google Drive as a case study, artefacts were identified that are likely to remain after the use of cloud storage, in the context of the experiments; on a computer hard drive and Apple iPhone3G, and the potential access point(s) for digital forensics examiners to secure evidence. Foreword | In the increasingly dynamic environment of mobile forensics, this paper provides an overview of the capabilities of three popular mobile forensic tools on three mobile phones based on Apple\\u2019s iOS, Google\\u2019s Android and RIM\\u2019s BlackBerry operating systems. The paper identifies where each specific tool is best applied and also describes the limitations of each in accessing contacts, call history, message data (SMS, MMS and emails), media files and other data. New releases of forensic tools and mobile operating systems may change the way the data are acquired and preserved in the future. It is therefore hoped that future research will continue to provide the digital forensics community with the most up-to-date overview of mobile forensics capabilities. In The International Conference on Practice and Theory in Public-Key Cryptography PKC'05, Baek et al. proposed the first multi-receiver identity-based encryption scheme. Their scheme is highly efficient in that it only needs one pairing computation to encrypt a single message for n receivers. However, the application scenario considered by Baek et al. is merely the ideal \\\"single domain environment,\\\" where all the n receivers are from the same administrative domain. When used in the real-world application scenario where the n receivers are from l different administrative domains i.e., a multiple domain environment, their scheme becomes inefficient as it requires l pairing computations for one message. In this paper, we present an efficient multiple domain multi-receiver identity-based encryption scheme that only requires \\\"one\\\" pairing computation to encrypt a single message for n receivers from l different administrative domains. We prove the security of the new scheme under the modified decisional bilinear Diffie-Hellman assumption in the random oracle model. In addition, the new scheme can be extended to be adaptive chosen ciphertext secure under the gap modified bilinear Diffie-Hellman assumption. Copyright \\u00a9 2013 John Wiley & Sons, Ltd. Purpose \\u2013 The purpose of this paper is to examine the identity and payment method verification procedures implemented by a number of popular massively multiplayer online games (MMOGs) and online financial service providers (OFSPs) to determine if the systems they currently have in place are sufficient to uncover the identities of those who may wish to use such environments to conduct money laundering or terrorism financing activity.Design/methodology/approach \\u2013 The paper investigates whether the payment instruments or methods used by account holders to place funds into their account(s) hinder or assist investigators to expose the real\\u2010world identity of the account holder. The paper then discusses whether it is feasible and/or desirable to introduce know your customer (KYC) and customer due diligence (CDD) legislation into virtual environments and illustrates an effective KYC approach which may assist MMOGs and OFSPs to correctly identify their account holders, should legislation be put in place.Findings \\u2013... Android mobile devices are becoming a popular alternative to computers. The rise in the number of tasks performed on smartphones means sensitive information is stored on the devices. Consequently, Android devices are a potential vector for criminal exploitation. Existing research on enhancing user privacy on Android devices can generally be classified as Android modifications. These solutions often require operating system modifications, which significantly reduce their potential. This research proposes the use of permissions removal, wherein a reverse engineering process is used to remove an app's permission to a resource. The repackaged app will run on all devices the original app supported. Our findings that are based on a study of seven popular social networking apps for Android mobile devices indicate that the difficulty of permissions removal may vary between types of permissions and how well-integrated a permission is within an app. In 2008, Das and Joshi proposed a new dynamic program update protocol for wireless sensor networks using the orthogonality principle. Recently, Zeng et al. showed that the Das-Joshi scheme is vulnerable to an impersonation attack and proposed an improved scheme to overcome the weakness. However, in this paper we present that Zeng et al.'s scheme still suffers from the impersonation attacks. The aim of this report was to identify the crime risks which will arise between 2007-09 out of the environment in which Australians use information and communications technologies. The aim of this report was to identify the crime risks which will arise over the next two years (2007-09) out of the environment in which Australians use information and communications technologies. In identifying future risk areas, particular focus is placed on the impact these will have for law enforcement, the need for additional resources, law reform, development of cooperative arrangements between Australian and overseas public and private sector organisations, and development of public information and educational resources to minimise the risk of widespread harm to the community. The report begins by identifying developments that will take place over the next two years that will be likely to facilitate technology-enabled crime. These include: changes arising from globalisation of business and the emergence of new economies in China and India; developments in digitisation of information, especially relating to the widespread use of broadband services and mobile and wireless technologies; the evolution of electronic payment systems, especially those being used in connection with online gambling and auctions; and changes in the use governments make of technology to allow members of the public to conduct transactions with government agencies securely and even to aloe participation in democracy online. These, and other developments, create not only benefits for the community but also risks. This report identifies the most likely areas in which opportunities for illegality may arise including fraud, identity-related crime, computer vandalism, theft of information, dissemination of objectionable material online, and risks of organised crime and terrorism. The implications for these developments are then assessed in terms of their impact for policing, policy making and legislation. Increasing interest in and use of cloud computing services presents both opportunities for criminal exploitation and challenges for law enforcement agencies (LEAs). For example, it is becoming easier for criminals to store incriminating files in the cloud computing environment but it may be extremely difficult for LEAs to seize these files as the latter could potentially be stored overseas. Two of the most widely used and accepted forensic frameworks \\u2013 McKemmish (1999) and NIST (Kent et al., 2006) \\u2013 are then reviewed to identify the required changes to current forensic practices needed to successfully conduct cloud computing investigations. We propose an integrated (iterative) conceptual digital forensic framework (based on McKemmish and NIST), which emphasises the differences in the preservation of forensic data and the collection of cloud computing data for forensic purposes. Cloud computing digital forensic issues are discussed within the context of this framework. Finally suggestions for future research are made to further examine this field and provide a library of digital forensic methodologies for the various cloud platforms and deployment models. Designed to assist prosecutors and members of the judiciary faced with proceedings involving technology-enabled crime, the report will be a useful general guide to concepts and terms for other non-technical people. Anti-mobile malware has attracted the attention of the research and security community in recent years due to the increasing threat of mobile malware and the significant increase in the number of mobile devices. M0Droid, a novel Android behavioral-based malware detection technique comprising a lightweight client agent and a server analyzer, is proposed here. The server analyzer generates a signature for every application (app) based on the system call requests of the app (termed app behavior) and normalizes the generated signature to improve accuracy. The analyzer then uses Spearman\\u2019s rank correlation coefficient to identify malware with similar behavior signatures in a previously generated blacklist of malwares signatures. The main contribution of this research is the proposed method to generate standardized mobile malware signatures based on their behavior and a method for comparing generated signatures. Preliminary experiments running M0Droid against Genome dataset and APK submissions of Android client... Proofs are invaluable tools in assuring protocol implementers about the security properties of protocols. However, several instances of undetected flaws in the proofs of protocols (resulting in flawed protocols) undermine the credibility of provably-secure protocols. In this work, we examine several protocols with claimed proofs of security by Boyd & Gonzalez Nieto (2003), Jakobsson & Pointcheval (2001), and Wong & Chan (2001), and an authenticator by Bellare, Canetti, & Krawczyk (1998). Using these protocols as case studies, we reveal previously unpublished flaws in these protocols and their proofs. We hope our analysis will enable similar mistakes to be avoided in the future. As our use of information and communication technologies increases and evolves, incidents of technology-enabled crime are likely to continue. Based on what we know today, this paper summarises a range of potential challenges that regulators and law enforcement agencies need to bear in mind. Key areas identified include infrastructure risks, the use of wireless and mobile technologies, more sophisticated malware, new identification and payment systems, computer-facilitated fraud, exploitation of younger persons, intellectual property infringement, and industrial espionage. Successful prosecution and appropriate sentencing for these crimes will require coordinated policing and on-going legislative reform. In 2004, Zhu and Ma proposed a new and efficient authentication scheme claiming to provide anonymity for wireless environments. Two years later, Lee et al. revealed several previously unpublished flaws in Zhu-Ma's authentication scheme and proposed a fix. More recently in 2008, Wu et al. pointed out that Lee et al.'s proposed fix fails to preserve anonymity as claimed and then proposed yet another fix to address the problem. In this paper, we use Wu et al.'s scheme as a case study and demonstrate that due to an inherent design flaw in Zhu-Ma's scheme, the latter and its successors are unlikely to provide anonymity. We hope that by identifying this design flaw, similar structural mistakes can be avoided in future designs. Identity-based cryptography has become extremely fashionable in the last few years. As a consequence many proposals for identity-based key establishment have emerged, the majority in the two party case. We survey the currently proposed protocols of this type, examining their security and efficiency. Problems with some published protocols are noted. We describe a mechanical approach to derive identity-based (ID-based) protocols from existing Diffie-Hellman-based ones. As case studies, we present the ID-based versions of the Unified Model protocol, UMP-ID, Blake-Wilson et al. (1997)'s protocol, BJM-ID, and Krawczyk (2005)'s HMQV protocol, HMQV-ID. We describe the calculations required to be modified in existing proofs. We conclude with a comparative security and efficiency of the three proposed ID-based protocols (relative to other similar published protocols) and demonstrate that our proposed ID-based protocols are computationally efficient. Digital video recorders (DVRs) for closed-circuit television (CCTV) commonly have an in-built capability to export stored video files to optical storage media. In the event that a DVR is damaged, its contents cannot be easily exported. This renders the forensically-sound recovery of proprietary-formatted video files with their timestamps from a DVR hard disk an expensive and challenging exercise. This paper presents and validates a technique that enables digital forensic practitioners to carve video files with timestamps without referring to the DVR hard disk filesystem. The traditional way of protecting a system against malicious threats and loss of personal data by using locally installed anti-malware software is unlikely to work on mobile devices due to the changing threat landscape and the mobile device resource limitations (e.g. storage and battery life). A number of anti-malware providers have migrated to the cloud where the computationally demanding tasks of analyzing malware is conducted by cloud-based server. However, the effectiveness of these anti-mobile apps has not been studied. Therefore, in this paper, we evaluate the effectiveness of ten popular free cloud-based anti-malware apps using a known Android malware dataset. We hope that this research will contribute towards a better understanding of the effectiveness of Android cloud-based anti-malware apps. JPEG thumbnail images are of interest in forensic investigations as images from the thumbnail cache could be intact even when the original pictures have been deleted. In addition, a deleted thumbnail is less likely to be fragmented due to its small size. The focus of existing literature is generally on the desktop environment. Considering the increasing capability of smart mobile devices, particularly Android devices, to take pictures and videos on the go, it is important to understand how thumbnails can be collected from these devices. In this paper, we examine and describe the various thumbnail sources in Android devices and propose a methodology for thumbnail collection and analysis from Android devices. We also demonstrate the utility of our proposed methodology using a case study (e.g. thumbnails could be recovered even when the file system is heavily fragmented). Our findings also indicate that collective information obtained from the recovered fragmented JPEG image (e.g. metadata) and the thumbnail could be akin to recovering the full image for forensic purposes. Conditional proxy re-encryption (CPRE) enables fine-grained delegation of decryption rights, and has many real-world applications. In this paper, we present a ciphertext-policy attribute based CPRE scheme, together with a formalization of the primitive and its security analysis. We demonstrate the utility of the scheme in a cloud deployment, which achieves fine-grained data sharing. This application implements cloud server-enabled user revocation, offering an alternative yet more efficient solution to the user revocation problem in the context of fine-grained encryption of cloud data. High user-side efficiency is another prominent feature of the application, which makes it possible for users to use resource constrained devices, e.g., mobile phones, to access cloud data. Our evaluations show promising results on the performance of the proposed scheme. Cloud services are now used by many organizations for the computation and storage of both public and sensitive data. Organizations expect that the data stored on clouds will be reasonably protected in terms of confidentiality, integrity and availability (CIA). In this paper, we review the cloud security literature to determine key contemporary attack consequences and mitigation strategies in the cloud environment. We categorize the consequences and mitigation strategies using the people, process and technology (PPT) and CIA classifications. We then construct a taxonomy of consequences and mitigation strategies, and use the themes discovered to present a conceptual privileged access management architecture. Android is one of the most popular and widely used mobile operating systems and one of the most actively researched products in the field of mobile forensics. However, analysis of Android caches has been, to date, an understudied research topic, which limits its potential use in forensic investigations. Due to the diversity of cache formats on Android, we propose a cache taxonomy based on app usage. Using this taxonomy as a base, a systematic process, known as the Android Cache Forensic Process, is proposed to forensically classify, extract and analyze Android caches. Various cache formats utilized by 11 popular Android apps are analyzed. As part of this analysis, a number of cache formats are decoded and several cache formats commonly used by Android apps are documented from a forensic perspective. Based on our technical findings, an Android Cache Viewer prototype was also developed. This prototype is able to decode a number of Android cache formats and display the contents in an accessible manner. The rapid growth in usage and application of Social Networking (SN) platforms make them a potential target by cyber criminals to conduct malicious activities such as identity theft, piracy, illegal trading, sexual harassment, cyber stalking and cyber terrorism. Many SN platforms are extending their services to mobile platforms, making them an important source of evidence in cyber investigation cases. Therefore, understanding the types of potential evidence of users\\u2019 SN activities available on mobile devices is crucial to forensic investigation and research. In this paper, we examine four popular SN applications: Facebook, Twitter, LinkedIn and Google+, on Android and iOS platforms, to detect remnants of users\\u2019 activities that are of forensic interest. We detect a variety of artefacts (e.g. usernames, passwords, login information, personal information, uploaded posts, exchanged messages and uploaded comments from SN applications) that could facilitate a criminal investigation. With the advances of information communication technologies, it is critical to improve the efficiency and accuracy of emergency management systems through modern data processing techniques. Geographic information system (GIS) models and simulation capabilities are used to exercise response and recovery plans during non-disaster times. They help the decision-makers understand near real-time possibilities during an event. In this paper, a participatory sensing-based model for mining spatial information of urban emergency events is introduced. Firstly, basic definitions of the proposed method are given. Secondly, positive samples are selected to mine the spatial information of urban emergency events. Thirdly, location and GIS information are extracted from positive samples. At last, the real spatial information is determined based on address and GIS information. Moreover, this study explores data mining, statistical analysis, and semantic analysis methods to obtain valuable information on public opinion and requirements based on Chinese microblog data. Typhoon Chan-hom is used as an example. Semantic analysis on microblog data is conducted and high-frequency keywords in different provinces are extracted for different stages of the event. With the geo-tagged and time-tagged data, the collected microblog data can be classified into different categories. Correspondingly, public opinion and requirements can be obtained from the spatial and temporal perspectives to enhance situation awareness and help the government offer more effective assistance. As sensor nodes are deployed in an open and hostile environment, they are vulnerable to various attacks. Due to the insecure nature of the environment, it is of critical importance to be able to revoke compromised nodes to ensure the confidentiality of data traversing in the network. Several approaches to resolving the challenges associated with key revocation have been proposed in the past years, which can be broadly categorized into four main classes, namely centralized, distributed, decentralized and hybrid. In this survey, we present an overview of existing key revocation schemes for wireless sensor networks, as well as a comparative summary (e.g. performance and security features). We also investigate rekeying schemes which are not designed for wireless sensor networks. We conclude this paper with a discussion of open research issues. With the increasing interest in large-scale, high-resolution and real-time geographic information system (GIS) applications and spatial big data processing, traditional GIS is not efficient enough to handle the required loads due to limited computational capabilities.Various attempts have been made to adopt high performance computation techniques from different applications, such as designs of advanced architectures, strategies of data partition and direct parallelization method of spatial analysis algorithm, to address such challenges. This paper surveys the current state of parallel GIS with respect to parallel GIS architectures, parallel processing strategies, and relevant topics. We present the general evolution of the GIS architecture which includes main two parallel GIS architectures based on high performance computing cluster and Hadoop cluster. Then we summarize the current spatial data partition strategies, key methods to realize parallel GIS in the view of data decomposition and progress of the special parallel GIS algorithms. We use the parallel processing of GRASS as a case study. We also identify key problems and future potential research directions of parallel GIS. The growing popularity of web applications makes them an attractive target for malicious users. Large amounts of private data commonly processed and stored by web applications are a valuable asset for attackers, resulting in more sophisticated web-oriented attacks. Therefore, multiple web application protections have been proposed. Such protections range from narrow, vector-specific solutions used to prevent some attacks only, to generic development practices aiming to build secure software from the ground up. However, due to the diversity of the proposed protection methods, choosing one to protect an existing or a planned application becomes an issue of its own.This paper surveys the web application protection techniques, aiming to systematise the existing approaches into a holistic big picture. First, a general background is presented to highlight the issues specific to web applications. Then, a novel classification of the protections is provided. A variety of existing protections is overviewed and systematised next, followed by a discussion of current issues and limitation inherent to the existing protection methods. Finally, the overall picture is summarised and future potentially beneficial research lines are discussed. An emergency event is an unexceptional event that exceeds the capacity of normal resources and organization to cope and a situation that poses an immediate risk to health, life, property, or environment. Crowdsourcing connects unobtrusive and ubiquitous sensing technologies, advanced data management and analytics models, and novel visualization methods, to create solutions that improve urban environment, human life quality, and city operation systems. The crowdsourcing on social media can be used to detect and analyze urban emergency events. In this paper, in order to detect and describe the real-time urban emergency event, the knowledge base model is proposed. The crowdsourcing-based knowledge base model is firstly introduced, which uses the information from social media. Secondly, the basic definition of the proposed knowledge base model including keywords, patterns, positive sentences, and knowledge graph is given. Thirdly, the temporal information is added to the proposed knowledge base model. The case study on real data sets shows that the proposed algorithm has good performance and high effectiveness in the analysis and detection of emergency events. Copyright \\u00a9 2016 John Wiley & Sons, Ltd. In this paper, we propose an adversary model to facilitate forensic investigations of mobile devices (e.g. Android, iOS and Windows smartphones) that can be readily adapted to the latest mobile device technologies. This is essential given the ongoing and rapidly changing nature of mobile device technologies. An integral principle and significant constraint upon forensic practitioners is that of forensic soundness. Our adversary model specifically considers and integrates the constraints of forensic soundness on the adversary, in our case, a forensic practitioner. One construction of the adversary model is an evidence collection and analysis methodology for Android devices. Using the methodology with six popular cloud apps, we were successful in extracting various information of forensic interest in both the external and internal storage of the mobile device. Voice over Internet Protocol (VoIP) has become increasingly popular among individuals and business organisations, with millions of users communicating using VoIP applications (apps) on their smart mobile devices. Since Android is one of the most popular mobile platforms, this research focuses on Android devices. In this paper we survey the research that examines the security and privacy of mVoIP published in English from January 2009 to January 2014. We also examine the ten most popular free mVoIP apps for Android devices, and analyse the communications to determine whether the voice and text communications using these mVoIP apps are encrypted. The results indicate that most of the apps encrypt text communications, but voice communications may not have been encrypted in Fring, ICQ, Tango, Viber, Vonage, WeChat and Yahoo. The findings described in this paper contribute to an in-depth understanding of the potential privacy risks inherent in the communications using these apps, a previously understudied app category. Six potential research topics are also outlined. As businesses continue to offer customers and employees increased access, improved software functionality, and continued improvements in supply chain management opportunities, it raises the risk of cyber-physical attacks on cyber-physical cloud systems (CPCS). In this article, the authors discuss the challenges associated with a CPCS attack and highlight the need for forensic-by-design, prior to presenting their conceptual CPCS forensic-by-design model. The six factors of the framework are discussed, namely, risk management principles and practices, forensic readiness principles and practices, incident handling principles and practices, laws and regulation, CPCS hardware and software requirements, and industry-specific requirements. Future research topics are also identified. The rapidly growing demand for cloud services in the current business practice has favored the success of the hybrid clouds and the advent of cloud federation. The available literature of this topic has focused on middleware abstraction to interoperate heterogeneous cloud platforms and orchestrate different management and business models. However, cloud federation implies serious security and privacy issues with respect to data sovereignty when data is outsourced across different judicial and legal systems. This column describes a solution that applies encryption to protect data sovereignty in federated clouds rather than restricting the elasticity and migration of data across federated clouds. Android forensics is one of the most studied topics in the mobile forensics literature, partly due to the popularity of Android devices and apps. However, there does not appear to have a formal model that captures the activities undertaken during a forensic investigation. In this paper, we adapt a widely used adversary model from the cryptographic literature to formally capture a forensic investigator's capabilities during the collection and analysis of evidentiary materials from mobile devices. We demonstrate the utility of the model using five popular Android social apps (Twitter, POF Dating, Snapchat, Fling and Pinterest). We recover various information of forensic interest, such as databases, user account information, sent-received images, profile pictures, contact lists, unviewed text messages. We are also able to determine when a notification was sent, a tweet was posted, as well as identifying the Facebook authentication token string used in the apps. Attribute-based encryption has the potential to be deployed in a cloud computing environment to provide scalable and fine-grained data sharing. However, user revocation within ABE deployment remains a challenging issue to overcome, particularly when there is a large number of users. In this work, we introduce an extended proxy-assisted approach, which weakens the trust required of the cloud server. Based on an all-or-nothing principle, our approach is designed to discourage a cloud server from colluding with a third party to hinder the user revocation functionality. We demonstrate the utility of our approach by presenting a construction of the proposed approach, designed to provide efficient cloud data sharing and user revocation. A prototype was then implemented to demonstrate the practicality of our proposed construction. Concurrency bugs usually manifest under very rare conditions, and reproducing such bugs can be a challenging task. To reproduce concurrency bugs with a given input, one would have to explore the vast interleaving space, searching for erroneous schedules. The challenges are compounded in a big data environment. This paper explores the topic of concurrency bug reproduction using runtime data. We approach the concurrency testing and bug reproduction problem differently from existing literature, by emphasizing on the preemptable synchronization points. In our approach, a light-weight profiler is implemented to monitor program runs, and collect synchronization points where thread scheduler could intervene and make scheduling decisions. Traces containing important synchronization API calls and shared memory accesses are recorded and analyzed. Based on the preemptable synchronization points, we build a reduced preemption set (RPS) to narrow down the search space for erroneous schedules. We implement an optimized preemption-bounded schedule search algorithm and an RPS directed search algorithm, in order to reproduce concurrency bugs more efficiently. Those schedule exploration algorithms are integrated into our prototype, Profile directed Event driven Dynamic AnaLysis (PEDAL). The runtime data consisting of synchronization points is used as a source of feedback for PEDAL. To demonstrate utility, we evaluate the performance of PEDAL against those of two systematic concurrency testing tools. The findings demonstrate that PEDAL can detect concurrency bugs more quickly with given inputs, and consuming less memory. To prove its scalability in a big data environment, we use PEDAL to analyze several real concurrency bugs in large scale multithread programs, namely: Apache, and MySQL. Despite the increasing popularity of cloud services, ensuring the security and availability of data, resources and services remains an ongoing research challenge. Distributed denial of service (DDoS) attacks are not a new threat, but remain a major security challenge and are a topic of ongoing research interest. Mitigating DDoS attack in cloud presents a new dimension to solutions proffered in traditional computing due to its architecture and features. This paper reviews 96 publications on DDoS attack and defense approaches in cloud computing published between January 2009 and December 2015, and discusses existing research trends. A taxonomy and a conceptual cloud DDoS mitigation framework based on change point detection are presented. Future research directions are also outlined. Instant messaging (IM) has changed the way people communicate with each other. However, the interactive and instant nature of these applications (apps) made them an attractive choice for malicious cyber activities such as phishing. The forensic examination of IM apps for modern Windows 8.1 (or later) has been largely unexplored, as the platform is relatively new. In this paper, we seek to determine the data remnants from the use of two popular Windows Store application software for instant messaging, namely Facebook and Skype on a Windows 8.1 client machine. This research contributes to an in-depth understanding of the types of terrestrial artefacts that are likely to remain after the use of instant messaging services and application software on a contemporary Windows operating system. Potential artefacts detected during the research include data relating to the installation or uninstallation of the instant messaging application software, log-in and log-off information, contact lists, conversations, and transferred files. An issue that continues to impact digital forensics is the increasing volume of data and the growing number of devices. One proposed method to deal with the problem of \\\"big digital forensic data\\\": the volume, variety, and velocity of digital forensic data, is to reduce the volume of data at either the collection stage or the processing stage. We have developed a novel approach which significantly improves on current practice, and in this paper we outline our data volume reduction process which focuses on imaging a selection of key files and data such as: registry, documents, spreadsheets, email, internet history, communications, logs, pictures, videos, and other relevant file types. When applied to test cases, a hundredfold reduction of original media volume was observed. When applied to real world cases of an Australian Law Enforcement Agency, the data volume further reduced to a small percentage of the original media volume, whilst retaining key evidential files and data. The reduction process was applied to a range of real world cases reviewed by experienced investigators and detectives and highlighted that evidential data was present in the data reduced forensic subset files. A data reduction approach is applicable in a range of areas, including: digital forensic triage, analysis, review, intelligence analysis, presentation, and archiving. In addition, the data reduction process outlined can be applied using common digital forensic hardware and software solutions available in appropriately equipped digital forensic labs without requiring additional purchase of software or hardware. The process can be applied to a wide variety of cases, such as terrorism and organised crime investigations, and the proposed data reduction process is intended to provide a capability to rapidly process data and gain an understanding of the information and/or locate key evidence or intelligence in a timely manner. Users interact with social media in a number of ways, providing a variety of data, from ratings and approvals to quantities of text. Public discussion for hotspots in particular generates significant volume and velocity of user-contributed text, frequently attributable to a user identifier or nom de plume. It may be feasible to determine authorship of various tracts of text on social media using n-gram analysis on the bit-level rendition of the text. This paper explores the facility of bit-level n-gram analysis with other statistical classification approaches for determining authorship on two months of captured user postings from an online news and opinion website with moderated discussion. The results show that this approach can achieve a good recognition rate with a low false negative rate. Display Omitted Bit-level n-gram based forensic authorship analysis on social media.Identifying individuals from linguistic profiles.Extraction of authors' linguistic features from the text of their postings. Radio propagation models (RPMs) are generally employed in Vehicular Ad Hoc Networks (VANETs) to predict path loss in multiple operating environments (e.g. modern road infrastructure such as flyovers, underpasses and road tunnels). For example, different RPMs have been developed to predict propagation behaviour in road tunnels. However, most existing RPMs for road tunnels are computationally complex and are based on field measurements in frequency band not suitable for VANET deployment. Furthermore, in tunnel applications, consequences of moving radio obstacles, such as large buses and delivery trucks, are generally not considered in existing RPMs. This paper proposes a computationally inexpensive RPM with minimal set of parameters to predict path loss in an acceptable range for road tunnels. The proposed RPM utilizes geometric properties of the tunnel, such as height and width along with the distance between sender and receiver, to predict the path loss. The proposed RPM also considers the additional attenuation caused by the moving radio obstacles in road tunnels, while requiring a negligible overhead in terms of computational complexity. To demonstrate the utility of our proposed RPM, we conduct a comparative summary and evaluate its performance. Specifically, an extensive data gathering campaign is carried out in order to evaluate the proposed RPM. The field measurements use the 5 GHz frequency band, which is suitable for vehicular communication. The results demonstrate that a close match exists between the predicted values and measured values of path loss. In particular, an average accuracy of 94% is found with R2 = 0.86. Due to growing user demand, web application development is becoming increasingly complicated. Multiple programming languages along with the complex multi-tier architecture commonly involved in web application development contribute to the probability of programming mistakes. Such mistakes may cause serious security vulnerabilities, which can then be exploited by malicious users. Current classifications include a wide variety of web application vulnerabilities, such as SQL injections, Cross-Site Scripting and File Inclusion. Various different protections exist against attacks associated with these vulnerabilities making it difficult to apply a single universal solution. This paper takes an alternative view of the core root of the vulnerabilities. Based on the discovered common traits, a unified extensible context-based model of web applications is proposed. A concept of context is introduced and different attacks are reformulated in terms of context boundary violation. The proposed model can be used to implement a more universal web application protection suitable against different types of attacks. In today\\u2019s Internet-connected world, mobile devices are increasingly used to access cloud storage services, which allow users to access data anywhere, anytime. Mobile devices have, however, been known to be used and/or targeted by cyber criminals to conduct malicious activities, such as data exfiltration, malware, identity theft, piracy, illegal trading, sexual harassment, cyber stalking and cyber terrorism. Consequently, mobile devices are an increasing important source of evidence in digital investigations. In this paper, we examine four popular cloud client apps, namely OneDrive, Box, GoogleDrive, and Dropbox, on both Android and iOS platforms (two of the most popular mobile operating systems). We identify artefacts of forensic interest, such as information generated during login, uploading, downloading, deletion, and the sharing of files. These findings may assist forensic examiners and practitioners in real-world examination of cloud client applications on Android and iOS platforms. Association rule mining and frequent itemset mining are two popular and widely studied data analysis techniques for a range of applications. In this paper, we focus on privacy-preserving mining on vertically partitioned databases. In such a scenario, data owners wish to learn the association rules or frequent itemsets from a collective data set and disclose as little information about their (sensitive) raw data as possible to other data owners and third parties. To ensure data privacy, we design an efficient homomorphic encryption scheme and a secure comparison scheme. We then propose a cloud-aided frequent itemset mining solution, which is used to build an association rule mining solution. Our solutions are designed for outsourced databases that allow multiple data owners to efficiently share their data securely without compromising on data privacy. Our solutions leak less information about the raw data than most existing solutions. In comparison to the only known solution achieving a similar privacy level as our proposed solutions, the performance of our proposed solutions is three to five orders of magnitude higher. Based on our experiment findings using different parameters and data sets, we demonstrate that the run time in each of our solutions is only one order higher than that in the best non-privacy-preserving data mining algorithms. Since both data and computing work are outsourced to the cloud servers, the resource consumption at the data owner end is very low. Widespread adoption of cloud computing has increased the attractiveness of such services to cybercriminals. Distributed denial of service (DDoS) attacks targeting the cloud\\u2019s bandwidth, services and resources to render the cloud unavailable to both cloud providers, and users are a common form of attacks. In recent times, feature selection has been identified as a pre-processing phase in cloud DDoS attack defence which can potentially increase classification accuracy and reduce computational complexity by identifying important features from the original dataset during supervised learning. In this work, we propose an ensemble-based multi-filter feature selection method that combines the output of four filter methods to achieve an optimum selection. We then perform an extensive experimental evaluation of our proposed method using intrusion detection benchmark dataset, NSL-KDD and decision tree classifier. The findings show that our proposed method can effectively reduce the number of features from 41 to 13 and has a high detection rate and classification accuracy when compared to other classification techniques. Protecting Web applications is increasingly important due to their high popularity and wide adoption. Therefore, a multitude of protection techniques emerged in effort to secure Web applications, specifically considering valuable and private data commonly processed by such applications. Based on an overview of currently existing protection techniques, a generic and extensible PHP-oriented protection framework is proposed. The concept of application developer intent is introduced and compared with other concepts such as enforced security policies commonly used in existing protection approaches. The proposed framework is mainly focused on application developer intention understanding. Supervising the application execution in real-time makes it possible to detect deviations from the intended behavior and prevent potentially malicious activity. The additional aspects of application behavior, such as database-related communications or generated Web page structure, can be analyzed due to the extensible architecture of the framework. Mobile Ad Hoc Networks (MANETs) are an active and challenging area in computer network research. One emerging research trend is the attempts in implementing or adapting existing voice protocols over MANETs. Successful implementation of voice over MANETs would allow an autonomous way to communicate. Session Initiation Protocol (SIP) is one of the most widely-used signaling protocols used in VoIP services. In order to implement a voice protocol over MANETs, SIP is generally required to be adapted for use over the decentralized environment instead of the overlay infrastructure-based networks. This paper proposes a Domain-Based Multi-cluster SIP solution for MANET. Our proposed solution eliminates the shortcomings of centralized approaches such as single point of failure and provides a scalable and reliable implementation. In addition, it reduces the overhead in existing fully distributed approaches. We then simulate and evaluate the proposed solution under different conditions and using metrics such as Trust Level, Proxy Server (PS) Load, Network Delay, Success Rate, and Network Management Packet. Cyber-physical systems CPS are a key component in industrial control systems ICS, which are widely used in the critical infrastructure sectors. The increasing reliance on CPS, however, affords exploitative opportunities for malicious actors targeting our critical infrastructure. The real-time requirement of control systems, coupled with the deployment of resource-constrained field devices, complicate efforts to secure our critical infrastructure. A key technical limitation for security solutions is that they should be lightweight. While lightweight cryptography is useful to some extent, enforcement of asymmetric key cryptographic primitives in control systems is known to be problematic. In this paper, we suggest investigating the enforcement of lightweight security solutions in ICS from a different perspective. Rather than focusing on designing lightweight individual cryptographic primitives, we propose taking a whole-of-system approach to 1 achieve system/collective lightweightness, 2 outsource expensive computations from resource-constrained field devices to neighboring devices and equipments that have more computational capacity, and 3 selectively protect critical data partial/selective protection of Data of Interest. With the increasingly popularity of mobile devices (e.g. iPhones and iPads), Mobile Ad hoc Networks (MANETs) have emerged as a topical research area in recent years, and adapting and implementing voice protocols over MANETs is a popular area of inquiry. Successful implementation of voice over MANETs would present a more efficient and cheaper way of communication. In this paper, we propose a cross-domain Session Initiation Protocol (SIP), a widely used voice over Internet Protocol (VoIP) protocol, solution for MANETs using dynamic clustering by extending the scheme of Aburumman and Choo. Our enhanced solution allows us to scale across domains, and deal with outbound requests using the reputation method. Advantages of this solution include avoiding the shortcomings associated with centralized approaches, such as a single point of failure. To demonstrate the utility of the solution, we simulate and evaluate the proposed solution under different conditions and using metrics such as trust level, overhead, network delay, success ratio, and network management packet. Digital camcorders commonly have an in-built capability to export entire video files or a single image to storage media such as a digital versatile disc (DVD). In the event that a DVD is not properly finalised, its contents might not be easily readable. It is generally accepted that recovering video evidence from an unfinalised DVD in a forensically sound manner is an expensive and a challenging exercise. In this paper, we propose a digital camcorder forensics technique that allows digital forensics examiners to carve video files with timestamps without referring to a file system (file system independent technique). We then conduct a forensic analysis to validate our proposed technique. The elastic provisioning of resources and the capability to adapt to changing resource demand and environmental conditions on-the-fly are, probably, key success factors of cloud computing. Live migration of virtual resources is of pivotal importance in achieving such key properties. However, the ability to effectively and efficiently determine which resource to be migrated and where, by satisfying proper objectives and constraints, remains a research challenge. The existing literature is generally based on metaheuristics running a central resolver. Such an approach is not suitable because it only considers the quality-of-service aspect during the decision-making performance while ignoring the regulatory challenges. This column highlights the regulatory challenges associated with the cross-border dataflow implication of migration and stresses the need to adopt alternative decision approaches. In this paper, we propose a toolkit for efficient and privacy-preserving outsourced calculation under multiple encrypted keys (EPOM). Using EPOM, a large scale of users can securely outsource their data to a cloud server for storage. Moreover, encrypted data belonging to multiple users can be processed without compromising on the security of the individual user\\u2019s (original) data and the final computed results. To reduce the associated key management cost and private key exposure risk in EPOM, we present a distributed two-trapdoor public-key cryptosystem, the core cryptographic primitive. We also present the toolkit to ensure that the commonly used integer operations can be securely handled across different encrypted domains. We then prove that the proposed EPOM achieves the goal of secure integer number processing without resulting in privacy leakage of data to unauthorized parties. Last, we demonstrate the utility and the efficiency of EPOM using simulations. In the hierarchical control paradigm of a smart grid cyber-physical system, decentralized local agents (LAs) can potentially be compromised by opportunistic attackers to manipulate electricity prices for illicit financial gains. In this paper, to address such opportunistic attacks, we propose a Dirichlet-based detection scheme, where a Dirichlet-based probabilistic model is built to assess the reputation levels of LAs. Initial reputation levels of the LAs are first trained using the proposed model, based on their historical operating observations. An adaptive detection algorithm with reputation incentive mechanism is then employed to detect opportunistic attackers. We demonstrate the utility of our proposed scheme using data collected from the IEEE 39-bus power system with the PowerWorld simulator. With the increased popularity of 3D printers in homes, and industry sectors, such as biomedical and manufacturing, the potential for cybersecurity risks must be carefully considered. Risks may arise from factors such as printer manufacturers not having the requisite levels of security awareness, and not fully understanding the need for security measures to protect intellectual property, and other sensitive data that are stored, accessed, and transmitted from such devices. This paper examines the security features of two different models of MakerBot Industries\\u2019 consumer-oriented 3D printers and proposes an attack technique that is able to, not only, exfiltrate sensitive data, but also allow for remote manipulation of these devices. The attack steps are discretely modeled using a threat model to enable formal representation of the attack. Specifically, we found that the printers stored the previously printed and currently printing objects on an unauthenticated web server. We also ascertain that the transport layer security implementation on these devices was flawed, which severely affected the security of these devices and allowed for remote exploitation. Countermeasures to the attack that are implementable by both the manufacturer and the user of the printer are presented. Preservation of individual privacy is an important issue in future IoT applications, which calls for lightweight anonymous entity authentication solutions that can be executed efficiently upon a wide range of resource-constrained IoT devices and gadgets. Existing anonymous credential techniques are not well fitted to the setting of IoT, and it is especially so when credential revocation support is considered. In this paper, leveraging on dynamic accumulator we propose a lightweight anonymous entity authentication scheme with outsource-able witness update, solving the main bottleneck of anonymous credentials. We further improve the performance of the scheme with the idea of self-blinding, in such a way that the computation by the prover works entirely in the compact bilinear group of bilinear map. Our performance evaluation shows that the proposed schemes are good for resource-constrained devices. This column addresses the need for a new dedicated bandwidth paradigm that allows flexible orchestration of resources from different sites (for example, a manufacturing organization and its upstream and downstream vendors) joining a cloud without belonging to the same administrative domain (that is, it supports cross-domain operations and advanced network support). The main design goal is to turn the network communication capabilities into virtualized resources that can be scheduled in conjunction with more traditional cloud resources and managed by a specific software layer within the federated (manufacturing) cloud system services to implement a service plane for use by cloud applications and resource-management services. Privacy-aware intersection set computation (PISC) can be modeled as secure multi-party computation. The basic idea is to compute the intersection of input sets without leaking privacy. Furthermore, PISC should be sufficiently flexible to recommend approximate intersection items. In this paper, we reveal two previously unpublished attacks against PISC, which can be used to reveal and link one input set to another input set, resulting in privacy leakage. We coin these as Set Linkage Attack and Set Reveal Attack. We then present a lightweight and flexible PISC scheme (LiPISC) and prove its security (including against Set Linkage Attack and Set Reveal Attack). Intrusion detection systems are important for detecting and reacting to the presence of unauthorised users of a network or system. They observe the actions of the system and its users and make decisions about the legitimacy of the activity and users. Much work on intrusion detection has focused on analysing the actions triggered by users, determining that atypical or disallowed actions may represent unauthorised use. It is also feasible to observe the users' own behaviour to see if they are acting in their'usual' way, reporting on any sufficiently-aberrant behaviour. Doing this requires a user profile, a feature found more often in marketing and education, but increasingly in security contexts. In this paper, we survey literature on intrusion detection and prevention systems from the viewpoint of exploiting the behaviour of the user in the context of their user profile to confirm or deny the legitimacy of their presence on the system (i.e. review of intrusion detection and prevention systems aimed at user profiling). User behaviour can be measured with both behavioural biometrics, such as keystroke speeds or mouse use, but also psychometrics which measure higher-order cognitive functions such as language and preferences. Display Omitted User profiling in intrusion detection.Robustness of behavioural characteristics.How to keep user profile secret.Falsify input data to fool the intrusion detection system.Behavioural profiling in intrusion detection. Data integrity is extremely important for cloud based storage services, where cloud users no longer have physical possession of their outsourced files. A number of data auditing mechanisms have been proposed to solve this problem. However, how to update a cloud user's private auditing key as well as the authenticators those keys are associated with without the user's re-possession of the data remains an open problem. In this paper, we propose a key-updating and authenticator-evolving mechanism with zero-knowledge privacy of the stored files for secure cloud data auditing, which incorporates zero knowledge proof systems, proxy re-signatures and homomorphic linear authenticators. We instantiate our proposal with the state-of-the-art Shacham-Waters auditing scheme. When the cloud user needs to update his key, instead of downloading the entire file and re-generating all the authenticators, the user can just download and update the authenticators. This approach dramatically reduces the communication and computation cost while maintaining the desirable security. We formalize the security model of zero knowledge data privacy for auditing schemes in the key-updating context and prove the soundness and zero-knowledge privacy of the proposed construction. Finally, we analyze the complexity of communication, computation and storage costs of the improved protocol which demonstrates the practicality of the proposal. Due to the increase in adoption of cloud storage services by organizations, ensuring the security and privacy of data stored in the cloud is of critical importance to these organizations. It is also important for organizations to have an effective cloud security incident handling strategy to minimize the impact of a security breach. In this chapter, we present a feasibility study of our proposed Cloud Incident Handling Model, which draws upon principles and practices from both incident handling and digital forensics. We demonstrated the utility of the proposed model using an ownCloud case study simulation. We also explained how the Situational Crime Prevention Theory can be used in our model to design mitigation strategies. Future work includes deploying the model in a real-world organization. Due to the popularity of Android devices and applications (apps), Android forensics is one of the most studied topics within mobile forensics. Communication apps, such as instant messaging and Voice over IP (VoIP), are one popular app category used by mobile device users, including criminals. Therefore, a taxonomy outlining artifacts of forensic interest involving the use of Android communication apps will facilitate the timely collection and analysis of evidentiary materials from such apps. In this paper, 30 popular Android communication apps were examined, where a logical extraction of the Android phone images was collected using XRY, a widely used mobile forensic tool. Various information of forensic interest, such as contact lists and chronology of messages, was recovered. Based on the findings, a two-dimensional taxonomy of the forensic artifacts of the communication apps is proposed, with the app categories in one dimension and the classes of artifacts in the other dimension. Finally, the artifacts identified in the study of the 30 communication apps are summarized using the taxonomy. It is expected that the proposed taxonomy and the forensic findings in this paper will assist forensic investigations involving Android communication apps. As highlighted in Chapter 1, there is a need for a methodology and framework to guide forensic investigations where cloud storage is involved. This chapter outlines our cloud (storage) forensic framework, which endeavors to enlarge upon the process used for traditional forensic computer analysis. This serves to expand the common framework, to be applied when dealing with cloud storage environments. This chapter introduces the reader to the initial developments of the cloud computing industry, consolidated cloud-related terminologies, and concepts, and explains the main reasons and causes of the cloud security and privacy concerns. With the increasing popularity of online social networks (OSNs) and the ability to access and exchange sensitive user information, user privacy concerns become an important issue which have attracted the attention of researchers and policymakers. For example, deleted pictures or pictures in deleted posts may not be deleted from the OSN server immediately, and hence accessible to another unauthorized user. In this paper, we highlight the deletion delay issue in seven popular OSNs, namely: Facebook, Instagram, MySpace, Tumblr, Flickr, Google+ and Weibo, which can be exploited by another unauthorized user to gain access to these pictures. To ensure OSN users are able to achieve a higher level of privacy, we propose a conceptual privacy-preserving tool for photo sharing, without compromising on transparency and real-time sharing features. We demonstrate the utility of the tool by prototyping a browser extension, which does not require modification of existing OSN systems. Major provisioning of cloud computing is mainly delivered via Software as a Service, Platform as a Service and Infrastructure as a Service. However, these service delivery models are vulnerable to a range of security attacks, exploiting both cloud specific and existing web service vulnerabilities. Taxonomies are a useful tool for system designers as they provide a systematic way of understanding, identifying and addressing security risks. In this research work, Cloud based attacks and vulnerabilities are collected and classify with respect to their cloud models. We also present taxonomy of cloud security attacks and potential mitigation strategies with the aim of providing an in-depth understanding of security requirements in the cloud environment. We also highlight the importance of intrusion detection and prevention as a service. Display Omitted Cloud Security Attacks Taxonomy.Cloud Intrusion Detection and Prevention as a Service.Intrusion detection in cloud computing service models.Need for in-depth advanced cloud protection systems. Information confidentiality is an essential requirement for cyber security in critical infrastructure. Identity-based cryptography, an increasingly popular branch of cryptography, is widely used to protect the information confidentiality in the critical infrastructure sector due to the ability to directly compute the user\\u2019s public key based on the user\\u2019s identity. However, computational requirements complicate the practical application of Identity-based cryptography. In order to improve the efficiency of identity-based cryptography, this paper presents an effective method to construct pairing-friendly elliptic curves with low hamming weight 4 under embedding degree 1. Based on the analysis of the Complex Multiplication(CM) method, the soundness of our method to calculate the characteristic of the finite field is proved. And then, three relative algorithms to construct pairing-friendly elliptic curve are put forward. 10 elliptic curves with low hamming weight 4 under 160 bits are presented to demonstrate the utility of our approach. Finally, the evaluation also indicates that it is more efficient to compute Tate pairing with our curves, than that of Bertoni et al. Encouraging cooperation among selfish individuals is crucial in many real-world systems, where individuals\\u2019 collective behaviors can be analyzed using evolutionary public goods game. Along this line, extensive studies have shown that reputation is an effective mechanism to investigate the evolution of cooperation. In most existing studies, participating individuals in a public goods game are assumed to contribute unconditionally into the public pool, or they can choose partners based on a common reputation standard (e.g., preferences or characters). However, to assign one reputation standard for all individuals is impractical in many real-world deployment. In this paper, we introduce a reputation tolerance mechanism that allows an individual to select its potential partners and decide whether or not to contribute an investment to the public pool based on its tolerance to other individuals\\u2019 reputation. Specifically, an individual takes part in a public goods game only if the number of participants with higher reputation exceeds the value of its tolerance. Moreover, in this paper, an individual\\u2019s reputation can increase or decrease in a bounded interval based on its historical behaviors. We explore the principle that how the reputation tolerance and conditional investment mechanisms can affect the evolution of cooperation in spatial lattice networks. Our simulation results demonstrate that a larger tolerance value can achieve an environment that promote the cooperation of participants. Summary As a well-known field of big data applications, smart city takes advantage of massive data analysis to achieve efficient management and sustainable development in the current worldwide urbanization process. An important problem in smart city is how to discover frequent trajectory sequence pattern and cluster trajectory. To solve this problem, this paper proposes a cloud-based taxi trajectory pattern mining and trajectory clustering framework for smart city. Our work mainly includes (1) preprocessing raw Global Positioning System trace by calling the Baidu API Geocoding; (2) proposing a distributed trajectory pattern mining (DTPM) algorithm based on Spark; and (3) proposing a distributed trajectory clustering (DTC) algorithm based on Spark. The proposed DTPM algorithm and DTC algorithm can overcome the high input/output overhead and communication overhead by adopting in-memory computation. In addition, the proposed DTPM algorithm can avoid generating redundant local trajectory patterns to significantly improve the overall performance. The proposed DTC algorithm can enhance the performance of trajectory similarity computation by transforming the trajectory similarity calculation into AND and OR operators. Experimental results indicate that DTPM algorithm and DTC algorithm can significantly improve the overall performance and scalability of trajectory pattern mining and trajectory clustering on massive taxi trace data. Copyright \\u00a9 2016 John Wiley & Sons, Ltd. To deal with the large number of malicious mobile applications (e.g. mobile malware), a number of malware detection systems have been proposed in the literature. In this paper, we propose a hybrid method to find the optimum parameters that can be used to facilitate mobile malware identification. We also present a multi agent system architecture comprising three system agents (i.e. sniffer, extraction and selection agent) to capture and manage the pcap file for data preparation phase. In our hybrid approach, we combine an adaptive neuro fuzzy inference system (ANFIS) and particle swarm optimization (PSO). Evaluations using data captured on a real-world Android device and the MalGenome dataset demonstrate the effectiveness of our approach, in comparison to two hybrid optimization methods which are differential evolution (ANFIS-DE) and ant colony optimization (ANFIS-ACO). The authenticity and reliability of digital images are increasingly important due to the ease in modifying such images. Thus, the capability to identify image manipulation is a current research focus, and a key domain in digital image authentication is Copy-move forgery detection (CMFD). Copy-move forgery is the process of copying and pasting from one region to another location within the same image. In this paper, we survey the recent developments in CMFD, and describe the entire CMFD process involved. Specifically, we characterize the common CMFD workflow of feature extraction and matching process using block or keypoint-based approaches. Instead of listing the datasets and validations used in the literature, we also categorize the types of copied regions. Finally, we also outline a number of future research directions. Display Omitted Copy-move forgery detection.Authenticity and reliability of digital images.Survey of copy-move forgery detection techniques.Feature extraction and matching process.Data inconsistencies and high scalability. As the web enters Big Data age, users and search engines may find it more and more difficult to effectively use and manage such big data. This paper proposes a method based on user tasks to perceive user intent, the meaningful user intent can be separated, and the elusiveness of user intent can be solved. One more thing, the method based on user task can predict the user behaviour to some extent. The main work of this paper is to propose such a concept space to fit the above features and the method to build the concept space on a large text database. We propose a new search pattern termed as ExNa, which can be incorporated into search engines to support more efficient search with better results. We conduct experimental studies upon a prototype search engine KNOWLE, so as to evaluate the validity and performance of our proposed ExNa mechanism. Cloud computing benefits from widespread adoption in every sector of our society and economy, due, at least in part, to the promise of cost-effective and elastic provisioning of computing resources. Manufacturing is one domain where cloud computing is starting to be seen as a promising solution to strengthen collaboration and cooperation capabilities, fostered by novel organization models and strategies. The combination of the cloud computing and manufacturing domains has brought the advent of \\\"cloud manufacturing,\\\" which is rethinking the way manufacturing enterprises are organized and how they can realize the advantages of cloud computing in their operations. However, the full application of cloud manufacturing is hindered by several issues, of which security is one of the most challenging. This column presents an overview of the key security issues relating to cloud manufacturing and briefly surveys the state of the art on this topic. Mobile devices have become ubiquitous in almost every sector of both private and commercial endeavors. As a result of such widespread use in everyday life, many users knowingly and unknowingly save significant amounts of personal and/or commercial data on these mobile devices. Thus, loss of mobile devices through accident or theft can expose users\\u2014and their businesses\\u2014to significant personal and corporate cost. To mitigate this data leakage issue, remote wiping features have been introduced to modern mobile devices. Given the destructive nature of such a feature, however, it may be subject to criminal exploitation (e.g., a criminal exploiting one or more vulnerabilities to issue a remote wiping command to the victim's device). To obtain a better understanding of remote wiping, we survey the literature, focusing on existing approaches to secure flash storage deletion and provide a critical analysis and comparison of a variety of published research in this area. In support of our analysis, we further provide prototype experimental results for three Android devices, thus providing both a theoretical and applied focus to this article as well as providing directions for further research. An effectively designed e-healthcare system can significantly enhance the quality of access and experience of healthcare users, including facilitating medical and healthcare providers in ensuring a smooth delivery of services. Ensuring the security of patients' electronic health records (EHRs) in the e-healthcare system is an active research area. EHRs may be outsourced to a third-party, such as a community healthcare cloud service provider for storage due to cost-saving measures. Generally, encrypting the EHRs when they are stored in the system (i.e. data-at-rest) or prior to outsourcing the data is used to ensure data confidentiality. Searchable encryption (SE) scheme is a promising technique that can ensure the protection of private information without compromising on performance. In this paper, we propose a novel framework for controlling access to EHRs stored in semi-trusted cloud servers (e.g. a private cloud or a community cloud). To achieve fine-grained access control for EHRs, we leverage the ciphertext-policy attribute-based encryption (CP-ABE) technique to encrypt tables published by hospitals, including patients' EHRs, and the table is stored in the database with the primary key being the patient's unique identity. Our framework can enable different users with different privileges to search on different database fields. Differ from previous attempts to secure outsourcing of data, we emphasize the control of the searches of the fields within the database. We demonstrate the utility of the scheme by evaluating the scheme using datasets from the University of California, Irvine. We present a novel design for stateless transitive signature ((mathrm {TS})) for undirected graph to authenticate dynamically growing graph data. Our construction is built on the widely studied (mathrm {ZSS}) signature technology [19] with bilinear mapping, and using general cryptographic hash functions (e.g., (mathrm {SHA})-512 and (mathrm {MD}6)). Compared with the existing stateless (mathrm {TS}) schemes for undirected graph in the literature, our scheme is more efficient. The scheme is also proven transitively unforgeable against adaptive chosen-message attack under the (mathrm {M2SDH}) assumption in the random oracle model. Cyberization, as a new big trend following computerization and informatization, is the process of forming a new cyberworld and transforming our current physical, social and mental worlds into novel cyber-combined worlds. Cyber science, responding to the cyberization trend, aims to create a new collection of knowledge about these cyber-enabled worlds, and provide a way of discovering what is in the cyber-enabled worlds and how they work. Cyber science is concerned with the study of phenomena caused or generated by the cyberworld and cyber-physical, cyber-social and cyber-mental worlds, as well as the complex intertwined integration of cyber physical, social and mental worlds. It fuels advances in cyber technology, beyond the existing cyber related technologies. In this paper, after discussing the cyberization background and process, and explaining cyber science and technology, we present our visions and perspectives on new opportunities, essential issues and major challenges for cyber science and technology. We further describe cyber related technologies and closely related existing research areas, and envision future research directions, in terms of cyber physical, cyber social, cyber life, cyber intelligence and cyber security, which are five basic dimensions of cyber science and technology. Internet of Things is a futuristic vision of networked objects able to take smart decisions and to assist humans in their daily activities. The founding factor of such a vision is the capacity of objects to exchange data by means of wireless networks, and the applied communication pattern is event-driven and multicast. Publish/subscribe services are a promising communication technology for building solutions according to the Internet of Things' vision. The available solutions consist in the adaptation of widely-known products for publish/subscribe services taken from other contexts and application domains, without considering the key peculiarities of the Internet of Things. Our driving idea is to consider beaconing as the basic communication means, and to build a publish/subscribe service based on it. Our solution can be used stand-alone for event-driven communications and/or integrated within standardized protocols to provide basic communication capabilities. We present a preliminary set of experiments showing the efficiency of our solution. In this paper, we seek to determine the residual artefacts of forensic value on Windows and Ubuntu client machines of using Syncany private cloud storage service. We demonstrate the types and the locations of the artefacts that can be forensically recovered (e.g. artefacts associated with the installation, uninstallation, log-in, log-off, and file synchronisation actions). Findings from this research contribute to an in-depth understanding of cloud-enabled big data storage forensics related to the collection of big data artefacts from a private cloud storage service, which have real-world implications and impacts (e.g. in criminal investigations and civil litigations). Echoing the observations of Ab Rahman et al. (2006), we reiterated the importance of forensic-by-design in future cloud-enabled big data storage solutions. The Internet of Things (IoT) is the latest Internet evolution that incorporates a diverse range of things such as sensors, actuators, and services deployed by different organizations and individuals to support a variety of applications. The information captured by IoT present an unprecedented opportunity to solve large-scale problems in those application domains to deliver services; example applications include precision agriculture, environment monitoring, smart health, smart manufacturing, and smart cities. Like all other Internet based services in the past, IoT-based services are also being developed and deployed without security consideration. By nature, IoT devices and services are vulnerable to malicious cyber threats as they cannot be given the same protection that is received by enterprise services within an enterprise perimeter. While IoT services will play an important role in our daily life resulting in improved productivity and quality of life, the trend has also \\u201cencouraged\\u201d cyber-exploitation and evolution and diversification of malicious cyber threats. Hence, there is a need for coordinated efforts from the research community to address resulting concerns, such as those presented in this special section. Several potential research topics are also identified in this special section. Mobile technologies can be, and have been, exploited in terrorist activities. In this paper, we highlight the importance of mobile forensics in the investigation of such activities. Specifically, using a series of controlled experiments on Android and Windows devices, we demonstrate how mobile forensic techniques can be used to recover evidentiary artefacts from client devices when popular cloud apps -- Google Drive, Dropbox, and OneDrive -- were used on the devices. A key issue in electronic health systems is the underlying security and privacy risk. For example, confidential patient information or medical records ending up in the hands of a person not privy to the information could have far-reaching consequences. With the trend toward cloud computing use in the healthcare industry continuing to grow (for example, using cloud platforms to digitally manage health-related data including electronic health records), security and privacy concerns must be adequately addressed, and regulations on data protections made to be in compliance. This column examines several key issues and requirements underpinning the use of cloud computing for managing healthcare-related data as well as potential solutions. : With a more Internet-savvy and sophisticated user base, there are more demands for interactive applications and services. However, it is a challenge for existing radio access networks (e.g. 3G and 4G) to cope with the increasingly demanding requirements such as higher data rates and wider coverage area. One potential solution is the inter-collaborative deployment of multiple radio devices in a 5G setting designed to meet exacting user demands, and facilitate the high data rate requirements in the underlying networks. These heterogeneous 5G networks can readily resolve the data rate and coverage challenges. Networks established using the hybridization of existing networks have diverse military and civilian applications. However, there are inherent limitations in such networks such as irregular breakdown, node failures, and halts during speed transmissions. In recent years, there have been attempts to integrate heterogeneous 5G networks with existing ad hoc networks to provide a robust solution for delay-tolerant transmissions in the form of packet switched networks. However, continuous connectivity is still required in these networks, in order to efficiently regulate the flow to allow the formation of a robust network. Therefore, in this paper, we present a novel network formation consisting of nodes from different network maneuvered by Unmanned Aircraft (UA). The proposed model utilizes the features of a biological aspect of genomes and forms a delay tolerant network with existing network models. This allows us to provide continuous and robust connectivity. We then demonstrate that the proposed network model has an efficient data delivery, lower overheads and lesser delays with high convergence rate in comparison to existing approaches, based on evaluations in both real-time testbed and simulation environment. There are known privacy concerns with the use of Tinder, a popular dating app. In this paper, we examine previous attacks on Tinder that have not been documented academically. We also documented the Tinder network API in order to test the previous attacks in a live environment. Although our testing revealed accurate user location data, which was the crux of the prior attacks, has since been patched; we were able to: associate a Facebook profile with a Tinder account due to their shared information, see Facebook pages a user had liked or was a member of, as well as gather user images, which Tinder sends via plain HTTP, for a reverse image search. We also demonstrated the potential for a less accurate location attack that takes into account Tinder\\u2019s updated security. Proof-of-Ownership (PoW) can be an effective deduplication technique to reduce storage requirements, by providing cloud storage servers the capability to guarantee that clients only upload and download files that they are in possession of. In this paper, we propose an attribute symmetric encryption PoW scheme (ase-PoW) for hierarchical environments such as corporations, in which (1) the external cloud service provider is honest-but-curious and (2) there is a flexible access control in place to ensure only users with the right privilege can access sensitive files. This is, to the best of our knowledge, the first such scheme and it is built upon the ce-PoW scheme of Gonzalez-Manzano and Orfila (2015). ase-PoW outperforms ce-PoW in thaact it does not suffer from content-guessing attacks, it reduces client storage needs and computational workload. Abstract Individuals involved in bribery and corruption will constantly seek to exploit new areas and opportunities to offend and launder their corruption proceeds and evade the scrutiny of law enforcement and other government agencies. The broad objective of this chapter is to examine whether cryptocurrencies and other virtual methods are potential instruments for laundering corruption proceeds. First, we review 75 FATF and FATF-style regional bodies\\u2019 mutual evaluation reports and identify compliance issues in areas that might afford exploitative opportunities for bribery and corruption. We then discuss the characteristics of crypto and virtual currencies that may be exploited to launder corruption proceeds. We propose a conceptual intelligence-led AML/CTF strategy, and identify three potential research questions to provide evidence and address specific gaps in knowledge concerning corruption, and money laundering/terrorism financing risks. Dating apps for mobile devices, one popular GeoSocial app category, are growing increasingly popular. These apps encourage the sharing of more personal information than conventional social media apps, including continuous location data. However, recent high profile incidents have highlighted the privacy risks inherent in using these apps. In this paper, we present a case study utilizing forensic techniques on nine popular proximity-based dating apps in order to determine the types of data that can be recovered from user devices. We recover a number of data types from these apps that raise concerns about user privacy. For example, we determine that chat messages could be recovered in at least half of the apps examined and, in some cases, the details of any users that had been discovered nearby could also be extracted. Using the evidence collection and analysis methodology for Android devices proposed by Martini et al. (2015) , we examined and analyzed seven popular Android cloud-based apps. First, we analyzed each app in order to see what information could be obtained from their private app storage and SD card directories. We collated the information and used it to aid our investigation of each app\\u2019s database files and AccountManager data. To complete our understanding of the forensic artifacts stored by apps we analyzed, we performed further analysis on the apps to determine if the user\\u2019s authentication credentials could be collected for each app based on the information gained in the initial analysis stages. The contributions of this research include a detailed description of artifacts, which are of general forensic interest, for each app analyzed. Android devices continue to grow in popularity and capability meaning the need for a forensically sound evidence collection methodology for these devices also increases. This chapter proposes a methodology for evidence collection and analysis for Android devices that is, as far as practical, device agnostic. Android devices may contain a significant amount of evidential data that could be essential to a forensic practitioner in their investigations. However, the retrieval of this data requires that the practitioner understand and utilize techniques to analyze information collected from the device. The major contribution of this research is an in-depth evidence collection and analysis methodology for forensic practitioners.\",\n",
            "  \"2139563917\": \"Abstract This paper presents an effective classification approach for hyperspectral image, based upon a novel discontinuity adaptive class-relative nonlocal means (DACNLM) algorithm and embedding it in the global energy function by energy fusion strategy. Inspired from recent works related to nonlocal means, we extend this framework to label space, assuming that nonlocal similar patches have similar label structures. Thus, similar local structures and nonlocal averaging process are combined by the proposed DACNLM algorithm. The Shannon entropy is adopted to define the distribution of energy. The energy function is then improved by fusion strategy that selects the energy corresponding to the lowest uncertainty. As a sequence, the hyperspectral image classification task stated in term of energy minimization is efficiently solved by graph cuts algorithm. Experiments on two real hyperspectral data sets are provided to demonstrate the effectiveness of our hyperspectral classification algorithm. This paper presents an unsupervised change detection approach for synthetic aperture radar (SAR) images based on a multiobjective clustering algorithm and selective ensemble strategy. A multiobjective clustering method based on the nondominated neighbor immune algorithm is proposed for classifying changed and unchanged regions in the difference image, which aims at reducing the effect of speckle noise and enhancing the cluster performance. The proposed multiobjective clustering method generates a set of mutually intermediate clustering solutions, which correspond to different trade-offs between the two objectives: restraining noise and preserving detail. Then the selective ensemble strategy is introduced to integrated theses intermediate change detection results. Experiments on real SAR images show that the proposed change detection method based on multiobjective clustering reduces the effect of speckle noise and enhancing the cluster performance. In general, the proposed method makes a balance between noise-immunity and the preservation of image detail. The final change detection results obtained by the selective ensemble strategy exhibit lower errors than other existing methods. High-order graph matching aims at establishing correspondences between two sets of feature points using high-order constraints. It is usually formulated as an NP-hard problem of maximizing an objective function. This paper introduces a discrete particle swarm optimization algorithm for resolving high-order graph matching problems, which incorporates several re-defined operations, a problem-specific initialization method based on heuristic information, and a problem-specific local search procedure. The proposed algorithm is evaluated on both synthetic and real-world datasets. Its outstanding performance is validated in comparison with three state-of-the-art approaches. Based on the Antibody Clonal Selection Theory and the dynamic process of immune response, a novel Immune Forgetting Multiobjective Optimization Algorithm (IFMOA) is proposed. IFMOA incorporates a Pareto-strength based antigen-antibody affinity assignment strategy, a clonal selection operation, and a technique simulating the progress of immune tolerance. The comparison of IFMOA with other two representative methods, Multi-objective Genetic Algorithm (MOGA) and Improved Strength Pareto Evolutionary Algorithm (SPEA2), on different test problems suggests that IFMOA extends the searching scope as well as increasing the diversity of the populations, resulting in more uniformly distributing global Pareto optimal solutions and more integrated Pareto fronts over the tradeoff surface. Based on the clonal selection theory, a new Dynamic Multiobjective Optimization (DMO) algorithm termed as Clonal Selection Algorithm for DMO (CSADMO) is presented. The clonal selection, the nonuniform mutation and the distance method are three main operators in the algorithm. CSADMO is designed for solving continuous DMO and is tested on two test problems. The simulation results show that CSADMO outperforms another Dynamic Evolutionary Multiobjective Optimization (EMO) Algorithm: a Direction-Based Method (DBM ) in terms of finding a diverse set of solutions and in converging near the true Pareto-optimal front (POF) in each time step. The cerebral cortex is composed of a large number of neurons. More and more evidences indicate the information is coded via a population approach in cerebrum, and is associated with the spatio-temporal pattern of spiking of neurons. In this paper, we present a novel model that represents the collective activity of neurons with spatio-temporal evolution. We get a density evolution equation of neuronal populations in phase space, which utilize the single neuron dynamics (integrate-and-fire neuron model). Both in theory analysis and applications, our method shows more predominance than direct simulation the large populations of neurons via single neuron. Based on the clonal selection theory, a novel artificial immune systems algorithm, immune clonal selection evolutionary strategy for constrained optimization (ICSCES), is put forward. The new algorithm uses the stochastic ranking constraint-handling technique, realizs local search using clonal proliferation and clonal selection, and global search using clonal deletion. The experimental results on ten benchmark problems show, compared with the (\\u00b5, \\u03bb)evolutionary strategies adopting stochastic ranking technique and dynamic penalty function method, ICSCES has the ability of significantly improving the search performance both in convergence speed and precision. Based on the concept of Immunodominance and Antibody Clonal Selection Theory, This paper proposes a new artificial immune system algorithm, Immune Dominance Clonal Multiobjective Algorithm (IDCMA), for multiobjective 0/1 knapsack problems. IDCMA divides the individual population into three sub-populations and adopts different evolution and selection strategies at them, but the update of each sub-population is not carried out all alone. The performance comparisons among IDCMA, SPEA, HLGA, NPGA, NSGA and VEGA show that IDCMA clearly outperforms the other five MOEAs in terms of solution quality. Unlike evaluation strategy (ES) and evaluation programming (EP), clonal selection algorithm (CSA) strongly depends on the given search space for the optimal solution problem. The interval of existing optimal solution is unknown in most practical problem, then the suitable search space can not be given and the performance of CSA are influence greatly. In this study, a self-adaptive search space expansion scheme and the clonal selection algorithm are integrated to form a new algorithm, Self Adaptive Clonal Selection Algorithm, termed as SACSA. It is proved that SACSA converges to global optimum with probability 1.Qualitative analyzes and experiments show that, compared with the standard genetic algorithm using the same search space expansion scheme, SACSA has a better performance in many aspects including the convergence speed, the solution precision and the stability. Then, we study more about the new algorithm on optimizing the time-variable function. SACSA has been confirmed that it is competent for solving global function optimization problems which the initial search space is unknown. Based on the Antibody Clonal Selection Theory of immunology, a novel artificial immune system algorithm, adaptive dynamic clone select algorithm, is put forward. The new algorithm is intended to integrate the local searching with the global and the probability evolution searching with the stochastic searching. Compared with the improved genetic algorithm and other clonal selection algorithms, the new algorithm prevents prematurity more effectively and has high convergence speed. Numeric experiments of function optimization indicate that the new algorithm is effective and useful. Based on the concept of Immunodominance and Antibody Clonal Selection Theory, we propose a new artificial immune system algorithm, Immune Dominance Clonal Multiobjective Algorithm (IDCMA). The influences of main parameters are analyzed empirically. The simulation comparisons among IDCMA, the Random-Weight Genetic Algorithm and the Strength Pareto Evolutionary Algorithm show that when low-dimensional multiobjective problems are concerned, IDCMA has the best performance in metrics such as Spacing and Coverage of Two Sets. In this study, we propose a novel evolutionary algorithm-based clustering method, named density-sensitive evolutionary clustering (DSEC). In DSEC, each individual is a sequence of real integer numbers representing the cluster representatives, and each data item is assigned to a cluster representative according to a novel density-sensitive dissimilarity measure which can measure the geodesic distance along the manifold. DSEC searches the optimal cluster representatives from a combinatorial optimization viewpoint using evolutionary algorithm. The experimental results on seven artificial data sets with different manifold structure show that the novel density-sensitive evolutionary clustering algorithm has the ability to identify complex non-convex clusters compared with the K-Means algorithm, a genetic algorithm-based clustering, and a modified K-Means algorithm with the density-sensitive distance metric. Social balance property is an eminent feature of social networks. Many creative efforts concerning social balance have been done. This paper presents an optimization idea to solve the social balance of complex social networks. A single objective optimization model integrating the network balance and the community properties is proposed towards the social balance problem. A discrete particle swarm optimization algorithm is introduced to solve the proposed optimization model. Extensive experiments on synthetic and real-world signed networks demonstrate that the proposed model does make sense and the introduced optimization algorithm is promissing for solving the social balance problem. Graphical abstractWe present two artificial immune models in mutitemporal synthetic aperture radar images change detection. The first model, as shown in Fig. (a), simulates the immune process to classify pixels in the difference image into two classes, changed class and unchanged class. The ag is one of the pixel and its affinity with antibodies and memory cells contains local information to reduce the impact of speckle noise. The second model, shown in Fig. (b), executes immune process in a fuzzy way to improve the accuracy of change detection results. Display Omitted HighlightsWe propose a novel change detection approach for synthetic aperture radar images based on unsupervised artificial immune systems.To reduce the impact of speckle noise, we incorporate local information into antibody-antigen affinity.We perform the immune response process in a fuzzy way to get an accurate result by retaining more image details.The proposed model performs well on several kinds of difference images and can be applied to the tasks of change detection. In this paper, we propose a novel change detection method for synthetic aperture radar images based on unsupervised artificial immune systems. After generating the difference image from the multitemporal images, we take each pixel as an antigen and build an immune model to deal with the antigens. By continuously stimulating the immune model, the antigens are classified into two groups, changed and unchanged. Firstly, the proposed method incorporates the local information in order to restrain the impact of speckle noise. Secondly, the proposed method simulates the immune response process in a fuzzy way to get an accurate result by retaining more image details. We introduce a fuzzy membership of the antigen and then update the antibodies and memory cells according to the membership. Compared with the clustering algorithms we have proposed in our previous works, the new method inherits immunological properties from immune systems and is robust to speckle noise due to the use of local information as well as fuzzy strategy. Experiments on real synthetic aperture radar images show that the proposed method performs well on several kinds of difference images and engenders more robust result than the other compared methods. A quaternion model of artificial immune response (AIR) is proposed in this paper. The model abstracts four elements to simulate the process of immune response, namely, antigen, antibody, rules of interaction among antibodies, and the drive algorithm describing how the rules are applied to antibodies. Inspired by the biologic immune system, we design the set of rules as three subsets, namely, the set of clonal selection rules, the set of immunological memory rules, and the set of immunoregulation rules. An example of the drive algorithm is given and a sufficient condition of its convergence is deduced. Personalized recommendation has become an increasing popular research topic, which aims to find future likes and interests based on users\\u2019 past preferences. Traditional recommendation algorithms pay more attention to forecast accuracy by calculating first-order relevance, while ignore the importance of diversity and novelty that provide comfortable experiences for customers. There are some levels of contradictions between these three metrics, so an algorithm based on bidirectional transfer is proposed in this paper to solve this dilemma. In this paper, we agree that an object that is associated with history records or has been purchased by similar users should be introduced to the specified user and recommendation approach based on heat bidirectional transfer is proposed. Compared with the state-of-the-art approaches based on bipartite network, experiments on two benchmark data sets, Movielens and Netflix, demonstrate that our algorithm has better performance on accuracy, diversity and novelty. Moreover, this method does better in exploiting long-tail commodities and cold-start problem. This paper introduces a computational model simulating the dynamic process of human immune response for solving Traveling Salesman Problems (TSPs). The new model is a quaternion (G, I, R, Al), where G denotes exterior stimulus or antigen, I denotes the set of valid antibodies, R denotes the set of reaction rules describing the interactions between antibodies, and Aldenotes the dynamic algorithm describing how the reaction rules are applied to antibody population. The set of immunodominance rules, the set of clonal selection rules, and a dynamic algorithm TSP-PAISA are designed. The immunodominance rules construct an immunodominance set based on the prior knowledge of the problem. The antibodies can gain the immunodominance from the set. The clonal selection rules strengthen these superior antibodies. The experiments indicate that TSP-PAISA is efficient in solving TSPs and outperforms a known TSP algorithm, the evolved integrated self-organizing map. Band selection is an important preprocessing step for hyperspectral image processing. Many valid criteria have been proposed for band selection, and these criteria model band selection as a single-objective optimization problem. In this paper, a novel multiobjective model is first built for band selection. In this model, two objective functions with a conflicting relationship are designed. One objective function is set as information entropy to represent the information contained in the selected band subsets, and the other one is set as the number of selected bands. Then, based on this model, a new unsupervised band selection method called multiobjective optimization band selection (MOBS) is proposed. In the MOBS method, these two objective functions are optimized simultaneously by a multiobjective evolutionary algorithm to find the best tradeoff solutions. The proposed method shows two unique characters. It can obtain a series of band subsets with different numbers of bands in a single run to offer more options for decision makers. Moreover, these band subsets with different numbers of bands can communicate with each other and have a coevolutionary relationship, which means that they can be optimized in a cooperative way. Since it is unsupervised, the proposed algorithm is compared with some related and recent unsupervised methods for hyperspectral image band selection to evaluate the quality of the obtained band subsets. Experimental results show that the proposed method can generate a set of band subsets with different numbers of bands in a single run and that these band subsets have a stable good performance on classification for different data sets. Recent years have seen the arising recognition of community detection in complex networks. Artificial immune systems, owing to their inherent properties, have been thoroughly studied and well applied to practical use. In this article, one of the well-known artificial immune system models, named clonal selection algorithm, is introduced to reveal community structures in complex networks. By introducing a novel antibody population initialization mechanism and a novel hypermutation strategy, the proposed approach could be applied to moderate-scale network. Besides, by optimizing an objective function called modularity density, the proposed algorithm is also capable of detecting community structure at multiple resolution levels. Experiments on both synthetic and real-world networks demonstrate the effectiveness of the proposed method. This paper presents an unsupervised distribution-free change detection approach for synthetic aperture radar (SAR) images based on an image fusion strategy and a novel fuzzy clustering algorithm. The image fusion technique is introduced to generate a difference image by using complementary information from a mean-ratio image and a log-ratio image. In order to restrain the background information and enhance the information of changed regions in the fused difference image, wavelet fusion rules based on an average operator and minimum local area energy are chosen to fuse the wavelet coefficients for a low-frequency band and a high-frequency band, respectively. A reformulated fuzzy local-information C-means clustering algorithm is proposed for classifying changed and unchanged regions in the fused difference image. It incorporates the information about spatial context in a novel fuzzy way for the purpose of enhancing the changed information and of reducing the effect of speckle noise. Experiments on real SAR images show that the image fusion strategy integrates the advantages of the log-ratio operator and the mean-ratio operator and gains a better performance. The change detection results obtained by the improved fuzzy clustering algorithm exhibited lower error than its preexistences. This letter presents a novel neighborhood-based ratio (NR) operator to produce a difference image for change detection in synthetic aperture radar (SAR) images. In order to reduce the negative influence of speckle noise on SAR images, the proposed NR operator produces a difference image by combining gray level information and spatial information of neighbor pixels. The performance comparisons of the proposed operator with a traditional ratio operator and a log-ratio operator indicate that the NR operator is superior to these traditional methods and produces better detection results. In this paper, we put forward a novel approach for change detection in synthetic aperture radar (SAR) images. The approach classifies changed and unchanged regions by fuzzy c-means (FCM) clustering with a novel Markov random field (MRF) energy function. In order to reduce the effect of speckle noise, a novel form of the MRF energy function with an additional term is established to modify the membership of each pixel. In addition, the degree of modification is determined by the relationship of the neighborhood pixels. The specific form of the additional term is contingent upon different situations, and it is established ultimately by utilizing the least-square method. There are two aspects to our contributions. First, in order to reduce the effect of speckle noise, the proposed approach focuses on modifying the membership instead of modifying the objective function. It is computationally simple in all the steps involved. Its objective function can just return to the original form of FCM, which leads to its consuming less time than that of some obviously recently improved FCM algorithms. Second, the proposed approach modifies the membership of each pixel according to a novel form of the MRF energy function through which the neighbors of each pixel, as well as their relationship, are concerned. Theoretical analysis and experimental results on real SAR datasets show that the proposed approach can detect the real changes as well as mitigate the effect of speckle noises. Theoretical analysis and experiments also demonstrate its low time complexity. Negative selection algorithm has been shown to be efficient for anomaly detection problems. This letter presents an improved negative selection algorithm by integrating a novel further training strategy into the training stage. The main process of further training is generating self-detectors to cover the self-region. A primary purpose of adopting further training is reducing self-samples to reduce computational cost in testing stage. It can also improve the self-region coverage. The testing stage focuses on the processing of testing samples lied within the holes. The experimental comparison among the proposed algorithm, the self-detector classification, and the V-detector on seven artificial and real-world data sets shows that the proposed algorithm can get the highest detection rate and the lowest false alarm rate in most cases. In this study, an orthogonal immune algorithm (OIA) is proposed for global optimization by incorporating orthogonal initialization, a novel neighborhood orthogonal cloning operator, a static hypermutation operator, and a novel diversity-based selection operator. The orthogonal initialization scans the feasible solution space once to locate good points for further exploration in subsequent iterations. Meanwhile, each row of the orthogonal array defines a sub-domain. The neighborhood orthogonal cloning operator uses orthogonal arrays to scan uniformly the neighborhood around each antibody. Then the new algorithm explores each clone by using hypermutation. The improved maturated progenies are selectively added to an external population by the diversity-based selection, which retains one and only one external antibody in each sub-domain. The OIA is unique in three aspects: First, a new selection method based on orthogonal arrays is provided in order to preserve diversity in the population. Second, the orthogonal design with a modified quantization technique is introduced to generate initial population. Third, the orthogonal design is introduced into the cloning operator. The performance comparisons of OIA with two known immune algorithms and three evolutionary algorithms in optimizing eight benchmark functions and six composition functions indicate that OIA is an effective algorithm for solving global optimization problems. Convergence speed and diversity of nondominated solutions are two important performance indicators for Multi-Objective Evolutionary Algorithms (MOEAs). In this paper, we propose a Resource Allocation (RA) model based on Game Theory to accelerate the convergence speed of MOEAs, and a novel Double-Sphere Crowding Distance (DSCD) measure to improve the diversity of nondominated solutions. The mechanism of RA model is that the individuals in each group cooperate with each other to get maximum benefits for their group, and then individuals in the same group compete for private interests. The DSCD measure uses hyper-spheres consisting of nearest neighbors to estimate the crowding degree. Experimental results on convergence speed and diversity of nondominated solutions for benchmark problems and a real-world problem show the efficiency of these two proposed techniques. Clustering the data evolve with time, which is termed evolutionary clustering, is an emerging and important research area in recent literature of data mining, and it is very effective to cluster the dynamic data. It needs to consider two conflicting criteria. One is the snapshot quality function; the other is the history cost function. Most state-of-the-art methods combine these two objectives into one and apply a single objective optimization method for optimizing it. In this paper, we propose a new evolutionary clustering approach by using a multi-objective evolutionary algorithm based on decomposition (MOEA/D) to optimize these two conflicting functions in evolutionary k-means algorithm (EKM). The experimental results demonstrate that our algorithm significantly outperforms EKM. Automatic image registration is a vital yet challenging task, particularly for remote sensing images. A fully automatic registration approach which is accurate, robust, and fast is required. For this purpose, a novel coarse-to-fine scheme for automatic image registration is proposed in this paper. This scheme consists of a preregistration process (coarse registration) and a fine-tuning process (fine registration). To begin with, the preregistration process is implemented by the scale-invariant feature transform approach equipped with a reliable outlier removal procedure. The coarse results provide a near-optimal initial solution for the optimizer in the fine-tuning process. Next, the fine-tuning process is implemented by the maximization of mutual information using a modified Marquardt-Levenberg search strategy in a multiresolution framework. The proposed algorithm is tested on various remote sensing optical and synthetic aperture radar images taken at different situations (multispectral, multisensor, and multitemporal) with the affine transformation model. The experimental results demonstrate the accuracy, robustness, and efficiency of the proposed algorithm. With the rapid development of the Internet, we have entered an era of information explosion. In this kind of situation, recommender systems appear. It is meaningful that recommender systems help people to get useful and valuable items from massive data (e.g. movies, music, books, jokes). The traditional recommender systems especially collaborative filtering make recommendations based on similarity between items or users. Under this strategy, the similarities between the items in the recommendation list are extremely high so as to guarantee the accuracy of a recommendation, but it easily results in the loss of diversity. Actually, the task of recommender systems can be modeled as a multi-objective optimization problem. By taking into account of the recommendation accuracy and diversity, a multi-objective evolutionary algorithm for recommender systems is proposed. Each run of the proposed algorithm can produce a set of nondominated solutions. Each solution denotes a unique recommendation list to a target user. Experiments demonstrate that the proposed algorithm is promising and effective in terms of recommendation diversity and novelty. In this study, we introduce a hybrid immune algorithm based on the intelligent recombination operator and clonal selection algorithm. The intelligent recombination operator uses orthogonal experimental design for factor analysis which identifies the potential gene segments from two individuals to improve their antigenic affinities. The new algorithm, termed as Hybrid Immune Algorithm with Recombination (HIAR), can avoid the decrease of gene diversity in evolutionary process. It evaluates the hamming distance before recombination and uses the two individuals which have the largest hamming distance between each other to implement intelligent recombination operator. It is shown empirically that HIAR has better performance in solving benchmark functions as compared with Intelligent Evolutionary Algorithm and Clonal Selection Algorithm. While spectral clustering can produce high-quality clusterings on small data sets, computational cost makes it infeasible for large data sets. Affinity Propagation (AP) has a limitation that it is hard to determine the value of parameter `preference' which can lead to an optimal clustering solution. These problems limit the scope of application of the two methods. In this paper, we develop a novel fast two-stage spectral clustering framework with local and global consistency. Under this framework, we propose a Fast density-Weighted low-rank Approximation Spectral Clustering (FWASC) algorithm to address the above issues. The proposed algorithm is a high-quality graph partitioning method, and simultaneously considers both the local and global structure information contained in the data sets. Specifically, we first present a new Fast Two-Stage AP (FTSAP) algorithm to coarsen the input sparse graph and produce a small number of final representative exemplars, which is a simple and efficient sampling scheme. Then we present a density-weighted low-rank approximation spectral clustering algorithm to operate those representative exemplars on the global underlying structure of data manifold. Experimental results show that our algorithm outperforms the state-of-the-art spectral clustering and original AP algorithms in terms of speed, memory usage, and quality. This paper addresses the problem of finding sparse solutions to linear systems. Although this problem involves two competing cost function terms (measurement error and a sparsity-inducing term), previous approaches combine these into a single cost term and solve the problem using conventional numerical optimization methods. In contrast, the main contribution of this paper is to use a multiobjective approach. The paper begins by investigating the sparse reconstruction problem, and presents data to show that knee regions do exist on the Pareto front (PF) for this problem and that optimal solutions can be found in these knee regions. Another contribution of the paper, a new soft-thresholding evolutionary multiobjective algorithm (StEMO), is then presented, which uses a soft-thresholding technique to incorporate two additional heuristics: one with greater chance to increase speed of convergence toward the PF, and another with higher probability to improve the spread of solutions along the PF, enabling an optimal solution to be found in the knee region. Experiments are presented, which show that StEMO significantly outperforms five other well known techniques that are commonly used for sparse reconstruction. Practical applications are also demonstrated to fundamental problems of recovering signals and images from noisy data. Coupled networks are extremely fragile because a node failure of a network would trigger a cascade of failures on the entire system. Existing studies mainly focused on the cascading failures and the robustness of coupled networks when the networks suffer from attacks. In reality, it is necessary to recover the damaged networks, and there are cascading failures in recovery processes. In this study, firstly, we analyze the cascading failures of coupled networks during recoveries. Then, a recovery robustness index is presented for evaluating the resilience of coupled networks to cascading failures in the recovery processes. Finally, we propose a technique aiming at protecting several influential nodes for enhancing robustness of coupled networks under the recoveries, and adopt six strategies based on the potential knowledge of network centrality to find the influential nodes. Experiments on three coupling networks demonstrate that with a small number of influential nodes protected, the robustness of coupled networks under the recoveries can be greatly enhanced. Artificial immune systems (AIS) are computational systems inspired by the principles and processes of the vertebrate immune system. The AIS-based algorithms typically exploit the immune system's characteristics of learning and adaptability to solve some complicated problems. Although, several AIS-based algorithms have proposed to solve multi-objective optimization problems (MOPs), little focus have been placed on the issues that adaptively use the online discovered solutions. Here, we proposed an adaptive selection scheme and an adaptive ranks clone scheme by the online discovered solutions in different ranks. Accordingly, the dynamic information of the online antibody population is efficiently exploited, which is beneficial to the search process. Furthermore, it has been widely approved that one-off deletion could not obtain excellent diversity in the final population; therefore, a k-nearest neighbor list (where k is the number of objectives) is established and maintained to eliminate the solutions in the archive population. The k-nearest neighbors of each antibody are founded and stored in a list memory. Once an antibody with minimal product of k-nearest neighbors is deleted, the neighborhood relations of the remaining antibodies in the list memory are updated. Finally, the proposed algorithm is tested on 10 well-known and frequently used multi-objective problems and two many-objective problems with 4, 6, and 8 objectives. Compared with five other state-of-the-art multi-objective algorithms, namely NSGA-II, SPEA2, IBEA, HYPE, and NNIA, our method achieves comparable results in terms of convergence, diversity metrics, and computational time. Resource allocation problems usually seek to find an optimal allocation of a limited amount of resources to a number of activities. The allocation solutions of different problems usually optimize different objectives under constraints [1, 2]. If the activities and constraints among them are presented as nodes and edges respectively, the resource allocation problem can be modeled as a k-coloring problem with additional optimization objectives [3, 4]. Since the amount of resources is limited, it is common that some of the activities (nodes) cannot obtain a resource (color). Because the importance of the nodes is usually different, let the weight of a node denote the cost if it cannot obtain a resource, then the resource allocation problem can be described by a node-weighted graph G(E,V), where E and V are the edge and node set, respectively. Some of the nodes that cannot obtain a resource will incur cost to the allocation solution. The optimization objective of the resource allocation problem formulated in this paper is to minimize the total cost of all the nodes that do not obtain a resource. If the total cost is zero, the obtained solution is a k-coloring of the graph; otherwise, the obtained solution is a k -coloring of the graph after removing the nodes that do not obtain a resource. So the resource allocation problem is a generalization of the k-coloring problem. Ng-Jordan-Weiss (NJW) method is one of the most widely used spectral clustering algorithms. For a K clustering problem, this method partitions data using the largest K eigenvectors of the normalized affinity matrix derived from the dataset. It has been demonstrated that the spectral relaxation solution of K-way grouping is located on the subspace of the largest K eigenvectors. However, we find from a lot of experiments that the top K eigenvectors cannot always detect the structure of the data for real pattern recognition problems. So it is necessary to select eigenvectors for spectral clustering. We propose an eigenvector selection method based on entropy ranking for spectral clustering (ESBER). In this method, first all the eigenvectors are ranked according to their importance on clustering, and then a suitable eigenvector combination is obtained from the ranking list. In this paper, we propose two strategies to select eigenvectors in the ranking list of eigenvectors. One is directly adopting the first K eigenvectors in the ranking list. Different to the largest K eigenvectors of NJW method, these K eigenvectors are the most important eigenvectors among all the eigenvectors. The other eigenvector selection strategy is to search a suitable eigenvector combination among the first Km (Km>K) eigenvectors in the ranking list. The eigenvector combination obtained by this strategy can reflect the structure of the original data and lead to a satisfying spectral clustering result. Furthermore, we also present computational complexity reduction strategies for ESBER method to deal with large-scale datasets. We have performed experiments on UCI benchmark datasets, MNIST handwritten digits datasets, and Brodatz texture datasets, adopting NJW method for a baseline comparison. The experimental results show that ESBER method is more robust than NJW method. Especially, ESBER method with the latter eigenvector selection strategy can obtain satisfying clustering results in most cases. Many immue-inspired algorithms are based on the abstractions of one or several immunology theories, such as clonal selection, negative selection, positive selection, rather than the whole process of immune response to solve computational problems. In order to build a general computational framework by simulating immune response process, this paper introduces a model for population-based artificial immune systems, termed as PAIS, and applies it to numerical optimization problems. PAIS models the dynamic process of human immune response as a quaternion (G, I, R, Al), where G denotes exterior stimulus or antigen, I denotes the set of valid antibodies, R denotes the set of reaction rules describing the interactions between antibodies, and Al denotes the dynamic algorithm describing how the reaction rules are applied to antibody population. Some general descriptions of reaction rules including the set of clonal selection rules and the set of immune memory rules are introduced in PAIS. Based on these reaction rules, a dynamic algorithm, termed as PAISA, is designed for numerical optimization. In order to validate the performance of PAISA, nine benchmark functions with 20-10,000 dimensions and a practical optimization problem, optimal approximation of linear systems are solved by PAISA, successively. The experimental results indicate that PAISA has high performance in optimizing some benchmark functions and practical optimization problems. We perform unsupervised image classification based on texture features by using a novel evolutionary clustering method, named manifold evolutionary clustering (MEC). In MEC, the clustering problem is considered from a combinatorial optimization viewpoint. Each individual is a sequence of real integers representing the cluster representatives. Each datum is assigned to a cluster representative according to a novel manifold-distance-based dissimilarity measure, which measures the geodesic distance along the manifold. After extracting texture features from an image, MEC determines partitioning of the feature vectors using evolutionary search. We apply MEC to solve seven benchmark clustering problems on artificial data sets, three artificial texture image classification problems, and two synthetic aperture radar image classification problems. The experimental results show that in terms of cluster quality and robustness, MEC outperforms the K-means algorithm, a modified K-means algorithm using the manifold-distance-based dissimilarity measure, and a genetic-algorithm-based clustering technique in partitioning most of the test problems. This letter presents a new unsupervised classification method for polarimetric synthetic aperture radar (POLSAR) images. Its novelties are reflected in three aspects: First, the scattering power entropy and the copolarized ratio are combined to produce initial segmentation. Second, an improved reduction technique is applied to the initial segmentation to obtain the desired number of categories. Finally, to improve the representation of each category, the data sets are classified by an iterative algorithm based on a complex Wishart density function. By using complementary information from the scattering power entropy and the copolarized ratio, the proposed method can increase the separability of terrains, which can be of benefit to POLSAR image processing. Three real POLSAR images, including the RADARSAT-2 C-band fully POLSAR image of western Xi'an, China, are used in the experiments. Compared with the other three state-of-the-art methods, H/\\u03b1 -Wishart method, Lee category-preserving classification method, and Freeman decomposition combined with the scattering entropy method, the final classification map based on the proposed method shows improvements in the accuracy and efficiency of the classification. Moreover, high adaptability and better connectivity are observed. There is an increasing recognition on community detection in complex networks in recent years. In this study, we improve a recently proposed memetic algorithm for community detection in networks. By introducing a Population Generation via Label Propagation (PGLP) tactic, an Elitism Strategy (ES) and an Improved Simulated Annealing Combined Local Search (ISACLS) strategy, the improved memetic algorithm called (iMeme-Net) is put forward for solving community detection problems. Experiments on both computer-generated and real-world networks show the effectiveness and the multi-resolution ability of the proposed method. Nondominated Neighbor Immune Algorithm (NNIA) is proposed for multiobjective optimization by using a novel nondominated neighbor-based selection technique, an immune inspired operator, two heuristic search operators, and elitism. The unique selection technique of NNIA only selects minority isolated nondominated individuals in the population. The selected individuals are then cloned proportionally to their crowding-distance values before heuristic search. By using the nondominated neighbor-based selection and proportional cloning, NNIA pays more attention to the less-crowded regions of the current trade-off front. We compare NNIA with NSGA-II, SPEA2, PESA-II, and MISA in solving five DTLZ problems, five ZDT problems, and three low-dimensional problems. The statistical analysis based on three performance metrics including the coverage of two sets, the convergence metric, and the spacing, show that the unique selection method is effective, and NNIA is an effective algorithm for solving multiobjective optimization problems. The empirical study on NNIA's scalability with respect to the number of objectives shows that the new algorithm scales well along the number of objectives. Abstract Complex network has become an important way to analyze the massive disordered information of complex systems, and its community structure property is indispensable to discover the potential functionality of these systems. The research on uncovering the community structure of networks has attracted great attentions from various fields in recent years. Many community detection approaches have been proposed based on the modularity optimization. Among them, the algorithms which optimize one initial solution to a better one are easy to get into local optima. Moreover, the algorithms which are susceptible to the optimized order are easy to obtain unstable solutions. In addition, the algorithms which simultaneously optimize a population of solutions have high computational complexity, and thus they are difficult to apply to practical problems. To solve the above problems, in this study, we propose a fast memetic algorithm with multi-level learning strategies for community detection by optimizing modularity. The proposed algorithm adopts genetic algorithm to optimize a population of solutions and uses the proposed multi-level learning strategies to accelerate the optimization process. The multi-level learning strategies are devised based on the potential knowledge of the node, community and partition structures of networks, and they work on the network at nodes, communities and network partitions levels, respectively. Extensive experiments on both benchmarks and real-world networks demonstrate that compared with the state-of-the-art community detection algorithms, the proposed algorithm has effective performance on discovering the community structure of networks. How to maintain the population diversity is an important issue in designing a multiobjective evolutionary algorithm. This paper presents an enhanced nondominated neighbor-based immune algorithm in which a multipopulation coevolutionary strategy is introduced for improving the population diversity. In the proposed algorithm, subpopulations evolve independently; thus the unique characteristics of each subpopulation can be effectively maintained, and the diversity of the entire population is effectively increased. Besides, the dynamic information of multiple subpopulations is obtained with the help of the designed cooperation operator which reflects a mutually beneficial relationship among subpopulations. Subpopulations gain the opportunity to exchange information, thereby expanding the search range of the entire population. Subpopulations make use of the reference experience from each other, thereby improving the efficiency of evolutionary search. Compared with several state-of-the-art multiobjective evolutionary algorithms on well-known and frequently used multiobjective and many-objective problems, the proposed algorithm achieves comparable results in terms of convergence, diversity metrics, and running time on most test problems. Structural balance is a large area of study in signed networks, and it is intrinsically a global property of the whole network. Computing global structural balance in signed networks, which has attracted some attention in recent years, is to measure how unbalanced a signed network is and it is a nondeterministic polynomial-time hard problem. Many approaches are developed to compute global balance. However, the results obtained by them are partial and unsatisfactory. In this study, the computation of global structural balance is solved as an optimization problem by using the Memetic Algorithm. The optimization algorithm, named Meme-SB, is proposed to optimize an evaluation function, energy function, which is used to compute a distance to exact balance. Our proposed algorithm combines Genetic Algorithm and a greedy strategy as the local search procedure. Experiments on social and biological networks show the excellent effectiveness and efficiency of the proposed method. This paper introduces a computational model simulating the dynamic process of human immune response to solve multidimensional knapsack problems. The new model is a quaternion (G, I, R, At), where G denotes exterior stimulus or antigen,denotes the set of valid antibodies, R denotes the set of reaction rules describing the interactions between antibodies, and Al denotes the dynamic algorithm describing how the reaction rules are applied to antibody population. The set of antibody-adjusting rules, the set of clonal selection rules, and a dynamic algorithm, named MKP-PAISA, are designed for solving multidimensional knapsack problems. The efficiency of the proposed algorithm was validated by testing on 57 benchmark problems and comparing with three genetic algorithms. The results indicated that the proposed algorithm was suitable for solving multidimensional knapsack problems. Multiobjective evolutionary algorithm based on decomposition (MOEA/D) has attracted a great deal of attention and has obtained enormous success in the field of evolutionary multiobjective optimization. It converts a multiobjective optimization problem (MOP) into a set of scalar optimization subproblems and then uses the evolutionary algorithm (EA) to optimize these subproblems simultaneously. However, there is a great deal of randomness in MOEA/D. Researchers in the field of evolutionary algorithm, reinforcement learning and neural network have reported that the simultaneous consideration of randomness and opposition has an advantage over pure randomness. A new scheme, called opposition-based learning (OBL), has been proposed in the machine learning field. In this paper, OBL has been integrated into the framework of MOEA/D to accelerate its convergence speed. Hence, our proposed approach is called opposition-based learning MOEA/D (MOEA/D-OBL). Compared with MOEA/D, MOEA/D-OBL uses an opposition-based initial population and opposition-based learning strategy to generate offspring during the evolutionary process. It is compared with its parent algorithm MOEA/D on four representative kinds of MOPs and many-objective optimization problems. Experimental results indicate that MOEA/D-OBL outperforms or performs similar to MOEA/D. Moreover, the parameter sensitivity of generalization opposite point and the probable to use OBL is experimentally investigated. A novel hybrid multiobjective estimation of distribution algorithm is proposed in this study. It combines an estimation of distribution algorithm based on local linear embedding and an immune inspired algorithm. Pareto set to the continuous multiobjective optimization problems, in the decision space, is a piecewise continuous (m-1)-dimensional manifold, where m is the number of objectives. By this regularity, a local linear embedding based manifold algorithm is introduced to build the distribution model of promising solutions. Besides, for enhancing local search ability of the EDA, an immune inspired sparse individual clone algorithm (SICA) is introduced and combined with the EDA. The novel hybrid multiobjective algorithm, named HMEDA, is proposed accordingly. Compared with three other state-of-the-art multiobjective algorithms, this hybrid algorithm achieves comparable results in terms of convergence and diversity. Besides, the tradeoff proportions of EDA to SICA in HMEDA are studied. Finally, the scalability to the number of decision variables of HMEDA is investigated too. This paper studies the replacement schemes in MOEA/D and proposes a new replacement named global replacement. It can improve the performance of MOEA/D. Moreover, trade-offs between convergence and diversity can be easily controlled in this replacement strategy. It also shows that different problems need different trade-offs between convergence and diversity. We test the MOEA/D with this global replacement on three sets of benchmark problems to demonstrate its effectiveness. Immune-inspired optimization algorithms encoded the parameters into individuals where each individual represents a search point in the space of potential solutions. A large number of parameters would result in a large search space. Nowadays, there is little report about immune algorithms effectively solving numerical optimization problems with more than 100 parameters. In this paper, we introduce an improved immune algorithm, termed as Dual-Population Immune Algorithm (DPIA), to solve large-scale optimization problems. DPIA adopts two side-by-side populations, antibody population and memory population. The antibody population employs the cloning, affinity maturation, and selection operators, which emphasizes the global search. The memory population stores current representative antibodies and the update of the memory population pay more attention to maintain the population diversity. Normalized decimal-string representation makes DPIA more suitable for solving large-scale optimization problems. Special mutation and recombination methods are adopted to simulate the somatic mutation and receptor editing process. Experimental results on eight benchmark problems show that DPIA is effective to solve large-scale numerical optimization problems. Artificial immune system (AIS) has been proven effective for pattern classification by its characteristics of learning and adaptability from the vertebrate immune system. However, little focus has been placed on the synthetic aperture radar (SAR) image segmentation by AIS. In this paper, we present an efficient automatic framework in AIS for SAR image segmentation. It aims at simultaneously solving the following three different crucial issues: (1) the automatic ability of searching true number of land-cover in SAR images; (2) the objective functions in guiding the segmentation of the images with complicated multiplicative noises; and (3) reduction of the computational complexity of segmenting SAR images with large sizes. By the proposed framework here, it can mitigate the above difficulties to a certain degree. Furthermore, a reasonable spatial filtering and watershed transformation are employed in the initial stage of the framework, and then a novel clustering index in Gaussian kernel is designed to lead the searching process, which is beneficial to find the partitions for the clustering problem with highly overlapping and contaminating samples. Besides, we also propose an efficient computing paradigm in AIS with variable length of chromosomes to search the optimal partitions of SAR images, which can find the optimal numbers of clusters automatically. Finally, in order to speed up the segmentation process of SAR image, we employ the histogram statistics to implement the pixels partition; therefore, the segmenting time is dependent on the small number of gray-levels, not the great amounts of whole image pixels. To test the segmentation performances of the proposed algorithm, a detailed experimental analysis was conducted on two simulated SAR images and four complicated real ones. Other four state-of-the-art image segmentation methods are employed for comparison, which are genetic clustering by variable string length encoding (VGA), fast generalized fuzzy C-means clustering (FGFCM), fuzzy local information C-means clustering algorithm (FLICM) and graph partitioning method: spectral clustering ensemble (SCE). Community structure is one of the most important properties in social networks. In dynamic networks, there are two conflicting criteria that need to be considered. One is the snapshot quality, which evaluates the quality of the community partitions at the current time step. The other is the temporal cost, which evaluates the difference between communities at different time steps. In this paper, we propose a decomposition-based multiobjective community detection algorithm to simultaneously optimize these two objectives to reveal community structure and its evolution in dynamic networks. It employs the framework of multiobjective evolutionary algorithm based on decomposition to simultaneously optimize the modularity and normalized mutual information, which quantitatively measure the quality of the community partitions and temporal cost, respectively. A local search strategy dealing with the problem-specific knowledge is incorporated to improve the effectiveness of the new algorithm. Experiments on computer-generated and real-world networks demonstrate that the proposed algorithm can not only find community structure and capture community evolution more accurately, but also be steadier than the two compared algorithms. Nonlocal means (NLM) algorithm has been proven to be an effective context-sensitive denoising approach, where many similar patches spatially far from a given patch could provide nonlocal constraint to the local structure. For hyperspectral image, however, the conventional NLM algorithm becomes inapplicable for the high number of spectral bands. In this letter, we incorporate the image nonlocal self-similarity into the maximum a posteriori estimation for hyperspectral classification. The main novelty lies in the following two aspects: The NLM algorithm is exploited to combine similar local structures and nonlocal averaging; a new class-relativity measurement is proposed to describe the self-similarity in the context of the hyperspectral classification. Several experiments on simulated and real hyperspectral data sets are provided to demonstrate the effectiveness of the proposed algorithm. Fuzzy c-means (FCM) algorithms have been shown effective for image segmentation. A series of enhanced FCM algorithms incorporating spatial information have been developed for reducing the effect of noises. This paper presents a robust FCM algorithm with non-local spatial information for image segmentation, termed as NLFCM. It incorporates two factors: one is the local similarity measure depending on the differences between the central pixel and its neighboring pixels in the image; the other is the non-local similarity measure depended on all pixels whose neighborhood configurations are similar to their neighborhood pixels. Furthermore, an adaptive weight is introduced to control the trade-off between local similarity measure and non-local similarity measure. The experimental results on synthetic images and real images under different types of noises show that the new algorithm is effective, and they are relatively independent to the types of noises. Abstract Image segmentation is an important and well-known ill-posed inverse problem in computer vision. It is a process of assigning a label to each pixel in a digital image so that pixels with the same label have similar characteristics. Chan\\u2013Vese model which belongs to partial differential equation approaches has been widely used in image segmentation tasks. Chan\\u2013Vese model has to optimize a non-convex problem. It usually converges to local minima. Furthermore, the length penalty item which is critical to the final results of Chan\\u2013Vese model makes the model be sensitive to parameter settings and costly in computation. In order to overcome these drawbacks, a novel bi-convex fuzzy variational image segmentation method is proposed. It is unique in two aspects: (1) introducing fuzzy logic to construct a bi-convex object function in order to simplify the procedure of finding global optima and (2) efficiently combining the length penalty item and the numerical remedy method to get better results and to bring robustness to parameter settings and greatly reduce computation costs. Experiments on synthetic, natural, medical and radar images have visually or quantitatively validated the superiorities of the proposed method compared with five state-of-the-art algorithms. In this paper, a novel change detection approach is proposed for multitemporal synthetic aperture radar (SAR) images. The approach is based on two difference images, which are constructed through intensity and texture information, respectively. In the extraction of the texture differences, robust principal component analysis technique is used to separate irrelevant and noisy elements from Gabor responses. Then graph cuts are improved by a novel energy function based on multivariate generalized Gaussian model for more accurately fitting. The effectiveness of the proposed method is proved by the experiment results obtained on several real SAR images data sets. Based on the mechanisms of immunodominance and clonal selection theory, we propose a new multiobjective optimization algorithm, immune dominance clonal multiobjective algorithm (IDCMA). IDCMA is unique in that its fitness values of current dominated individuals are assigned as the values of a custom distance measure, termed as Ab-Ab affinity, between the dominated individuals and one of the nondominated individuals found so far. According to the values of Ab-Ab affinity, all dominated individuals (antibodies) are divided into two kinds, subdominant antibodies and cryptic antibodies. Moreover, local search only applies to the subdominant antibodies, while the cryptic antibodies are redundant and have no function during local search, but they can become subdominant (active) antibodies during the subsequent evolution. Furthermore, a new immune operation, clonal proliferation is provided to enhance local search. Using the clonal proliferation operation, IDCMA reproduces individuals and selects their improved maturated progenies after local search, so single individuals can exploit their surrounding space effectively and the newcomers yield a broader exploration of the search space. The performan ce comparison of IDCMA with MISA, NSGA-II, SPEA, PAES, NSGA, VEGA, NPGA, and HLGA in solving six well-known multiobjective function optimization problems and nine multiobjective 0/1 knapsack problems shows that IDCMA has a good performance in converging to approximate Pareto-optimal fronts with a good distribution. An adaptive hybrid model (AHM) based on nondominated solutions is presented in this study for multi-objective optimization problems (MOPs). In this model, three search phases are devised according to the number of nondominated solutions in the current population: 1) emphasizing the dominated solutions when the population contains very few nondominated solutions; 2) maintaining the balance between nondominated and dominated solutions when nondominated ones become more; 3) when the population consists of adequate nondominated solutions, dominated ones could be ignored and the isolated nondominated ones are allocated more computational budget by their crowding distance values for heuristic search. To exploit local information efficiently, a local incremental search algorithm, LISA, is proposed and merged into the model. This model maintains the adaptive mechanism between the optimization process by the online discovered nondominated solutions. The proposed model is validated using five ZDT and five DTLZ problems. Compared with three other state-of-the-art multi-objective algorithms, namely NSGA-II, SPEA2, and PESA-II, AHM achieves comparable results in terms of convergence and diversity metrics. Finally, the sensitivity of introduced parameters and scalability to the number of objectives are investigated. In this paper, we establish a deep neural network using stacked Restricted Boltzmann Machines (RBMs) to analyze the difference images and detect changes between multitemporal synthetic aperture radar (SAR) images. Given the two multitemporal images, a difference image which shows difference degrees between corresponding pixels is generated. Then, RBMs are stacked to form a deep hierarchical neural network to learn to analyze the difference image and recognize the changed pixels and unchanged pixels. The learning process includes unsupervised layer-wise feature learning and supervised fine-tuning of network parameters. Unsupervised learning aims to learn the representation of the difference image. Supervised fine-tuning aims to learn to classify the changed and unchanged pixels. The network can learn from datasets that have few labeled data. The labeled data can be selected from the results obtained by other methods because there is no prior information in image change detection. The system learns to detect the changes instead of recognizing the changes by fixed equations as in traditional change detection algorithms. We test the network with real synthetic aperture radar datasets and the labeled samples are extracted from the results obtained, respectively, by several methods, including a thresholding method, a level set method and two clustering methods. The results achieved by the trained network outperform that of other methods. By replacing the selection component, a well researched evolutionary algorithm for scalar optimization problems (SOPs) can be directly used to solve multi-objective optimization problems (MOPs). Therefore, in most of existing multi-objective evolutionary algorithms (MOEAs), selection and diversity maintenance have attracted a lot of research effort. However, conventional reproduction operators designed for SOPs might not be suitable for MOPs due to the different optima structures between them. At present, few works have been done to improve the searching efficiency of MOEAs according to the characteristic of MOPs. Based on the regularity of continues MOPs, a Baldwinian learning strategy is designed for improving the nondominated neighbor immune algorithm and a multi-objective immune algorithm with Baldwinian learning (MIAB) is proposed in this study. The Baldwinian learning strategy extracts the evolving environment of current population by building a probability distribution model and generates a predictive improving direction by combining the environment information and the evolving history of the parent individual. Experimental results based on ten representative benchmark problems indicate that, MIAB outperforms the original immune algorithm, it performs better or similarly the other two outstanding approached NSGAII and MOEA/D in solution quality on most of the eight testing MOPs. The efficiency of the proposed Baldwinian learning strategy has also been experimentally investigated in this work. Abstract The biological immune system is a highly parallel and distributed adaptive system. The information processing abilities of the immune system provide important insights into the field of computation. Based on immunodominance in the biological immune system and the clonal selection mechanism, a novel data mining method, Immune Dominance Clonal Multiobjective Clustering algorithm (IDCMC), is presented. The algorithm divides an individual population into three sub-populations according to three different measurements, and adopts different evolution and selection strategies for each sub-population. The update of each sub-population, however, is not carried out in isolation. The periodic combination operation of the analysis of the three sub-populations represents considerable advantages in its global search ability. The clustering task is a multiobjective optimization problem, which is more robust with respect to the variety of cluster structures of different datasets than a single-objective clustering algorithm. In addition, the new algorithm can determine the number of clusters automatically, which should identify the most promising clustering solutions in the candidate set. The experimental results, using artificial datasets with different manifold structure and handwritten digit datasets, show that the IDCMC outperforms the PESA-II-based clustering method, the genetic algorithm-based clustering technique and the original K -Means algorithm in solving most of the problems tested. This letter presents a novel method based on wavelet fusion for change detection in synthetic aperture radar (SAR) images. The proposed approach is applied to generate the difference image (DI) by using complementary information from mean-ratio and log-ratio images. To restrain the background (unchanged areas) information and enhance the information of changed regions in the fused DI, fusion rules based on weight averaging and minimum standard deviation are chosen to fuse the wavelet coefficients for low- and high-frequency bands, respectively. Experiments on real SAR images confirm that the proposed approach does better than the mean-ratio, log-ratio, and Rayleigh-distribution-ratio operators. In this paper, based on Baldwin effect, an improved clonal selection algorithm, Baldwin clonal selection algorithm, termed as BCSA, is proposed to deal with complex multimodal optimization problems. BCSA evolves and improves antibody population by three operations: clonal proliferation operation, Baldwinian learning operation and clonal selection operation. By introducing Baldwin effect, BCSA can make the most of experience of antibodies, accelerate the convergence, and obtain the global optimization quickly. In experiments, BCSA is tested on four types of functions and compared with the clonal selection algorithm and other optimization methods. Experimental results indicate that BCSA achieves a good performance, and is also an effective and robust technique for optimization. Community structure is one of the most important properties in social networks, and community detection has received an enormous amount of attention in recent years. In dynamic networks, the communities may evolve over time so that pose more challenging tasks than in static ones. Community detection in dynamic networks is a problem which can naturally be formulated with two contradictory objectives and consequently be solved by multiobjective optimization algorithms. In this paper, a novel multiobjective immune algorithm is proposed to solve the community detection problem in dynamic networks. It employs the framework of nondominated neighbor immune algorithm to simultaneously optimize the modularity and normalized mutual information, which quantitatively measure the quality of the community partitions and temporal cost, respectively. The problem-specific knowledge is incorporated in genetic operators and local search to improve the effectiveness and efficiency of our method. Experimental studies based on four synthetic datasets and two real-world social networks demonstrate that our algorithm can not only find community structure and capture community evolution more accurately but also be more steadily than the state-of-the-art algorithms. Robustness and accuracy are the two main challenging problems in feature-based remote sensing image registration. In this letter, a novel point-matching algorithm is proposed. An improved random sample consensus (RANSAC) algorithm called fast sample consensus (FSC) is proposed. It divides the data set in RANSAC into two parts: the sample set and the consensus set. Sample set has high correct rate and consensus set has a large number of correct matches. An iterative method is put forward to increase the number of correct correspondences. A set of measures has been used to evaluate the registration result. The performance of the proposed method is validated on the evaluation of these measures and the mosaic images. FSC can get more correct matches than RANSAC in less number of iterations, iterative selection of correct matches algorithm and removal of the imprecise points algorithm effectively increase the accuracy of the result. Extensive experimental studies compared with three state-of-the-art methods prove that the proposed algorithm is robust and accurate. Image change detection is a process to analyze multi-temproal images of the same scene for identifying the changes that have occurred. In this paper, we propose a novel difference image analysis approach based on deep neural networks for image change detection problems. The deep neural network learning algorithm for classification includes unsupervised feature learning and supervised fine-tuning. Some samples with the labels of high accuracy obtained by a pre-classification are used for fine-tuning. Since a deep neural network can learn complicated functions that can represent high-level abstractions, it can obtain satisfactory results. Theoretical analysis and experiment results on real datasets show that the proposed method outperforms some other methods. Research on network structural balance has been of great concern to scholars from diverse fields. In this paper, a two-step approach is proposed for the first time to address the network structural balance problem. The proposed approach involves evolutionary multiobjective optimization, followed by model selection. In the first step, an improved version of the multiobjective discrete particle swarm optimization framework developed in our previous work is suggested. The suggested framework is then employed to implement network multiresolution clustering. In the second step, a problem-specific model selection strategy is devised to select the best Pareto solution (PS) from the Pareto front produced by the first step. The best PS is then decoded into the corresponding network community structure. Based on the discovered community structure, imbalanced edges are determined. Afterward, imbalanced edges are flipped so as to make the network structurally balanced. Extensive experiments on synthetic and real-world signed networks demonstrate the effectiveness of the proposed approach. Social computing is a new paradigm for information and communication technology. Social network analysis is one of the theoretical underpinnings of social computing. Community structure detection is believed to be an effective tool for social network analysis. Uncovering community structures in social networks can be regarded as clustering optimization problems. Because social networks are characterized by dynamics and huge volumes of data, conventional nature-inspired optimization algorithms encounter serious challenges when applied to solving large-scale social network clustering optimization problems. In this study, we put forward a novel particle swarm optimization algorithm to reveal community structures in social networks. The particle statuses are redefined under a discrete scenario. The status updating rules are reconsidered based on the network topology. A greedy strategy is designed to drive particles to a promising region. To this end, a greedy discrete particle swarm optimization framework for large-scale social network clustering is suggested. To determine the performance of the algorithm, extensive experiments on both synthetic and real-world social networks are carried out. We also compare the proposed algorithm with several state-of-the-art network community clustering methods. The experimental results demonstrate that the proposed method is effective and promising for social network clustering. The Yellow River Estuary area of China is under great pressure from both human intervention and natural processes. For analysis of the changes in this area, this article presents a novel change-detection method based on a local fit-search model and kernel-induced graph cuts in multitemporal synthetic aperture radar images. Change detection involves assigning a label to every pixel. This task is naturally formulated in terms of energy minimization, which can be effectively solved by graph cuts. The difference image is transformed implicitly by a kernel function so that an alternative to complex modelling of the original data makes the piecewise constant model become applicable for graph cuts formulation. An issue is that graph cuts are sensitive to the initial estimate. The local fit-search model is proposed to approximate to the local histogram while selecting an optimal threshold for the initial labelling, which leads to an effective constraint for graph cuts and computational benefits as well. Visual an... In this paper, we propose a novel Fast Affinity Propagation clustering approach (FAP). FAP simultaneously considers both local and global structure information contained in datasets, and is a high-quality multilevel graph partitioning method that can implement both vector-based and graph-based clustering. First, a new Fast Sampling algorithm (FS) is proposed to coarsen the input sparse graph and choose a small number of final representative exemplars. Then a density-weighted spectral clustering method is presented to partition those exemplars on the global underlying structure of data manifold. Finally, the cluster assignments of all data points can be achieved through their corresponding representative exemplars. Experimental results on two synthetic datasets and many real-world datasets show that our algorithm outperforms the state-of-the-art original affinity propagation and spectral clustering algorithms in terms of speed, memory usage, and quality on both vector-based and graph-based clustering. This paper addresses the problem of globally stable direct adaptive backstepping neural network (NN) tracking control design for a class of uncertain strict-feedback systems under the assumption that the accuracy of the ultimate tracking error is given a priori . In contrast to the classical adaptive backstepping NN control schemes, this paper analyzes the convergence of the tracking error using Barbalat\\u2019s Lemma via some nonnegative functions rather than the positive-definite Lyapunov functions. Thus, the accuracy of the ultimate tracking error can be determined and adjusted accurately a priori , and the closed-loop system is guaranteed to be globally uniformly ultimately bounded. The main technical novelty is to construct three new $n$ th-order continuously differentiable functions, which are used to design the control law, the virtual control variables, and the adaptive laws. Finally, two simulation examples are given to illustrate the effectiveness and advantages of the proposed control method. Change detection in synthetic aperture radar SAR images has become increasingly important along with the development of SAR techniques. In this study, a novel locally fitting and expectation-maximization EM approach is proposed for unsupervised change detection tasks in SAR images. A difference image is generated first, and an in-depth study of the inherent characteristics of the histogram of the difference image is then made. Thus, the new approach is proposed corresponding to these characteristics. In this approach, the locally fitting model orientated to deal with the unchanged class is put forward to reach a best fit, and the semi-EM algorithm for the changed class is used to tackle the phenomenon of overlapping. Then, through the Bayesian decision rule, the optimal threshold is determined. Our contributions lie in two aspects. First, in the locally fitting model, the location of the optimal threshold can be determined, which leads to an accurate fit over a short interval. Second, the use of the semi-EM algorithm not only retains the efficacy of the EM algorithm to cope with overlapping for the changed class but also simplifies the computing process. Experiments on real data sets confirm the effectiveness of the proposed approach, which results in final maps very similar to the ground truth and is more effective in determining the optimal threshold in comparison with others. The experimental results also demonstrate its effectiveness when the changed areas are of different geometrical shapes. In this study, we introduced a novel multiobjective optimization algorithm, Nondominated Neighbor Immune Algorithm (NNIA), to solve the multiobjective clustering problems. NNIA solves multiobjective optimization problems by using a nondominated neighbor-based selection technique, an immune inspired operator, two heuristic search operators and elitism. The main novelty of NNIA is that the selection technique only selects minority isolated nondominated individuals in current population to clone proportionally to the crowding-distance values, recombine and mutate. As a result, NNIA pays more attention to the less-crowded regions in the current trade-off front. The experimental results on seven artificial data sets with different manifold structure and six real-world data sets show that the NNIA is an effective algorithm for solving multiobjective clustering problems, and the NNIA based multiobjective clustering technique is a cogent unsupervised learning method. In this paper, we present an improved fuzzy C-means (FCM) algorithm for image segmentation by introducing a tradeoff weighted fuzzy factor and a kernel metric. The tradeoff weighted fuzzy factor depends on the space distance of all neighboring pixels and their gray-level difference simultaneously. By using this factor, the new algorithm can accurately estimate the damping extent of neighboring pixels. In order to further enhance its robustness to noise and outliers, we introduce a kernel distance measure to its objective function. The new algorithm adaptively determines the kernel parameter by using a fast bandwidth selection rule based on the distance variance of all data points in the collection. Furthermore, the tradeoff weighted fuzzy factor and the kernel distance measure are both parameter free. Experimental results on synthetic and real images show that the new algorithm is effective and efficient, and is relatively independent of this type of noise. In this paper, we propose a novel unsupervised evolutionary clustering algorithm for mixed type data, evolutionary k-prototype algorithm (EKP). As a partitional clustering algorithm, k-prototype (KP) algorithm is a well-known one for mixed type data. However, it is sensitive to initialization and converges to local optimum easily. Global searching ability is one of the most important advantages of evolutionary algorithm (EA), so an EA framework is introduced to help KP overcome its flaws. In this study, KP is applied as a local search strategy, and runs under the control of the EA framework. Experiments on synthetic and real-life datasets show that EKP is more robust and generates much better results than KP for mixed type data. In this paper, an interactive version of the decomposition based multiobjective evolutionary algorithm (iMOEA/D) is proposed for interaction between the decision maker (DM) and the algorithm. In MOEA/D, a multi-objective problem (MOP) can be decomposed into several single-objective sub-problems. Thus, the preference incorporation mechanism in our algorithm is implemented by selecting the preferred sub-problems rather than the preferred region in the objective space. At each interaction, iMOEA/D offers a set of current solutions and asks the DM to choose the most preferred one. Then, the search will be guided to the neighborhood of the selected. iMOEA/D is tested on some benchmark problems, and various utility functions are used to simulate the DM's responses. The experimental studies show that iMOEA/D can handle the preference information very well and successfully converge to the expected preferred regions. Community structure is one of the most important properties in complex networks, and the field of community detection has received an enormous amount of attention in the past several years. Many quality metrics and methods have been proposed for revealing community structures at multiple resolution levels, while most existing methods need a tunable parameter in their quality metrics to determine the resolution level in advance. In this study, a multi-objective evolutionary algorithm (MOEA) for revealing multi-resolution community structures is proposed. The proposed MOEA-based community detection algorithm aims to find a set of tradeoff solutions which represent network partitions at different resolution levels in a single run. It adopts an efficient multi-objective immune algorithm to simultaneously optimize two contradictory objective functions, Modified Ratio Association and Ratio Cut. The optimization of Modified Ratio Association tends to divide a network into small communities, while the optimization of Ratio Cut tends to divide a network into large communities. The simultaneous optimization of these two contradictory objectives returns a set of tradeoff solutions between the two objectives. Each of these solutions corresponds to a network partition at one resolution level. Experiments on artificial and real-world networks show that the proposed method has the ability to reveal community structures of networks at different resolution levels in a single run. Modern science of networks has facilitated us with enormous convenience to the understanding of complex systems. Community structure is believed to be one of the notable features of complex networks representing real complicated systems. Very often, uncovering community structures in networks can be regarded as an optimization problem, thus, many evolutionary algorithms based approaches have been put forward. Particle swarm optimization (PSO) is an artificial intelligent algorithm originated from social behavior such as birds flocking and fish schooling. PSO has been proved to be an effective optimization technique. However, PSO was originally designed for continuous optimization which confounds its applications to discrete contexts. In this paper, a novel discrete PSO algorithm is suggested for identifying community structures in signed networks. In the suggested method, particles' status has been redesigned in discrete form so as to make PSO proper for discrete scenarios, and particles' updating rules have been reformulated by making use of the topology of the signed network. Extensive experiments compared with three state-of-the-art approaches on both synthetic and real-world signed networks demonstrate that the proposed method is effective and promising. Lamarckian learning has been introduced into evolutionary computation as local search mechanism. The relevant research topic, memetic computation, has received significant amount of interests. In this study, a novel Lamarckian learning strategy is designed for improving the Nondominated Neighbor Immune Algorithm, a novel hybrid multi-objective optimization algorithm, Multi-objective Lamarckian Immune Algorithm (MLIA), is proposed. The Lamarckian learning performs a greedy search which proceeds towards the goal along the direction obtained by Tchebycheff approach and generates the improved progenies or improved decision vectors, so single individual will be optimized locally and the newcomers yield an enhanced exploitation around the nondominated individuals in less-crowded regions of the current trade-off front. Simulation results based on twelve benchmark problems show that MLIA outperforms the original immune algorithm and NSGA-II in approximating Pareto-optimal front in most of the test problems. When compared with the state of the art algorithm MOEA/D, MLIA shows better performance in terms of the coverage of two sets metric, although it is laggard in the hypervolume metric. Recent years have witnessed a growing recognition on the community detection in networks. Diverse techniques have been devoted to uncovering community structures in complex networks and amongst which are the density-based methods. Density-based avenues are very popular in data clustering field. They rely on two parameters which are utilized by us to process the community detection problem. In this paper, a novel view to look deep into the network structure from the community level is tested and a heuristic density-based approach for community detection is put forward. In the proposed method, firstly, both of the two parameters are under consideration and all the possible parameter pairs are exploited. These parameter pairs produce all kinds of partitions through the classic method. Secondly, these partitions are processed by our proposed strategy consisting of classification, mergence, decomposition and recombination. After employing the proposed strategy, a community division with high quality is uncovered. Experiments on both synthetic and real-world networks demonstrate the effectiveness of the proposed method. Traditional recommendation techniques in recommender systems mainly focus on improving recommendation accuracy. However, personalized recommendation, which considers the multiple needs of users and can make both accurate and diverse recommendations, is more suitable for modern recommender systems. In this paper, the task of personalized recommendation is modeled as a multi-objective optimization problem. A multiobjective recommendation model is proposed. The proposed model maximizes two conflicting performance metrics termed as accuracy and diversity. The accuracy is evaluated by the probabilistic spreading method, while the diversity is measured by recommendation coverage. The proposed MOEA-based recommendation method can simultaneously provide multiple recommendations for multiple users in only one run. Our experimental results demonstrate the effectiveness of the proposed algorithm. Comparison experiments also indicate that the proposed algorithm can make more diverse yet accurate recommendations. Artificial immune systems (AIS) are a kind of new computational intelligence methods which draw inspiration from the human immune system. In this study, we introduce an AIS-based optimization algorithm, called clonal selection algorithm, to solve the multi-user detection problem in code-division multiple-access communications system based on the maximum-likelihood decision rule. Through proportional cloning, hypermutation, clonal selection and clonal death, the new method performs a greedy search which reproduces individuals and selects their improved maturated progenies after the affinity maturation process. Theoretical analysis indicates that the clonal selection algorithm is suitable for solving the multi-user detection problem. Computer simulations show that the proposed approach outperforms some other approaches including two genetic algorithm-based detectors and the matched filters detector, and has the ability to find the most likely combinations. Artificial immune systems (AIS) are the computational systems inspired by the principles and processes of the vertebrate immune system. AIS-based algorithms typically mimic the human immune system's characteristics of learning and adaptability to solve some complicated problems. Here, an artificial immune multi-objective optimization framework is formulated and applied to synthetic aperture radar (SAR) image segmentation. The important innovations of the framework are listed as follows: (1) an efficient and robust immune, multi-objective optimization algorithm is proposed, which has the features of adaptive rank clones and diversity maintenance by K-nearest-neighbor list; (2) besides, two conflicting, fuzzy clustering validity indices are incorporated into this framework and optimized simultaneously and (3) moreover, an effective, fused feature set for texture representation and discrimination is constructed and researched, which utilizes both the Gabor filter's ability to precisely extract texture features in low- and mid-frequency components and the gray level co-occurrence probability's (GLCP) ability to measure information in high-frequency. Two experiments with synthetic texture images and SAR images are implemented to evaluate the performance of the proposed framework in comparison with other five clustering algorithms: fuzzy C-means (FCM), single-objective genetic algorithm (SOGA), self-organizing map (SOM), wavelet-domain hidden Markov models (HMTseg), and spectral clustering ensemble (SCE). Experimental results show the proposed framework has obtained the better performance in segmenting SAR images than other five algorithms and behaves insensitive to the speckle noise. Fuzzy c-means (FCM) algorithm has been proven effective for image segmentation; nevertheless it is sensitive to different types of noises. Up to now, a series of improved FCM algorithms incorporating spatial information have been developed, which are robust for Gaussian, uniform, and salt and pepper noises. However, limited effort has been placed on tackling the problem of a large amount of intrinsic and undesired multiplicative speckle in synthetic aperture radar (SAR) images. A crucial problem for SAR image segmentation is to guarantee speckle insensitiveness and edge detail preservation simultaneously. To address this problem, a robust and specific non-local FCM algorithm with edge preservation for SAR image segmentation is proposed. In this study, a new image is constructed using the non-local information and rectifying the edge parts, which is robust for speckle without sacrificing edge sharpness. To measure the patch-similarity in non-local method effectively, a novel generalized ratio distance based on SAR multiplicative speckle is defined. To locate and rectify the edge parts, coefficient of variation (CV) based threshold and orientation based statistics methods are designed. At last, this new image is clustered by FCM algorithm. Compared with six improved FCM algorithms and two state-of-the-art segmentation algorithms (spectral clustering and normalized cuts), the proposed algorithm obtains the best performance in terms of region uniformity and boundary localization. In this letter, we proposed a novel band selection algorithm for hyperspectral images (HSIs) based on column subset selection. The main idea of the proposed algorithm comes from the column subset selection problem in numerical linear algebra. It selects a group of bands, which maximizes the volume of the selected subset of columns. Since the high dimensionality decreases the contrast between bands, we use Manhattan distance to obtain a higher selection quality. Experimental results on real HSIs show that the proposed algorithm obtains competitively good results, in terms of classification accuracy, and is robust to noisy bands. Artificial immune systems are a kind of new computational intelligence methods which draw inspiration from the human immune system. Most immune system inspired optimization algorithms are based on the applications of clonal selection and hypermutation, and known as clonal selection algorithms. These clonal selection algorithms simulate the immune response process based on principles of Darwinian evolution by using various forms of hypermutation as variation operators. The generation of new individuals is a form of the trial and error process. It seems very wasteful not to make use of the Baldwin effect in immune system to direct the genotypic changes. In this paper, based on the Baldwin effect, an improved clonal selection algorithm, Baldwinian Clonal Selection Algorithm, termed as BCSA, is proposed to deal with optimization problems. BCSA evolves and improves antibody population by four operators, clonal proliferation, Baldwinian learning, hypermutation, and clonal selection. It is the first time to introduce the Baldwinian learning into artificial immune systems. The Baldwinian learning operator simulates the learning mechanism in immune system by employing information from within the antibody population to alter the search space. It makes use of the exploration performed by the phenotype to facilitate the evolutionary search for good genotypes. In order to validate the effectiveness of BCSA, eight benchmark functions, six rotated functions, six composition functions and a real-world problem, optimal approximation of linear systems are solved by BCSA, successively. Experimental results indicate that BCSA performs very well in solving most of the test problems and is an effective and robust algorithm for optimization. Based on the concepts and principles of quantum computing, a quantum-inspired evolutionary algorithm for data clustering (QECA) is proposed in this paper. And a novel distance measurement index called manifold distance is introduced. These attribute data are the main source of clustering problem, due to its complex distribution, most clustering algorithms available are only suitable for these types of characteristic data. In this study, a new algorithm which can deal with these data with manifold distribution is more effective. The main motives of using QECA consist in searching for appropriate cluster center so that a similarity metric of clusters are optimized more quickly and effectively. The superiority of QECA over fuzzy c-means (FCM) algorithm and immune evolutionary clustering algorithm (IECA) is extensively demonstrated in our experiments. In this paper, a novel constrained multiobjective immune algorithm for optimizing detector distribution in V-detector negative selection is proposed. The theory of artificial immune system (AIS) and the spirit of population evolution are introduced to generate detectors. By combining the constraint handling technique and AIS-based multiobjective optimization, the algorithm is able to steadily maximize the anomaly coverage with little extra cost, which means the distribution with maximized coverage of the non-self space and minimized overlapping among detectors with fixed size will be well realized. Furthermore, the new approach is tested on some benchmark problems. The experimental results show that compared with some state-of-the-art methods, our algorithm can remarkably outperform them in terms of enhancing the detection rate by optimizing distribution without increasing the number of detectors. Clustering validity indices play a core role in the unsupervised pattern classification. To date, some indices have been proposed and their individual performances were compared on different artificial clustering test instances and real image segmentation tasks. However, little focus has been placed on the issue that how about their combinational performances are, although multi-objective optimization has attracted much interest from current researchers in unsupervised classification. Here, we firstly evaluate the performance of five state-of-the-art clustering indices with different characteristics in a single-objective optimization algorithm designed in artificial immune system AIS. Then, a multi-objective optimization algorithm in AIS with fair ability of adaptability and diversity maintaining is introduced in this study. After that, the clustering performances of combinational clustering indices are investigated in the multi-objective optimization framework. To test the effectiveness of the combinational clustering validity indices, 27 benchmark functions with different geometric structures and three complicated remote sensing images are employed in this study. Based on the computer simulations, some meaningful empirical guidelines are obtained for selecting the suitable combinational clustering indices for formulating an effective and robust multi-objective clustering algorithm in different clustering tasks. An unsupervised change detection method based on an improved rough fuzzy c-means clustering method (SRFPCM) for synthetic aperture radar and optical remote sensing images is proposed. SRFPCM incorporates the local spatial information and gray level information in a novel fuzzy way, aiming at guaranteeing noise insensitiveness and image detail preservation. Inspired by the idea of a robust fuzzy local information c-means clustering algorithm, this new algorithm can overcome the disadvantages of rough fuzzy c-means clustering algorithm and enhance the clustering performance at the same time. SRFPCM is employed to cluster the difference image into two clusters (changed and unchanged regions) and get the change map. Experimental results confirm the effectiveness of the proposed algorithm. The field of complex network clustering has been very active in the past several years. In this paper, a discrete framework of the particle swarm optimization algorithm is proposed. Based on the proposed discrete framework, a multiobjective discrete particle swarm optimization algorithm is proposed to solve the network clustering problem. The decomposition mechanism is adopted. A problem-specific population initialization method based on label propagation and a turbulence operator are introduced. In the proposed method, two evaluation objectives termed as kernel k-means and ratio cut are to be minimized. However, the two objectives can only be used to handle unsigned networks. In order to deal with signed networks, they have been extended to the signed version. The clustering performances of the proposed algorithm have been validated on signed networks and unsigned networks. Extensive experimental studies compared with ten state-of-the-art approaches prove that the proposed algorithm is effective and promising. Spectral clustering (SC) has been used with success in the field of computer vision for data clustering. In this paper, a new algorithm named SC ensemble (SCE) is proposed for the segmentation of synthetic aperture radar (SAR) images. The gray-level cooccurrence matrix-based statistic features and the energy features from the undecimated wavelet decomposition extracted for each pixel being the input, our algorithm performs segmentation by combining multiple SC results as opposed to using outcomes of a single clustering process in the existing literature. The random subspace, random scaling parameter, and Nystrom approximation for component SC are applied to construct the SCE. This technique provides necessary diversity as well as high quality of component learners for an efficient ensemble. It also overcomes the shortcomings faced by the SC, such as the selection of scaling parameter, and the instability resulted from the Nystrom approximation method in image segmentation. Experimental results show that the proposed method is effective for SAR image segmentation and insensitive to the scaling parameter. In this paper, we introduce Lamarckian learning theory into the Clonal Selection Algorithm and propose a sort of Lamarckian Clonal Selection Algorithm, termed as LCSA. The major aim is to utilize effectively the information of each individual to reinforce the exploitation with the help of Lamarckian local search. Recombination operator and tournament selection operator are incorporated into LCSA to further enhance the ability of global exploration. We compare LCSA with the Clonal Selection Algorithm in solving twenty benchmark problems to evaluate the performance of LCSA. The results demonstrate that the Lamarckian local search makes LCSA more effective and efficient in solving numerical optimization problems. Based on the antibody clonal selection theory of immunology and differential evolution paradigm, we put forward a novel artificial immune system algorithm, differential immune clonal selection algorithm, termed as DICSA. The DICSA evolves and improves the population by clonal proliferation operator, differential mutation operator, differential crossover operator and (mu+lambda)-selection operator. In experiments, six composition functions are used to validate the performance of the DICSA. The results confirm its potential to solve the global optimization problems. In this letter, a statistical active contour model exploiting the local information is proposed for classifying changed and unchanged regions in the difference image. It incorporates the local information and fuzzy logic for the purpose of enhancing the changed information and of reducing the effect of speckle noise. The fuzzy length term and the fuzzy penalty term are added into the fuzzy energy function. In particular, in order to avoid deriving the complex fuzzy membership updating function, we solve the Euler-Lagrange equation to minimize the fuzzy energy function instead of calculating the fuzzy energy alterations directly. The experimental results on three synthetic aperture radar images confirm the performance of the proposed model over some existing methods. Lymph Node Metastasis (LNM) in gastric cancer is an important prognostic factor regarding long-term survival. As it is difficult for doctors to combine multiple factors for a comprehensive analysis, Clinical Decision Support System (CDSS) is desired to help the analysis. In this paper, a novel Bi-level Belief Rule Based (BBRB) prototype CDSS is proposed. The CDSS consists of a two-layer Belief Rule Base (BRB) system. It can be used to handle uncertainty in both clinical data and specific domain knowledge. Initial BRBs are constructed by domain specific knowledge, which may not be accurate. Traditional methods for optimizing BRB are sensitive to initialization and are limited by their weak local searching abilities. In this paper, a new Clonal Selection Algorithm (CSA) is proposed to train a BRB system. Based on CSA, efficient global search can be achieved by reproducing individuals and selecting their improved maturated progenies after the affinity maturation process. The proposed prototype CDSS is validated using a set of real patient data and performs extremely well. In particular, BBRB is capable of providing more reliable and informative diagnosis than a single-layer BRB system in the case study. Compared with conventional optimization method, the new CSA could improve the diagnostic performance further by trying to avoid immature convergence to local optima. Based on the Antibody Clonal Selection Theory of immunology, we put forward a novel clonal selection algorithm for multiuser detection in Code-division Multiple-access Systems. By using the clonal selection operator, the new algorithm can carry out the global search and the local search in many directions rather than one direction around the same individual simultaneously. After discussing the main characters of the new algorithm, especially the convergence and complexity, the performance of the proposed receiver, named by CAMUD, is evaluated via computer simulations and compared to that of other suboptimal schemes as well as to that of Optimal Multiuser detector (OMD) and conventional detector in CDMA systems over Multi-Path Channels. When compared with the OMD scheme, the CAMUD is capable of reducing the computational complexity significantly. On the other hand, when compared with standard genetic algorithm and improved genetic algorithm, theoretical analysis and Monte Carlo simulations show that the CAMUD with same complexity has optimal performance in eliminating MAI and \\\"near-far\\\" resistance. The simulations also show that the CAMUD performs quite well even when the number of active users and the length of the transmitted packet are considerably large. Hierarchical deep neural networks are currently popular learning models for imitating the hierarchical architecture of human brain. Single-layer feature extractors are the bricks to build deep networks. Sparse feature learning models are popular models that can learn useful representations. But most of those models need a user-defined constant to control the sparsity of representations. In this paper, we propose a multiobjective sparse feature learning model based on the autoencoder. The parameters of the model are learnt by optimizing two objectives, reconstruction error and the sparsity of hidden units simultaneously to find a reasonable compromise between them automatically. We design a multiobjective induced learning procedure for this model based on a multiobjective evolutionary algorithm. In the experiments, we demonstrate that the learning procedure is effective, and the proposed multiobjective model can learn useful sparse features. The study of complex networks has received an enormous amount of attention from the scientific community in recent years. In this paper, we propose a multi-objective approach, named NNIA-Net, to discover communities in networks by employing Non-dominated Neighbor Immune Algorithm (NNIA). Our algorithm optimizes two objectives to find communities in networks' groups of vertices within which connections are dense, but between which connections are sparser. The method can produce a series of solutions which represent various divisions to the networks at different hierarchical levels. The number of subdivisions is automatically determined by the non-dominated individuals resulting from our algorithm. We demonstrate that our algorithm is highly efficient at discovering quality community structure in both synthetic and real-world network data. What's more, a new initialization method is proposed to improve the traditional initialization method by about 30% in running time. Based on the Antibody Clonal Selection Theory of immunology, we put forward a novel artificial immune system algorithm, Immune Clonal Selection Algorithm for Multiuser Detection in DS-CDMA Systems The performance of the new detector, named by ICSMUD, is evaluated via computer simulations When compared with Optimal Multiuser detection, ICSMUD can reduce the computational complexity significantly When compared with detectors based on Standard Genetic Algorithm and A Novel Genetic Algorithm, ICSMUD has the best performance in eliminating multiple-access interference and \\u201cnear-far\\u201d resistance and performs quite well even when the number of active users and the packet length are considerably large. Change detection is an important technique in damage assessment area. As the amount of remote sensing images and the complexity of algorithms rise, the demand for processing power is increasing. In this paper, we propose PLog-FLCM, a parallel algorithm for change detection. It is implemented on AMD Accelerated Parallel Processing (APP) SDK v2 based on Open Computing Language. The parallel characteristics and implementation details of the proposed PLog-FLICM algorithm are presented. Experiments on several Synthetic Aperture Radar(SAR) images demonstrate that the proposed algorithm outperform other algorithms, and the designed parallel algorithm can greatly reduce the computational time of change detection algorithm. It has achieved speedups of between 63 and 145 times on AMD Radeon HD 6870 Graphics Processing Unit (GPU). Based on the concepts and principles of quantum computing, a novel immune clonal algorithm, called a quantum-inspired immune clonal algorithm (QICA), is proposed to deal with the problem of global optimization. In QICA, the antibody is proliferated and divided into a set of subpopulation groups. The antibodies in a subpopulation group are represented by multistate gene quantum bits. In the antibody's updating, the general quantum rotation gate strategy and the dynamic adjusting angle mechanism are applied to accelerate convergence. The quantum NOT gate is used to realize quantum mutation to avoid premature convergences. The proposed quantum recombination realizes the information communication between subpopulation groups to improve the search efficiency. Theoretical analysis proves that QICA converges to the global optimum. In the first part of the experiments, 10 unconstrained and 13 constrained benchmark functions are used to test the performance of QICA. The results show that QICA performs much better than the other improved genetic algorithms in terms of the quality of solution and computational cost. In the second part of the experiments, QICA is applied to a practical problem (i.e., multiuser detection in direct-sequence code-division multiple-access systems) with a satisfying result. We provide a position-patch based face hallucination method using convex optimization. Recently, a novel position-patch based face hallucination method has been proposed to save computational time and achieve high-quality hallucinated results. This method has employed least square estimation to obtain the optimal weights for face hallucination. However, the least square estimation approach can provide biased solutions when the number of the training position-patches is much larger than the dimension of the patch. To overcome this problem, this letter proposes a new position-patch based face hallucination method which is based on convex optimization. Experimental results demonstrate that our method is very effective in producing high-quality hallucinated face images. In this paper, we introduce Lamarckian learning theory into the clonal selection algorithm and propose a sort of Lamarckian clonal selection algorithm, termed as LCSA. The major aim is to utilize effectively the information of each individual to reinforce the exploitation with the help of Lamarckian local search. Recombination operator and tournament selection operator are incorporated into LCSA to further enhance the ability of global exploration. We compared LCSA with the clonal selection algorithm (CSA) in solving twenty benchmark problems to test the performance of LCSA. The results demonstrate that LCSA is effective and efficient in solving numerical optimization problems. In order to solve the image segmentation problem which assigns a label to every pixel in an image such that pixels with the same label share certain visual characteristics more effectively, a novel approach based on memetic algorithm (MISA) is proposed. Watershed segmentation is applied to segment original images into non-overlap small regions before performing the portioning process by MISA. MISA adopts a straightforward representation method to find the optimal combination of watershed regions under the criteria of interclass variance in feature space. After implementing cluster-based crossover and mutation, an individual learning procedure moves exocentric regions in current cluster to the one they should belong to according to the distance between these regions and cluster centers in feature space. In order to evaluate the new algorithm, six texture images, three remote sensing images and three natural images are employed in experiments. The experimental results show that MISA outperforms its genetic version, the Fuzzy c-means algorithm, and K-means algorithm in partitioning most of the test problems, and is an effective approach when compared with two state-ofthe-art image segmentation algorithms including an efficient graph-based algorithm and a spectral clustering ensemble-based algorithm. This paper provides a novel pointwise-adaptive speckle filter based on local homogeneous-region segmentation with pixel-relativity measurement. A ratio distance is proposed to measure the distance between two speckled-image patches. The theoretical proofs indicate that the ratio distance is valid for multiplicative speckle, while the traditional Euclidean distance failed in this case. The probability density function of the ratio distance is deduced to map the distance into a relativity value. This new relativity-measurement method is free of parameter setting and more functional compared with the Gaussian kernel-projection-based ones. The new measurement method is successfully applied to segment a local shape-adaptive homogeneous region for each pixel, and a simplified strategy for the segmentation implementation is given in this paper. After segmentation, the maximum likelihood rule is introduced to estimate the true signal within every homogeneous region. A novel evaluation metric of edge-preservation degree based on ratio of average is also provided for more precise quantitative assessment. The visual and numerical experimental results show that the proposed filter outperforms the existing state-of-the-art despeckling filters. In this study, a novel clustering-based selection strategy of nondominated individuals for evolutionary multi-objective optimization is proposed. The new strategy partitions the nondominated individuals in current Pareto front adaptively into desired clusters. Then one representative individual will be selected in each cluster for pruning nondominated individuals. In order to evaluate the validity of the new strategy, we apply it into one state of the art multi-objective evolutionary algorithm. The experimental results based on thirteen benchmark problems show that the new strategy improves the performance obviously in terms of breadth and uniformity of nondominated solutions. Graphical abstractIn this paper, we put forward a multiobjective fuzzy clustering method for change detection in SAR images. As shown in the figure, the change detection problem is modeled as a multiobjective optimization problem, and two conflicting objective functions are constructed from the perspective of preserving detail and removing noise, respectively. A number of solutions representing different trade-off relationships between preserving detail and restraining noise are given by the proposed method. The decision makers can judge relatively and select one or more solutions according to the problem requirements. Display Omitted On account of the presence of speckle noise, the trade-off between removing noise and preserving detail is crucial for the change detection task in Synthetic Aperture Radar (SAR) images. In this paper, we put forward a multiobjective fuzzy clustering method for change detection in SAR images. The change detection problem is modeled as a multiobjective optimization problem, and two conflicting objective functions are constructed from the perspective of preserving detail and removing noise, respectively. We optimize the two constructed objective functions simultaneously by using a multiobjective fuzzy clustering method, which updates the membership values according to the weights of the two objectives to find the optimal trade-off. The proposed method obtains a set of solutions with different trade-off relationships between the two objectives, and users can choose one or more appropriate solutions according to requirements for diverse problems. Experiments conducted on real SAR images demonstrate the superiority of the proposed method. This paper aims at reducing the potential conflicts in real-world signed networks.We model the computation of structural balance as the optimization of energy index.We present a more general energy function Hw incorporated with transformation cost.We model the transformation of structural balance as the optimization of Hw.We propose a fast memetic algorithm to solve the modeled optimization problems. Structural balance enables a comprehensive understanding of the potential tensions and conflicts of signed networks, and its computation and transformation have attracted increasing attention in recent years. The balance computation aims at evaluating the distance from an unbalanced network to a balanced one, and the balance transformation is to convert an unbalanced network into a balanced one. In this paper, firstly, we model the balance computation of signed networks as the optimization of an energy function. Secondly, we model the balance transformation as the optimization of a more general energy function incorporated with transformation cost. Finally, a multilevel learning based memetic algorithm, which incorporates network-specific knowledge such as the neighborhoods of node, cluster and partition, is proposed to solve the modeled optimization problems. Systematical experiments in real-world social networks demonstrate the superior performance of the proposed algorithm compared with the state-of-the-art algorithms on the computation and transformation of structural balance. The results also show that our method can resolve the potential conflicts of signed networks with the minimum cost. Lower and upper approximations of a set. In real applications, it may be impossible to obtain complete information of a given pattern set. Uncertain information will cause imperfect description for a pattern set in various pattern recognition algorithms. It is natural to expend great effort on analyzing the belonging of patterns with large uncertainties. Fuzzy sets theories are reputed to handle vague phenomena through membership functions which measure degrees of a pattern belonging to different clusters. Rough sets theory is a new paradigm to deal with uncertainty and incompleteness, forming an interesting region which consisted of patterns with large uncertainties. By integrating fuzzy sets and rough sets, a hybrid unsupervised learning algorithm is designed for analyzing patterns with large uncertainties. Furthermore, a fuzzy weighted factor is designed to work with membership degrees together for further determining where patterns with large uncertainties belong to. Besides, the partition criterion utilized in original hybrid clustering algorithms cannot guarantee basic rough sets properties to be fully satisfied. In this study, a modified partition criterion is proposed to overcome this issue. Experimental results on synthetic datasets, real-life datasets, and image segmentation problems indicate that the proposed method outperforms its counterparts in most cases. Graphical abstractDisplay Omitted HighlightsWe propose a model which incorporates kernel metric and fuzzy logic for image segmentation.The updating of region prototypes is more robust against outliers and noise.The evolution of contour in our model is stable and accurate.The proposed model achieves good balance between accuracy and efficiency. In this paper, a novel region-based fuzzy active contour model with kernel metric is proposed for a robust and stable image segmentation. This model can detect the boundaries precisely and work well with images in the presence of noise, outliers and low contrast. It segments an image into two regions - the object and the background by the minimization of a predefined energy function. Due to the kernel metric incorporated in the energy and the fuzziness of the energy, the active contour evolves very stably without the reinitialization for the level set function during the evolution. Here the fuzziness provides the model with a strong ability to reject local minima and the kernel metric is employed to construct a nonlinear version of energy function based on a level set framework. This new fuzzy and nonlinear version of energy function makes the updating of region centers more robust against the noise and outliers in an image. Theoretical analysis and experimental results show that the proposed model achieves a much better balance between accuracy and efficiency compared with other active contour models. This paper presents a novel change detection approach for synthetic aperture radar images based on deep learning. The approach accomplishes the detection of the changed and unchanged areas by designing a deep neural network. The main guideline is to produce a change detection map directly from two images with the trained deep neural network. The method can omit the process of generating a difference image (DI) that shows difference degrees between multitemporal synthetic aperture radar images. Thus, it can avoid the effect of the DI on the change detection results. The learning algorithm for deep architectures includes unsupervised feature learning and supervised fine-tuning to complete classification. The unsupervised feature learning aims at learning the representation of the relationships between the two images. In addition, the supervised fine-tuning aims at learning the concepts of the changed and unchanged pixels. Experiments on real data sets and theoretical analysis indicate the advantages, feasibility, and potential of the proposed method. Moreover, based on the results achieved by various traditional algorithms, respectively, deep learning can further improve the detection performance. Multi-spatial-resolution change detection is a newly proposed issue and it is of great significance in remote sensing, environmental and land use monitoring, etc. Though multi-spatial-resolution image-pair are two kinds of representations of the same reality, they are often incommensurable superficially due to their different modalities and properties. In this paper, we present a novel multi-spatial-resolution change detection framework, which incorporates deep-architecture-based unsupervised feature learning and mapping-based feature change analysis. Firstly, we transform multi-resolution image-pair into the same pixel-resolution through co-registration, followed by details recovery, which is designed to remedy the spatial details lost in the registration. Secondly, the denoising autoencoder is stacked to learn local and high-level representation/feature from the local neighborhood of the given pixel, in an unsupervised fashion. Thirdly, motivated by the fact that multi-resolution image-pair share the same reality in the unchanged regions, we try to explore the inner relationships between them by building a mapping neural network. And it can be used to learn a mapping function based on the most-unlikely-changed feature-pairs, which are selected from all the feature-pairs via a coarse initial change map generated in advance. The learned mapping function can bridge the different representations and highlight changes. Finally, we can build a robust and contractive change map through feature similarity analysis, and the change detection result is obtained through the segmentation of the final change map. Experiments are carried out on four real datasets, and the results confirmed the effectiveness and superiority of the proposed method. Learning can be of great use when dealing with problems in various fields. Inspired by locally linear embedding from manifold, we propose a novel automatic change-detection method through an offline learning approach. The proposed method comprises three steps. First, two coupled dictionaries of the difference image (DI) patches and change-detection map patches are generated from known image pairs. Second, we approximately represent each patch of the input DI with respect to the DI dictionary by using the matching the pursuit algorithm. Third, the coefficients of this representation are applied with the change-detection map dictionary to generate the output change-detection map. This way, we exploit the relationship between the DI patches and the corresponding change-detection map patches based on two coupled dictionaries. In addition, the relationship guides us to construct the change-detection map for any given input DI. Experimental results on real synthetic aperture radar databases show that the proposed method is superior to its counterparts. Our method can obtain promising results, even though the dictionaries are prepared by simple random sampling from fixed training images. Many researchers have introduced tag information to recommender systems to improve the performance of traditional recommendation techniques. However, user-defined tags will usually suffer from many problems, such as sparsity, redundancy, and ambiguity. To address these problems, we propose a new recommendation algorithm based on deep neural networks. In the proposed algorithm, users' profiles are initially represented by tags and then a deep neural network model is used to extract the in-depth features from tag space layer by layer. In this way, representations of the raw data will become more abstract and advanced, and therefore the unique structure of tag space will be revealed automatically. Based on those extracted abstract features, users' profiles are updated and used for making recommendations. The experimental results demonstrate the usefulness of the proposed algorithm and show its superior performance over the clustering based recommendation algorithms. In addition, the impact of network depth on the algorithm performance is also investigated. : Community structure is one of the most important properties in networks, and community detection has received an enormous amount of attention in recent years. Modularity is by far the most used and best known quality function for measuring the quality of a partition of a network, and many community detection algorithms are developed to optimize it. However, there is a resolution limit problem in modularity optimization methods. In this study, a memetic algorithm, named Meme-Net, is proposed to optimize another quality function, modularity density, which includes a tunable parameter that allows one to explore the network at different resolutions. Our proposed algorithm is a synergy of a genetic algorithm with a hill-climbing strategy as the local search procedure. Experiments on computer-generated and real-world networks show the effectiveness and the multiresolution ability of the proposed method. : The modular structure of a network is closely related to the dynamics toward clustering. In this paper, a method for community detection is proposed via the clustering dynamics of a network. The initial phases of the nodes in the network are given randomly, and then they evolve according to a set of dedicatedly designed differential equations. The phases of the nodes are naturally separated into several clusters after a period of evolution, and each cluster corresponds to a community in the network. For the networks with overlapping communities, the phases of the overlapping nodes will evolve to the interspace of the two communities. The proposed method is illustrated with applications to both synthetically generated and real-world complex networks. State-of-the-art multiobjective evolutionary algorithms (MOEAs) treat all the decision variables as a whole to optimize performance. Inspired by the cooperative coevolution and linkage learning methods in the field of single objective optimization, it is interesting to decompose a difficult high-dimensional problem into a set of simpler and low-dimensional subproblems that are easier to solve. However, with no prior knowledge about the objective function, it is not clear how to decompose the objective function. Moreover, it is difficult to use such a decomposition method to solve multiobjective optimization problems (MOPs) because their objective functions are commonly conflicting with one another. That is to say, changing decision variables will generate incomparable solutions. This paper introduces interdependence variable analysis and control variable analysis to deal with the above two difficulties. Thereby, an MOEA based on decision variable analyses (DVAs) is proposed in this paper. Control variable analysis is used to recognize the conflicts among objective functions. More specifically, which variables affect the diversity of generated solutions and which variables play an important role in the convergence of population. Based on learned variable linkages, interdependence variable analysis decomposes decision variables into a set of low-dimensional subcomponents. The empirical studies show that DVA can improve the solution quality on most difficult MOPs. The code and supplementary material of the proposed algorithm are available at http://web.xidian.edu.cn/fliu/paper.html . Recommender systems are tools to suggest items to target users. Accuracy-focused recommender systems tend to recommend popular items, while suggesting items with few ratings (long tail items) is also of great importance in practice. Recommending long tail items may cause an accuracy loss of recommendation results. Thus, it is necessary to have a recommendation framework that recommends unpopular items meanwhile minimizing the accuracy loss. In this paper, we formulate a multi-objective framework for long tail items recommendation. Under this framework, two contradictory objective functions are designed to describe the abilities of recommender system to recommend accurate and unpopular items, respectively. To optimize these two objective functions, a novel multi-objective evolutionary algorithm is proposed. This multi-objective evolutionary algorithm aims to find a set of tradeoff solutions by optimizing two objective functions simultaneously. Experiments show that the proposed framework is effective to suggest accurate and novel items. The proposed recommendation algorithm could suggest many high-quality recommendation lists for the target user based on the concept of Pareto dominance in one run. High-throughput experimental screening techniques have resulted in a large number of biological network data such as protein-protein interactions (PPI) data. The analysis of these data can enhance our understanding of cellular processes. PPI network alignment is one of the comparative analysis methods for analyzing biological networks. Research on PPI networks can identify conserved subgraphs and help us to understand evolutionary trajectories across species. Some evolutionary algorithms have been proposed for coping with PPI network alignment, but most of them are limited by the lower search efficiency due to the lack of the priori knowledge. In this paper, we propose a memetic algorithm, denoted as MeAlgn, to solve the biological network alignment by optimizing an objective function which introduces topological structure and sequence similarities. MeAlign combines genetic algorithm with a local search refinement. The genetic algorithm is to find interesting alignment solution regions, and the local search is to find optimal solutions around the regions. The proposed algorithm first develops a coarse similarity score matrix for initialization and then it uses a specific neighborhood heuristic local search strategy to find an optimal alignment. In MeAlign, the information of topological structure and sequence similarities is used to guide our mapping. Experimental results demonstrate that our algorithm can achieve a better mapping than some state-of-the-art algorithms and it makes a better balance between the network topology and nodes sequence similarities. Abstract Finding the shortest path (SP) in a large-scale network analysis between any two nodes is a tough but very significant task. The SP can help us to analyze the information spreading performance and research the latent relationship in the weighted social network, and so on. As the size of the social network increases, the traditional SP algorithms have poor performance and there is not a suitable algorithm for weighted social network. Some features of the network analysis are beneficial to solve this problem, and community structure ignored by the traditional methods is one of the most important features. In this paper, we propose a shortest path algorithm based on community detection (SPCD) by integrating community detection algorithm with traditional search methods. SPCD constructs a community graph by using community structure to narrow the searching scope. The algorithm presented improves the time efficiency and maintains the accuracy scale of the SP. Experimental results on five real-world networks demonstrate the effectiveness of the proposed methods for the SP problem. A decomposition approach decomposes a multiobjective optimization problem into a number of scalar objective optimization subproblems. It plays a key role in decomposition-based multiobjective evolutionary algorithms. However, many widely used decomposition approaches, originally proposed for mathematical programming algorithms, may not be very suitable for evolutionary algorithms. To help decomposition-based multiobjective evolutionary algorithms balance the population diversity and convergence in an appropriate manner, this letter proposes to impose some constraints on the subproblems. Experiments have been conducted to demonstrate that our proposed constrained decomposition approach works well on most test instances. We further propose a strategy for adaptively adjusting constraints by using information collected from the search. Experimental results show that it can significantly improve the algorithm performance. In this paper, we propose a novel three-class change detection approach for synthetic aperture radar images (SAR) based on deep learning. In most literatures, change detection in images is a method that classifies the ratio images into two parts: the changed and unchanged classes. However, multitemporal SAR images have either increase or decrease in the backscattering values, so it is significative to further classify the changed areas into the positive and negative changed classes. We accomplish this novel three-class change detection method through Deep Learning. Given the multitemporal images, a difference image which shows difference degrees between corresponding pixels is generated by modified log-ratio operator. Then, we establish a deep belief network to analyze the difference image and recognize the positive changed pixels, negative changed pixels and unchanged pixels. Influence maximization in social networks aims to find a small group of individuals, which have maximal influence cascades. In this study, an optimization model based on a local influence criterion is established for the influence maximization problem. The local influence criterion can provide a reliable estimation for the influence propagations in independent and weighted cascade models. A discrete particle swarm optimization algorithm is then proposed to optimize the local influence criterion. The representations and update rules for the particles are redefined in the proposed algorithm. Moreover, a degree based heuristic initialization strategy and a network-specific local search strategy are introduced to speed up the convergence. Experimental results on four real-world social networks demonstrate the effectiveness and efficiency of the proposed algorithm for influence maximization. AdaBoost has attracted much attention in the machine learning community because of its excellent performance in combining weak classifiers into strong classifiers. However, AdaBoost tends to overfit to the noisy data in many applications. Accordingly, improving the antinoise ability of AdaBoost plays an important role in many applications. The sensitiveness to the noisy data of AdaBoost stems from the exponential loss function, which puts unrestricted penalties to the misclassified samples with very large margins. In this paper, we propose two boosting algorithms, referred to as RBoost1 and RBoost2, which are more robust to the noisy data compared with AdaBoost. RBoost1 and RBoost2 optimize a nonconvex loss function of the classification margin. Because the penalties to the misclassified samples are restricted to an amount less than one, RBoost1 and RBoost2 do not overfocus on the samples that are always misclassified by the previous base learners. Besides the loss function, at each boosting iteration, RBoost1 and RBoost2 use numerically stable ways to compute the base learners. These two improvements contribute to the robustness of the proposed algorithms to the noisy training and testing samples. Experimental results on the synthetic Gaussian data set, the UCI data sets, and a real malware behavior data set illustrate that the proposed RBoost1 and RBoost2 algorithms perform better when the training data sets contain noisy data. Uncovering community structures of a complex network can help us to understand how the network functions. Over the past few decades, network community detection has attracted growing research interest from many fields. Many community detection methods have been developed. Network community structure detection can be modelled as optimisation problems. Due to their inherent complexity, these problems often cannot be well solved by traditional optimisation methods. For this reason, evolutionary algorithms have been adopted as a major tool for dealing with community detection problems. This paper presents a survey on evolutionary algorithms for network community detection. The evolutionary algorithms in this survey cover both single objective and multiobjective optimisations. The network models involve weighted/unweighted, signed/unsigned, overlapping/non-overlapping and static/dynamic ones. Influence maximization is to extract a small set of nodes from a social network which influences the propagation maximally under a cascade model. In this paper, we propose a memetic algorithm for community-based influence maximization in social networks. The proposed memetic algorithm optimizes the 2-hop influence spread to find the most influential nodes. Problem-specific population initialization and similarity-based local search are designed to accelerate the convergence of the algorithm. Experiments on three realworld datasets demonstrate that our algorithm has competitive performances to the comparing algorithms in terms of effectiveness and efficiency. For example, on a real-world network of 15233 nodes and 58891 edges, the influence spread of the proposed algorithm is 12.5%, 13.2% and 173.5% higher than the three comparing algorithms Degree, PageRank and Random, respectively. Abstract Many social spreading phenomena can be modeled as epidemic spreading models over networks, and the studies of these phenomena are important to avoid epidemic outbreaks. Epidemic threshold of the network, which fundamentally depends on the network structure itself, is a critical measure to judge whether the epidemic dies out or results in an epidemic breakout. In this study, epidemic threshold is regarded as the objective function to control the spreading process. In addition, an efficient structure optimization strategy based on memetic algorithm is proposed to adjust the spreading threshold without changing the degree of each node. Lowering the threshold can promote the spreading process whereas heightening the threshold can prevent the spreading process. In the proposed algorithm, genetic algorithm is adopted as the global search strategy and a modified simulated annealing algorithm combined with the properties of networks is proposed as the local search strategy. Experiments on computer-generated and real-world networks demonstrate that the proposed algorithm has superior performances for both the threshold minimization and maximization problems. Due to the noise interference and redundancy in multispectral images, it is promising to transform the available spectral channels into a suitable feature space for relieving noise and reducing the redundancy. The booming of deep learning provides a flexible tool to learn abstract and invariant features directly from the data in their raw forms. In this letter, we propose an unsupervised change detection technique for multispectral images, in which we combine deep belief networks (DBNs) and feature change analysis to highlight changes. First, a DBN is established to capture the key information for discrimination and suppress the irrelevant variations. Second, we map bitemporal change feature into a 2-D polar domain to characterize the change information. Finally, an unsupervised clustering algorithm is adopted to distinguish the changed and unchanged pixels, and then, the changed types can be identified by classifying the changed pixels into several classes according to the directions of feature changes. The experimental results demonstrate the effectiveness and robustness of the proposed method. With the increase of multisource data available from remote sensing platforms, it is demanding to develop unsupervised techniques for change detection from multisource data. The difference in imaging mechanism makes it difficult to carry out a direct comparison between multisource data in original observation spaces. Different sensors provide different descriptions on the same truth in low-dimension observation spaces, but the same truth indicates the comparability of multisource data in some high-dimensional feature spaces. Inspired by this, we try to solve this problem by transforming multisource data into a common high-dimension feature space. In this paper, an iterative coupled dictionary learning (CDL) model is proposed for multisource image change detection. This model aims to establish a pair of coupled dictionaries, one of which is responsible for the data from one sensor, whereas the other is responsible for the data from another sensor. The atoms from these two coupled dictionaries have a one-to-one correspondence at the same location. Such a property guarantees the transferability of the reconstruction coefficients between bitemporal patch pairs and provides us a desired mechanism to bridge multisource data and highlight changes. The contributions can be summarized as follows: CDL is designed to explore the intrinsic difference of multisource data for change detection in a high-dimension feature space, and an iterative scheme for unsupervised sample selection is proposed to keep the purity of training samples and gradually optimize the current coupled dictionaries. The experimental results have demonstrated the feasibility, effectiveness, and robustness of the proposed framework. Abstract Many image inverse problems are ill-posed for no unique solutions. Most of them have incommensurable or mixed-type objectives. In this study, a multi-objective optimization framework is introduced to model such ill-posed inverse problems. The conflicting objectives are designed according to the properties of ill-posedness and certain techniques. Multi-objective evolutionary algorithms have capability to optimize multiple objectives simultaneously and obtain a set of trade-off solutions. For that reason, we use multi-objective evolutionary algorithms to keep the trade-off between these objectives for image ill-posed problems. Two case studies of sparse reconstruction and change detection are implemented. In the case study of sparse reconstruction, the measurement error term and the sparsity term are optimized by multi-objective evolutionary algorithms, which aims at balancing the trade-off between enforcing sparsity and reducing measurement error. In the case study of image change detection, two conflicting objectives are constructed to keep the trade-off between robustness to noise and preserving the image details. Experimental results of the two case studies confirm the multi-objective optimization framework for ill-posed inverse problems in image processing is effective. Chaos theory describes complex motion and the dynamics of nonlinear systems. As a complex nonlinear system, the immune system is chaotic. This paper introduces chaos into the clonal selection algorithm by a novel mutation method, self-adaptive chaotic mutation (SACM). In detail, based on the logistic chaotic sequence, SACM extracts antibody's affinity and distribution to adjust the mutation scale. Compared with the clonal selection algorithm using random mutation and standard genetic algorithm, the improved clonal selection algorithm can enhance the precision and stability, and overcome prematurity effectively with a high convergence speed. Abstract Evolutionary computation (EC) has received significant attention in China during the last two decades. In this paper, we present an overview of the current state of this rapidly growing field in China. Chinese research in theoretical foundations of EC, EC-based optimization, EC-based data mining, and EC-based real-world applications are summarized. Current self-paced learning (SPL) regimes adopt the greedy strategy to obtain the solution with a gradually increasing pace parameter while where to optimally terminate this increasing process is difficult to determine. Besides, most SPL implementations are very sensitive to initialization and short of a theoretical result to clarify where SPL converges to with pace parameter increasing. In this paper, we propose a novel multi-objective self-paced learning (MOSPL) method to address these issues. Specifically, we decompose the objective functions as two terms, including the loss and the self-paced regularizer, respectively, and treat the problem as the compromise between these two objectives. This naturally reformulates the SPL problem as a standard multi-objective issue. A multi-objective evolutionary algorithm is used to optimize the two objectives simultaneously to facilitate the rational selection of a proper pace parameter. The proposed technique is capable of ameliorating a set of solutions with respect to a range of pace parameters through finely compromising these solutions inbetween, and making them perform robustly even under bad initialization. A good solution can then be naturally achieved from these solutions by making use of some off-the-shelf tools in multi-objective optimization. Experimental results on matrix factorization and action recognition demonstrate the superiority of the proposed method against the existing issues in current SPL research.\",\n",
            "  \"2075225989\": \"This study focuses on various trader behaviors that affect market dynamics. In particular, the effects of a covering mechanism, learning mechanism and bias mechanism are analyzed through agent-based financial market model. An XCS classifier system is used to model trader learning mechanism. A trader model is proposed to formulate a trader decision model that combines bias mechanisms with learning mechanisms. The results reveal that biased traders survive under evolving markets and affect price dynamics. The model contributes to understanding the market behavior and potential sources of deviation from efficient market equilibrium. This paper describes the use of a neuro-fuzzy-genetic data mining architecture for finding hidden knowledge and modeling the data of the 1997 donation campaign of an American charitable organization. This data was used during the 1998 KDD Cup competition. In the architecture, all input variables are first preprocessed and all continuous variables are fuzzified. Principal component analysis (PCA) is then applied to reduce the dimensions of the input variables in finding combinations of variables, or factors, that describe major trends in the data. The reduced dimensions of the input variables are then used to train probabilistic neural networks (PNN) to classify the dataset according to the groups considered. A rule extraction technique is then applied in order to extract hidden knowledge from the trained neural networks and represent the knowledge in the form of crisp and fuzzy if-then-rules. In the final stage a genetic algorithm is used as a rule-pruning module to eliminate weak rules that are still in the rule base while insuring that the classification accuracy of the rule base is improved or not changed. The pruned rule base helps the charitable organization to maximize the donation and to understand the characteristics of the respondents of the direct mail fund raising campaign. The dynamic, non-linear, volatile and complex nature of interest rates makes it hard to predict their future movements. In order to deal with these complexities, the authors propose a two-stage neuro-hybrid forecasting model. In the initial data preprocessing stage, multiple regression analysis is implemented to determine the variables that have the strongest prediction ability. The selected variables are then provided as inputs to a Fuzzy Inference Neural Network to forecast future interest rate values. The proposed hybrid model is implemented using data from the U.S. interest rate market. It has been widely accepted by many studies that non-linearity exists in the financial markets and that neural networks can be effectively used to uncover this relationship. Unfortunately, many of these studies fail to consider alternative forecasting techniques, the relevance of input variables, or the performance of the models when using different trading strategies. This paper introduces an information gain technique used in machine learning for data mining to evaluate the predictive relationships of numerous financial and economic variables. Neural network models for level estimation and classification are then examined for their ability to provide an effective forecast of future values. A cross-validation technique is also employed to improve the generalization ability of several models. The results show that the trading strategies guided by the classification models generate higher risk-adjusted profits than the buy-and-hold strategy, as well as those guided by the level-estimation based forecasts of the neural network and linear regression models. The following research implements a differential evolution-based fuzzy-type clustering method with a fuzzy inference neural network after input preprocessing with regression analysis in order to predict future interest rates, particularly 3-month T-bill rates. The empirical results of the proposed model is compared against nonparametric models, such as locally weighted regression and least squares support vector machines, along with two linear benchmark models, the autoregressive model and the random walk model. The root mean square error is reported for comparison. Abstract Stock market forecasting research offers many challenges and opportunities, with the forecasting of individual stocks or indexes focusing on forecasting either the level (value) of future market prices, or the direction of market price movement. A three-stage stock market prediction system is introduced in this article. In the first phase, Multiple Regression Analysis is applied to define the economic and financial variables which have a strong relationship with the output. In the second phase, Differential Evolution-based type-2 Fuzzy Clustering is implemented to create a prediction model. For the third phase, a Fuzzy type-2 Neural Network is used to perform the reasoning for future stock price prediction. The results of the network simulation show that the suggested model outperforms traditional models for forecasting stock market prices. Abstract Volatility forecasting in the financial markets, along with the development of financial models, is important in the areas of risk management and asset pricing, among others. Previous testing has shown that asymmetric GARCH models outperform other GARCH family models with regard to volatility prediction. Utilizing this information, three popular Neural Network models (Feed-Forward with Back Propagation, Generalized Regression, and Radial Basis Function) are implemented to help improve the performance of the GJR(1,1) method for estimating volatility over the next forty-four trading days. During training and testing, four different economic cycles have been considered between 1997-2011 to represent real and contemporary periods of market calm and crisis. In addition to stress testing for different neural network architectures to assess their performance under various turmoil and normal situations in the U.S. market, their synergy along with another econometric model is also accessed. In recent years, the opening of worldwide markets to many products has forced manufacturing companies to compete on a global basis. This high level of competition amongst manufacturers has led to rapid developments in the areas of computer integrated manufacturing, flexible manufacturing, agile manufacturing, and intelligent manufacturing. These developments have in turn generated a need for intelligent sensing and decision making systems capable of automatically performing many tasks traditionally executed by human beings. Visual inspection is one such task, and there is a need for effective automated visual inspection systems in today's competitive manufacturing environments. The following will discuss the advantages of implementing an intelligent visual inspection system based on natural human vision. Results from current models will also be presented. It has been long recognized that trading volume provides valuable information for understanding stock price movement. As such, equivolume charting was developed to consider how stocks appear to move in a volume frame of reference as opposed to a time frame of reference. Two technical indicators, namely the volume adjusted moving average (VAMA) and the ease of movement (EMV) indicator, are developed from equivolume charting. This paper explores the profitability of stock trading by using a neural network model developed to assist the trading decisions of the VAMA and EMV. The generalized regression neural network (GRNN) is chosen and utilized on past S&P 500 index data. For the VAMA, the GRNN is used to predict the future stock prices, as well as the future width size of the equivolume boxes typically utilized on an equivolume chart, for calculating the future value of the VAMA. For the EMV, the GRNN is also used to predict the future value of the EMV. The idea is to further exploit the equivolume potential by using a forecasting system to predict the future equivolume measurements, allowing investors to enter or exit trades earlier. The results show that the stock trading using the neural network with the VAMA and EMV outperforms the results of stock trading generated from the VAMA and EMV without neural network assistance, the simple moving averages (MA) in isolation, and the buy-and-hold trading strategy. Abstract The following paper discusses the use of a hybrid model for the prediction of short-term US interest rates. The model consists of a differential evolution-based fuzzy type-2 clustering with a fuzzy type-2 inference neural network, after input preprocessing with multiple regression analysis. The model was applied to forecast the US 3- Month T-bill rates. Promising model performance was obtained as measured using root mean square error. Abstract One of the key issues in constructing monetary policy is accurate prediction of the inflation level. The complex behavior and non-linear nature of the financial markets makes it hard to forecast the inflation rate precisely. This paper introduces a hybrid model that attempts to forecast the inflation rate with a combination of a subtractive clustering technique and a fuzzy inference neural network to overcome the shortcomings of the individual methodologies. Selected macroeconomic factors were used to predict the historical CPI data from the US Markets. The results of the proposed hybrid model are measured in RMSE. Abstract Recently, there has been a spike in the prices and popularity of commodities. On a macroeconomic level, developing countries are increasing production; while on a microeconomic level, speculative traders are becoming more involved in the market. Agricultural products have a diverse array of factors that can affect the price (i.e. political, government, population, weather, supply and demand). Commodity prices can suffer from extreme volatility in the short term, changing as much as 50% in one year. This research uses the soybean crush spread as a model. The soybean complex adds an interesting component as the underlying soybean product can be crushed into soymeal and soy oil. All three products (soybeans, soymeal, and soy oil) currently have contracts on the Chicago Mercantile Exchange. The crush margin represents the profit margin a processor will receive from crushing the soybeans into the underlying products (soymeal and soy oil). This research adds to the literature of agricultural price forecasting models, using artificial intelligence and nonlinear modeling. The performance of different neural network architectures and inputs to discover desirable returns for both speculative trading and hedging are investigated. This paper presents the use of an intelligent hybrid stock trading system that integrates neural networks, fuzzy logic, and genetic algorithms techniques to increase the efficiency of stock trading when using a volume adjusted moving average (VAMA), a technical indicator developed from equivolume charting. For this research, a neuro-fuzzy-based genetic algorithm (NF-GA) system utilizing a VAMA membership function is introduced. The results show that the intelligent hybrid system takes advantage of the synergy among these different techniques to intelligently generate more optimal trading decisions for the VAMA, allowing investors to make better stock trading decisions. The Black\\u2013Scholes (BS) model is the standard approach used for pricing financial options. However, although being theoretically strong, option prices valued by the model often differ from the prices observed in the financial markets. This paper applies a hybrid neural network which preprocesses financial input data for improving the estimation of option market prices. This model is comprised of two parts. The first part is a neural network developed to estimate volatility. The second part is an additional neural network developed to value the difference between the BS model results and the actual market option prices. The resulting option price is then a summation between the BS model and the network response. The hybrid system with a neural network for estimating volatility provides better performance in terms of pricing accuracy than either the BS model with historical volatility (HV), or the BS model with volatility valued by the neural network. Abstract The ISO 9000 quality management system has been widely accepted and adapted as a national standard by most industrial countries. Despite its high popularity and the urgent demand from customers to implement ISO 9000, some major concerns for those organizations that are seeking registration to ISO 9000 include the expensive cost and the lengthy time to implement. The purpose of this paper is to describe an expert advisory system for ISO 9001 implementation by using an expert system shell called Visual Rules Studio. This expert advisory system integrated the ISO 9001 quality system guidelines and an evaluation approach based on the Malcolm Baldrige National Quality Award (MBNQA) criteria into a knowledge-based expert system. By identifying the critical ISO elements and comparing the company's current quality performance with ISO standards, this advisory system provides assessment results and implementation suggestions to the organization. The advisory system has been validated by a group of quality professionals. The following contains a description of the system and a discussion of the validation results. Limitations of the system and recommendations for future research are also discussed. Abstract It has been widely accepted that predicting stock returns is not a simple task since many market factors are involved and their structural relationships are not perfectly linear. Recently, a promising data mining technique in machine learning has been proposed to uncover the predictive relationships of numerous financial and economic variables. Inspired by the fact that the determinant between these variables and their interrelationships over stock returns changes over time, we explore this issue further by using data mining to uncover the recent relevant variables with the greatest predictive ability. The objective is to examine whether using the recent relevant variables leads to additional improvements in stock return forecasting. Given evidence of non-linearity in the financial market, the resulting variables are then provided to neural networks, including probabilistic and feed-forward neural networks, for predicting the directions of future excess stock return. The results show that redeveloped neural network models that use the recent relevant variables generate higher profits with lower risks than the buy-and-hold strategy, conventional linear regression, and the random walk model, as well as the neural network models that use constant relevant variables. There are fundamental difficulties when only using a supervised learning philosophy to predict financial stock short-term movements. We present a reinforcement-oriented forecasting framework in which the solution is converted from a typical error-based learning approach to a goal-directed match-based learning method. The real market timing ability in forecasting is addressed as well as traditional goodness-of-fit-based criteria. We develop two applicable hybrid prediction systems by adopting actor-only and actor-critic reinforcement learning, respectively, and compare them to both a supervised-only model and a classical random walk benchmark in forecasting three daily-based stock indices series within a 21-year learning and testing period. The performance of actor-critic-based systems was demonstrated to be superior to that of other alternatives, while the proposed actor-only systems also showed efficacy Abstract : A major challenge to the successful planning and evolution of an acknowledged System of Systems (SoS) is the current lack of understanding of the impact that the presence or absence of a set of constituent systems has on the overall SoS capability. Since the candidate elements of an SoS are fully functioning, stand-alone Systems in their own right, they have goals and objectives of their own to satisfy, some of which may compete with those of the overarching SoS. These system-level concerns drive decisions to participate (or not) in the SoS. Individual systems typically must be requested to join the SoS construct, and persuaded to interface and cooperate with other Systems to create the new capability of the proposed SoS. Current SoS evolution strategies lack a means for modeling the impact of decisions concerning participation. The goal of this research is to model the evolution of the architecture of an acknowledged SoS that accounts for the ability and willingness of constituent systems to support the SoS capability development. Since DoD Systems of Systems (SoS) development efforts do not typically follow the normal program acquisition process described in DoDI 5000.02, the Wave Model proposed by Dahmann and Rebovich is used as the basis for this research on SoS capability evolution. The Wave Process Model provides a framework for an agent-based modeling methodology, which is used to abstract the non-utopian behavioral aspects of the constituent systems and their interactions with the SoS. In particular, the research focuses on the impact of individual system behavior on the SoS capability and architecture evolution processes. A proof of concept agent-based model (ABM) of the system interactions is developed and integrated with a genetic algorithm (GA) to explore the potential architectural design space, using a fuzzy associative memory (FAM) to evaluate candidate architectures for simulating SoS creation and evolution. Genetic algorithms are design tools used in generating optimal solutions. While they can often be shown to outperform various heuristic methods and hybrid approaches, using a combination of evolutionary algorithms and heuristic approaches can generate an optimal solution more quickly than either of the two methods independently. Our purpose is to provide an overview of genetic algorithms, to discuss the types of problems that lend themselves to being solved by genetic algorithms, and to identify heuristics that have been shown to aid genetic algorithms in their quest for optimal solutions. While the sample problems discussed in this paper are generally of textbook variety, genetic algorithms can be applied to problems of interest to systems engineers. Such problems include (1) up-front trade studies to look for potential feasible concepts based on combinations of key system attributes within system constraints and (2) resource selection problems. A military example of a resource selection problem is autonomously recommending air attack resources to prosecute evolving targets. The decision space in this problem is bounded by available fuel, available number and types of weapons, current aircraft locations and current target priority rules of engagement. Abstract Complex Event Processing (CEP) is a novel and promising methodology that enables the real-time analysis of stream event data. The main purpose of CEP is detection of the complex event patterns from the atomic and semantically low-level events such as sensor, log, or RFID data. Determination of the rule patterns for matching these simple events based on the temporal, semantic, or spatial correlations is the central task of CEP systems. In the current design of the CEP systems, experts provide event rule patterns. Having reached maturity, the Big Data Systems and Internet of Things (IoT) technology require the implementation of advanced machine learning approaches for automation in the CEP domain. The goal of this research is proposing a machine learning model to replace the manual identification of rule patterns. After a pre-processing stage (dealing with missing values, data outliers, etc.), various rule-based machine learning approaches were applied to detect complex events. Promising results with high preciseness were obtained. A comparative analysis of the performance of classifiers is discussed. Abstract Volatility forecasting models are becoming more accurate, but noise looks to be an inseparable part of these forecasts. Nonetheless, using adaptive filters to cancel the noise should help improve the performance of the forecasting models. Adaptive filters have the advantage of changing based on the environment. This feature is vital when they are used along with a model for volatility forecasting and error cancellation in the financial markets. Nonlinear Autoregressive (NAR) neural networks have simple structures, but they are efficient tools in error cancelation systems when working with non-stationary and random walk noise processes. For this research, an adaptive threshold filter is designed to respond to changes in its environment when a GARCH(1,1) model makes errors in its volatility forecast. It is shown that this filter can forecast the noise (errors) in the GARCH(1,1) outputs when there is a non-stationary time series of errors. The model reduces the mean squared errors by 42.9%. A sample portfolio of five stocks from the S&P 500 index from 4/2007 to 12/2010 is studied to illustrate the performance of the model. The system provides an automated and adaptive model selection process.The system predicts the stock price direction, rather than the forecasted level.Particle swarm optimization is used to reduce computation time.Denoising is used to deal with stock market volatility. Predicting the direction and movement of stock index prices is difficult, often leading to excessive trading, transaction costs, and missed opportunities. Often traders need a systematic method to not only spot trading opportunities, but to also provide a consistent approach, thereby minimizing trading errors and costs. While mechanical trading systems exist, they are usually designed for a specific stock, stock index, or other financial asset, and are often highly dependent on preselected inputs and model parameters that are expected to continue providing trading information well after the initial training or back-tested model development period. The following research leads to a detailed trading model that provides a more effective and intelligent way for recognizing trading signals and assisting investors with trading decisions by utilizing a system that adapts both the inputs and the prediction model based on the desired output. To illustrate the adaptive approach, multiple inputs and modeling techniques are utilized, including neural networks, particle swarm optimization, and denoising. Simulations with stock indexes illustrate how traders can generate higher returns using the developed adaptive decision support system model. The benefits of adding adaptive and intelligent decision making to forecasts are also discussed. This study proposes a unique rule change trading system for the futures market.Rough set analysis is adopted for generating trading rules.A genetic algorithm is used to optimize the thresholds for buying and selling signals.To verify the proposed system, a sliding window method is applied. Many technical indicators have been selected as input variables in order to develop an automated trading system that determines buying and selling trading decision using optimal trading rules within the futures market. However, optimal technical trading rules alone may not be sufficient for real-world application given the endlessly changing futures market. In this study, a rule change trading system (RCTS) that consists of numerous trading rules generated using rough set analysis is developed in order to cover diverse market conditions. To change the trading rules, a rule change mechanism based on previous trading results is proposed. Simultaneously, a genetic algorithm is employed with the objective function of maximizing the payoff ratio to determine the thresholds of market timing for both buying and selling in the futures market. An empirical study of the proposed system was conducted in the Korea Composite Stock Price Index 200 (KOSPI 200) futures market. The proposed trading system yields profitable results as compared to both the buy-and-hold strategy, and a system not utilizing a genetic algorithm for maximizing the payoff ratio. Abstract The objective of this study is to use artificial neural networks for volatility forecasting to enhance the ability of an asset allocation strategy based on the target volatility. The target volatility level is achieved by dynamically allocating between a risky asset and a risk-free cash position. However, a challenge to data-driven approaches is the limited availability of data since periods of high volatility, such as during financial crises, are relatively rare. To resolve this issue, we apply a stability-oriented approach to compare data for the current period to a past set of data for a period of low volatility, providing a much more abundant source of data for comparison. In order to explore the impact of the proposed model, the results of this approach will be compared to different volatility forecast methodologies, such as the volatility index, the historical volatility, the exponentially weighted moving average (EWMA), and the generalized autoregressive conditional heteroskedasticity (GARCH) model. Trading measures are used to evaluate the performance of the models for forecasting volatility. An empirical study of the proposed model is conducted using the Korea Composite Stock Price Index 200 (KOSPI 200) and certificate of deposit interest rates from January, 2006 to February, 2016.\",\n",
            "  \"2096449736\": \"It is the science of room acoustics that offers an understanding of the physical processes by which sound waves propagate in enclosed spaces and the manner in which acoustic reflections combine to give the effect that we refer to as reverberation. This chapter aims to summarize some of the main concepts of room acoustics that are relevant to the subsequent material. In particular, this discussion will focus on models and simulation techniques for room acoustics that aid the description of reverberation and the development of dereverberation algorithms. Examples will be given involving both simulated room impulse responses and measured responses of real rooms. The issue of evaluation of dereverberation processing will then be addressed. Measures that aim to characterize the quantity and perceived effect of reverberation in a speech signal will be described and discussed. The chapter ends by considering the well known delay-and-sum beamformer, which is often considered to be a baseline spatial filtering approach, and presents an analysis of the dereverberation performance levels that can be expected at such a baseline. The paper presents a voice source waveform modeling techniques based on principal component analysis (PCA) and Gaussian mixture modeling (GMM). The voice source is obtained by inverse-filteirng speech with the estimated vocal tract filter. This decomposition is useful in speech analysis, synthesis, recognition and coding. Existing models of the voice source signal are based on function-fitting or physically motivated assumptions and although they are well defined, estimation of their parameters is not well understood and few are capable of reproducing the large variety of voice source waveforms. Here, a data-driven approach is presented for signal decomposition and classification based on the principal components of the voice source. The principal components are analyzed and the \\u2018prototype\\u2019 voice source signals corresponding to the Gaussian mixture means are examined. We show how an unknown signal can be decomposed into its components and/or prototypes and resynthesized. We show how the techniques are suited for both low bitrate or high quality analysis/synthesis schemes. A system, method and computer program product are described for voice activity detection (VAD) within a digitally encoded bitstream. A parameter extraction module is configured to extract parameters from a sequence of coded frames from a digitally encoded bitstream containing speech. A VAD classifier is configured to operate with input of the digitally encoded bitstream to evaluate each coded frame based on bitstream coding parameter classification features to output a VAD decision indicative of whether or not speech is present in one or more of the coded frames. A system and method for non-intrusive acoustic parameter estimation is included. The method may include receiving, at a computing device, a first speech signal associated with a particular user. The method may include extracting one or more short-term features from the first speech signal. The method may also include determining one or more statistics of each of the one or more short-term features from the first speech signal. The method may further include classifying the one or more statistics as belonging to one or more acoustic parameter classes. Reverberation affects the quality and intelligibility of distant speech recorded in a room. Direct-to-Reverberant Ratio (DRR) is a useful measure for assessing the acoustic configuration and can be used to inform dereverberation algorithms. We describe a novel DRR estimation algorithm applicable where the signal was recorded with two or more microphones, such as mobile communications devices and laptops. The method uses a null-steered beamformer. In simulations the proposed method yields accurate DRR estimates to within \\u00b14 dB across a wide variety of room sizes, reverberation times and source-receiver distances. It is also shown that the proposed method is more robust to background noise than a baseline approach. The best estimation accuracy is obtained in the region from \\u22125 to 5 dB which is a relevant range for portable devices. Hands-free operation is an important feature of many communication terminals such as telephones and conferencing systems. In general, the quality of speech from a hands-free terminal is degraded by ambient noise and by acoustic echo. This paper presents some of the activities in acoustic echo control of a cec Esprit consortium. A short description is given of a database collected specifically for testing acoustic echo cancellers and of a simple methodology for comparative evaluation of adaptive acoustic echo control methods. Eight methods have been investigated so far. Annotated bibliographies are given for each method and comparative performance results are presented. The invention provides a method and device for enhancing the listening qualities of an audio file by providing the listener with a plurality of modified equalized audio files. Each modified equalized audio file having a consistent loudness level but different audio characteristics. Hence, for an input audio file the current invention allows the listener to individually select the best audio characteristics for them to listen to the content of the input audio file according to their particular requirements without them needing to adjust the loudness level in playback. The invention further enables the listener to switch between the multiple equalized audio files during playback. The invention further includes a SN detector and reducer to eliminate the adverse effects of the presence of sudden, strong noise in the input audio file in the process of generating the plurality of modified equalized audio files. In this paper we present a simple yet effective method for estimating the geometry of an acoustic enclosure in three-dimensions. By capturing the acoustic impulse responses using a microphone array and a loudspeaker at different spatial locations we transform the localization of planar reflectors into the estimation of multiple linear reflectors. By decomposing the microphone array into co-planar sub-arrays the line parameters of the reflectors lying on the corresponding planes can be inferred using a geometric constraint. By intersecting these lines the actual lying plane of each reflector can be estimated. The proposed method is evaluated using a three-dimensional microphone array in a real conference room. We propose an algorithm for blind estimation of the magnitude response of a channel using the observations of a single microphone. The algorithm employs channel robust RASTA filtered Mel-frequency cepstral coefficients as features and a Gaussian mixture model based classifier to generate a dictionary of average speech spectra. These are then used to infer the channel response from speech that has undergone spectral modification in the capturing process. Simulation results using babble noise, car noise and white Gaussian noise are presented, which demonstrate that the proposed method is able to estimate a variety of channel responses to within 3\\u20134 dB in terms of weighted spectral distance; and it is more accurate than a previously published method. Many multichannel algorithms for blind channel identification and deconvolution rely on the identifiability condition that the channels are coprime, i.e. they do not have common zeros. This property has not received much attention in the literature, partly due to the difficulty of factoring the high order channel polynomials that arise in room acoustics. In this paper we propose a novel method for adaptive identification of the common roots of two polynomials. The algorithm is further used to gain some insight into the problem of common zeros in the context of adaptive blind channel identification. Simulation results are provided to demonstrate the identification and the detection of common zeros. We also consider approximately common zeros and show that they do not have to be exactly identical in order to degrade the adaptive channel identification performance. Assessment of speech quality of law-enforcement audio recordings is important as degradations introduced by non-ideal recording conditions can reduce the intelligence value of such recordings. Furthermore a model that predicts speech quality could be beneficial for assessing the performance of audio collection and enhancement systems. The Perceptual Evaluation of Speech Quality (PESQ) algorithm (ITU-T P.862) has been validated for degradations common in telecommunications. In this paper we apply PESQ to degradations typically encountered in law-enforcement. Also we present a subjectively labeled database (C-Qual) containing distortions encountered in law enforcement scenarios. Comparing the prediction by PESQ and the observed opinions provided by the listeners shows that PESQ is less suitable for estimating the speech quality in this context. This paper focuses on speaker tracking in robot audition for human-robot interaction. Using only acoustic signals, speaker tracking in enclosed spaces is subject to missing detections and spurious clutter measurements due to speech inactivity, reverberation and interference. Furthermore, many acoustic localization approaches estimate speaker direction, hence providing bearing-only measurements without range information. This paper presents a probability hypothesis density (PHD) tracker that augments the bearing-only speaker directions of arrival with a cloud of range hypotheses at speaker initiation and propagates the random variates through time. Furthermore, due to their formulation PHD filters explicitly model, and hence provide robustness against, clutter and missing detections. The approach is verified using experimental results. We present single-channel approaches to robust automatic speech recognition (ASR) in reverberant environments based on non-intrusive estimation of the clarity index (C 50). Our best performing method includes the estimated value of C 50 in the ASR feature vector and also uses C 50 to select the most suitable ASR acoustic model according to the reverberation level. We evaluate our method on the REVERB Challenge database employing two different C 50 estimators and show that our method outperforms the best baseline of the challenge achieved without unsupervised acoustic model adaptation, i.e. using multi-condition hidden Markov models (HMMs). Our approach achieves a 22.4 % relative word error rate reduction in comparison to the best baseline of the challenge. Subband adaptive filters suffer degraded performance when high input energy occurs at frequencies coincident with subband boundaries. This is seen as increased error in critically sampled systems and as reduced asymptotic convergence speed in oversampled systems. To address this problem a dynamic frequency decomposition scheme is presented which aims to control the frequency of subband boundaries such that they avoid spectral regions of high input energy. An efficient structure for this is described, which maintains the low-complexity advantage of subband systems. Simulation results show reductions in MSE of around 5-10 dB in the critical case and convergence improvement in the oversampled case, in addition to increased robustness to coloured inputs in both cases. Recognition and synthesis of speech require accurate models of speech production. Inherent in linear predictive modelling is the assumption that the formant frequencies and bandwidths of speech do not change within the analysis frame-usually at least a larynx cycle. It is shown that there can be significant differences in formant characteristics between the closed and open glottis phases of the larynx cycle. A development of the lossless tube model is presented in which the formant variations between closed and open glottis phases are modelled by a time varying glottal reflection coefficient. A numerically based method for estimating the parameters of such a model is outlined. The results of analysis and re-synthesis of female voiced speech obtained using this model are compared to those obtained using conventional LPC. > Simulated room impulse responses have been proven to be both useful and indispensable for comprehensive testing of acoustic signal processing algorithms while controlling parameters such as the reverberation time, room dimensions, and source\\u2013array distance. In this work, a method is proposed for simulating the room impulse responses between a sound source and the microphones positioned on a spherical array. The method takes into account specular reflections of the source by employing the well-known image method, and scattering from the rigid sphere by employing spherical harmonic decomposition. Pseudocode for the proposed method is provided, taking into account various optimizations to reduce the computational complexity. The magnitude and phase errors that result from the finite order spherical harmonic decomposition are analyzed and general guidelines for the order selection are provided. Three examples are presented: an analysis of a diffuse reverberant sound field, a study of binaural cues in the presence of reverberation, and an illustration of the algorithm\\u2019s use as a mouth simulator.Simulated room impulse responses have been proven to be both useful and indispensable for comprehensive testing of acoustic signal processing algorithms while controlling parameters such as the reverberation time, room dimensions, and source\\u2013array distance. In this work, a method is proposed for simulating the room impulse responses between a sound source and the microphones positioned on a spherical array. The method takes into account specular reflections of the source by employing the well-known image method, and scattering from the rigid sphere by employing spherical harmonic decomposition. Pseudocode for the proposed method is provided, taking into account various optimizations to reduce the computational complexity. The magnitude and phase errors that result from the finite order spherical harmonic decomposition are analyzed and general guidelines for the order selection are provided. Three examples are presented: an analysis of a diffuse reverberant sound field, a study of binaural cues in the pres... Spherical arrays facilitate processing and analysis of sound fields with the potential for high resolution in three dimensions in the spherical harmonic domain. Using the captured sound field, robust source localisation systems are required for speech acquisition, speaker tracking and environment mapping. Source localisation becomes a challenging problem in reverberant environments and under noisy conditions, leading to potentially poor performance in cocktail party scenarios. This paper evaluates the performance of a low-complexity localisation approach using spherical harmonics in reverberant environments for multiple speakers. Eigen-beams are used to estimate pseudo-intensity vectors pointing in the direction of sound intensity. This paper proposes a clustering approach in which the intensity vectors of active sound sources and strong reflections are extracted, yielding an estimate of the source direction in azimuth and inclination as an approach to source localisation. We propose a new low complexity, low delay, and fast converging frequency-domain adaptive algorithm for network echo cancellation in VoIP exploiting MMax and sparse partial (SP) tap-selection criteria in the frequency domain. We incorporate these tap-selection techniques into the multidelay filtering (MDF) algorithm in order to mitigate the delay inherent in frequency-domain algorithms. We illustrate two such approaches and discuss their tradeoff between convergence performance and computational complexity. Simulation results show an improvement in convergence rate for the proposed algorithm over MDF and significantly reduced complexity. The proposed algorithm achieves a convergence performance close to that of the recently proposed, but substantially more complex improved proportionate MDF (IPMDF) algorithm. A novel method for parameter estimation of minimum-phase autoregressive moving average (ARMA) systems in noise is presented. The ARMA parameters are estimated using a damped sinusoidal model representation of the autocorrelation function of the noise-free ARMA signal. The AR parameters are obtained directly from the estimates of the damped sinusoidal model parameters with guaranteed stability. The MA parameters are estimated using a correlation matching technique. The simulation results show that the proposed method can estimate the ARMA parameters with better accuracy as compared to other reported methods, in particular for low SNRs. We motivate and propose an alternative criterion for the dynamical control of regularization in the context of the standard RLS algorithm. The proposed criterion explores the fact that in finite precision the numerical solution of a regularized linear system of equations may be closer to the analytical (unknown) solution of the original (unregularized) system than the numerical solution of the latter. We develop a measure of accuracy for such solutions and use it to automatically adjust the regularization parameter via a simple feedback mechanism. In order to keep computational complexity low, regularization is implemented indirectly via dithering of the input signal. Simulations show that the proposed criterion can effectively react, and compensate for large condition numbers, the precision available and unnecessarily large levels of regularization. The cancellation of echoes is a vital component of telephony networks. In some cases the echo response that must be identified by the echo canceller is sparse, as for example when telephony traffic is routed over networks with unknown delay such as packet-switched networks. The sparse nature of such a response causes standard adaptive algorithms including normalized LMS to perform poorly. This paper begins by providing a review of techniques that aim to give improved echo cancellation performance when the echo response is sparse. In addition, adaptive filters can also be designed to exploit sparseness in the input signal by using partial update procedures. This concept is discussed and the MMax procedure is reviewed. We proceed to present a new high performance sparse adaptive algorithm and provide comparative echo cancellation results to show the relative performance of the existing and new algorithms. Finally, an efficient low cost implementation of our new algorithm using partial update adaptation is presented and evaluated. This algorithm exploits both sparseness of the echo response and also sparseness of the input signal in order to achieve high performance without high computational cost. An algorithm to generate representations of system identification (SI) errors, which enables systematic testing of the performance of system equalization techniques, is proposed. With this algorithm, the normalized projection misalignment (NPM) of the generated error representation can be chosen to suit the particular characteristics of the application under test. Additionally, the generated error representation can represent all the error vectors corresponding to different scaling factors in the estimates of the system impulse response (SIR), without influencing the signal-to-distortion ratio (SDR) of the equalized impulse response. In this paper we discuss a method for localizing acoustic reflectors in space based on acoustic measurements on source-to-microphone reflective paths. The method converts Time of Arrival (TOA) and Time Difference of Arrival (TDOA) into quadratic constraints on the line corresponding to the reflector. In order to be robust against measurement errors we derive an exact solution for the minimization of a cost function that combines an arbitrary number of quadratic constraints. Moreover we propose a new method for the analytic prediction of reflector localization accuracy. This method is sufficiently general to be applicable to a wide range of estimation problems. A novel approach is proposed for robust multichannel dereverberation in the presence of system identification error (SIEs), based on channel shortening. A mathematical link is derived between the well known multiple-input/output inverse theorem (MINT) algorithm and channel shortening. The relaxed multichannel least squares (RMCLS) algorithm is then proposed as an efficient realization within the channel shortening paradigm and is shown through experimental results to outperform MINT in the presence of SIEs. While the RMCLS is robust to SIEs, the coloration of the output cannot be controlled. Two extensions to RMCLS are proposed to control the level of coloration and the performances of both extensions are evaluated comparatively. It is shown that both substantially maintain the dereverberation performance and robustness to SIEs obtained from RMCLS while effectively controlling the level of coloration introduced. In noise reduction, a common approach is to use a microphone array with a beamformer that combines the individual microphone signals to extract a desired speech signal. The beamformer weights usually depend on the statistics of the noise and desired speech signals, which cannot be directly observed and must be estimated. Estimators based on the speech presence probability (SPP) seek to update the statistics estimates only when desired speech is known to be absent or present. However, they do not normally distinguish between desired and undesired speech sources. In this contribution, an algorithm is proposed to distinguish between these two types of sources using additional spatial information, by estimating a desired speech presence probability based on the combination of a multichannel SPP and a direction of arrival (DOA) based probability. The DOA-based probability is computed using DOA estimates for each time-frequency bin. The estimated statistics are then used to compute the weights of a spherical harmonic domain tradeoff beamformer, which achieves a balance between noise reduction and speech distortion. The performance evaluation demonstrates the effectiveness of the proposed approach at suppressing both background noise and spatially coherent noise. A number of audio examples and sample spectrograms are also provided. Reverberant speech can be described as sounding distant with noticeable coloration and echo. These detrimental perceptual effects are caused by early and late reflections, respectively, and reduces the fidelity and intelligibility of speech. It is well-known that the echo density of the reflections increases with time. Therefore, the temporal structure of early and late reflections differs. In this paper, we combine two different dereverberation techniques that were recently developed to suppress early and late reverberation separately. First, late reverberation is suppressed using a spectral processing technique that is based on a statistical reverberation model. Secondly, early reverberation and residual late reverberation are suppressed using a linear prediction (LP) residual processing technique. In addition, an objective measure based on the kurtosis of the LP residual is proposed to measure the coloration caused by early reflections. Experimental results demonstrate the beneficial use of the new single microphone system that reduces echo and coloration with little speech distortion. Localization of early room reflections can be achieved by estimating the time-differences-of-arrival (TDOAs) of reflected waves between elements of a microphone array. For an unknown source, we propose to apply sparse blind system identification (BSI) methods to identify the acoustic impulse responses, from which the TDOAs of temporally sparse reflections are estimated. The proposed time- and frequency-domain adaptive algorithms based on crossrelation formulation are regularized by incorporating an l1 -norm sparseness constraint, which is realized using a split Bregman method. These algorithms are shown to outperform standard crossrelation-based BSI techniques when estimating TDOAs of reflections in the presence of background noise. The pseudo-periodicity of voiced speech can be exploited in several speech processing applications. This requires however that the precise locations of the glottal closure instants (GCIs) are available. The focus of this paper is the evaluation of automatic methods for the detection of GCIs directly from the speech waveform. Five state-of-the-art GCI detection algorithms are compared using six different databases with contemporaneous electroglottographic recordings as ground truth, and containing many hours of speech by multiple speakers. The five techniques compared are the Hilbert Envelope-based detection (HE), the Zero Frequency Resonator-based method (ZFR), the Dynamic Programming Phase Slope Algorithm (DYPSA), the Speech Event Detection using the Residual Excitation And a Mean-based Signal (SEDREAMS) and the Yet Another GCI Algorithm (YAGA). The efficacy of these methods is first evaluated on clean speech, both in terms of reliabililty and accuracy. Their robustness to additive noise and to reverberation is also assessed. A further contribution of the paper is the evaluation of their performance on a concrete application of speech processing: the causal-anticausal decomposition of speech. It is shown that for clean speech, SEDREAMS and YAGA are the best performing techniques, both in terms of identification rate and accuracy. ZFR and SEDREAMS also show a superior robustness to additive noise and reverberation. Analysis of the leaky least mean square (LMS) adaptive algorithm has justified the use of a leakage factor in many applications. In this work, a similar leakage factor is introduced in the two-channel LMS and the extended LMS (XLMS) algorithms for use in stereophonic acoustic echo cancellation. This is compared with the alternative of adding random white noise to the input stereo signals. Simulations and experimental results indicate that introduction of the leakage factor is superior to the direct addition of random white noise. Performance measures used are based on output error and weight error vector norms. Die Analyse des adaptiven 'leaky least mean square (LMS)' Algorithmus rechtfertigt fur viele Anwendungen die Verwendung eines 'Leakage'-Faktors. In dieser Arbeit wird einahnlicher Faktor fur einen zweikanaligen LMS Algorithmus und einen erweiterten LMS (XLMS) zur zweikanaligen Echokompensation vorgestellt. Dies wird mit der Addition von weisem Rauschen zu den Eingangssignalen verglichen. Simulationen zeigen, das die Einfuhrung eines 'Leakage'-Faktors der einfachen Addition von weisem Rauschenuberlegen ist. Die verwendeten Mase zur Beurteilung der Verfahren basieren auf dem Ausgangsfehler und dem Systemabstand. L'analyse de l'algorithme de gradient stochastique avec perte (leaky LMS) a permis de justifier l'usage d'un facteur de perte dans de nombreuses applications. Un facteur de perte similaire est introduit dans cet article pour les algorithmes LMS deux-canaux et LMSetendu (XLMS) dans le contexte d'annulation d'echo stereophonique. Cette approche est comparee avec celle consistanti?additionner un bruit blanc aux signaux stereo d'entree. Des simulations et des resultats experimentaux indiquent que l'introduction du facteur de perte est preferablei?l'addition directe de bruit blanc. Les mesures de performance utilisees sont basees sur l'erreur de sortie et les normes des vecteurs d'erreur sur les coefficients. Spatially distributed acoustic sensors find increasingly many new applications in speech-based human-machine interfaces. One well researched topic is the localization of sound sources from Time Differences Of Arrival (TDOAs) measurements. Typically, the propagation speed of sound is considered a known constant. However due to temperature variations its value is known only up to some uncertainty. This paper exploits TDOA-based localization techniques in order to estimate accurately the actual speed of sound. Experimental results using both simulated and real data demonstrate the feasibility of the proposed method. Furthermore, the practical validation of this work considers two distinct experiments that are aimed at inferring information about enclosed sound fields. The first experiment concerns the calculation of the air temperature from the estimated speed of sound. The second experiment highlights the effects of temperature variations on the inference of the physical location of reflective boundaries of the acoustic enclosure. In the latter case it is shown that the position estimates of the reflective surfaces in a room can be improved when the correct propagation speed is first estimated using this method. Reverberation Time (T60) is an important measure of the acoustic properties of a room. It can provide information about the acoustic environment, the intelligibility, and quality of speech recorded in the room, and help improve the performance of speech processing algorithms with reverberant speech. Where the acoustic impulse response of the room is not available, the T60 must be estimated non-intrusively from reverberant speech. State-of-the-art non-intrusive T60 estimators have been shown to be strongly biased in the presence of noise. We describe a novel T60 estimation algorithm based on spectral decay distributions that provides robustness to additive noise for a range of realistic noise types for signal-to-noise ratios in the range 0 to 35 dB and T60s between 200 and 950 ms. The proposed method also has much reduced computational cost. The tracking performance of adaptive filters is crucially important in practical applications involving time-varying systems. We present an analysis of the tracking performance for IPNLMS, one of the best known and best performing algorithms originally targeted at sparse system identification. We then validate our analytic results in practical simulations for echo cancellation for sparse and dispersive time-varying unknown echo path systems. These results show the analysis to be highly accurate in all the cases studied. The diffuseness of sound fields has previously been estimated in the spatial domain using the spatial coherence between a pair of microphones (omnidirectional or first-order). In this paper, we propose a diffuseness estimator for spherical microphone arrays based on the coherence between eigenbeams, which result from a spherical harmonic decomposition of the sound field. The weighted averaging of the diffuseness estimates over all eigenbeam pairs is shown to significantly reduce the variance of the estimates, particularly in fields with low diffuseness. Signals captured by microphone arrays provide spatial diversity that can be exploited by multichannel processing algorithms to suppress noise and reverberation. Beamforming is a class of approaches that treats the problem with respect to the spatial location of wanted and competing sources, leveraging properties of propagation of waves in free space. A related class of algorithms is channel equalization that exploits knowledge of the acoustic impulse response between a source and microphones with a view to near-perfect dereverberation. Beamforming has been shown to be a very powerful and practical tool in a number of domains, whereas channel equalizers are notoriously sensitive to noise and channel mismatch leading to limited practical applicability. This paper investigates some of the common properties of these algorithms and presents a solution incorporating approaches from both disciplines. In this paper, we consider the problem of remote vector Gaussian source coding for a wireless acoustic sensor network. Each node receives messages from multiple nodes in the network and decodes these messages using its own measurement of the sound field as side information. The node's measurement and the estimates of the source resulting from decoding the received messages are then jointly encoded and transmitted to a neighbouring node in the network. We show that for this distributed source coding scenario, one can encode a so-called conditional sufficient statistic of the sources instead of jointly encoding multiple sources. We focus on the case where node measurements are in form of noisy linearly mixed combinations of the sources and the acoustic channel mixing matrices are invertible. For this problem, we derive the rate-distortion function for vector Gaussian sources and under covariance distortion constraints. Blind identification of single-input multiple-output (SIMO) systems is not normally possible if common zeros exist in the channels. Studies of measured acoustic SIMO systems show that near-common zeros occur in such systems as encountered in the speech dereverberation task. We therefore introduce a method to add additional diversity to the SIMO system to be identified which we term forced spectral diversity (FSD) and we show that its use leads to an identification-equalization approach that gives improved dereverberation. As part of this work, we show the link between channel diversity and the effect of common zeros. We also define and discuss in more detail the concept and impact of near-common zeros. The proposed algorithm is presented specifically for a two-channel system where such near-common zeros exist. In hands-free communications, speech received by a microphone is distorted by room reverberation that can reduce the intelligibility of speech. An approach to dereverberation is firstly to estimate the impulse responses of the acoustic channels between the speaker and the microphones and secondly to design a multichannel equalization system based on the estimated impulse responses. Traditional equalization techniques are designed without the consideration of estimation errors that are commonly introduced by the system identification process. In this work, a System-Identification-Error-Robust Equalization Method (SIEREM) for the equalization of multichannel room acoustic systems is presented. Experimental results for dereverberation using SIEREM applied to estimates of single-input multiple-output acoustic systems with known level of estimation errors show that the proposed equalization design significantly outperforms existing methods in the presence of both synthetic and real system identification errors. We present an algorithm for blind estimation of the magnitude response of an acoustic channel from single microphone observations of a speech signal. The algorithm employs channel robust RASTA filtered Mel-frequency cepstral coefficients as features to train a Gaussian mixture model based classifier and average clean speech spectra are associated with each mixture; these are then used to blindly estimate the acoustic channel magnitude response from speech that has undergone spectral modification due to the channel. Experimental results using a variety of simulated and measured acoustic channels and additive babble noise, car noise and white Gaussian noise are presented. The results demonstrate that the proposed method is able to estimate a variety of channel magnitude responses to within an Itakura distance of dI \\u22640.5 for SNR \\u226510 dB. The common zeros problem for Blind System Identification (BSI) has been well known to degrade the performance of classic BSI algorithms and therefore limits performance of subsequent speech dereverberation. Recently, we have shown that multichannel systems cannot be well identified if near-common zeros are present. In this work, we further study the near-common zeros problem using channel diversity measure. We then investigate the use of forced spectral diversity (FSD) based on a combination of spectral shaping filters and effective channel under modelling. Simulation results show the effectiveness of the proposed approach. A sparse system identification algorithm for network echo cancellation is presented. This new approach exploits both the fast convergence of the improved proportionate normalized least mean square (IPNLMS) algorithm and the efficient implementation of the multidelay adaptive filtering (MDF) algorithm inheriting the beneficial properties of both. The proposed IPMDF algorithm is evaluated using impulse responses with various degrees of sparseness. Simulation results are also presented for both speech and white Gaussian noise input sequences. It has been shown that the IPMDF algorithm outperforms the MDF and IPNLMS algorithms for both sparse and dispersive echo path impulse responses. Computational complexity of the proposed algorithm is also discussed. Reverberation is a process that distorts a wanted signal and impairs perceived speech quality. In the context of multichannel dereverberation, channel-based methods and beamforming are two common approaches. Channel-based methods such as the multiple input/output inverse theorem (MINT) can provide perfect dereverberation provided the exact acoustic impulse responses (AIRs) are known. However, they have been shown to be very sensitive to AIR estimation errors for which several modifications have consequently been proposed. Conversely, beamformers are significantly more robust but provide comparatively modest dereverberation. While the two approaches are conventionally considered independent, both can be formulated as a filter-and-sum operation with differing filter design criteria. We propose a unified framework, termed MINT-Forming, that exploits this similarity and introduces a mixing parameter to control the tradeoff between the potential performance of MINT and the robustness of beamforming. Empirical results show that the mixing parameter is a monotonic function of channel estimation error, whereby a MINT solution is preferred when channel estimation error is low. The problem of blind estimation of the room acoustic clarity index C 50 from single-channel reverberant speech signals is presented in this paper. We analyze the performance of several machine learning methods for a regression task using 309 features derived from the speech signal and modeled with a Deep Belief Network (DBN), Classification And Regression Tree (CART) and Linear Regression (LR). These techniques are evaluated on a large test database (86 hours) that includes babble noise and reverberation using both artificial and real room impulses responses (RIRs). All methods are trained on a database which contains noise, speech and simulated RIRs different from the test set. The performance results show that the DBN model gives the lowest error for the simulated RIRs whereas the LR model gives the best generalization performance with the highest accuracy for real RIRs. 1Department of Electrical and Computer Engineering, Curtin University of Technology, Perth, WA 6845, Australia 2College of Engineering & Computer Science, The Australian National University, Canberra, ACT 0200, Australia 3 Institute of Physics, Signal Processing Group, University of Oldenburg, 26111 Oldenburg, Germany 4 School of Engineering, Bar-Ilan University, 52900 Tel Aviv, Israel 5Department of Electrical and Electronic Engineering, Imperial College, London SW7 2AZ, UK 6Microsoft Research, USA Acoustic scene reconstruction is a process that aims to infer characteristics of the environment from acoustic measurements. We investigate the problem of locating planar reflectors in rooms, such as walls and furniture, from signals obtained using distributed microphones. Specifically, localization of multiple two- dimensional (2-D) reflectors is achieved by estimation of the time of arrival (TOA) of reflected signals by analysis of acoustic impulse responses (AIRs). The estimated TOAs are converted into elliptical constraints about the location of the line reflector, which is then localized by combining multiple constraints. When multiple walls are present in the acoustic scene, an ambiguity problem arises, which we show can be addressed using the Hough transform. Additionally, the Hough transform significantly improves the robustness of the estimation for noisy measurements. The proposed approach is evaluated using simulated rooms under a variety of different controlled conditions where the floor and ceiling are perfectly absorbing. Results using AIRs measured in a real environment are also given. Additionally, results showing the robustness to additive noise in the TOA information are presented, with particular reference to the improvement achieved through the use of the Hough transform. In this paper, we consider the problem of source coding for a wireless acoustic sensor network where each node in the network makes its own noisy measurement of the sound field, and communicates with other nodes in the network by sending and receiving encoded versions of the measurements. To make use of the correlation between the sources available at the nodes, we consider the possibility of combining the measurement and the received messages into one single message at each node instead of forwarding the received messages and separate encoding of the measurement. Moreover, to exploit the correlation between the messages received by a node and the node?s measurement of the source, we propose to use the measurement as side information and thereby form a distributed source coding (DSC) problem. Assuming that the sources are Gaussian, we then derive the rate-distortion function (RDF) for the resulting remote DSC problem under covariance matrix distortion constraints. We further show that for this problem, the Gaussian source is the worst to code. Thus, the Gaussian RDF provides an upper bound to other sources such as audio signals. We then turn our attention to audio signals. We consider an acoustical model based on the room impulse response (RIR) and provide simulation results for the rate-distortion performance in a practical setup where a set of microphones record the sound in a standard listening room. Since our reconstruction scheme and distortion measure are defined over the direct sound source, coding and dereverberation are performed in a joint manner. HighlightsWe treat the problem of source coding for wireless acoustic sensor networks.We consider vector sources to make use of the time correlation in the audio sequences.We use the measurements at the receiving nodes as side information using distributed source coding.We derive local rate-distortion functions to be used for rate allocation for an optimal sum-rate.Our encoding/decoding process is joint with dereverberation. The inverse-filtering of acoustic impulse responses (AIRs) can be achieved with existing methods provided a good estimate of the channel is available and the observed signals contain little or no noise. Such assumptions are not generally valid in practical scenarios, leading to much interest in the issue of robustness. In particular, channel shortening (CS) techniques have been shown to be more robust to channel estimation error than existing approaches. In this paper we investigate CS using the relaxed multichannel least-squares (RMCLS) algorithm in the presence of both channel error and additive noise. It is shown quantitatively that shortening the acoustic channel to a few ms duration is more robust than attempting to equalize the channel fully, giving better resultant sound quality for dereverberation. A key point of this paper is to provide an explanation for this added robustness in terms of the equalization filter gain. We provide simulation results and results for practical settings using speech recordings and room impulse response measurements from a real acoustic environment. A roomprint is a quantifiable description of an acoustic environment which can be measured under controlled conditions and estimated from a monophonic recording made in that space. We here identify the properties required of a roomprint in forensic audio applications and review the observable characteristics of a room that, when extracted from recordings, could form the basis of a room-print. Frequency-dependent reverberation time is investigated as a promising characteristic and used in a room identification experiment giving correct identification in 96% of trials. Artificial bandwidth extension (ABWE) of speech signals aims to estimate wideband speech (50 Hz \\u2013 7 kHz) from narrowband signals (300 Hz \\u2013 3.4 kHz). Applying the source-filter model of speech, many existing algorithms estimate vocal tract filter parameters independently of the source signal. However, many current methods for extending the narrowband voice source signal are limited to straightforward signal processing techniques which are only effective for high-band estimation. This paper presents a method for ABWE that employs novel data-driven modelling and an existing spectral mirroring technique to estimate the wideband source signal in both the high and low extension bands. A state-of-the-art Hidden Markov Model-based estimator evaluates the temporal and spectral envelopes in the missing frequency bands, with which the ABWE speech signal is synthesized. Informal listening tests comparing two existing source estimation techniques and two permutations of the proposed approach show an improvement in the perceived bandwidth of speech signals, in particular towards low frequencies. Subjective tests on the same data show a preference for the proposed techniques over the existing methods under test. We address the measurement of reverberation in terms of the (DRR) in the context of the assessment of dereverberation algorithms for which we wish to quantify the level of reverberation before and after processing. The DRR is normally calculated from the impulse response of the reverberating system. However, several important dereverberation algorithms involve nonlinear and/or time-varying processing and therefore their effect cannot conveniently be represented in terms of modifications to the impulse response of the reverberating system. In such cases, we show that a good estimate of DRR can be obtained from the input/output signals alone using the Signal-to-Reverberant Ratio (SRR) only if the source signal is spectrally white and correctly normalized. We study alternative normalization schemes and conclude by showing a least squares optimal normalization procedure for estimating DRR using signal-based SRR measurement. Simulation results illustrate the accuracy of DRR estimation using SRR. Selective-tap algorithms employing the MMax tap selection criterion were originally proposed for low-complexity adaptive filtering. The concept has recently been extended to multichannel adaptive filtering and applied to stereophonic acoustic echo cancellation. This paper first briefly reviews least mean square versions of MMax selective-tap adaptive filtering and then introduces new recursive least squares and affine projection MMax algorithms. We subsequently formulate an analysis of the MMax algorithms for time-varying system identification by modeling the unknown system using a modified Markov process. Analytical results are derived for the tracking performance of MMax selective tap algorithms for normalized least mean square, recursive least squares, and affine projection algorithms. Simulation results are shown to verify the analysis. We propose a new low complexity and fast converging frequency-domain adaptive algorithm for sparse system identification. This is achieved by exploiting the MMax and SP tap-selection criteria for complexity reduction and fast convergence respectively. We incorporate these tap-selection techniques into the multi-delay filtering (MDF) algorithm in order to reduce the delay inherent in frequency-domain algorithms. We illustrate two such approaches and discuss the tradeoff between convergence performance and computational complexity for these approaches. Simulation results show an improvement in convergence rate for the proposed algorithm over MDF with reduced complexity. The proposed algorithm achieves a convergence performance close to that of the recently proposed but substantially more complex improved proportionate MDF algorithm. Speech signals acquired in a reverberant room with microphones positioned at a distance from the talker are degraded in quality due to reverberation and measurement noise. Therefore, enhancement of reverberant speech is important in hands-free telecommunications applications. The perceptual effects of reverberation can be linked to the room impulse response (RIR) between the talker and the microphone and are characterized by: (i) colouration, due to the strong early reflections and (ii) a distant 'echoey' quality due to the decaying tail of the RIR. Accordingly, we present a two-stage multimicrophone method for speech dereverberation. First, spatiotemporal averaging is performed on the linear prediction residual, which primarily reduces the effects of the early reflections. Secondly, a spectral subtraction method is employed to reduce late reverberation. Simulation results with measured RIRs and additive white Gaussian noise illustrate the performance of this method and show that the combined approach performs better than each of the two stages individually. In theory the linearly constrained minimum variance (LCMV) beamformer can achieve perfect dereverberation and noise cancellation when the acoustic transfer functions (ATFs) between all sources (including interferences) and the microphones are known. However, blind estimation of the ATFs remains a difficult task. In this paper the noise reduction of the LCMV beamformer is analyzed and compared with the noise reduction of the minimum variance distortionless response (MVDR) beamformer. In addition, it is shown that the constraint of the LCMV can be modified such that we only require relative transfer functions rather than ATFs to achieve perfect cancellation of coherent interferences. Finally, we evaluate the noise reduction performance achieved by the LCMV and MVDR beamformers for two coherent sources: one desired and one undesired. All-pass polyphase networks (APN) are particularly attractive for acoustical echo cancellation (AEC) arranged in sub-bands. They provide lower inter-band aliasing, delay and computational complexity than their FIR counterparts. Moreover, APNs achieve higher echo return loss enhancement (ERLE) performance and faster convergence than full-band processing. In the paper, the finite precision implementation of APNs is addressed. A procedure is presented for re-optimising the all-pass coefficients of the prototype low-pass filter for finite precision operation. Robust finite precision implementation of a prototype low-pass filter is discussed. The results of a set of AEC experiments are reported with full and 16-bit precision implementation. This paper deals with the blind adaptive identification of single-input multi-output (SIMO) finite impulse response acoustic channels from noise-corrupted observations. The normalized multichannel frequency-domain least-mean-squares (NMCFLMS) algorithm [1] is known to be a very effective and efficient technique for identification of such channels when noise effects can be ignored. It, however, misconverges in presence of noise [2]. In this paper, we present an analysis of noise effects on the NMCFLMS algorithm and propose a novel technique for ameliorating such misconvergence characteristics of the NMCFLMS algorithm for blind channel identification (BCI) with noise by attaching a spectral constraint in the adaptation rule. Experimental results demonstrate that the robustness of the NMCFLMS algorithm for BCI can be significantly improved using such a constraint. Adaptive filtering in subbands is an attractive alternative to full-band schemes in many applications because of the potential for faster convergence and lower computational cost. However, the analysis of a signal into a subband representation and the synthesis back into its original full-band form carries three main penalties. These are that (1) the subsampling process often introduces aliasing, (2) the subband analysis and synthesis processes carry a computational overhead, thereby reducing the gain in efficiency, and (3) the subband analysis and synthesis processes introduce delay into the signal path. A subband scheme is presented that aims to minimize these penalties, thereby allowing the potential advantages of the subband approach to be more fully realized. The scheme is based on infinite impulse response (IIR) filterbanks, formed from allpass polyphase filters, which exhibit very high quality filtering compared to typical finite impulse response (FIR) implementations, have relatively low complexity, introduce a limited degree of phase distortion and have low delay. The scheme, in conjunction with normalized least mean squares (NLMS) adaptive filters, is tested in an acoustic echo control application and shown to give better convergence, lower delay, and lower computational cost than a comparable FIR subband scheme. This paper presents a data-driven approach to the modelling of voice source waveforms. The voice source is a signal that is estimated by inverse-filtering speech signals with an estimate of the vocal tract filter. It is used in speech analysis, synthesis, recognition and coding to decompose a speech signal into its source and vocal tract filter components. Existing approaches parameterize the voice source signal with physically- or mathematically-motivated models. Though the models are well-defined, estimation of their parameters is not well understood and few are capable of reproducing the large variety of voice source waveforms. Here we present a data-driven approach to classify types of voice source waveforms based upon their melfrequency cepstrum coefficients with Gaussian mixture modelling. A set of \\u201cprototype\\u201d waveform classes is derived from a weighted average of voice source cycles from real data. An unknown speech signal is then decomposed into its prototype components and resynthesized. Results indicate that with sixteen voice source classes, low resynthesis errors can be achieved. A frequency-domain adaptive algorithm for acoustic echo cancellation is proposed. This new algorithm dynamically adjusts its step-size according to the sparseness variation in acoustic impulse responses that arise in a mobile environment. Inheriting the beneficial properties of both the fast convergence of the improved proportionate normalized least mean squares (IPNLMS) and the efficient implementation of the multidelay filtering (MDF) algorithm, the proposed sparseness-controlled improved proportionate MDF (SC-IPMDF) algorithm is evaluated using white Gaussian noise (WGN) and speech input signals with acoustic impulse responses of various degrees of sparseness. Simulation results show an improved performance over the MDF and improved proportionate MDF (IPMDF) algorithms with only a modest increase in computational complexity. Partial update adaptive algorithms have been proposed as a means of reducing complexity for adaptive filtering. The MMax tap-selection is one of the most popular tap-selection algorithms. It is well known that the performance of such partial update algorithm reduces with reducing number of filter coefficients selected for adaptation. We propose a low complexity and fast converging adaptive algorithm that exploits the MMax tap-selection. We achieve fast convergence with low complexity by deriving a variable step-size for the MMax normalized least-mean-square (MMax-NLMS) algorithm using its mean square deviation. Simulation results verify that the proposed algorithm achieves higher rate of convergence with lower computational complexity compared to the NLMS algorithm. Blind system identification (BSI) and equalization algorithms have been applied to multichannel systems with high order such as found in acoustic impulse responses. Studies on the performance of such algorithms in the presence of near-common zeros have been limited to low order systems. In this work, we propose two high order clustering algorithms which efficiently extract clusters of near-common zeros within a specified pairwise distance in the z-plane. Using these algorithms, we then quantify the number of common zeros that exist in acoustic systems. In addition, we show how these algorithms can be applied to study of BSI and equalization algorithms in the presence of near-common zeros for such acoustic systems. A data-driven approach is introduced for studying, analyzing and processing the voice source signal. Existing approaches parameterize the voice source signal by using models that are motivated, for example, by a physical model or function-fitting. Such parameterization is often difficult to achieve and it produces a poor approximation to a large variety of real voice source waveforms of the human voice. This paper presents a novel data-driven approach to analyze different types of voice source waveforms using principal component analysis and Gaussian mixture modeling. This approach models certain voice source features that many other approaches fail to model. Prototype voice source waveforms are obtained from each mixture component and analyzed with respect to speaker, phone and pitch. An analysis/synthesis scheme was set up to demonstrate the effectiveness of the method. Compression of the proposed voice source by discarding 75% of the features yields a segmental signal-to-reconstruction error ratio of 13dB and a Bark spectral distortion of 0.14. We present a reverberant speech enhancement algorithm which, operating on the linear prediction residual of spatially averaged multi- microphone observations, utilizes temporal averaging of neighbouring larynx cycles. The enhanced larynx cycles are used to design an equalization filter, which is applied in order to dereverberate both voiced and unvoiced speech. The DYPSA algorithm is employed and evaluated for larynx cycle segmentation of reverberant speech. Simulation results show reverberation reduction with up to 5 dB in terms of segmental signal-to-reverberant ratio and 0.34 in terms of normalized Bark spectral distortion score. We present order K affine projection and recursive least squares adaptive filters employing partial update schemes. The starting point of the work is the MMax tap-selection criterion in which, given a filter length L, only M coefficients are updated that correspond to the M largest magnitude elements of the regression vector. We extend this approach from its existing form of MMax-NLMS to new affine projection and recursive least squares schemes with supporting analysis and simulation results. We discuss the computational complexity of these approaches for two alternative sort procedures. Finally, we extend the MMax criterion to a multichannel case by introducing an exclusivity constraint and show the effectiveness of the resulting XM tap-selection criterion for application to stereophonic acoustic echo cancellation. We propose a data driven, non-intrusive method for speech intelligibility estimation. We begin with a large set of speech signal specific features and use a dimensionality reduction approach based on correlation and principal component analysis to find the most relevant features for intelligibility prediction. These are then used to train a Gaussian mixture model from which the intelligibility of unseen data is inferred. Experimental results show that our method gives a correlation with subjective intelligibility of 0.92 and a correlation of 0.96 with the ANSI standard Speech Intelligibility Index. Pitch estimation has a central role in many speech processing applications. In voiced speech, pitch can be objectively defined as the rate of vibration of the vocal folds. However, pitch is an inherently subjective quantity and cannot be directly measured from the speech signal. It is a nonlinear function of the signal's spectral and temporal energy distribution. A number of methods for pitch estimation have been developed but none can claim to work accurately in the presence of high levels of additive noise or reverberation. Any system of practical importance must be robust to additive noise and reverberation as these are encountered frequently in the field of operation of voice telecommunications systems. In non-intrusive speech quality measurement algorithms, such as the P.563 and LCQA, pitch is used as a feature for quality assessment. The accuracy of this feature in noisy speech signals will be shown to correlate with the accuracy of the objective measure of the quality of the speech signal. In this paper we evaluate the performance of four established state-of-the-art algorithms for pitch estimation in additive noise and reverberation. Furthermore, we show how accurate estimation of the pitch of a speech signal can influence objective speech quality measurement algorithms. Accurate estimation of glottal closing instants (GCIs) and opening instants (GOIs) is important for speech processing applications that benefit from glottal-synchronous processing including pitch tracking, prosodic speech modification, speech dereverberation, synthesis and study of pathological voice. We propose the Yet Another GCI/GOI Algorithm (YAGA) to detect GCIs from speech signals by employing multiscale analysis, the group delay function, and N-best dynamic programming. A novel GOI detector based upon the consistency of the candidates' closed quotients relative to the estimated GCIs is also presented. Particular attention is paid to the precise definition of the glottal closed phase, which we define as the analysis interval that produces minimum deviation from an all-pole model of the speech signal with closed-phase linear prediction (LP). A reference algorithm analyzing both electroglottograph (EGG) and speech signals is described for evaluation of the proposed speech-based algorithm. In addition to the development of a GCI/GOI detector, an important outcome of this work is in demonstrating that GOIs derived from the EGG signal are not necessarily well-suited to closed-phase LP analysis. Evaluation of YAGA against the APLAWD and SAM databases show that GCI identification rates of up to 99.3% can be achieved with an accuracy of 0.3 ms and GOI detection can be achieved equally reliably with an accuracy of 0.5 ms. This paper addresses the performance of objective methods for speech quality assessment in signals with realistic, block-varying degradations. A block algorithm is presented which employs an existing data-driven approach and is shown to outperform current standard algorithms. We present test results performed on a block-varying extension to the C-Qual database. The effects of block size on the accuracy and distribution of errors is also investigated. We present fast-converging adaptive blind channel identification algorithms for acoustic room impulse responses. These new algorithms exploit the fast-convergence of the improved proportionate normalized least-mean-square (IPNLMS) algorithm and address the problem of delay inherent in frequency domain algorithms by employing the multi-delay filter (MDF) structure. Simulation results for both speech and white Guassian noise show that the proposed algorithms outperform current frequency domain blind channel estimation algorithms. Identification of glottal closure instants (GCIs) is important in speech applications which benefit from larynx-synchronous processing. In modern telecommunication applications, speech signals are often obtained inside office rooms, with one or more microphones placed at a distance from the talker. Such speech signals are affected by reverberation due to the reflections from surrounding walls and objects, which distort the observed speech signals and degrade the performance of speech processing algorithms. This paper presents a study of the identifiability of GCIs from reverberant speech using the Dynamic Programming Projected Phase-Slope Algorithm (DYPSA) and new extensions to the multimicrophone case. Two multichannel algorithms are proposed and evaluated; in both cases, considerable performance gains over a single microphone are obtained, with detection rates improved by up to 29% in highly reverberant environments. In the context of acoustic echo cancellation (AEC), it is shown that the level of sparseness in acoustic impulse responses can vary greatly in a mobile environment. When the response is strongly sparse, convergence of conventional approaches is poor. Drawing on techniques originally developed for network echo cancellation (NEC), we propose a class of AEC algorithms that can not only work well in both sparse and dispersive circumstances, but also adapt dynamically to the level of sparseness using a new sparseness-controlled approach. Simulation results, using white Gaussian noise (WGN) and speech input signals, show improved performance over existing methods. The proposed algorithms achieve these improvement with only a modest increase in computational complexity. Speech signals for hands-free telecommunication applications are received by one or more microphones placed at some distance from the talker. In an office environment, for example, unwanted signals such as reverberation and background noise from computers and other talkers will degrade the quality of the received signal. These unwanted components have an adverse effect upon speech processing algorithms and impair intelligibility. This paper demonstrates the use of the Multichannel DYPSA algorithm to identify glottal closure instants (GCIs) from noisy, reverberant speech. Using the estimated GCIs, a spatiotemporal averaging technique is applied to attenuate the unwanted components. Experiments with a microphone array demonstrate the dereverberation and noise suppression of the spatiotemporal averaging method, showing up to a 5 dB improvement in segmental SNR and 0.33 in normalized Bark spectral distortion score. We present the DYPSA algorithm for automatic and reliable estimation of glottal closure instants (GCIs) in voiced speech. Reliable GCI estimation is essential for closed-phase speech analysis, from which can be derived features of the vocal tract and, separately, the voice source. It has been shown that such features can be used with significant advantages in applications such as speaker recognition. DYPSA is automatic and operates using the speech signal alone without the need for an EGG or Laryngograph signal. It incorporates a new technique for estimating GCI candidates and employs dynamic programming to select the most likely candidates according to a defined cost function. We review and evaluate three existing methods and compare our new algorithm to them. Results for DYPSA show GCI detection accuracy to within \\u00b10.25ms on 87% of the test database and fewer than 1% false alarms and misses. A method is proposed for simulating the sound pressure signals on a spherical microphone array in a reverberant enclosure. The method employs spherical harmonic decomposition and takes into account scattering from a solid sphere. An analysis shows that the error in the decomposition can be made arbitrarily small given a sufficient number of spherical harmonics. This paper presents an automatic method to determine the instants of glottal closure (GCIs), or epochs, from the speech signal alone without the need of a laryngograph signal. The proposed algorithm incorporate a new technique for estimating GCI candidates and dynamic programming to select the best candidates according to predefined cost functions. Results show accuracy in estimation to within /spl plusmn/0.25ms on 87% of the test database and less that 1% false alarms and misses. Preliminary experiments using the telephone -degraded NTIMIT database have shown that the algorithm continues to perform well even in the presence of noise. We study the effect of reverberation and source movement on the performance of blind source separation and deconvolution (BSSD) algorithms. Using the model of statistical room acoustics we derive theoretical performance measures for a class of unmixing algorithms when these are used in a reverberant room. We specifically investigate the cases: 1) where separation of only direct paths is performed and 2) the case where unmixing of the full reverberant paths is attempted. We develop closed-form performance measures that are dependent on the geometry used and the chosen unmixing system. Using these measures allows us to draw general conclusions on the robustness to source movement of typical BSSD algorithms. Results indicate that performance of systems that show very good separation in static reverberant environments is significantly reduced when sources move, with performance degrading to that of simple direct-path separation This paper exploits the displacement structure of the coefficient matrix of the linear system of equations pertinent to the Affine Projection Algorithm (APA), to obtain the exact solution in a way faster than any other existing exact method. The main emphasis of the paper is to present the concepts of displacement structure theory and how these are applied to the APA context. We consider the blind signal separation (BSS) problem of instantaneous mixtures using penalty term and natural gradient. A class of Frobenius norm-based algorithms consisting of the offline/block processing (BP), online processing (OP) algorithms, and their normalized versions is proposed for separating nonstationary and nonwhite signals. The BP and OP algorithms, respectively, suitable for blind separation with offline and online data, are derived by using the nonstationarity and nonwhiteness of signals and the natural gradient method in conjunction with an appropriate penalty term. Associated with almost all algorithms employing a gradient method is a gradient noise problem. We thus develop, from BP and OP, their normalized versions in which the update of an unknown demixing matrix is based on the minimal disturbance principle. We show that the resulting updates are in the same direction as those of the original algorithms but with a scaling factor whose upper bound is unity. Algorithms using the nonstationarity and nonwhiteness properties have been proposed before but, due to the use of logarithms in their derivation, they are not capable of separating signals that are not persistently active and require regularization parameters to mitigate the problem. In this paper, the superior performance of the proposed algorithms to the previously proposed logarithm-based algorithms with and without regularization when separating nonpersistently active source signals is presented through some illustrative numerical experiments. Geometric inference is an approach for localizing reflectors in a closed acoustic space. It is based on a simple observation that turns time differences of arrival (TDOA) or time of arrival (TOA) measurements from the signals of a microphone array into a geometric constraint. The reflector localization methodology relies on accurate TDOA which is directly dependent on speed of sound information. Estimating the actual speed of sound at the ambient temperature therefore greatly improves the accuracy of the reflector localization in uncontrolled environments. This manuscript shows how to use the geometric inference jointly with the speed of sound estimation for a more accurate reflector localization. Simulations and experiments show the validity of the proposed approach. Stereophonic acoustic echo cancellation has generated much interest in recent years due to the nonuniqueness and misalignment problems that are caused by the strong interchannel signal coherence. In this paper, we introduce a novel adaptive filtering approach to reduce interchannel coherence which is based on a selective-tap updating procedure. This tap-selection technique is then applied to the normalized least-mean-square, affine projection and recursive least squares algorithms for stereophonic acoustic echo cancellation. Simulation results for the proposed algorithms have shown a significant improvement in convergence rate compared with existing techniques. Measures based on the group delay of the LPC residual have been used by a number of authors to identify the time instants of glottal closure in voiced speech. In this paper, we discuss the theoretical properties of three such measures and we also present a new measure having useful properties. We give a quantitative assessment of each measure's ability to detect glottal closure instants evaluated using a speech database that includes a direct measurement of glottal activity from a Laryngograph/EGG signal. We find that when using a fixed-length analysis window, the best measures can detect the instant of glottal closure in 97% of larynx cycles with a standard deviation of 0.6 ms and that in 9% of these cycles an additional excitation instant is found that normally corresponds to glottal opening. We show that some improvement in detection rate may be obtained if the analysis window length is adapted to the speech pitch. If the measures are applied to the preemphasized speech instead of to the LPC residual, we find that the timing accuracy worsens but the detection rate improves slightly. We assess the computational cost of evaluating the measures and we present new recursive algorithms that give a substantial reduction in computation in all cases. The reverberation time is one of the most prominent acoustic characteristics of an enclosure. Its value can be used to predict speech intelligibility, and is used by speech enhancement techniques to suppress reverberation. The reverberation time is usually obtained by analysing the decay rate of (i) the energy decay curve that is observed when a noise source is switched off, and (ii) the energy decay curve of the room impulse response. Estimating the reverberation time using only the observed reverberant speech signal, i.e., blind estimation, is required for speech evaluation and enhancement techniques. Recently, (semi) blind methods have been developed. Unfortunately, these methods are not very accurate when the source consists of a human speaker, and unnatural speech pauses are required to detect and/or track the decay. In this paper we extract and analyse the decay rate of the energy envelope blindly from the observed reverberation speech signal in the short-time Fourier transform domain. We develop a method to estimate the reverberation time using a property of the distribution of the decay rates. Experimental results using simulated and real reverberant speech signals demonstrate the performance of the new method. The concept underlying most on-line gradient-based algorithms for blind signal separation (BSS) is that the unknown demixing matrix is adjusted with an appropriate step-size in the direction of the gradient computed at each sample instant. Associated with these algorithms is a gradient noise problem. In this paper, we develop, from the on-line processing (OP) algorithm derived using the nonstationarity and nonwhiteness properties, a normalized algorithm in which the update of the demixing matrix is based on the minimal disturbance principle. We show that the resulting updates are in the same direction as those of the original algorithm but with a scaling factor whose upper bound is unity. We evaluate the convergence speed and robustness to gradient noise of the new algorithm. Accurate estimation of glottal closure instants (GCIs) and opening instants (GOIs) is important for speech processing applications that benefit from glottal-synchronous processing. The majority of existing approaches detect GCIs by comparing the differentiated EGG signal to a threshold and are able to provide accurate results during voiced speech. More recent algorithms use a similar approach across multiple dyadic scales using the stationary wavelet transform. All existing approaches are however prone to errors around the transition regions at the end of voiced segments of speech. This paper describes a new method for EGG-based glottal activity detection which exhibits high accuracy over the entirety of voiced segments, including, in particular, the transition regions, thereby giving significant improvement over existing methods. Following a stationary wavelet transform-based preprocessor, detection of excitation due to glottal closure is performed using a group delay function and then true and false detections are discriminated by Gaussian mixture modeling. GOI detection involves additional processing using the estimated GCIs. The main purpose of our algorithm is to provide a ground-truth for GCIs and GOIs. This is essential in order to evaluate algorithms that estimate GCIs and GOIs from the speech signal only, and is also of high value in the analysis of pathological speech where knowledge of GCIs and GOIs is often needed. We compare our algorithm with two previous algorithms against a hand-labeled database. Evaluation has shown an average GCI hit rate of 99.47% and GOI of 99.35%, compared to 96.08 and 92.54 for the best-performing existing algorithm. This paper presents a method for speech time scale modification. Voiced speech is pseudo-periodic, allowing time scale modification by the repetition or removal of cycles as necessary. However, in the case of unvoiced speech and at the boundaries of voiced speech, no such periodicity exists so the speech should not be modified. To address this issue, the proposed approach is novel in its use of the DYPSA algorithm to derive speech periodicity from glottal closure instants (GCIs), followed by a Gaussian Mixture model-based voiced/unvoiced/silence (VUS) classifier. A listening test based on ITU-T P800 has been conducted and has shown that, by employing VUS detection, the average mean opinion score of the perceptual quality of processed speech exceeds that of a method without VUS detection by 0.61 over a range of modification factors. Results are presented as a function of modification factor for normal and fast original talking rate. Reliable time scale modification of high audio quality enables many applications, such as time scale compression for fast scanning of recorded voicemail messages, slowing talking rate for improved intelligibility in forensics and lip synchronization in motion video. Equalization techniques for high order, multichannel, FIR systems are important for dereverberation of speech observed in reverberation using multiple microphones. In this case the multichannel system represents the room impulse responses (RIRs). The existence of near-common zeros in multichannel RIRs can slow down the convergence rate of adaptive inverse filtering algorithms. In this paper, the effect of common and near-common zeros on both the closed-form and the adaptive inverse filtering algorithms is studied. An adaptive shortening algorithm of room acoustics is presented based on this study. The effect of additive sensor noise on single-input-multiple-output (SIMO) blind system identification (BSI) algorithms based upon cross-relation (CR) error is investigated. Previous studies have shown that additive noise in the observed signal results in systems comprising the true estimated channels convolved with an erroneous \\u2018common filter\\u2019, and additionally that identification and removal of this filter significantly improves estimation error. However, the source of the common filter remained an open question. This paper explains the common filter through a first-order perturbation analysis of the CR matrix, showing that it be estimated from the perturbation and the eigenvectors of the noiseless CR matrix. The analysis given in this paper provides a new insight into the effect of noise on SIMO BSI algorithms and forms the first step towards an overall noise robust solution. A class of adaptive blind channel identification algorithms were proposed recently and were demonstrated to be able to successfully identify various types of channels when the observed signals are free from significant levels of measurement noise. In this paper, we provide a study of the effects of noise on these algorithms and show that they misconverge even at moderate values of SNR. We introduce a spectral constraint into the adaptation rule and show that the robustness to noise can be considerably improved. Simulation results are presented for the new algorithm, which demonstrate a significant performance improvement in terms of normalized projection misalignment. We investigate stereophonic acoustic echo cancellation in which solutions for the system can be nonunique and propose the use of selective-tap adaptive filters to address this problem. The main concept is to employ tap selection to optimize jointly for minimum interchannel coherence and maximum L/sub 2/-norm of the subselected tap-input vectors. The exclusive maximum (XM) tap-selection approach is proposed and applied to normalized least-mean squares (NLMS) and recursive least-squares (RLS) algorithm. We propose an approach for solving the nonuniqueness problem employing XM tap selection in combination with a nonlinear preprocessor. Simulation results show a significant improvement in convergence rate compared with existing techniques. We present the Dynamic Programming Projected Phase-Slope Algorithm (DYPSA) for automatic estimation of glottal closure instants (GCIs) in voiced speech. Accurate estimation of GCIs is an important tool that can be applied to a wide range of speech processing tasks including speech analysis, synthesis and coding. DYPSA is automatic and operates using the speech signal alone without the need for an EGG signal. The algorithm employs the phase-slope function and a novel phase-slope projection technique for estimating GCI candidates from the speech signal. The most likely candidates are then selected using a dynamic programming technique to minimize a cost function that we define. We review and evaluate three existing methods of GCI estimation and compare the new DYPSA algorithm to them. Results are presented for the APLAWD and SAM databases for which 95.7% and 93.1% of GCIs are correctly identified A novel algorithm for use in stereophonic acoustic echo cancellation is introduced. The alternating fixed-point structure of the algorithm descends each of the channels alternatively and avoids nonuniqueness in the solutions by employing inter- and intra-channel orthogonal projections. For a noisy environment, the algorithm is shown to provide improved misalignment performance and echo cancellation over leading competitors at a low computational cost. The inherent noise robustness is uncovered using H\\u221e estimation techniques and noise-error bounds are derived. The problem of localizing reflective boundaries in an acoustic environment from acoustic measurements is considered. Specifically, localization of multiple two-dimensional (2-D) line reflectors is achieved by estimation of the time of arrival (TOA) of reflected signals by analysis of acoustic impulse responses (AIRs). The estimated TOAs are used in conjunction with the source and receiver locations to find the loci of solutions whose common tangents correspond to the location of a reflector. The solution to the common tangent estimation is a nonlinear and non-convex problem that can yield local sub-optimal solutions using existing approaches. We therefore propose an analytic method, based on a closed-form estimator, that is guaranteed to converge to the global minimum in an error-free scenario. We further improve the robustness of the approach when errors are introduced in the estimated TOAs by using the Hough transform to find the optimal solution. The proposed approach is evaluated through Monte Carlo runs, using simulated rooms, that demonstrate the feasibility of the proposed approach. Hands-free speech input is required in many modern telecommunication applications that employ autoregressive (AR) techniques such as linear predictive coding. When the hands-free input is obtained in enclosed reverberant spaces such as typical office rooms, the speech signal is distorted by the room transfer function. This paper utilizes theoretical results from statistical room acoustics to analyze the AR modeling of speech under these reverberant conditions. Three cases are considered: (i) AR coefficients calculated from a single observation; (ii) AR coefficients calculated jointly from an M-channel observation (M>1); and (iii) AR coefficients calculated from the output of a delay-and sum beamformer. The statistical analysis, with supporting simulations, shows that the spatial expectation of the AR coefficients for cases (i) and (ii) are approximately equal to those from the original speech, while for case (iii) there is a discrepancy due to spatial correlation between the microphones which can be signi... Accurate estimation of glottal closure instants (GCIs) in voiced speech is important for speech analysis applications which benefit from glottal-synchronous processing. Electroglottograph (EGG) recordings give a measure of the electrical conductance of the glottis, providing a signal which is proportional to its contact area. EGG signals contain little noise or distortion, providing a good reference from which GCIs can be extracted to evaluate GCI estimation from speech recordings. Many approaches impose a threshold on the differentiated EGG signal which provide accurate results during voiced speech but are prone to errors at the onset and end of voicing; modern algorithms use a similar approach across multiple dyadic scales using the stationary wavelet transform. This paper describes a new method for EGG-based GCI estimation named SIGMA, which is based upon the stationary wavelet transform, peak detection with a group delay function and Gaussian Mixture Modelling for discrimination between true and false GCI candidates. In most real-world environments, it is necessary to estimate GCIs from a speech signal recorded with a microphone placed at some distance from the talker. The presence of reverberation, noise and filtering by the vocal tract render GCI detection from real speech signals relatively difficult to achieve compared with the EGG, so EGG-based references have often been used to evaluate GCI detection from speech signals. Evaluation against 500 hand-labelled sentences has shown an accuracy of 99.35%, a 4.7% improvement over a popular existing method. We present an improved adaptive echo cancellation algorithm designed for use with sparse echo path impulse responses such as arise from packet-switched networks. The new approach implicitly segments the impulse response into 'active' and 'inactive' regions, and employs different proportionate updating in each region. An efficient partial updating scheme is then formulated for the new algorithm. Evaluation results are presented to compare the new algorithm against three existing methods in terms of convergence and computational complexity. The results show that the new algorithm outperforms the best existing technique and has lower complexity. This work addresses the I/Q mismatch problem which arises due to analog component tolerances in zero-IF OFDM receivers. The application of this work is to digital audio broadcasting (DAB). The approach is to employ a decision directed LMS-based frequency-adaptive equalizer, applied to OFDM demodulated carriers. The equalizer is trained initially using the phase reference symbols in each frame and is switched to a decision directed mode when the error reaches a defined level. In the decision directed mode, the output is taken prior to the threshold decision device in order to present to the subsequent Viterbi decoder a signal suitable for soft decisions. Simulation results demonstrate convergence within six frames; the particular convergence rate depending on choice of step-size and number of carriers per group employed. Even with large phase and amplitude imbalances, 25/spl deg/ and 5 dB respectively, in a DAB system the compensation algorithm attains a reduction in raw BER from 10/sup -1.5/ to 10/sup -5/ with a 15 dB SNR. We present a novel frequency domain blind multichannel identification approach. This is a dual filter based approach comprising a background and a foreground filter. The foreground filter tracks the changes in the cost function of the background filter and employs an update decision criterion making it robust to observation noise and facilitating tracking of changes in the unknown system. Simulation results for both speech and white Gaussian noise input signals are presented to illustrate the algorithm's performance for acoustic systems. Signals captured by a set of microphones in a speech communication system are mixtures of desired and undesired signals and ambient noise. Existing beamformers can be divided into those that preserve or distort the desired signal. Beamformers that preserve the desired signal are, for example, the linearly constrained minimum variance (LCMV) beamformer that is supposed, ideally, to reject the undesired signal and reduce the ambient noise power, and the minimum variance distortionless response (MVDR) beamformer that reduces the interference-plus-noise power. The multichannel Wiener filter, on the other hand, reduces the interference-plus-noise power without preserving the desired signal. In this paper, a speech distortion and interference rejection constraint (SDIRC) beamformer is derived that minimizes the ambient noise power subject to specific constraints that allow a tradeoff between speech distortion and interference-plus-noise reduction on the one hand, and undesired signal and ambient noise reductions on the other hand. Closed-form expressions for the performance measures of the SDIRC beamformer are derived and the relations to the aforementioned beamformers are derived. The performance evaluation demonstrates the tradeoffs that can be made using the SDIRC beamformer. We propose a noise robust adaptive blind multichannel identification algorithm for acoustic impulse responses. It has been known that the normalized multichannel frequency domain least-mean-square (NMCFLMS) algorithm misconverges under low signal-to- noise ratio. The coefficients of NMCFLMS converge initially toward the true impulse response after which they then misconverge. The extended NMCFLMS (ext-NMCFLMS) algorithm which has been proposed to mitigate this misconvergence problem assumes the knowledge of magnitude and time-differences-of-arrival (TDOA) of the direct paths for the acoustic impulse responses. In this work, we show how the TDOA can be obtained. More importantly, we present a novel approach to estimate the magnitude of the direct path component under practical conditions. We then show how these estimates can be incorporated to the proposed ext-NMCFLMS with direct path estimation algorithm. We analyze how errors in these estimates affect the performance of the proposed algorithm. The performance in terms of misalignment of adaptive algorithms, in general, is dependent on the conditioning of the input signal covariance matrix. The performance of two-channel adaptive algorithms is further degraded by the high interchannel coherence between the two input signals. In this letter, we establish the relationship between interchannel coherence of the two input signals and condition of the corresponding covariance matrix for stereo acoustic echo cancellation application. We show how this relationship affects the misalignment of a frequency-domain adaptive algorithm. We provide simulation results for both white Gaussian noise and speech input to verify our mathematical analysis. Equalization of room transfer functions (RTFs) is an important topic with several applications in acoustic signal processing. RTFs are often modeled as finite-impulse response filters, characterized by orders of thousands of taps and non-minimum phase. In practice, only approximate estimates of the actual RTFs are available due to measurement noise, limited estimation accuracy, and temporal variation of source-receiver position. These issues make equalization a difficult problem. In this paper, we discuss multichannel equalization with focus on inexact RTF estimates. We present a multichannel method for the equalization filter design utilizing decimated and oversampled subbands, where the full-band acoustic impulse response is decomposed into equivalent subband filters prior to equalization. This technique is not only more computationally efficient but also more robust to impulse response inaccuracies compared with the full-band counterpart. In this paper, we consider a distributed remote source coding problem, where a sequence of observations of source vectors is available at the encoder. The problem is to specify the optimal rate for encoding the observations subject to a covariance matrix distortion constraint and in the presence of side information at the decoder. For this problem, we derive lower and upper bounds on the rate-distortion function (RDF) for the Gaussian case, which in general do not coincide. We then provide some cases, where the RDF can be derived exactly. We also show that previous results on specific instances of this problem can be generalized using our results. We finally show that if the distortion measure is the mean squared error, or if it is replaced by a certain mutual information constraint, the optimal rate can be derived from our main result. Knowledge of the Direct-to-Reverberant Ratio (DRR) and Reverberation Time (T 60 ) can be used to better perform speech and audio processing such as dereverberation. Established methods compute these parameters from measured Acoustic Impulse Responses (AIRs). However, in many practical situations the AIR is not available and the parameters must be estimated non-intrusively directly from noisy speech or audio signals. The Acoustic Characterization of Environments (ACE) Challenge is a competition to identify the most promising non-intrusive DRR and T 60 estimation methods using real noisy reverberant speech. We describe the ACE corpus comprising multi-channel AIRs, and multi-channel noise including ambient, fan and babble noise recorded in the same environment as the measured AIRs, along with the corresponding DRR and T 60 measurements. The evaluation methodology is discussed and comparative results are shown. Reverberation time, or T 60 , is a key parameter used for characterizing acoustic spaces. Blind T 60 estimation is useful for many applications including speech intelligibility estimation, acoustic scene analysis and dereverberation. In our previous work, a single-channel blind T 60 estimator was proposed employing spectral analysis in the modulation frequency domain. It was shown that the estimation accuracy is crucially affected by the window lengths used for transformation to the modulation domain. In this work, we propose the use of a sliding window length that is dynamically updated depending on the length of the detected decay region. Experimental results demonstrated that in the presence of noise, estimation accuracy was improved over our previous work for T 60 up to 700 ms. When compared against two alternative algorithms from the literature, the proposed approach demonstrated higher accuracy for T 60 between 500 ms and 1 s. Finally, the proposed approach was shown to be more computationally efficient compared to two of the three alternative algorithms. The accuracy of direction of arrival estimation tends to degrade under reverberant conditions due to the presence of reflected signal components which are correlated with the direct path. The recently proposed direct-path dominance test provides a means of identifying time-frequency regions in which a single signal path is dominant. By analysing only these regions it was shown that the accuracy of the FS-MUSIC algorithm could be significantly improved. However, for real-time implementation a less computationally demanding localisation algorithm would be preferable. In the present contribution we investigate the direct-path dominance test as a preprocessing step to pseudo-intensity vector-based localisation. A novel formulation of the pseudo-intensity vector is proposed which further exploits the direct path dominance test and leads to improved localisation performance. A system and method for speech quality detection is included. The method may include receiving, at a computing device, a first speech signal associated with a particular user. The method may include extracting one or more short-term features from the first speech signal wherein extracting short-term features includes extracting a time frame of between 10-50 ms. The method may also include determining one or more statistics of each of the one or more short-term features from the first speech signal. The method may further include classifying the one or more statistics as belonging to one of a set of quality classes. Acoustic scene mapping creates a representation of positions of audio sources such as talkers within the surrounding environment of a microphone array. By allowing the array to move, the acoustic scene can be explored in order to improve the map. Furthermore, the spatial diversity of the kinematic array allows for estimation of the source-sensor distance in scenarios where source directions of arrival are measured. As sound source localization is performed relative to the array position, mapping of acoustic sources requires knowledge of the absolute position of the microphone array in the room. If the array is moving, its absolute position is unknown in practice. Hence, Simultaneous Localization and Mapping (SLAM) is required in order to localize the microphone array position and map the surrounding sound sources. In realistic environments, microphone arrays receive a convolutive mixture of direct-path speech signals, noise and reflections due to reverberation. A key challenge of Acoustic SLAM (a-SLAM) is robustness against reverberant clutter measurements and missing source detections. This paper proposes a novel bearing-only a-SLAM approach using a Single-Cluster Probability Hypothesis Density filter. Results demonstrate convergence to accurate estimates of the array trajectory and source positions. An approach for 3D source localization using a spherical microphone array is proposed that gives improved accuracy compared to intensity-based methods. First order spherical harmonics are first used to obtain an initial approximate localization result and then the initial result is improved based on an optimized grid search in the local vicinity using the method of least squares and high-order spherical harmonics. We show that this approach outperforms the first-order approach and shows strong robustness to reverberation and noise. The worst average error of 3 degrees was found in our experiments in the presence of realistic reverberation and noise. We present a novel adaptive algorithm exploiting the sparseness of an impulse response for network echo cancellation. This sparseness-controlled improved proportionate normalized least mean square (SC-IPNLMS) algorithm is based on IPNLMS which allocates a step-size gain proportional to each filter coefficient. The proposed SC-IPNLMS algorithm achieves improved convergence over IPNLMS by estimating the sparseness of the impulse response and allocating gains for each step- size such that a higher weighting is given to the proportionate term of the IPNLMS for sparse impulse responses. For a less sparse impulse response, a higher weighting will be allocated to the NLMS term. Simulation results presented show improved performance over the IPNLMS algorithm during convergence before and after an echo path change has been introduced. We also discuss the computational complexity of the proposed algorithm. In this work, an adaptive acoustic source tracking algorithm is proposed. It is based on eigenbeams which perform spatial decomposition of the sound field, works in two dimensions for tracking of azimuth and elevation, has low computational complexity, and is robust to noise and room reverberation. The tracking is performed using an adaptive principal component analysis of the particle velocity vector, which points from the acoustic source to the sensor. The particle velocity vector is estimated using a spherical microphone array, and is formed by combining the first-order eigenbeams. Direction of arrival estimation using a spherical microphone array is an important and growing research area. One promising algorithm is the recently proposed Subspace Pseudo-Intensity Vector method. In this contribution the Subspace Pseudo-Intensity Vector method is combined with a state-of-the-art method for robustly estimating the centres of mass in a 2D histogram based on matching pursuits. The performance of the improved Subspace Pseudo-Intensity Vector method is evaluated in the context of localising multiple moving sources where it is shown to outperform competing methods in terms of clutter rate and the number of missed detections whilst remaining comparable in terms of localisation accuracy. Speech dereverberation is a signal processing technique of key importance for successful hands-free speech acquisition in applications of telecommunications and automatic speech recognition. Over the last few years, speech dereverberation has become a hot research topic driven by consumer demand, the availability of terminals based on Skype which encourage hands-free operation and the development of promising signal processing algorithms. Speech Dereverberation gathers together an overview, a mathematical formulation of the problem and the state-of-the-art solutions for dereverberation. Speech Dereverberation presents the most important current approaches to the problem of reverberation. It begins by providing a focused and digestible review of the relevant topics in room acoustics and also describes key performance measures for dereverberation. The algorithms are then explained together with relevant mathematical analysis and supporting examples that enable the reader to see the relative strengths and weaknesses of the various techniques, as well as giving a clear understanding of the open questions still to be addressed in this topic. Techniques rooted in speech enhancement are included, in addition to a substantial treatment of multichannel blind acoustic system identification and inversion. The TRINICON framework is shown in the context of dereverberation to be a powerful generalization of the signal processing for a important range of analysis and enhancement techniques. Speech Dereverberation offers the reader an overview of the subject area, as well as an in-depth text on the advanced signal processing involved. The book benefits the reader by providing such a wealth of information in one place, defines the current state of the art and, lastly, encourages further work on this topic by offering open research questions to exercise the curiosity of the reader. It is suitable for students at masters and doctoral level, as well as established researchers. In this paper, we consider the problem of remote vector Gaussian source coding for a wireless acoustic sensor network. Each node receives messages from multiple nodes in the network and decodes these messages using its own measurement of the sound field as side information. The node's measurement and the estimates of the source resulting from decoding the received messages are then jointly encoded and transmitted to a neighboring node in the network. We show that for this distributed source coding scenario, one can encode a so-called conditional sufficient statistic of the sources instead of jointly encoding multiple sources. We focus on the case where node measurements are in form of noisy linearly mixed combinations of the sources and the acoustic channel mixing matrices are invertible. For this problem, we derive the rate-distortion function for vector Gaussian sources and under covariance distortion constraints.\",\n",
            "  \"2029994413\": \"In this paper, a new state-of-the-art multi-cell MMSE scheme is proposed for massive MIMO networks, which includes an uplink MMSE detector and a downlink MMSE precoder. The main novelty is that it exploits all available pilots for interference suppression. Specifically, let $K$ and $B$ denote the number of users per cell and the number of orthogonal pilot sequences in the network, respectively, where $eta = B/K$ is the pilot reuse factor. Then our multi-cell MMSE scheme utilizes all $B$ channel directions, that can be estimated locally at each base station, to actively suppress both intra-cell and inter-cell interference. The proposed scheme is particularly practical and general, since power control for the pilot and payload, imperfect channel estimation and arbitrary pilot allocation are all accounted for. Simulations show that significant spectral efficiency (SE) gains are obtained over the single-cell MMSE scheme and the multi-cell ZF, particularly for large $eta$ and/or $K$. Furthermore, large-scale approximations of the uplink and downlink SINRs are derived, which are asymptotically tight in the large-system limit. The approximations are easy to compute and very accurate even for small system dimensions. Using these SINR approximations, a low-complexity power control algorithm is also proposed to maximize the sum SE. Pilot-based estimation of the squared Euclidean norm of the channel vector of a Rayleigh fading system is considered. Unlike most previous work in the area of estimation of multiple antenna channels, we consider Bayesian estimation where the long-term channel statistics are known a priori. Closed-form expressions of the minimum mean square error (MMSE) estimator and its mean squared error (MSE) are derived for the cases of either an unweighted or a weighted unitary pilot matrix. The problem of finding the optimal pilot weighting, in the sense of minimizing the average MSE, is solved and a simple algorithm is proposed to achieve this power allocation numerically. The numerical evaluation shows that an optimal weighting can significantly improve the estimation quality in spatially correlated environments. What would a cellular network designed for maximal energy efficiency look like? To answer this fundamental question, tools from stochastic geometry are used in this paper to model future cellular networks and obtain a new lower bound on the average uplink spectral efficiency. This enables us to formulate a tractable uplink energy efficiency (EE) maximization problem and solve it analytically with respect to the density of base stations (BSs), the transmit power levels, the number of BS antennas and users per cell, and the pilot reuse factor. The closed-form expressions obtained from this general EE maximization framework provide valuable insights on the interplay between the optimization variables, hardware characteristics, and propagation environment. Small cells are proved to give high EE, but the EE improvement saturates quickly with the BS density. Interestingly, the maximal EE is achieved by also equipping the BSs with multiple antennas and operate in a \\u201cmassive MIMO\\u201d fashion, where the array gain from coherent detection mitigates interference and the multiplexing of many users reduces the energy cost per user. In this paper, we examine the feasibility of wireless energy transfer (WET) using arrays with multiple antennas. Specifically, we compute the probability of outage in energy transfer over a Rician fading channel when the base station (BS) with multiple antennas transfers energy to a wireless sensor node (WSN). Through our analytical and numerical results, we prove that by deploying more antennas at the BS, the range of WET can be increased while maintaining a target outage probability. We observe that the use of massive antenna arrays at the BS results into huge savings of radiated energy. We show that for typical energy levels used in WET, the outage performance with imperfect channel state information (CSI) is essentially the same as that obtained based on perfect CSI. We also observe that a strong line-of-sight component between the BS and the WSN lowers the probability of outage in energy transfer. This paper considers a scenario of short-range communication, known as device-to-device (D2D) communication, where D2D users reuse the downlink resources of a cellular network to transmit directly to their corresponding receivers. In addition, multiple antennas at the base station (BS) are used in order to simultaneously support multiple cellular users using multiuser or massive MIMO. The network model considers a fixed number of cellular users and that D2D users are distributed according to a homogeneous Poisson point process (PPP). Two metrics are studied, namely, average sum rate (ASR) and energy efficiency (EE). We derive tractable expressions and study the tradeoffs between the ASR and EE as functions of the number of BS antennas and density of D2D users for a given coverage area. In this paper, the problem of optimal maximum likelihood detection in a single user single-input multiple-output (SIMO) channel with phase noise at the receiver is considered. The optimal detection rules under training are derived for two operation modes, namely when the phase increments are fully correlated among the M receiver antennas (synchronous operation) and when they are independent (non-synchronous operation). The phase noise increments are parameterized by a very general distribution, which includes the Wiener phase noise model as a special case. It is proven that phase noise creates a symbol-error-rate (SER) floor for both operation modes. In the synchronous operation this error floor is independent of M, while it goes to zero exponentially with M in the non-synchronous operation. This paper considers the downlink of a heterogeneous network, where multiple base stations (BSs) can serve the users by non-coherent multiflow beamforming. We assume imperfect channel state information at both BSs and users. The objective is to jointly optimize the precoding, load balancing, and BS operation mode (active or sleep) for improving the energy efficiency of the network. The considered problem is to minimize the weighted total power consumption (both circuit power and dynamic transmit power), while satisfying per-user quality of service constraints and per-BS transmit power constraints. This problem is non-convex, but we prove that for each combination of BS modes, the considered problem has a hidden convexity structure. Thus, the global optimal solution is obtained by an exhaustive search over all possible BS mode combinations. Furthermore, by iterative convex approximations of the non-convex power consumption functions, a heuristic algorithm is proposed to obtain a local optimal solution with low complexity. Simulation results illustrate that our proposed algorithms significantly reduce the total power consumption, compared to the scheme where all BSs are continuously active. This implies that putting a BS into sleep mode by proper load balancing is an important solution for energy savings in heterogeneous networks. Distributed massive multiple-input multiple-output (MIMO) combines the array gain of coherent MIMO processing with the proximity gains of distributed antenna setups. In this paper, we analyze how transceiver hardware impairments affect the downlink with maximum ratio transmission. We derive closed-form spectral efficiencies expressions and study their asymptotic behavior as the number of the antennas increases. We prove a scaling law on the hardware quality, which reveals that massive MIMO is resilient to additive distortions, while multiplicative phase noise is a limiting factor. It is also better to have separate oscillators at each antenna than one per BS. Wireless communications is one of the most successful technologies in modern years, given that an exponential growth rate in wireless traffic has been sustained for over a century (known as Cooper's law). This trend will certainly continue driven by new innovative applications; for example, augmented reality and internet-of-things. Massive MIMO (multiple-input multiple-output) has been identified as a key technology to handle orders of magnitude more data traffic. Despite the attention it is receiving from the communication community, we have personally witnessed that Massive MIMO is subject to several widespread misunderstandings, as epitomized by following (fictional) abstract: \\\"The Massive MIMO technology uses a nearly infinite number of high-quality antennas at the base stations. By having at least an order of magnitude more antennas than active terminals, one can exploit asymptotic behaviors that some special kinds of wireless channels have. This technology looks great at first sight, but unfortunately the signal processing complexity is off the charts and the antenna arrays would be so huge that it can only be implemented in millimeter wave bands.\\\" The \\\"properties\\\" above are, in fact, completely false. In this article, we identify 10 myths and explain why they are not true. We also ask a grand question that will require intense future research activities to answer properly. In this paper, we illustrate the potential benefits of using large transmitter arrays for wireless power transfer. Specifically, we analyze the probability of outage in energy transfer over fading channels when the base station (BS) with multiple antennas beamforms energy to a wireless sensor node. Our analytical and numerical results show that by using large transmitter arrays, the range of wireless power transfer can be increased while maintaining a target outage probability. We also observe and quantify that by using multi-antenna arrays at the BS, a lower downlink energy is required to get the same outage performance. For both maximum ratio transmission (MRT) and zero forcing (ZF) precoding schemes and given any specific rate requirement the optimal transmit power, number of antennas to be used, number of users to be served and number of pilots spent on channel training are found with the objective to minimize the total consumed power at the base station. The optimization problem is solved by finding closed form expressions of the optimal transmit power and then search over the remaining discrete variables. The analysis consists of two parts, the first part investigates the situation when only power consumed in the RF amplifiers is considered. The second part includes both the power consumed in the RF amplifiers and in other transceiver circuits. In the former case having all antennas active while reducing the transmit power is optimal. Adaptive scheme to switch off some of the antennas at the base stations is found to be optimal in the latter case. Massive multiple-input multiple-output (MIMO) techniques have the potential to bring tremendous improvements in spectral efficiency to future communication systems. Counterintuitively, the practical issues of having uncertain channel knowledge, high propagation losses, and implementing optimal non-linear precoding are solved more or less automatically by enlarging system dimensions. However, the computational precoding complexity grows with the system dimensions. For example, the close-to-optimal and relatively \\u201cantenna-efficient\\u201d regularized zero-forcing (RZF) precoding is very complicated to implement in practice, since it requires fast inversions of large matrices in every coherence period. Motivated by the high performance of RZF, we propose to replace the matrix inversion and multiplication by a truncated polynomial expansion (TPE), thereby obtaining the new TPE precoding scheme which is more suitable for real-time hardware implementation and significantly reduces the delay to the first transmitted symbol. The degree of the matrix polynomial can be adapted to the available hardware resources and enables smooth transition between simple maximum ratio transmission and more advanced RZF. By deriving new random matrix results, we obtain a deterministic expression for the asymptotic signal-to-interference-and-noise ratio (SINR) achieved by TPE precoding in massive MIMO systems. Furthermore, we provide a closed-form expression for the polynomial coefficients that maximizes this SINR. To maintain a fixed per-user rate loss as compared to RZF, the polynomial degree does not need to scale with the system, but it should be increased with the quality of the channel knowledge and the signal-to-noise ratio. This paper considers a downlink heterogeneous network, where different types of multiantenna base stations (BSs) communicate with a number of single-antenna users. Multiple BSs can serve the users by spatial multiflow transmission techniques. Assuming imperfect channel state information at both BSs and users, the precoding, load balancing, and BS operation mode are jointly optimized for improving the network energy efficiency. We minimize the weighted total power consumption while satisfying quality-of-service constraints at the users. This problem is nonconvex, but we prove that for each BS mode combination, the considered problem has a hidden convexity structure. Thus, the optimal solution is obtained by an exhaustive search over all possible BS mode combinations. Furthermore, by iterative convex approximations of the nonconvex objective function, a heuristic algorithm is proposed to obtain a suboptimal solution of low complexity. We show that although multicell joint transmission is allowed, in most cases, it is optimal for each user to be served by a single BS. The optimal BS association condition is parameterized, which reveals how it is impacted by different system parameters. Simulation results indicate that putting a BS into sleep mode by proper load balancing is an important solution for energy savings. How would a cellular network designed for high energy efficiency look like? To answer this fundamental question, we model cellular networks using stochastic geometry and optimize the energy efficiency with respect to the density of base stations, the number of antennas and users per cell, the transmit power levels, and the pilot reuse. The highest efficiency is neither achieved by a pure small-cell approach, nor by a pure massive MIMO solution. Interestingly, it is the combination of these approaches that provides the highest energy efficiency; small cells contributes by reducing the propagation losses while massive MIMO enables multiplexing of users with controlled interference. The main usage of cellular networks has changed from voice to data traffic, mostly requested by static users. In this paper, we analyze how a cellular network should be designed to provide such wireless broadband access with maximal energy efficiency (EE). Using stochastic geometry and a detailed power consumption model, we optimize the density of access points (APs), number of antennas and users per AP, and transmission power for maximal EE. Small cells are of course a key technology in this direction, but the analysis shows that the EE improvement of a small-cell network saturates quickly with the AP density and then \\u201cmassive MIMO\\u201d techniques can further improve the EE. The evolution of cellular networks is driven by the dream of ubiquitous wireless connectivity: any data service is instantly accessible everywhere. With each generation of cellular networks, we have moved closer to this wireless dream; first by delivering wireless access to voice communications, then by providing wireless data services, and recently by delivering a Wi-Fi-like experience with wide-area coverage and user mobility management. The support for high data rates has been the main objective in recent years [1], as seen from the academic focus on sum-rate optimization and the efforts from standardization bodies to meet the peak rate requirements specified in IMT-Advanced. In contrast, a variety of metrics/objectives are put forward in the technological preparations for fifth-generation (5G) networks: higher peak rates, improved coverage with uniform user experience, higher reliability and lower latency, better energy efficiency (EE), lower-cost user devices and services, better scalability with number of devices, etc. These multiple objectives are coupled, often in a conflicting manner such that improvements in one objective lead to degradation in the other objectives. Hence, the design of future networks calls for new optimization tools that properly handle the existence of multiple objectives and tradeoffs between them. Modern cellular networks need to serve user terminals with large disparities in mobility, which incurs different accuracy of the channel state information for each user. The impact of such heterogeneous mobility on the multi-cell downlink is analyzed in this paper. The base stations serve a multitude of users by coordinated beamforming. We derive deterministic equivalents for the user performance in a large scale system where the number of transmit antennas and user terminals grow large at a fixed ratio. We show that low and high mobility users can coexist and be served simultaneously, since the CSI imperfections of a user only harms the performance of this particular user. Simulations are used to verify the applicability of our large scale approximations for systems of practical dimensions. Furthermore, we show that the performance of high mobility users can be improved by explicitly managing the user priorities in the network. Radio-frequency (RF) impairments in the transceiver hardware of communication systems (e.g., phase noise (PN), high power amplifier (HPA) nonlinearities, or in- phase/quadrature-phase (I/Q) imbalance) can severely degrade the performance of traditional multiple-input multiple-output (MIMO) systems. Although calibration algorithms can partially compensate these impairments, the remaining distortion still has substantial impact. Despite this, most prior works have not analyzed this type of distortion. In this paper, we investigate the impact of residual transceiver hardware impairments on the MIMO system performance. In particular, we consider a transceiver impairment model, which has been experimentally validated, and derive analytical ergodic capacity expressions for both exact and high signal-to-noise ratios (SNRs). We demonstrate that the capacity saturates in the high-SNR regime, thereby creating a finite capacity ceiling. We also present a linear approximation for the ergodic capacity in the low-SNR regime, and show that impairments have only a second-order impact on the capacity. Furthermore, we analyze the effect of transceiver impairments on large-scale MIMO systems; interestingly, we prove that if one increases the number of antennas at one side only, the capacity behaves similar to the finite-dimensional case. On the contrary, if the number of antennas on both sides increases with a fixed ratio, the capacity ceiling vanishes; thus, impairments cause only a bounded offset in the capacity compared to the ideal transceiver hardware case. In wireless multiple antenna and multi-user systems, the spatial dimensions may be exploited to increase the performance by means of antenna gain, spatial diversity, and multi-user diversity. A limiting factor in such systems is the channel information required by the transmitter to control the intra-cell interference. Herein, the properties of spatially correlated channels with long-term statistical information at the transmitter and fixed-rate feedback of the quantized Euclidean channel norm are analyzed using a spectral subspace decomposition framework. A spatial division multiple access scheme is proposed with interference suppression at the receiver and joint scheduling and zero-forcing beamforming at the transmitter. Closed-form expressions for first and second order moments of the feedback conditional channel statistics are derived. It is shown that only a few bits of feedback are required to achieve reliable rate estimation and weighted sum-rate maximization. Hardware impairments in radio-frequency components of a wireless system cause unavoidable distortions to transmission that are not captured by the conventional linear channel model. In this paper, a \\\"binoisy\\\" single-user multiple-input multiple-output (SU-MIMO) relation is considered where the additional distortions are modeled via an additive noise term at the transmit side. Through this extended SU-MIMO channel model, the effects of transceiver hardware impairments on the achievable rate of multi-antenna point-to-point systems are studied. Channel input distributions encompassing practical discrete modulation schemes, such as, QAM and PSK, as well as Gaussian signaling are covered. In addition, the impact of mismatched detection and decoding when the receiver has insufficient information about the non-idealities is investigated. The numerical results show that for realistic system parameters, the effects of transmit-side noise and mismatched decoding become significant only at high modulation orders. In this work, we consider the downlink of a single-cell multi-user multiple-input multiple-output system in which zero-forcing precoding is used at the base station (BS) to serve a certain number of user equipments (UEs). A fixed data rate is guaranteed at each UE. The UEs move around in the cell according to a Brownian motion, thus the path losses change over time and the energy consumption fluctuates accordingly. We aim at determining the distribution of the energy consumption. To this end, we analyze the asymptotic regime where the number of antennas at the BS and the number of UEs grow large with a given ratio. It turns out that the energy consumption is asymptotically a Gaussian random variable whose mean and variance are derived analytically. These results can, for example, be used to approximate the probability that a battery-powered BS runs out of energy within a certain time period. Massive multiple-input multiple-output (MIMO) techniques have been proposed as a solution to satisfy many requirements of next generation cellular systems. One downside of massive MIMO is the incre ... Physical transceivers exhibit distortions from hardware impairments, of which traces remain even after compensation and calibration. Multicell MIMO coordinated beamforming methods that ignore these ... The capacity of ideal MIMO channels has a high-SNR slope that equals the minimum of the number of transmit and receive antennas. This letter analyzes if this result holds when there are distortions from physical transceiver impairments. We prove analytically that such physical MIMO channels have a finite upper capacity limit, for any channel distribution and SNR. The high-SNR slope thus collapses to zero. This appears discouraging, but we prove the encouraging result that the relative capacity gain of employing MIMO is at least as large as with ideal transceivers. Densification is a key to greater throughput in cellular networks. The full potential of coordinated multipoint (CoMP) can be realized by massive multiple-input multiple-output (MIMO) systems, where each base station (BS) has very many antennas. However, the improved throughput comes at the price of more infrastructure; hardware cost and circuit power consumption scale linearly/affinely with the number of antennas. In this paper, we show that one can make the circuit power increase with only the square root of the number of antennas by circuit-aware system design. To this end, we derive achievable user rates for a system model with hardware imperfections and show how the level of imperfections can be gradually increased while maintaining high throughput. The connection between this scaling law and the circuit power consumption is established for different circuits at the BS. We study the performance region of a general multicell downlink scenario with multiantenna transmitters, hardware impairments, and low-complexity receivers that treat interference as noise. The Pareto boundary of this region describes all efficient resource allocations, but is generally hard to compute. We propose a novel explicit characterization that gives Pareto optimal transmit strategies using a set of positive parameters-fewer than in prior work. We also propose an implicit characterization that requires even fewer parameters and guarantees to find the Pareto boundary for every choice of parameters, but at the expense of solving quasi-convex optimization problems. The merits of the two characterizations are illustrated for interference channels and ideal network multiple-input multiple-output (MIMO). Massive MIMO is a promising technique to increase the spectral efficiency of cellular networks, by deploying antenna arrays with hundreds or thousands of active elements at the base stations and performing coherent beamforming. A common rule-of-thumb is that these systems should have an order of magnitude more antennas, N, than scheduled users, K, because the users' channels are then likely to be quasi-orthogonal. However, it has not been proved that this rule-of-thumb actually maximizes the spectral efficiency. In this paper, we analyze how the optimal number of scheduled users, K\\u2217, depends on N and other system parameters. The value of K\\u2217 in the large-N regime is derived in closed form, while simulations are used to show what happens at finite N, in different interference scenarios, and for different beamforming. Base station cooperative transmission is an effective strategy to mitigate inter-cell interference. Centralized multi-cell transmission provides considerable performance gains but is impractical in large cellular systems, due to its prohibitive complexity and large amount of overhead. Dividing cells into small clusters enables practical channel acquisition and coordination within each cluster but still suffers from out-of-cluster interference. In this paper, we propose a dynamic cooperative framework for large cellular systems, which divides cells into groups such that neighboring cells belong to different groups. Based on the cell-grouping, a distributed scheduling strategy is proposed which can effectively coordinate the interference between cell-groups. With limited signalling among BSs and lower complexity, the cell-grouping based distributed scheduling and beamforming shows performance advantages over the fixed clustering based centralized scheduling and beamforming. Massive multiple-input multiple-output (MIMO) systems are cellular networks where the base stations (BSs) are equipped with unconventionally many antennas, deployed on co-located or distributed arrays. Huge spatial degrees-of-freedom are achieved by coherent processing over these massive arrays, which provide strong signal gains, resilience to imperfect channel knowledge, and low interference. This comes at the price of more infrastructure; the hardware cost and circuit power consumption scale linearly/affinely with the number of BS antennas $N$ . Hence, the key to cost-efficient deployment of large arrays is low-cost antenna branches with low circuit power, in contrast to today's conventional expensive and power-hungry BS antenna branches. Such low-cost transceivers are prone to hardware imperfections, but it has been conjectured that the huge degrees-of-freedom would bring robustness to such imperfections. We prove this claim for a generalized uplink system with multiplicative phase-drifts, additive distortion noise, and noise amplification. Specifically, we derive closed-form expressions for the user rates and a scaling law that shows how fast the hardware imperfections can increase with $N$ while maintaining high rates. The connection between this scaling law and the power consumption of different transceiver circuits is rigorously exemplified. This reveals that one can make the circuit power increase as $sqrt{N} $ , instead of linearly, by careful circuit-aware system design. In this work, we consider the downlink of a single-cell multi-user MIMO system in which the base station (BS) makes use of $N$ antennas to communicate with $K$ single-antenna user equipments (UEs) . The UEs move around in the cell according to a random walk mobility model. We aim at determining the energy consumption distribution when different linear precoding techniques are used at the BS to guarantee target rates within a finite time interval $T$ . The analysis is conducted in the asymptotic regime where $protect{N}$ and $K$ grow large with fixed ratio under the assumption of perfect channel state information (CSI) . Both recent and standard results from large system analysis are used to provide concise formulae for the asymptotic transmit powers and beamforming vectors for all considered schemes. These results are eventually used to provide a deterministic approximation of the energy consumption and to study its fluctuations around this value in the form of a central limit theorem. Closed-form expressions for the asymptotic means and variances are given. Numerical results are used to validate the accuracy of the theoretical analysis and to make comparisons. We show how the results can be used to approximate the probability that a battery-powered BS runs out of energy and also to design the cell radius for minimizing the energy consumption per unit area. The imperfect CSI case is also briefly considered. Large-scale MIMO systems can yield a substantial improvements in spectral efficiency for future communication systems. Due to the finer spatial resolution and array gain achieved by a massive number of antennas at the base station, these systems have shown to be robust to inter-user interference and the use of linear precoding appears to be asymptotically optimal. However, from a practical point of view, most precoding schemes exhibit prohibitively high computational complexity as the system dimensions increase. For example, the near-optimal regularized zero forcing (RZF) precoding requires the inversion of a large matrix. To solve this issue, we propose in this paper to approximate the matrix inverse by a truncated polynomial expansion (TPE), where the polynomial coefficients are optimized to maximize the system performance. This technique has been recently applied in single cell scenarios and it was shown that a small number of coefficients is sufficient to reach performance similar to that of RZF, while it was not possible to surpass RZF. In a realistic multi-cell scenario involving large-scale multi-user MIMO systems, the optimization of RZF precoding has, thus far, not been feasible. This is mainly attributed to the high complexity of the scenario and the non-linear impact of the necessary regularizing parameters. On the other hand, the scalar coefficients in TPE precoding give hope for possible throughput optimization. To this end, we exploit random matrix theory to derive a deterministic expression of the asymptotic signal-to-interference-and-noise ratio for each user based on channel statistics. We also provide an optimization algorithm to approximate the coefficients that maximize the network-wide weighted max-min fairness. The optimization weights can be used to mimic the user throughput distribution of RZF precoding. Using simulations, we compare the network throughput of the proposed TPE precoding with that of the suboptimal RZF scheme and show that our scheme can achieve higher throughput using a TPE order of only 5. Physical wireless transceivers suffer from a variety of impairments that distort the transmitted and received signals. Their degrading impact is particularly evident in modern systems with multiuser transmission, high transmit power, and low-cost devices, but their existence is routinely ignored in the optimization literature for multicell transmission. This paper provides a detailed analysis of coordinated beamforming in the multicell downlink. We solve two optimization problems under a transceiver impairment model and derive the structure of the optimal solutions. We show numerically that these solutions greatly reduce the impact of impairments, compared with beamforming developed for ideal transceivers. Although the so-called multiplexing gain is zero under transceiver impairments, we show that the gain of multiplexing can be large at practical SNRs. Coordinated beamforming can significantly improve the performance of cellular systems through joint interference management. Unfortunately, such beamforming optimization problems are typically NP-hard in multicell scenarios, making heuristic beamforming the only feasible choice in practice. This paper proposes a new branch-reduce-and-bound algorithm that solves such optimization problems globally, with a complexity suitable for benchmarking and analysis. Compared to prior work, the framework handles robustness to uncertain intercell interference and numerical analysis shows higher efficiency. Hardware impairments in physical transceivers are known to have a deleterious effect on communication systems; however, very few contributions have investigated their impact on relaying. This paper quantifies the impact of transceiver impairments in a two-way amplify-and-forward configuration. More specifically, the effective signal-to-noise-and-distortion ratios at both transmitter nodes are obtained. These are used to deduce exact and asymptotic closed-form expressions for the outage probabilities (OPs), as well as tractable formulations for the symbol error rates (SERs). It is explicitly shown that non-zero lower bounds on the OP and SER exist in the high-power regime-this stands in contrast to the special case of ideal hardware, where the OP and SER go asymptotically to zero. Recently, it was shown that transceiver hardware impairments have a detrimental impact on the performance of communication systems, especially for high-rate systems. The vast majority of technical contributions in the area of relaying assume ideal transceiver hardware. This paper quantifies the impact of transceiver hardware impairments in dual-hop Amplify-and-Forward (AF) relaying, both for fixed and variable gain relays. The outage probability (OP) in this practical scenario is a function of the instantaneous end-to-end signal-to-noise-and-distortion ratio (SNDR). This paper derives closed-form expressions for the exact and asymptotic OPs under Rayleigh fading, accounting for hardware impairments at both the transmitter and the relay. The performance loss is small at low spectral efficiency, but can otherwise be very substantial. In particular, it turns out that for high signal-to-noise ratio (SNR), the instantaneous end-to-end SNDR converges to a deterministic constant, called the SNDR ceiling, which is inversely proportional to the level of impairments. This stands in stark contrast to the ideal hardware case for which the end-to-end SNDR grows without bound in the high SNR regime. Recently, a structure of an optimal linear precoder for multi cell downlink systems has been described, and many other references have used simplified versions of this precoder to obtain promising performance gains. These gains have been hypothesized to stem from the additional degrees of freedom that allow for interference mitigation through interference relegation to orthogonal subspaces. However, no conclusive or rigorous understanding has yet been developed. In this paper, we build on an intuitive interference induction trade-off and the aforementioned preceding structure to propose an interference aware RZF (iaRZF) preceding scheme for multi cell downlink systems, and we analyze its rate performance. Special emphasis is placed on the induced interference mitigation mechanism of iaRZF. For example, we will verify the intuitive expectation that the precoder structure can either completely remove induced inter-cell or intra-cell interference. We state new results from large-scale random matrix theory that make it possible to give more intuitive and insightful explanations of the precoder behavior, also for cases involving imperfect channel state information (CSI). We remark especially that the interference-aware precoder makes use of all available information about interfering channels to improve performance. Even very poor CSI allows for significant sum-rate gains. Our obtained insights are then used to propose heuristic precoder parameters for arbitrary systems, whose effectiveness are shown in more involved system scenarios. Furthermore, calculation and implementation of these parameters does not require explicit inter base station cooperation. This paper considers pilot-based channel estimation in large-scale multiple-input multiple-output (MIMO) communication systems, also known as \\u201cmassive MIMO\\u201d, where there are hundreds of antennas at ... Cooperative precoding is an attractive way of improving the performance in multicell downlink scenarios. By serving each terminal through multiple surrounding base stations, inter-cell interference can be coordinated and higher spectral efficiency achieved, especially for terminals at cell edges. The optimal performance of multicell precoding is well-known as it can be treated as a single cell with distributed antennas. However, the requirements on backhaul signaling and computational power scales rapidly in large and dense networks, which often makes such fully centralized approaches impractical. In this paper, we review and generalize some recent work on multicell precoding with both centralized and distributed cooperation. We propose practical precoding strategies under Rician channel conditions, and illustrate how the major gain of multicell precoding originates from having good base station synchronization and not from making centralized precoding decisions. Transmit beamforming is a versatile technique for signal transmission from an array of antennas to one or multiple users [1]. In wireless communications, the goal is to increase the signal power at the intended user and reduce interference to nonintended users. A high signal power is achieved by transmitting the same data signal from all antennas but with different amplitudes and phases, such that the signal components add coherently at the user. Low interference is accomplished by making the signal components add destructively at nonintended users. This corresponds mathematically to designing beamforming vectors (that describe the amplitudes and phases) to have large inner products with the vectors describing the intended channels and small inner products with nonintended user channels. We consider the downlink of a single-cell multi-user MIMO system in which the base station makes use of N antennas to communicate with K single-antenna user equipments (UEs) randomly positioned in the coverage area. In particular, we focus on the problem of designing the optimal linear precoding for minimizing the total power consumption while satisfying a set of target signal-to-interference-plus-noise ratios (SINRs). To gain insights into the structure of the optimal solution and reduce the computational complexity for its evaluation, we analyze the asymptotic regime where N and K grow large with a given ratio and make use of recent results from large system analysis to compute the asymptotic solution. Then, we concentrate on the asymptotically design of heuristic linear precoding techniques. Interestingly, it turns out that the regularized zero-forcing (RZF) precoder is equivalent to the optimal one when the ratio between the SINR requirement and the average channel attenuation is the same for all UEs. If this condition does not hold true but only the same SINR constraint is imposed for all UEs, then the RZF can be modified to still achieve optimality if statistical information of the UE positions is available at the BS. Numerical results are used to evaluate the performance gap in the finite system regime and to make comparisons among the precoding techniques. This paper considers pilot-based channel estimation in large-scale multiple-input multiple-output (MIMO) communication systems, also known as \\u201cmassive MIMO\\u201d. Unlike previous works on this topic, which mainly considered the impact of inter-cell disturbance due to pilot reuse (so-called pilot contamination), we are concerned with the computational complexity. The conventional minimum mean square error (MMSE) and minimum variance unbiased (MVU) channel estimators rely on inverting covariance matrices, which has cubic complexity in the multiplication of number of antennas at each side. Since this is extremely expensive when there are hundreds of antennas, we propose to approximate the inversion by an L-order matrix polynomial. A set of low-complexity Bayesian channel estimators, coined Polynomial ExpAnsion CHannel (PEACH) estimators, are introduced. The coefficients of the polynomials are optimized to yield small mean square error (MSE). We show numerically that near-optimal performance is achieved with low polynomial orders. In practice, the order L can be selected to balance between complexity and MSE. Interestingly, pilot contamination is beneficial to the PEACH estimators in the sense that smaller L can be used to achieve near-optimal MSEs. This paper considers the mode selection problem for network-assisted device-to-device (D2D) communications with multiple antennas at the base station. We study transmission in both dedicated and shared frequency bands. Given the type of resources (i.e., dedicated or shared), the user equipment (UE) decides to transmit in the conventional cellular mode or directly to its corresponding receiver in the D2D mode. We formulate this problem under two different objectives. The first problem is to maximize the quality-of-service (QoS) given a transmit power, and the second problem is to minimize the transmit power given a QoS requirement. We derive closed-form results for the optimal decision and show that the two problem formulations behave differently. Taking a geometrical approach, we study the area around the transmitter UE where the receiving UE should be to have D2D mode optimality, and how it is affected by the transmit power, QoS, and the number of base station antennas. Massive MIMO is a promising technique for increasing the spectral efficiency (SE)\\u00a0of cellular networks, by deploying antenna arrays with hundreds or thousands of active elements at the base stations and performing coherent transceiver processing. A common rule-of-thumb is that these systems should have an order of magnitude more antennas $M$ than scheduled users $K$ because the users\\u2019 channels are likely to be near-orthogonal when $M/K > 10$. However, it has not been proved that this rule-of-thumb actually maximizes the SE. In this paper, we analyze how the optimal number of scheduled users $K^star$ depends on $M$ and other system parameters. To this end, new SE expressions are derived to enable efficient system-level analysis with power control, arbitrary pilot reuse, and random user locations. The value of $K^star$ in the large-$M$ regime is derived in closed form, while simulations are used to show what happens at finite $M$, in different interference scenarios, with different pilot reuse factors, and for different processing schemes. Up to half the coherence block should be dedicated to pilots and the optimal $M/K$ is less than 10\\u00a0in many cases of practical relevance. Interestingly, $K^star$ depends strongly on the processing scheme and hence it is unfair to compare different schemes using the same $K$. Massive multiple-input multiple-output (MIMO) systems are cellular networks where the base stations (BSs) are equipped with unconventionally many antennas. Such large antenna arrays offer huge spatial degrees-of-freedom for transmission optimization; in particular, great signal gains, resilience to imperfect channel knowledge, and small inter-user interference are all achievable without extensive inter-cell coordination. The key to cost-efficient deployment of large arrays is the use of hardware-constrained base stations with low-cost antenna elements, as compared to today's expensive and power-hungry BSs. Low-cost transceivers are prone to hardware imperfections, but it has been conjectured that the excessive degrees-of-freedom of massive MIMO would bring robustness to such imperfections. We herein prove this claim for an uplink channel with multiplicative phase-drift, additive distortion noise, and noise amplification. Specifically, we derive a closed-form scaling law that shows how fast the imperfections increase with the number of antennas. In this paper, the problem of training optimization for estimating a multiple-input multiple-output (MIMO) flat fading channel in the presence of spatially and temporally correlated Gaussian noise is studied in an application-oriented setup. So far, the problem of MIMO channel estimation has mostly been treated within the context of minimizing the mean square error (MSE) of the channel estimate subject to various constraints, such as an upper bound on the available training energy. We introduce a more general framework for the task of training sequence design in MIMO systems, which can treat not only the minimization of channel estimator\\u2019s MSE but also the optimization of a final performance metric of interest related to the use of the channel estimate in the communication system. First, we show that the proposed framework can be used to minimize the training energy budget subject to a quality constraint on the MSE of the channel estimator. A deterministic version of the 'dual\\u2019 problem is also provided. We then focus on four specific applications, where the training sequence can be optimized with respect to the classical channel estimation MSE, a weighted channel estimation MSE and the MSE of the equalization error due to the use of an equalizer at the receiver or an appropriate linear precoder at the transmitter. In this way, the intended use of the channel estimate is explicitly accounted for. The superiority of the proposed designs over existing methods is demonstrated via numerical simulations. Base station cooperation is an attractive way of increasing the spectral efficiency in multiantenna communication. By serving each terminal through several base stations in a given area, intercell interference can be coordinated and higher performance achieved, especially for terminals at cell edges. Most previous work in the area has assumed that base stations have common knowledge of both data dedicated to all terminals and full or partial channel state information (CSI) of all links. Herein, we analyze the case of distributed cooperation where each base station has only local CSI, either instantaneous or statistical. In the case of instantaneous CSI, the beamforming vectors that can attain the outer boundary of the achievable rate region are characterized for an arbitrary number of multiantenna transmitters and single-antenna receivers. This characterization only requires local CSI and justifies distributed precoding design based on a novel virtual signal-to-interference noise ratio (SINR) framework, which can handle an arbitrary SNR and achieves the optimal multiplexing gain. The local power allocation between terminals is solved heuristically. Conceptually, analogous results for the achievable rate region characterization and precoding design are derived in the case of local statistical CSI. The benefits of distributed cooperative transmission are illustrated numerically, and it is shown that most of the performance with centralized cooperation can be obtained using only local CSI. The symbol error performance of spatially correlated multi-antenna systems is analyzed herein. When the transmitter only has statistical channel information, the use of space-time block codes still permits spatial multiplexing and mitigation of fading. The statistical information can be used for precoding to optimize some quality measure. Herein, we analyze the performance in terms of the symbol error rate (SER). It is shown analytically that spatial correlation at the receiver decreases the performance both without precoding and with an SER minimizing precoder. Without precoding, correlation should also be avoided at the transmitter side, but with an SER minimizing precoder the performance is actually improved by increasing spatial correlation at the transmitter. The structure of the optimized precoder is analyzed and the asymptotic properties at high and low SNRs are characterized and illustrated numerically. Large-scale multiple-input multiple-output (MIMO) communication systems can bring substantial improvement in spectral efficiency and/or energy efficiency, due to the excessive degrees-of-freedom and huge array gain. However, large-scale MIMO is expected to deploy lower-cost radio frequency (RF) components, which are particularly prone to hardware impairments. Unfortunately, compensation schemes are not able to remove the impact of hardware impairments completely, such that a certain amount of residual impairments always exists. In this paper, we investigate the impact of residual transmit RF impairments (RTRI) on the spectral and energy efficiency of training-based point-to-point large-scale MIMO systems, and seek to determine the optimal training length and number of antennas which maximize the energy efficiency. We derive deterministic equivalents of the signal-to-noise-and-interference ratio (SINR) with zero-forcing (ZF) receivers, as well as the corresponding spectral and energy efficiency, which are shown to be accurate even for small number of antennas. Through an iterative sequential optimization, we find that the optimal training length of systems with RTRI can be smaller compared to ideal hardware systems in the moderate SNR regime, while larger in the high SNR regime. Moreover, it is observed that RTRI can significantly decrease the optimal number of transmit and receive antennas. Radio-frequency (RF) impairments, which intimately exist in wireless communication systems, can severely limit the performance of multiple-input-multiple-output (MIMO) systems. Although we can resort to compensation schemes to mitigate some of these impairments, a certain amount of residual impairments always persists. In this paper, we consider a training-based point-to-point MIMO system with residual transmit RF impairments (RTRI) using spatial multiplexing transmission. Specifically, we derive a new linear channel estimator for the proposed model, and show that RTRI create an estimation error floor in the high signal-to-noise ratio (SNR) regime. Moreover, we derive closed-form expressions for the signal-to-noise-plus-interference ratio (SINR) distributions, along with analytical expressions for the ergodic achievable rates of zero-forcing, maximum ratio combining, and minimum mean-squared error receivers, respectively. In addition, we optimize the ergodic achievable rates with respect to the training sequence length and demonstrate that finite dimensional systems with RTRI generally require more training at high SNRs than those with ideal hardware. Finally, we extend our analysis to large-scale MIMO configurations, and derive deterministic equivalents of the ergodic achievable rates. It is shown that, by deploying large receive antenna arrays, the extra training requirements due to RTRI can be eliminated. In fact, with a sufficiently large number of receive antennas, systems with RTRI may even need less training than systems with ideal hardware. Physical transceivers have hardware impairments that create distortions which degrade the performance of communication systems. The vast majority of technical contributions in the area of relaying neglect hardware impairments and, thus, assume ideal hardware. Such approximations make sense in low-rate systems, but can lead to very misleading results when analyzing future high-rate systems. This paper quantifies the impact of hardware impairments on dual-hop relaying, for both amplify-and-forward and decode-and-forward protocols. The outage probability (OP) in these practical scenarios is a function of the effective end-to-end signal-to-noise-and-distortion ratio (SNDR). This paper derives new closed-form expressions for the exact and asymptotic OPs, accounting for hardware impairments at the source, relay, and destination. A similar analysis for the ergodic capacity is also pursued, resulting in new upper bounds. We assume that both hops are subject to independent but non-identically distributed Nakagami-m fading. This paper validates that the performance loss is small at low rates, but otherwise can be very substantial. In particular, it is proved that for high signal-to-noise ratio (SNR), the end-to-end SNDR converges to a deterministic constant, coined the SNDR ceiling, which is inversely proportional to the level of impairments. This stands in contrast to the ideal hardware case in which the end-to-end SNDR grows without bound in the high-SNR regime. Finally, we provide fundamental design guidelines for selecting hardware that satisfies the requirements of a practical relaying system. Assume that a multi-user multiple-input multiple-output (MIMO) communication system must be designed to cover a given area with maximal energy efficiency (bits/Joule). What are the optimal values for the number of antennas, active users, and transmit power? By using a new model that describes how these three parameters affect the total energy efficiency of the system, this work provides closed-form expressions for their optimal values and interactions. In sharp contrast to common belief, the transmit power is found to increase (not decrease) with the number of antennas. This implies that energy efficient systems can operate at high signal-to-noise ratio (SNR) regimes in which the use of interference-suppressing precoding schemes is essential. Numerical results show that the maximal energy efficiency is achieved by a massive MIMO setup wherein hundreds of antennas are deployed to serve relatively many users using interference-suppressing regularized zero-forcing precoding. To improve the cellular energy efficiency, without sacrificing quality-of-service (QoS) at the users, the network topology must be densified to enable higher spatial reuse. We analyze a combination of two densification approaches, namely \\u201cmassive\\u201d multiple-input multiple-output (MIMO) base stations and small-cell access points. If the latter are operator-deployed, a spatial soft-cell approach can be taken where the multiple transmitters serve the users by joint non-coherent multiflow beamforming. We minimize the total power consumption (both dynamic emitted power and static hardware power) while satisfying QoS constraints. This problem is proved to have a hidden convexity that enables efficient solution algorithms. Interestingly, the optimal solution promotes exclusive assignment of users to transmitters. Furthermore, we provide promising simulation results showing how the total power consumption can be greatly improved by combining massive MIMO and small cells; this is possible with both optimal and low-complexity beamforming. The performance of multiuser systems is both difficult to measure fairly and to optimize. Most resource allocation problems are nonconvex and NP-hard, even under simplifying assumptions such as perfect channel knowledge, homogeneous channel properties among users, and simple power constraints. We establish a general optimization framework that systematically solves these problems to global optimality. The proposed branch-reduce-and-bound (BRB) algorithm handles general multicell downlink systems with single-antenna users, multiantenna transmitters, arbitrary quadratic power constraints, and robust- ness to channel uncertainty. A robust fairness-profile optimization (RFO) problem is solved at each iteration, which is a quasiconvex problem and a novel generalization of max-min fairness. The BRB algorithm is computationally costly, but it shows better convergence than the previously proposed outer polyblock approximation algorithm. Our framework is suitable for computing benchmarks in general multicell systems with or without channel uncertainty. We illustrate this by deriving and evaluating a zero-forcing solution to the general problem. In this paper, we propose a multiple-state other-cell interference (OCI) model, which takes into account the interference variability and uncertainty created by scheduling and other fast resource allocation adaptivity. This situation is modeled by a set of states, each described by a spatial covariance matrix and a probability. In order to illustrate the usefulness of such a model, we study two receive beamforming design problems, one maximizing the average data rate and one maximizing the worst-case data rate. We compare the resulting performance with what can be obtained when state information is not available. We show theoretically and numerically that exploring the multiple-state OCI structure can improve the receive beamforming design efficiency, especially for cell-edge users in a interference-limited system. This paper considers quantization and exact minimum mean square error (MMSE) estimation of the squared Frobenius norm and the squared spectral norm of a Rayleigh fading multiple-input multiple-output (MIMO) channel with one-sided spatial correlation. The Frobenius and spectral norms are of great importance when describing the achievable capacity of many wireless communication systems; in particularly, they correspond to the signal-to-noise ratio (SNR) of space-time block coded and maximum ratio combining transmissions, respectively. Herein, a general quantization framework is presented, where the quantization levels are determined to maximize the feedback entropy. Quantization based on the post-user-selection distribution is discussed, and analyzed for a specific scheduler. Finally, exact results on MMSE estimation of the capacity and the SNR, conditioned on a quantized channel norm, are presented. The impact of transmission design and spatial correlation on the symbol error rate (SER) is analyzed for multi-antenna communication links. The receiver has perfect channel state information (CSI), while the transmitter has either statistical or no CSI. The transmission is based on orthogonal space-time block codes (OSTBCs) and linear precoding. The precoding strategy that minimizes the worst-case SER is derived for the case when the transmitter has no CSI. Based on this strategy, the intuitive result that spatial correlation degrades the SER performance is proved mathematically. In the case when the transmitter knows the channel statistics, the correlation matrix is assumed to be jointly-correlated (a generalization of the Kronecker model). The eigenvectors of the SER-optimal precoding matrix are shown to originate from the correlation matrix and the remaining power allocation is a convex problem. Equal power allocation is SER-optimal at high SNR. Beamforming is SER-optimal at low SNR, or for increasing constellation sizes, and its optimality range is characterized. A heuristic low-complexity power allocation is proposed and evaluated numerically. Finally, it is proved analytically that receive-side correlation always degrades the SER. Transmit-side correlation will however improve the SER at low to medium SNR, while its impact is negligible at high SNR. The use of large-scale antenna arrays can bring substantial improvements in energy and/or spectral efficiency to wireless systems due to the greatly improved spatial resolution and array gain. Recent works in the field of massive multiple-input multiple-output (MIMO) show that the user channels decorrelate when the number of antennas at the base stations (BSs) increases, thus strong signal gains are achievable with little interuser interference. Since these results rely on asymptotics, it is important to investigate whether the conventional system models are reasonable in this asymptotic regime. This paper considers a new system model that incorporates general transceiver hardware impairments at both the BSs (equipped with large antenna arrays) and the single-antenna user equipments (UEs). As opposed to the conventional case of ideal hardware, we show that hardware impairments create finite ceilings on the channel estimation accuracy and on the downlink/uplink capacity of each UE. Surprisingly, the capacity is mainly limited by the hardware at the UE, while the impact of impairments in the large-scale arrays vanishes asymptotically and interuser interference (in particular, pilot contamination) becomes negligible. Furthermore, we prove that the huge degrees of freedom offered by massive MIMO can be used to reduce the transmit power and/or to tolerate larger hardware impairments, which allows for the use of inexpensive and energy-efficient antenna elements. The problem of beamforming and rate estimation in a multi-user downlink multiple-input multiple-output (MIMO) system with limited feedback and statistical channel information at the transmitter is considered. In order to exploit the spatial properties of the channel, the norm of the channel to each receive antenna is computed. We propose to feed back the largest norm to the transmitter and derive the conditional second and fourth order channel moments in order to design the downlink beamforming weights. Similar approaches have previously been presented for multiuser multiple-input single-output (MISO) systems. Herein, these techniques are generalized to MIMO systems, by either antenna selection or receive beamforming at the receiver. Two eigenbeamform- ing strategies are proposed and shown to outperform opportunistic beamforming, based on similar feedback information. Assume that a multi-user multiple-input multiple-output (MIMO) system is designed from scratch to uniformly cover a given area with maximal energy efficiency (EE). What are the optimal number of antennas, active users, and transmit power? The aim of this paper is to answer this fundamental question. We consider jointly the uplink and downlink with different processing schemes at the base station and propose a new realistic power consumption model that reveals how the above parameters affect the EE. Closed-form expressions for the EE-optimal value of each parameter, when the other two are fixed, are provided for zero-forcing (ZF) processing in single-cell scenarios. These expressions prove how the parameters interact. For example, in sharp contrast to common belief, the transmit power is found to increase (not to decrease) with the number of antennas. This implies that energy-efficient systems can operate in high signal-to-noise ratio regimes in which interference-suppressing signal processing is mandatory. Numerical and analytical results show that the maximal EE is achieved by a massive MIMO setup wherein hundreds of antennas are deployed to serve a relatively large number of users using ZF processing. The numerical results show the same behavior under imperfect channel state information and in symmetric multi-cell scenarios. The throughput of multicell systems is inherently limited by interference and the available communication resources. Coordinated resource allocation is the key to efficient performance, but the demand on backhaul signaling and computational resources grows rapidly with number of cells, terminals, and subcarriers. To handle this, we propose a novel multicell framework with dynamic cooperation clusters where each terminal is jointly served by a small set of base stations. Each base station coordinates interference to neighboring terminals only, thus limiting backhaul signalling and making the framework scalable. This framework can describe anything from interference channels to ideal joint multicell transmission. The resource allocation (i.e., precoding and scheduling) is formulated as an optimization problem (P1) with performance described by arbitrary monotonic functions of the signal-to-interference-and-noise ratios (SINRs) and arbitrary linear power constraints. Although (P1) is nonconvex and difficult to solve optimally, we are able to prove: 1) optimality of single-stream beamforming; 2) conditions for full power usage; and 3) a precoding parametrization based on a few parameters between zero and one. These optimality properties are used to propose low-complexity strategies: both a centralized scheme and a distributed version that only requires local channel knowledge and processing. We evaluate the performance on measured multicell channels and observe that the proposed strategies achieve close-to-optimal performance among centralized and distributed solutions, respectively. In addition, we show that multicell interference coordination can give substantial improvements in sum performance, but that joint transmission is very sensitive to synchronization errors and that some terminals can experience performance degradations. This paper considers downlink multiantenna communication with base stations that perform cooperative precoding in a distributed fashion. Most previous work in the area has assumed that transmitters have common knowledge of both data symbols of all users and full or partial channel state information (CSI). Herein, we assume that each base station only has local CSI, either instantaneous or statistical. For the case of instantaneous CSI, a parametrization of the beamforming vectors used to achieve the outer boundary of the achievable rate region is obtained for two multi-antenna transmitters and two single-antenna receivers. Distributed generalizations of classical beamforming approaches that satisfy this parametrization are provided, and it is shown how the distributed precoding design can be improved using the so-called virtual SINR framework [1]. Conceptually analog results for both the parametrization and the beamforming design are derived in the case of local statistical CSI. Heuristics on the distributed power allocation are provided in both cases, and the performance is illustrated numerically. Feedback of channel state information (CSI) is necessary to achieve high throughput and low outage probability in multiuser multiantenna systems. There are two types of CSI: directional and quality information. Many papers have analyzed the importance of these in asymptotic regimes. However, we show that such results should be handled with care, as very different conclusions can be drawn depending on the spatial correlation and number of users. Therefore, we propose a quantization framework and evaluate the tradeoff between directional and quality feedback under practical conditions. This paper summarizes four novel advanced antenna concepts explored in the framework of the WINNER+ project. The concepts are related to multiuser MIMO communication in cellular networks, focusing on the acquisition and application of channel state information (CSI) at the transmitter in time-division-duplex (TDD) mode. The concepts include new ideas for CSI modeling and sounding for the purposes of multiuser precoding, and methods for pilot signal design with the aim to support the estimation of different CSI quantities. Furthermore, a new relaying strategy for terminal-to-terminal communication is described. All the ideas are feasible for adoption into practical upcoming communication systems such as LTE-Advanced, and most of the proposed concepts have only a minor impact on standards. Our study indicates that the CSI at its best is not only about estimating the channel responses between different antenna pairs. What counts is the nature of the intended communication link as well as the form in which CSI is applied. Base station cooperation can theoretically improve the throughput of multicell systems by coordinating interference and serving cell edge terminals through multiple base stations. In practice, the extent of cooperation is limited by the increase in backhaul signaling and computational demands. To address these concerns, we propose a novel distributed cooperation structure where each base station has responsibility for the interference towards a set of terminals, while only serving a subset of them with data. Weighted sum rate maximization is considered, and conditions for beamforming optimality and the optimal transmission structure are derived using Lagrange duality theory. This leads to distributed low-complexity transmission strategies, which are evaluated on measured multiantenna channels in a typical urban multicell environment. In the design of narrowband multi-antenna systems, a limiting factor is the amount of channel state information (CSI) available at the transmitter. This is especially evident in multi-user systems, where the spatial user separability determines the multiplexing gain, but it is also important for transmission-rate adaptation in single-user systems. To limit the feedback load, the unknown and multi-dimensional channel needs to be represented by a limited number of bits. When combined with long-term channel statistics, the norm of the channel matrix has been shown to provide substantial CSI that permits efficient user selection, linear precoder design, and rate adaptation. Herein, we consider quantized feedback of the squared Frobenius norm in a Rayleigh fading environment with arbitrary spatial correlation. The conditional channel statistics are characterized and their moments are derived for both identical, distinct, and sets of repeated eigenvalues. These results are applied for minimum mean square error (MMSE) estimation of signal and interference powers in single- and multi-user systems, for the purpose of reliable rate adaptation and resource allocation. The problem of efficient feedback quantization is discussed and an entropy-maximizing framework is developed where the post-user-selection distribution can be taken into account in the design of the quantization levels. The analytic results of this paper are directly applicable in many widely used communication techniques, such as space-time block codes, linear precoding, space division multiple access (SDMA), and scheduling. The use of large-scale antenna arrays has the potential to bring substantial improvements in energy efficiency and/or spectral efficiency to future wireless systems, due to the greatly improved spatial beamforming resolution. Recent asymptotic results show that by increasing the number of antennas one can achieve a large array gain and at the same time naturally decorrelate the user channels; thus, the available energy can be focused very accurately at the intended destinations without causing much inter-user interference. Since these results rely on asymptotics, it is important to investigate whether the conventional system models are still reasonable in the asymptotic regimes. This paper analyzes the fundamental limits of large-scale multiple-input single-output (MISO) communication systems using a generalized system model that accounts for transceiver hardware impairments. As opposed to the case of ideal hardware, we show that these practical impairments create finite ceilings on the estimation accuracy and capacity of large-scale MISO systems. Surprisingly, the performance is only limited by the hardware at the single-antenna user terminal, while the impact of impairments at the large-scale array vanishes asymptotically. Furthermore, we show that an arbitrarily high energy efficiency can be achieved by reducing the power while increasing the number of antennas. Massive MIMO systems, where the base stations are equipped with hundreds of antenna elements, are an attractive way to attain unprecedented spectral efficiency in future wireless networks. In the \\\"classical\\\" massive MIMO setting, the terminals are assumed fully loaded and a main impairment to the performance comes from the inter-cell pilot contamination, i.e., interference from terminals in neighboring cells using the same pilots as in the home cell. However, when the terminals are active intermittently, it is viable to avoid inter-cell contamination by pre-allocation of pilots, while same-cell terminals use random access to select the allocated pilot sequences. This leads to the problem of intra-cell pilot contamination. We propose a framework for random access in massive MIMO networks and derive new uplink sum rate expressions that take intra-cell pilot collisions, intermittent terminal activity, and interference into account. We use these expressions to optimize the terminal activation probability and pilot length. Channel measurements show that significant spatially non-wide-sense-stationary characteristics rise in massive MIMO channels. Notable parameter variations are experienced along the base station array, such as the average received energy at each antenna, and the directions of arrival of signals impinging on different parts of the array. In this paper, a new channel model is proposed to describe this spatial non-stationarity in massive MIMO channels by incorporating the concepts of partially visible clusters and wholly visible clusters. Furthermore, a closed-form expression of an upper bound on the ergodic sum capacity is derived for the new model, and the influence of the spatial non-stationarity on the sum capacity is analyzed. Analysis shows that for non-identically-and-independent-distributed (i.i.d.) Rayleigh fading channels, the non-stationarity benefits the sum capacity by bringing a more even spread of channel eigenvalues. Specifically, more partially visible clusters, smaller cluster visibility regions, and a larger antenna array can all help to yield a well-conditioned channel, and benefit the sum capacity. This shows the advantage of using a large antenna array in a non-i.i.d. channel: the sum capacity benefits not only from a higher array gain, but also from a more spatially non-stationary channel. Numerical results demonstrate our analysis and the tightness of the upper bound. We illustrate potential benefits of using massive antenna arrays for wireless energy transfer (WET). Specifically, we analyze probability of outage in WET over fading channels when a base station (BS)\\u00a0with multiple antennas beamforms energy to a wireless sensor node (WSN). Our analytical results show that by using massive antenna arrays, the range of WET can be increased for a given target outage probability. We prove that by using multiple-antenna arrays at the BS, a lower downlink energy is required to get the same outage performance, resulting in savings of radiated energy. We show that for energy levels used in WET, the outage performance with least-squares or minimum mean-square-error channel estimates is the same as that obtained based on perfect channel estimates. We observe that a strong line-of-sight component between the BS and WSN lowers outage probability. Furthermore, by deploying more antennas at the BS, a larger energy can be transferred reliably to the WSN at a given target outage performance for the sensor to be able to perform its main tasks. In our numerical examples, the RF power received at the input of the sensor is assumed to be on the order of a\\u00a0mW, such that the rectenna operates at an efficiency in the order of 50%. This paper considers three aspects of Massive MIMO (multiple- input multiple-output) communication networks that have received little attention in previous works, but are important to understand when designing and implementing this promising wireless technology. First, we analyze how bursty data traffic behaviors affect the system. Using a probabilistic model for intermittent user activity, we show that the spectral efficiency (SE) scales gracefully with reduced user activity. Then, we make an analytic comparison between synchronous and asynchronous pilot signaling, and prove that the choice between these has no impact on the SE. Finally, we provide an analytical and numerical study of the SE achieved with random network deployment. This paper considers the jointly optimal pilot and data power allocation in single cell uplink massive MIMO systems. A closed form solution for the optimal length of the training interval is derived. Using the spectral efficiency (SE) as performance metric and setting a total energy budget per co- herence interval the power control is formulated as optimization problems for two different objective functions: the minimum SE among the users and the sum SE. The optimal power control policy is found for the case of maximizing the minimum SE by converting it to a geometric program (GP). Since maximizing the sum SE is an NP-hard problem, an efficient algorithm is developed for finding KKT (local maximum) points. Simulation results show the advantage of optimizing the power control over both pilot and data power, as compared to heuristic power control policies. Transceiver hardware impairments (e.g., phase noise, in-phase/quadrature-phase imbalance, amplifier nonlinearities, and quantization errors) have obvious degradation effects on the performance of wireless communications. While prior works have improved our knowledge of the influence of hardware impairments of single-user multiple-input multiple-output (MIMO) systems over Rayleigh fading channels, an analysis encompassing the Rician fading channel is not yet available. In this paper, we pursue a detailed analysis of regular and large-scale (LS) MIMO systems over Rician fading channels by deriving new closed-form expressions for the achievable rate to provide several important insights for practical system design. More specifically, for regular MIMO systems with hardware impairments, there is always a finite achievable rate ceiling, which is irrespective of the transmit power and fading conditions. For LS-MIMO systems, it is interesting to find that the achievable rate loss depends on the Rician $K$ - factor, which reveals that the favorable propagation in LS-MIMO systems can remove the influence of hardware impairments. However, we show that the nonideal LS-MIMO system can still achieve high spectral efficiency due to its huge degrees of freedom. Wireless communications is one of the most successful technologies in modern years, given that an exponential growth rate in wireless traffic has been sustained for over a century (known as Cooper\\u017as law). This trend will certainly continue, driven by new innovative applications; for example, augmented reality and the Internet of Things. Massive MIMO has been identified as a key technology to handle orders of magnitude more data traffic. Despite the attention it is receiving from the communication community, we have personally witnessed that Massive MIMO is subject to several widespread misunderstandings, as epitomized by following (fictional) abstract: \\u017aThe Massive MIMO technology uses a nearly infinite number of high-quality antennas at the base stations. By having at least an order of magnitude more antennas than active terminals, one can exploit asymptotic behaviors that some special kinds of wireless channels have. This technology looks great at first sight, but unfortunately the signal processing complexity is off the charts and the antenna arrays would be so huge that it can only be implemented in millimeter-wave bands.\\u017a These statements are, in fact, completely false. In this overview article, we identify 10 myths and explain why they are not true. We also ask a question that is critical for the practical adoption of the technology and which will require intense future research activities to answer properly. We provide references to key technical papers that support our claims, while a further list of related overview and technical papers can be found at the Massive MIMO Info Point: http://massivemimo. eu We consider the uplink of a cellular massive multiple-input multiple-output network. Acquiring channel state information at the base stations (BSs) requires uplink pilot signaling. Since the number of orthogonal pilot sequences is limited by the channel coherence, pilot reuse across cells is necessary to achieve high spectral efficiency. However, finding efficient pilot reuse patterns is non-trivial, especially in practical asymmetric BS deployments. We approach this problem using the coalitional game theory. Each BS has a few unique pilots and can form coalitions with other BSs to gain access to more pilots. The BSs in a coalition, thus, benefit from serving more users in their cells at the expense of higher pilot contamination and interference. Given that a cell\\u2019s average spectral efficiency depends on the overall pilot reuse pattern, the suitable coalitional game model is in the partition form. We develop a low-complexity distributed coalition formation based on individual stability. By incorporating a BS intercommunication budget constraint, we are able to control the overhead in message exchange between the BSs and ensure the algorithm\\u2019s convergence to a solution of the game called individually stable coalition structure. Simulation results reveal fast algorithmic convergence and substantial performance gains over the baseline schemes with no pilot reuse, full pilot reuse, or random pilot reuse pattern. State-of-the-art algorithms for energy-efficient power allocation in wireless networks are based on fractional programming theory, and allow to find the global maximum of the energy efficiency only in noise-limited scenarios. In interference-limited scenarios, several sub-optimal solutions have been proposed, but an efficient framework to globally maximize energy-efficient metrics is lacking. The goal of this work is to fill this gap by making use of fractional programming theory jointly with monotonic optimization. The resulting optimization framework is useful for at least two main reasons. First, it sheds light on the ultimate energy-efficiency performance of wireless networks. Second, it provides the means to benchmark the energy efficiency of state-of-the-art, but sub-optimal, solutions. We consider uplink transmission of a massive multiuser multiple-input multiple-output (MU-MIMO) system in the presence of a smart jammer. The jammer aims to degrade the sum spectral efficiency of the legitimate system by attacking both the training and data transmission phases. First, we derive a closed-form expression for the sum spectral efficiency by taking into account the presence of a smart jammer. Then, we determine how a jammer with a given energy budget should attack the training and data transmission phases to induce the maximum loss to the sum spectral efficiency. Numerical results illustrate the impact of optimal jamming specifically in the large limit of the number of base station (BS)\\u00a0antennas. The development of 5G enabling technologies brings new challenges to the design of power amplifiers (PAs). In particular, there is a strong demand for low-cost, nonlinear PAs which, however, introduce nonlinear distortions. On the other hand, contemporary expensive PAs show great power efficiency in their nonlinear region. Inspired by this trade-off between nonlinearity distortions and efficiency, finding an optimal operating point is highly desirable. Hence, it is first necessary to fully understand how and how much the performance of multiple-input multiple-output (MIMO) systems deteriorates with PA nonlinearities. In this paper, we first reduce the ergodic achievable rate (EAR) optimization from a power allocation to a power control problem with only one optimization variable, i.e. total input power. Then, we develop a closed-form expression for the EAR, where this variable is fixed. Since this expression is complicated for further analysis, two simple lower bounds and one upper bound are proposed. These bounds enable us to find the best input power and approach the channel capacity. Finally, our simulation results evaluate the EAR of MIMO channels in the presence of nonlinearities. An important observation is that the MIMO performance can be significantly degraded if we utilize the whole power budget. In this paper, we investigate the coexistence of two technologies that have been put forward for the fifth generation (5G) of cellular networks, namely, network-assisted device-to-device (D2D) communications and massive MIMO (multiple-input multiple-output). Potential benefits of both technologies are known individually, but the tradeoffs resulting from their coexistence have not been adequately addressed. To this end, we assume that D2D users reuse the downlink resources of cellular networks in an underlay fashion. In addition, multiple antennas at the BS are used in order to obtain precoding gains and simultaneously support multiple cellular users using multiuser or massive MIMO technique. Two metrics are considered, namely the average sum rate (ASR) and energy efficiency (EE). We derive tractable and directly computable expressions and study the tradeoffs between the ASR and EE as functions of the number of BS antennas, the number of cellular users and the density of D2D users within a given coverage area. Our results show that both the ASR and EE behave differently in scenarios with low and high density of D2D users, and that coexistence of underlay D2D communications and massive MIMO is mainly beneficial in low densities of D2D users. An analysis of broadcasting in massive MIMO (multiple-input and multiple-output) systems with a limited coherence interval is presented. When broadcasting common information, such as control signals, the base station does not have channel state information to the terminals. We propose that the base station broadcasts this common information using a low dimensional orthogonal space-time block code (OSTBC). This code is mapped onto the large antenna array with the use of a dimension reducing matrix, effectively \\u201cshrinking\\u201d the channel. The terminal can estimate the effective channel and decode the information, even when the coherence interval is short compared to the number of base station antennas. Different OSTBCs are compared in terms of outage capacity in practical scenarios using estimated CSI. In particular, the trade-off between diversity and rate, when little or no time/frequency diversity is available, is investigated. The performance of multi-user massive multiple-input multiple-output orthogonal frequency division multiplexing uplink systems in the presence of base station (BS) phase noise impairments is investigated. Closed-form achievable rate expressions are rigorously derived under two different operations, namely, the case of a common oscillator (synchronous operation) at the BS and the case of independent oscillators at each BS antenna (non-synchronous operation). It is observed that the non-synchronous operation exhibits superior performance due to the averaging of a portion of the intercarrier interference. Furthermore, radiated power scaling laws are derived, which are identical to the phase-noise-free case. We consider a multi-pair two-way amplify-and-forward relaying system with a massive antenna array at the relay and estimated channel state information, assuming maximum-ratio combining/transmission processing. Closed-form approximations of the sum spectral efficiency are developed and simple analytical power scaling laws are presented, which reveal a fundamental trade-off between the transmit powers of each user/the relay and of each pilot symbol. Finally, the optimal power allocation problem is studied. Consider a multicell multiuser MIMO (multiple-input multiple-output) system with a very large number of antennas at each base station (BS). The number of users in each cell is assumed to be fixed as the number of BS antennas grows large. Under certain conditions on the powers of the transmitting users, the signal eigenvalue spectrum is asymptotically separated from the interference-plus-noise spectrum as the number of BS antennas grows large. As it was observed in [1], this phenomenon allows to mitigate the pilot contamination problem. We provide the power limits for each user in the cell of interest above which such a separation occurs asymptotically. Unlike the approximative methods used in [1], we obtain these power limits by making use of the exact asymptotic characterizations of the interference-plus-noise spectrum. The results are based on the theory of small rank perturbations of large dimensional random matrices. In high-density low-bitrate Internet-of-Things (IoT) use case of 5G networks, the terminals and sensors are to be of extremely low-cost and low energy-consuming. Typically, the analog-to-digital converters (ADCs) dominate the power-budget of receiver chains, in particular if the quantization resolution is high. Hence, receiver architectures deploying 1-bit ADCs are of high interest towards realizing low-cost, high energy-efficiency device solutions. In this paper, we study the waveform design and optimization for a narrowband low-bitrate massive MISO downlink targeting to achieve rates higher than 1 bits/sec (per real-dimension) where the terminal receivers adopt only simple 1-bit quantization (per real-dimension) with oversampling. In this respect, first we show that for a particular precoder structure, the overall link is equivalent to that of an AWGN SISO with controlled intersymbol interference (ISI). The filter design problem for generating the desired ISI in such SISO links has been studied in previous works, however, the only known method in literature is a computationally demanding brute force search method. As a novel contribution, we develop models and tools that elaborate on the conditions to be satisfied for unique detection and existence of solution for the filter coefficients. Then, as a concrete example, the developed models and tools are utilized to show that in the absence of noise, five-times oversampling is required for unique detection of 16-QAM input alphabet. Building on these findings, we then develop novel algorithms that can efficiently design the filter coefficients. Examples and simulations are provided to elaborate on filter coefficient design and optimization, and to illustrate good SER performance of the MISO link with 1-bit receiver even at SNRs down to 5 dB. Consider the uplink of a single-cell multiuser MIMO system with a very large number of antennas, M, at the base station (BS) and K single-antenna users. A jamming device equipped with KJ antennas transmitting signals attempts to degrade the transmission between the users and the BS. In this paper, we propose a detection algorithm of the jamming attack as well as a method for its rejection. The proposed results are based on the application of results from random matrix theory. We assume that K and KJ are fixed as M converges to infinity while the coherence interval \\u03c4 is assumed to be of the same order of magnitude as M. This work focuses on the hardware design for the efficient operation of Massive multiple-input multiple-output (MIMO) systems. A closed-form uplink achievable data rate expression is derived considering imperfect channel state information (CSI) and hardware impairments. We formulate an optimization problem to maximize the sum data rate subject to a constraint on the total power consumption. A general power consumption model accounting for the level of hardware impairments is utilized. The optimization variables are the number of base station (BS) antennas and the level of impairments per BS antenna. The resolution of the analog-to-digital converter (ADC) is a primary source of such impairments. The results show the trade-off between the number of BS antennas and the level of hardware impairments, which is important for practical hardware design. Moreover, the maximum power consumption can be tuned to achieve maximum energy efficiency (EE). Numerical results suggest that the optimal level of hardware impairments yields ADCs of 4 to 5 quantization bits. This work investigates the impact of imperfect statistical information in the uplink of massive MIMO systems. In particular, we first show why covariance information is needed and then propose two schemes for covariance matrix estimation. A lower bound on the spectral efficiency (SE) of any combining scheme is derived, under imperfect covariance knowledge, and a closed-form expression is computed for maximum-ratio combining. We show that having covariance information is not critical, but that it is relatively easy to acquire it and to achieve SE close to the ideal case of having perfect statistical information. Massive MIMO is a promising technique to increase the spectral efficiency of cellular networks, by deploying antenna arrays with hundreds or thousands of active elements at the base stations and performing coherent beamforming. A common rule-of-thumb is that these systems should have an order of magnitude more antennas, $N$, than scheduled users, $K$, because the users' channels are then likely to be quasi-orthogonal. However, it has not been proved that this rule-of-thumb actually maximizes the spectral efficiency. In this paper, we analyze how the optimal number of scheduled users, $K^star$, depends on $N$ and other system parameters. The value of $K^star$ in the large-$N$ regime is derived in closed form, while simulations are used to show what happens at finite $N$, in different interference scenarios, and for different beamforming. Assume that a multi-user multiple-input multiple-output (MIMO) system is designed from scratch to uniformly cover a given area with maximal energy efficiency (EE). What are the optimal number of antennas, active users, and transmit power? The aim of this paper is to answer this fundamental question. We consider jointly the uplink and downlink with different processing schemes at the base station and propose a new realistic power consumption model that reveals how the above parameters affect the EE. Closed-form expressions for the EE-optimal value of each parameter, when the other two are fixed, are provided for zero-forcing (ZF) processing in single-cell scenarios. These expressions prove how the parameters interact. For example, in sharp contrast to common belief, the transmit power is found to increase (not to decrease) with the number of antennas. This implies that energy-efficient systems can operate in high signal-to-noise ratio regimes in which interference-suppressing signal processing is mandatory. Numerical and analytical results show that the maximal EE is achieved by a massive MIMO setup wherein hundreds of antennas are deployed to serve a relatively large number of users using ZF processing. The numerical results show the same behavior under imperfect channel state information and in symmetric multi-cell scenarios. Massive MIMO is a promising technique to increase the spectral efficiency (SE) of cellular networks, by deploying antenna arrays with hundreds or thousands of active elements at the base stations and performing coherent transceiver processing. A common rule-of-thumb is that these systems should have an order of magnitude more antennas, $M$, than scheduled users, $K$, because the users' channels are likely to be near-orthogonal when $M/K > 10$. However, it has not been proved that this rule-of-thumb actually maximizes the SE. In this paper, we analyze how the optimal number of scheduled users, $K^star$, depends on $M$ and other system parameters. To this end, new SE expressions are derived to enable efficient system-level analysis with power control, arbitrary pilot reuse, and random user locations. The value of $K^star$ in the large-$M$ regime is derived in closed form, while simulations are used to show what happens at finite $M$, in different interference scenarios, with different pilot reuse factors, and for different processing schemes. Up to half the coherence block should be dedicated to pilots and the optimal $M/K$ is less than 10 in many cases of practical relevance. Interestingly, $K^star$ depends strongly on the processing scheme and hence it is unfair to compare different schemes using the same $K$. Massive MIMO (multiple-input multiple-output) provides great improvements in spectral efficiency over legacy cellular networks, by coherent combining of the signals over a large antenna array and by spatial multiplexing of many users. Since its inception, the coherent interference caused by pilot contamination has been believed to be an impairment that does not vanish, even with an unlimited number of antennas. In this work, we show that this belief is incorrect and an artifact from using simplistic channel models and suboptimal signal processing schemes. We focus on the uplink and prove that with multicell MMSE combining, the spectral efficiency grows without bound as the number of antennas increases, even under pilot contamination, under a condition of linear independence between the channel covariance matrices. This condition is generally satisfied, except in special cases that are hardly found in practice. How would a cellular network designed for maximal energy efficiency look like? To answer this fundamental question, tools from stochastic geometry are used in this paper to model future cellular networks and obtain a new lower bound on the average uplink spectral efficiency. This enables us to formulate a tractable uplink energy efficiency (EE) maximization problem and solve it analytically with respect to the density of base stations (BSs), the transmit power levels, the number of BS antennas and users per cell, and the pilot reuse factor. The closed-form expressions obtained from this general EE maximization framework provide valuable insights on the interplay between the optimization variables, hardware characteristics, and propagation environment. Small cells are proved to give high EE, but the EE improvement saturates quickly with the BS density. Interestingly, the maximal EE is achieved by also equipping the BSs with multiple antennas and operate in a \\\"massive MIMO\\\" fashion, where the array gain from coherent detection mitigates interference and the multiplexing of many users reduces the energy cost per user. Transmit beamforming is a versatile technique for signal transmission from an array of $N$ antennas to one or multiple users [1]. In wireless communications, the goal is to increase the signal power at the intended user and reduce interference to non-intended users. A high signal power is achieved by transmitting the same data signal from all antennas, but with different amplitudes and phases, such that the signal components add coherently at the user. Low interference is accomplished by making the signal components add destructively at non-intended users. This corresponds mathematically to designing beamforming vectors (that describe the amplitudes and phases) to have large inner products with the vectors describing the intended channels and small inner products with non-intended user channels. While it is fairly easy to design a beamforming vector that maximizes the signal power at the intended user, it is difficult to strike a perfect balance between maximizing the signal power and minimizing the interference leakage. In fact, the optimization of multiuser transmit beamforming is generally a nondeterministic polynomial-time (NP) hard problem [2]. Nevertheless, this lecture shows that the optimal transmit beamforming has a simple structure with very intuitive properties and interpretations. This structure provides a theoretical foundation for practical low-complexity beamforming schemes. (See this lecture note for the complete abstract/introduction) Distributed massive multiple-input multiple-output (MIMO) combines the array gain of coherent MIMO processing with the proximity gains of distributed antenna setups. In this paper, we analyze how transceiver hardware impairments affect the downlink with maximum ratio transmission. We derive closed-form spectral efficiencies expressions and study their asymptotic behavior as the number of the antennas increases. We prove a scaling law on the hardware quality, which reveals that massive MIMO is resilient to additive distortions, while multiplicative phase noise is a limiting factor. It is also better to have separate oscillators at each antenna than one per BS. Massive MIMO systems, where the base stations are equipped with hundreds of antenna elements, are an attractive way to attain unprecedented spectral efficiency in future wireless networks. In the \\\"classical\\\" massive MIMO setting, the terminals are assumed fully loaded and a main impairment to the performance comes from the inter-cell pilot contamination, i.e., interference from terminals in neighboring cells using the same pilots as in the home cell. However, when the terminals are active intermittently, it is viable to avoid inter-cell contamination by pre-allocation of pilots, while same-cell terminals use random access to select the allocated pilot sequences. This leads to the problem of intra-cell pilot contamination. We propose a framework for random access in massive MIMO networks and derive new uplink sum rate expressions that take intra-cell pilot collisions, intermittent terminal activity, and interference into account. We use these expressions to optimize the terminal activation probability and pilot length. This paper considers three aspects of Massive MIMO (multiple-input multiple-output) communication networks that have received little attention in previous works, but are important to understand when designing and implementing this promising wireless technology. First, we analyze how bursty data traffic behaviors affect the system. Using a probabilistic model for intermittent user activity, we show that the spectral efficiency (SE) scales gracefully with reduced user activity. Then, we make an analytic comparison between synchronous and asynchronous pilot signaling, and prove that the choice between these has no impact on the SE. Finally, we provide an analytical and numerical study of the SE achieved with random network deployment. We consider the uplink of a cellular massive MIMO network. Since the spectral efficiency of these networks is limited by pilot contamination, the pilot allocation across cells is of paramount importance. However, finding efficient pilot reuse patterns is non-trivial especially in practical asymmetric base station deployments. In this paper, we approach this problem using coalitional game theory. Each cell has its own unique pilots and can form coalitions with other cells to gain access to more pilots. We develop a low-complexity distributed algorithm and prove convergence to an individually stable coalition structure. Simulations reveal fast algorithmic convergence and substantial performance gains over one-cell coalitions and full pilot reuse. This paper considers the jointly optimal pilot and data power allocation in single cell uplink massive MIMO systems. A closed form solution for the optimal length of the training interval is derived. Using the spectral efficiency (SE) as performance metric and setting a total energy budget per coherence interval the power control is formulated as optimization problems for two different objective functions: the minimum SE among the users and the sum SE. The optimal power control policy is found for the case of maximizing the minimum SE by converting it to a geometric program (GP). Since maximizing the sum SE is an NP-hard problem, an efficient algorithm is developed for finding KKT (local maximum) points. Simulation results show the advantage of optimizing the power control over both pilot and data power, as compared to heuristic power control policies. Wireless networks with many antennas at the base stations and multiplexing of many users, known as Massive MIMO systems, are key to handle the rapid growth of data traffic. As the number of users increases, the random access in contemporary networks will be flooded by user collisions. In this paper, we propose a reengineered random access protocol, coined strongest-user collision resolution (SUCR). It exploits the channel hardening feature of Massive MIMO channels to enable each user to detect collisions, determine how strong the contenders' channels are, and only keep transmitting if it has the strongest channel gain. The proposed SUCR protocol can quickly and distributively resolve the vast majority of all pilot collisions. This paper investigates the spectral efficiency (SE) of multi-cell Massive Multi-Input Multi-Output (MIMO) using different channel models. Prior works have derived closed-form SE bounds and approximations for Gaussian distributed channels, while we consider the double scattering model \\u2014 a prime example of a non-Gaussian channel for which it is intractable to obtain closed form SE expressions. The channels are estimated using limited resources, which gives rise to pilot contamination, and the estimates are used for linear detection and to compute the SE numerically. Analytical and numerical examples are used to describe the key behaviors of the double scattering models, which differ from conventional Massive MIMO models. Finally, we provide multi-cell simulation results that compare the double scattering model with uncorrelated Rayleigh fading and explain under what conditions we can expect to achieve similar SEs. In this paper, a new multi-cell MMSE precoder is proposed for massive MIMO systems. We consider a multi-cell network where each cell has K users and B orthogonal pilot sequences are available, with B = \\u03b2K and \\u03b2 \\u2265 1 being the pilot reuse factor over the network. In comparison with conventional single-cell precoding which only uses the K intra-cell channel estimates, the proposed multi-cell MMSE precoder utilizes all B channel directions that can be estimated locally at a base station, so that the transmission is designed spatially to suppress both parts of the inter-cell and intra-cell interference. To evaluate the performance, a large-scale approximation of the downlink SINR for the proposed multi-cell MMSE precoder is derived and the approximation is tight in the large-system limit. Power control for the pilot and payload, imperfect channel estimation and arbitrary pilot allocation are accounted for in our precoder. Numerical results show that the proposed multi-cell MMSE precoder achieves a significant sum spectral efficiency gain over the classical single-cell MMSE precoder and the gain increases as K or \\u03b2 grows. Compared with the recent M-ZF precoder, whose performance degrades drastically for a large K, our M-MMSE can always guarantee a high and stable performance. Moreover, the large-scale approximation is easy to compute and shown to be accurate even for small system dimensions.\",\n",
            "  \"2133814358\": \"Workloads generated by the real-world parallel applications that are executed on a multicomputer have a strong effect on the performance of its interconnection network\\u2014the hardware fabric supporting communication among individual processors. Existing multicomputer networks have been primarily designed and analysed under the assumption that the workload follows the non-bursty Poisson arrival process. As a step towards obtaining a clear understanding of network performance under various workloads, this paper presents a new analytical model for computing message latency in wormhole switched torus networks in the presence of bursty traffic, based on the well-known Markov-Modulated Poisson Process (MMPP). In order to derive the model, the approach for accurately capturing the properties of the composite MMPPs is applied to characterize traffic on network channels. Moreover, a general method has been proposed for calculating the probability of virtual channel occupancy when the traffic on network channels follows a multi-state MMPP process. Simulation experiments reveal that the model exhibits a good degree of accuracy. Real-time precision retrieval based on big data images has become a key technical issue recently. The vocabulary tree is an efficient method for addressing this issue owing to high precision and fast retrieval time. Most of the existing construction methods for the vocabulary tree are centralized. However, under a centralized scheme, it is almost impossible to train a big vocabulary tree with limited memory to retrieve a similar image with high precision. In this paper, a new scheme of the distributed in-memory vocabulary tree based on MapReduce model for massive image training and retrieval is proposed. Firstly, the distributed image feature exaction mechanism is presented to preprocess massive images. Secondly, a distributed K-means algorithm based on MapReduce model is proposed to build the first level of the vocabulary tree concurrently. Thirdly, the big vocabulary tree is divided into many subtrees. The entire training task for computing the vocabulary tree is divided into many subtasks. These training subtasks are performed in parallel in the memory of the cluster nodes. This distributed vocabulary tree strategy can support massive image training in memory. Therefore, a similar image can be retrieved in a distributed manner based on MapReduce model. Besides, the training time and memory overhead of our proposed scheme are analyzed in detail. The experimental results demonstrate that, with an increase in computer nodes, the training time and memory overhead on each node are linearly reduced, and the retrieval time is relatively reduced compared with centralized scheme without a loss of retrieval precision. The IEEE 802.11 Medium Access Control (MAC) protocols have gained widespread popularity and become ubiquitous in Wireless Local Area Networks (WLANs) owing to their attractive properties, such as easy deployment and low cost. The Distributed Coordination Function (DCF) is the fundamental MAC scheme of the IEEE 802.11 standard which can provide best-effort service but lacks support for differentiated Quality-of-Service (QoS). The Enhanced Distributed Channel Access (EDCA) protocol has been proposed in the IEEE 802.11e standard for provisioning of the MAC-level QoS differentiation. The EDCA specifies three important QoS differentiation schemes including Arbitrary Inter-frame Space (AIFS), Contention Window (CW), and Transmission Opportunity (TXOP). Analytical models of EDCA in the current literature have been primarily developed for the AIFS, CW, and TXOP schemes, separately. With the aim of obtaining a thorough and deep understanding of the performance of EDCA, in this chapter we first present a detailed survey of the existing work on modelling the DCF and EDCA protocols and then develop a comprehensive analytical model to accommodate the combination of the three QoS differentiation schemes in WLANs under unsaturated traffic conditions. We derive the QoS performance metrics in terms of throughput, end-to-end delay, and frame loss probability. The accuracy of the proposed model is verified by comparing the analytical results with those obtained from extensive NS-2 simulation experiments. The switching method determines the way messages visit intermediate routers, and has a great impact on network performance. This paper conducts an extensive comparative performance analysis, by means of analytical modelling, of three well-known switching methods proposed for multicomputer networks, namely wormhole switching, circuit switching, and pipelined circuit switching. The results reveal that wormhole switching is the most efficient when messages are short. Pipelined circuit switching can perform better than circuit switching and provides superior performance over wormhole switching when messages are relatively long. In delay-tolerant disconnected Mobile Ad-hoc Networks (MANETs), mobile devices communicate with each other as they meet opportunistically establishing peer-to-peer (P2P) connections. Trust management in this type of networks is a challenging task due to the dynamic topology and decentralized nature and has attracted mant recent research interests. In this paper we propose a trust based framework for P2P services in a delay-tolerant disconnected MANET. The proposed framework utilizes a decentralized trust model and a P2P communication protocol for secure file exchange in an opportunistic store-carry and forward mechanism. We discuss the implementation of the framework and experiment with a gnutella style P2P file sharing application in a disconnected MANET. Our results provide important insight into the key challenges and short-comings face by researchers and engineers when designing and deploying such systems. This chapter presents a detailed review of existing handover schemes and focuses on an analytical model developed for a Dynamic Guard Channel Scheme (DGCS), which manages adaptively the channels reserved for handover calls depending on the current status of the handover queue. The Poisson process and Markov-Modulated-Poisson-Process (MMPP) are used to model the arrival processes of new and handover calls, respectively. The accuracy of this model is demonstrated through the extensive comparisons of the analytical results against those obtained from discrete-event simulation experiments. Moreover, the analytical model is used to assess the effects of the number of channels originally reserved for handover calls, the number of dynamic channels and the burstiness of handover calls on the performance of the DGCS scheme. Analytical models for adaptive routing in multicomputer interconnection networks with the traditional non-bursty Poisson traffic have been widely reported in the literature. However, traffic loads generated by many real-world parallel applications may exhibit bursty and batch arrival properties, which can significantly affect network performance. This paper develops a new and concise analytical model for hypercubic networks in the presence of bursty and batch arrival traffic modelled by the Compound Poisson Process (CPP) with geometrically distributed batch sizes. The computation complexity of the model is independent of network size. The analytical results are validated through comparison to those obtained from the simulation experiments. The model is used to evaluate the effects of the bursty traffic with batch arrivals on the performance of interconnection networks. Social networks have recently been gaining popularity. In a peer-to-peer mobile social network (MSN), users with similar interests establish groups or communities and share information without relying on a centralized infrastructure. Users socially interact with each other using handheld mobile devices and membership in a group/community of MSNs is granted by a pre-existing group member. However, it is possible that a group of malicious users can collude to promote another untrustworthy user in becoming a group member. Moreover, revoking membership without the existence of a central authority in a group is also a grant challenge. To address these problems in peer-to-peer MSNs, we propose a decentralized framework and the related algorithms for trusted information exchange and social interaction among users based on the dynamicity aware graph relabeling system. In contrast to the existing implementations of social networks based on a client/server paradigm, the proposed framework utilizes a lightweight trust model for identifying trustworthy users and aims at creating communities of trusted users while isolating and reducing interactions with untrustworthy users. Simulation results demonstrated the effectiveness of the proposed framework compared with the traditional dynamicity aware graph relabeling system algorithm.Copyright \\u00a9 2011 John Wiley & Sons, Ltd. The problem of controlling access to multimedia multicasts requires the distribution and maintenance of keying information. This paper proposes a new key embedded video codec for the media-dependent approach that involves the use of a data-dependent channel, and can be achieved for multimedia by using data embedding techniques. Using data embedding to convey keying information can provide an additional security and reduce the demand of bandwidth. In this paper, we develop a new statistical data embedding algorithm on compression-domain, then, by combining FEC (Forward Error Correction) algorithm and our proposed key information frame format. After a series of simulation experiments, we find that the new codec can satisfy the special demand of media-dependent approach. At the same time, the codec provides good performance. This paper proposes the first analytical model of pipelined circuit switching (PCS) in cube networks. The model uses a Markov chain to analyse the backtracking actions of the header flit during the path set-up phase in PCS. One of the main features of the present model is its ability to capture the effects of using virtual channels. The validity of the model is demonstrated by comparing analytical results to those obtained through simulation experiments. Coverage of interest points and network connectivity are two main challenging and practically important issues of Wireless Sensor Networks (WSNs). Although many studies have exploited the mobility of sensors to improve the quality of coverage and connectivity, little attention has been paid to the minimization of sensors' movement, which often consumes the majority of the limited energy of sensors and thus shortens the network lifetime significantly. To fill in this gap, this paper addresses the challenges of the Mobile Sensor Deployment (MSD) problem and investigates how to deploy mobile sensors with minimum movement to form a WSN that provides both target coverage and network connectivity. To this end, the MSD problem is decomposed into two sub-problems: the Target COVerage (TCOV) problem and the Network CONnectivity (NCON) problem. We then solve TCOV and NCON one by one and combine their solutions to address the MSD problem. The NP-hardness of TCOV is proved. For a special case of TCOV where targets disperse from each other farther than double of the coverage radius, an exact algorithm based on the Hungarian method is proposed to find the optimal solution. For general cases of TCOV, two heuristic algorithms, i.e., the Basic algorithm based on clique partition and the TV-Greedy algorithm based on Voronoi partition of the deployment region, are proposed to reduce the total movement distance of sensors. For NCON, an efficient solution based on the Steiner minimum tree with constrained edge length is proposed. The combination of the solutions to TCOV and NCON, as demonstrated by extensive simulation experiments, offers a promising solution to the original MSD problem that balances the load of different sensors and prolongs the network lifetime consequently. In secure video multicast systems, access control is a very important and challenging issue. A common practice is to encrypt the video content using a group key, shared only by legitimate clients. Since the clients can join and leave the multicast session frequently, the group key needs to be updated and delivered to the legitimate clients accordingly in order to provide forward/backward secrecy. Therefore, key management and distribution plays a crucial role in building a secure and scalable video multicast system. Conventional approaches usually use a dedicated secure channel independent of the video data channel, which is expensive and inefficient. In this paper, we propose a novel and scalable Media-dependent Key Management and Distribution (MKMD) solution that embeds key management messages in video data so as to save network bandwidth and enhance the security level, and most importantly, without sacrificing the quality of video content too much. We have built a prototype system of MKMD, and our extensive experimental results show that MKMD is a secure, scalable, reliable and efficient key management and distribution solution for video multicast system. Large-scale behavioral simulations are widely used to study real-world multi-agent systems. Such programs normally run in discrete time-steps or ticks, with simulated space decomposed into domains that are distributed over a set of workers to achieve parallelism. A distinguishing feature of behavioral simulations is their frequent and high-volume group migration , the phenomenon in which simulated objects traverse domains in groups at massive scale in each tick. This results in continual and significant load imbalance among domains. To tackle this problem, traditional load balancing approaches either require excessive load re-profiling and redistribution, which lead to high computation/communication costs, or perform poorly because their statically partitioned data domains cannot reflect load changes brought by group migration. In this paper, we propose an effective and low-cost load balancing scheme, named Inc-part , based on a key observation that an object is unlikely to move a long distance (across many domains) within a single tick. This localized mobility property allows one to efficiently estimate the load of a dynamic domain incrementally, based on merely the load changes occurring in its neighborhood. The domains experiencing significant load changes are then partitioned or merged, and redistributed to redress load imbalance among the workers. Experiments on a 64-node (1,024-core) platform show that Inc-part can attain excellent load balance with dramatically lowered costs compared to state-of-the-art solutions. Time-stepped applications are pervasive in scientific computing domain but perform poorly in the cloud because these applications execute in discrete time-step or tick and use logical synchronization barriers at tick boundaries to ensure correctness. As a result, the accumulated computational skew and communication skew that were unsolved in each tick can slow downtime-stepped applications significantly. However, the existing solutions have focused only on the skew in each tick and thus cannot resist the accumulation of skew. To fill in this gap, an efficient approach to resisting the accumulation of skew is proposed in this paper via fully exploiting parallelism among ticks. This new approach allows the user to decompose much computational part (also called asynchronous part) of the processing for an object, into several asynchronous sub-processes which are dependent on one data object. Each sub-process from different ticks can then proceed in advance using the idle time whenever the needed data object is available, redressing the negative effects caused by accumulated unsolved computational and communication skew. To efficiently support such an approach, a data-centric programming model and also a runtime system, namely AsyTick , coupled with an ad hoc scheduler are developed. Experimental results show that the proposed approach can improve the performance of time-stepped applications over a state-of-the-art computational skew-resistant approach up to $2.53$ times. The Generalized Processor Sharing (GPS) is an important service differentiation scheme in telecommunication systems and has attracted great research interests due to the desirable properties of fairness, traffic isolation, and work conservation. The major limitation of the existing analytical performance models of GPS systems subject to self-similar traffic is that these models have been restricted to a simplified scenario with only two traffic flows. To overcome such a limitation, this study proposes a new flow-decomposition approach to modelling GPS systems subject to multiple self-similar traffic flows. We present and prove the existence of a feasible ordering of GPS systems based on the required and guaranteed service rates of traffic flows. With the aid of the feasible ordering, we decompose the GPS system into a group of single-queue systems and then derive the queue length distributions and loss probabilities of individual self-similar traffic flows in the original GPS system. Extensive simulation experiments are conducted to validate the accuracy of the developed analytical model. k-Clique detection enables computer scientists and sociologists to analyze social networks' latent structure and thus understand their structural and functional properties. However, the existing k-clique-detection approaches are not applicable to signed social networks directly because of positive and negative links. The authors' approach to detecting k-balanced trusted cliques in such networks bases the detection algorithm on formal context analysis. It constructs formal contexts using the modified adjacency matrix after converting a signed social network into an unweighted one. Experimental results demonstrate that their algorithm can efficiently identify the trusted cliques. Switching techniques have a strong effect on the performance of interconnection networks. Many studies have shown that traffic loads in parallel computation environments can exhibit burstiness property. In order to obtain a deep understanding of the efficiency of switching techniques, this study proposes an analytical performance model for pipelined circuit switching (PCS) in binary n-cube networks under bursty traffic, which is modeled by a switched Poisson process (SPP). Simulation experiments demonstrate that the proposed model exhibits a good degree of accuracy. This model and the one proposed in Ref. [13] for wormhole switching (WS) are then applied to investigate the relative performance merits of PCS and WS in the presence of bursty loads. The Access Point (AP) is the bottleneck of infrastructure-based Wireless Local Area Networks (WLANs) due to its responsibility for forwarding all the incoming and outgoing frames. As a result, the network performance is significantly deteriorated by the unbalanced traffic loads at AP and other mobile stations. A potential solution to the bottleneck problem is to assign different Transmission Opportunity (TXOP) limits to the AP and mobile stations, respectively. This study develops a new analytical model to investigate the Quality-of-Service (QoS) performance metrics including throughput, total frame delay, and frame loss probability in infrastructure-based WLANs with TXOP differentiation under the unbalanced traffic loads. To this end, the transmission queue is modeled as a bulk service queueing system in order to address the challenge of queueing analysis arising from the TXOP burst transmission mechanism. The accuracy of this model is validated through extensive NS2 simulation experiments. The analytical results demonstrate that the implementation of TXOP differentiation can substantially alleviate the bottleneck effects of unbalanced traffic loads, increase the aggregate throughput and decrease the frame delay and loss probability. Moreover, the developed model can be adopted as a cost-efficient tool to evaluate the impact of TXOP limits on the network performance and to investigate the optimal setting of TXOP limits under different working conditions. The hypercube and torus are two important message-passing network architectures of high-performance multicomputers. Analytical models of multicomputer networks under the non-bursty Poisson traffic have been widely reported. Motivated by the convincing evidence of bursty and batch arrival nature of traffic generated by many real-world parallel applications in high-performance computing environments, we develop a new and concise analytical model in this paper for hypercube and torus networks in the presence of batch message arrivals modelled by the compound Poisson process with geometrically distributed batch sizes. The average degree of virtual channel multiplexing is derived by employing a Markov chain which can capture the batch arrival nature. An attractive advantage of the model is its constant computation complexity independent of the network size. The accuracy of the analytical performance results is validated against those obtained from simulation experiments of an actual system. Wireless information networks satisfy the users' desirable requirements for mobility and relocation and thus have experienced tremendous growth in recent years. The Enhanced Distributed Channel Access (EDCA) is a promising Medium Access Control (MAC) protocol for provisioning of Quality-of-Service (QoS) in Wireless Local Area Networks (WLANs). This protocol specifies three important QoS schemes including Arbitrary Inter-Frame Space (AIFS), Contention Window (CW) and Transmission Opportunity (TXOP). Analytical models of EDCA reported in the literature have been primarily developed for the AIFS, CW, and TXOP schemes, separately. This paper proposes a comprehensive analytical model to accommodate the integration of these three QoS schemes in WLANs with finite buffer capacity under unsaturated traffic loads. The important QoS performance metrics in terms of throughput, delay, delay jitter, and frame loss probability are derived. The accuracy of the model is validated through extensive simulation experiments. Performance results reveal that the TXOP can support QoS differentiation, improve the system performance significantly and outperform AIFS and CW owing to its unique burst transmission property. Furthermore, the results demonstrate that the buffer size has considerable impact on the QoS of EDCA, highlighting the importance of taking the buffer size into account for design and analysis of MAC protocols. Abstract The Wireless Mesh Network (WMN) is a highly capable technology that can offer low-cost and easy-deployable network connectivity to both small-size community networks and large-scale metropolitan networks. As a key emerging technology to provide the next generation broadband networking, WMN combines the advantages of both mobile ad hoc network (MANET) and traditional fixed network, attracting significant industrial and academic attentions. In WMN, the load balancing has already shown its key role in enhancing communication quality by providing reduced latency, jitter and error rate as well as improving bandwidth utilisation and allocation polices. Although there are a number of proposals on using load-aware strategy in WMN, the neighbourhood load has not been considered within the context of load balancing and high efficient WMNs. In this paper, we propose a Neighbourhood Load Routing metric to further improve the performance of existing routing protocols such as AODV in WMN. We have conducted extensive simulation experiments and our results confirm the superiority of our proposed scheme over its well-known counterparts, especially in grid topologies. A heterogeneous, sensor-based Web of Things system model with ubiquitous attributes and a human-attention inspired resource allocation scheme facilitates dynamic resource interaction. Radio Frequency Identification (RFID) has attracted much research attention in recent years. RFID can support automatic information tracing and management during the management process in many fields. A typical field that uses RFID is modern warehouse management, where products are attached with tags and the inventory of products is managed by retrieving tag IDs. Many practical applications require searching a group of tags to determine whether they are in the system or not. The existing studies on tag searching mainly focused on improving the time efficiency but paid little attention to energy efficiency which is extremely important for active tags powered by built-in batteries. To fill in this gap, this paper investigates the tag searching problem from the energy efficiency perspective. We first propose an Energy-efficient tag Searching protocol in Multiple reader RFID systems, namely ESiM, which pushes per tag energy consumption to the limit as each tag needs to exchange only one bit data with the reader. We then develop a time efficiency enhanced version of ESiM, namely TESiM, which can dramatically reduce the execution time while only slightly increasing the transmission overhead. Extensive simulation experiments reveal that, compared to state-of-the-art solution in the current literature, TESiM reduces per tag energy consumption by more than one order of magnitude subject to comparable execution time. In most considered scenarios, TESiM even reduces the execution time by more than 50%. Many high-quality measurement studies have demonstrated that wireless network traffic exhibits the noticeable Long-Range-Dependent (LRD) property. Moreover, the fading nature of wireless channels can lead to variable service capacity. Due to the inherent difficulty and high complexity of modelling the fractal-like LRD traffic, existing analytical models of queuing systems with LRD arrival processes have been primarily limited to the simplified scenarios where the service capacity is assumed to be constant. Given the time-varying nature of wireless channels in the real-world working environments, it is very important and necessary to investigate system performance in the presence of variable service capacity. To this end, this paper presents a comprehensive analytical model for queuing systems subject to LRD traffic and variable service capacity. We extend the application of a Large Deviation Principle and derive the closed-form expressions of Quality-of-Service (QoS) metrics. The accuracy of the model validated through extensive simulation experiments makes it a cost-effective evaluation tool for performance analysis of communication networks. To illustrate its applications, the model is adopted to investigate the effects of LRD traffic and variable service capacity on the system performance and resource configuration. Owing to its attractive features such as fast identification and relatively long interrogating range over the classical barcode systems, radio-frequency identification (RFID) technology possesses a promising prospect in many practical applications such as inventory control and supply chain management. However, unknown tags appear in RFID systems when the tagged objects are misplaced or unregistered tagged objects are moved in, which often causes huge economic losses. This paper addresses an important and challenging problem of unknown tag identification in large-scale RFID systems. The existing protocols leverage the Aloha-like schemes to distinguish the unknown tags from known tags at the slot level, which are of low time-efficiency, and thus can hardly satisfy the delay-sensitive applications. To fill in this gap, two filtering-based protocols (at the bit level) are proposed in this paper to address the problem of unknown tag identification efficiently. Theoretical analysis of the protocol parameters is performed to minimize the execution time of the proposed protocols. Extensive simulation experiments are conducted to evaluate the performance of the protocols. The results demonstrate that the proposed protocols significantly outperform the currently most promising protocols. Trust management in applications for peer-to-peer (P2P) mobile networks has recently gained enormous interests. In a P2P mobile social network, users interact with each other to share information without relying on a centralized infrastructure. Users with similar interests establish groups or communities thus encouraging sharing of information such as interest profiles, personal information, documents, audio or video files etc. Gaining membership in a group/community in a mobile social network is based on recommendation from a pre-existing member of a group; revoking membership of a user from a group could be a challenging task without the existence of a central authority. In contrast to the existing implementations of social networks based on a client/server paradigm, we propose a decentralized (P2P) framework for trusted information exchange and social interaction among users based on the Dynamicity Aware Graph Relabeling System (DA-GRS). The proposed framework utilizes a light weight trust model for identifying trustworthy users and aims at creating communities of trusted users while isolating and reducing interactions with untrustworthy users. Simulations in three different environments show the effectiveness of the framework compared to the original DA-GRS algorithm. Mobile social networks (MSNs) facilitate connections between mobile users and allow them to find other potential users who have similar interests through mobile devices, communicate with them, and benefit from their information. As MSNs are distributed public virtual social spaces, the available information may not be trustworthy to all. Therefore, mobile users are often at risk since they may not have any prior knowledge about others who are socially connected. To address this problem, trust inference plays a critical role for establishing social links between mobile users in MSNs. Taking into account the nonsemantical representation of trust between users of the existing trust models in social networks, this paper proposes a new fuzzy inference mechanism, namely MobiFuzzyTrust, for inferring trust semantically from one mobile user to another that may not be directly connected in the trust graph of MSNs. First, a mobile context including an intersection of prestige of users, location, time, and social context is constructed. Second, a mobile context aware trust model is devised to evaluate the trust value between two mobile users efficiently. Finally, the fuzzy linguistic technique is used to express the trust between two mobile users and enhance the human's understanding of trust. Real-world mobile dataset is adopted to evaluate the performance of the MobiFuzzyTrust inference mechanism. The experimental results demonstrate that MobiFuzzyTrust can efficiently infer trust with a high precision. Abstract Analytical models for wormhole routing in multicomputer networks have been widely reported in the literature. Although several recent studies have revealed that pipelined circuit switching (PCS) can provide superior performance characteristics over wormhole routing, there has been hardly any study that describes performance models for PCS. As a result, all existing studies have resorted to simulation when evaluating the performance merits of PCS. In an effort to fill this gap, this paper proposes the first analytical model for PCS in hypercube networks. The validity of the model is demonstrated by comparing analytical results with those obtained through simulation experiments. Pipelined circuit switching (PCS) that combines the advantages of both circuit switching and wormhole switching is an efficient method for passing messages in interconnection networks. Analytical modelling is a cost-effective tool and plays an important role in achieving a clear understanding of the network performance. However, most of the existing models for PCS are unable to capture the realistic nature of message behaviours generated by real-world applications, which have a significant impact on the design and performance of communication networks. This paper presents a new analytical model for PCS in interconnection networks in the presence of bursty and correlated message arrivals coupled with hot-spot destinations, which can capture the bursty message arrival process and non-uniform distribution of message destinations. Such a traffic pattern has been found in many practical communication environments. The accuracy of the proposed analytical model is validated through extensive simulation experiments. The model is then applied to investigate the effects of the bursty message arrivals and hot-spot destinations on the performance of interconnection networks with PCS. The increasing demand for the coverage of high-speed wireless local area networks (WLANs) is driving the installation of a very large number of access points (APs). Wireless mesh networks (WMNs) have emerged as a promising technology in the next generation networks to provide economical and scalable broadband access to wireless interconnection of multiple APs that manage individual WLANs in order to extend the coverage of the conventional single WLANs. In this paper, we develop a new analytical model to investigate the quality-of-service (QoS) performance metrics in WMNs interconnecting multiple WLANs. The model takes the phenomenon of communication locality into consideration. Extensive simulation experiments are conducted to validate the accuracy of the analytical model. The proposed model is then used to obtain the maximum achievable throughput, which is a key performance metrics in wireless networks. Moreover, we utilize the model to evaluate the impact of communication locality on network performance and investigate its effects on the optimization of integrated WLANs and multi-hop mesh networks. Copyright \\u00a9 2009 John Wiley & Sons, Ltd.  This paper develops a new analytical model to investigate the Quality-of-Service (QoS) performance metrics in Wireless Mesh Networks (WMNs) interconnecting multiple Wireless Local Area Networks (WLANs). The proposed model is then used to obtain the maximum achievable throughput, which is a key performance metric in wireless networks. Moreover, we utilize the model to evaluate the impact of communication locality on the network performance and investigate its effects on the optimization of integrated WLANs and multi-hop mesh networks. Copyright \\u00a9 2009 John Wiley & Sons, Ltd. The interconnection network is one of the key architectural components in any parallel computer. The distribution of the traffic injected into the network is among the factors that greatly influences network performance. The uniform traffic pattern has been adopted in many existing network performance evaluation studies due to the tractability of the resulting analytical modelling approach. However, many real applications exhibit non-uniform traffic patterns such as hot-spot traffic. K-ary n-cubes have been the mostly widely used in the implementation of practical parallel systems. Extensive research studies have been conducted on the performance modelling and evaluation of these networks. Nonetheless, most of these studies have been confined to uniform traffic distributions and have been based on software simulation. The present paper proposes a new stochastic model to predict message latency in k-ary n-cubes with deterministic routing in the presence of hot-spot traffic. The model has been validated through simulation experiments and has shown a close agreement with simulation results. Abstract Generalized Processor Sharing (GPS) is an idealized scheduling mechanism with given weights assigned to individual traffic flows for service differentiation. Based on the principle of GPS, a number of variants have been proposed and deployed in real-world communication systems where the service capacity of network channels is usually variable. This paper proposes an analytical performance model for GPS systems with the variable service capacity characterized by Long Range Dependent (LRD) processes. The model is able to accommodate heterogeneous fractional Brownian motion (fBm) and Poisson traffic in multi-service networks. We derive the expressions of the queue length distributions of the GPS system and individual traffic flows. The accuracy of the analytical model is validated through comparison between analytical and simulation results. Traffic loads generated by many parallel applications can exhibit strong burstiness, which may significantly affect the performance of interconnection networks supporting communication among individual processors. As a step towards obtaining a deeper understanding of network performance under various traffic conditions, this paper develops an analytical model for pipelined circuit-switched k-ary n-cubes augmented with virtual channels support in the presence of bursty traffic by extending the application of the well-known Markov-modulated Poisson process (MMPP) to queuing networks. The model is then used to investigate the performance implications of using virtual channels when the network is subjected to bursty traffic. The results reveal the importance of virtual channel flow control for reducing the degrading effects of bursty traffic on network performance. Our analysis also shows that when the number of virtual channels reaches a certain threshold, adding more virtual channels does not lead to considerable performance gains. During the last decades, many researchers in image processing and AI community have been focused on developing image and video analysis and understanding. However, despite their extensive efforts on these, there are still several significant challenges such as robust object segmentation and tracking, motion feature extraction, context modeling, and machine learning algorithms. Therefore, more advanced related issues should be taken into account. We have selected nine research papers whose topics are strongly related to the intelligent surveillance system in smart home environment. Contention free bursting (CFB) is an innovative quality-of-service (QoS) scheme specified in the IEEE 802.11e standard. To reduce the contention overheads, this scheme enables the stations that gain the channel to transmit multiple frames back-to-back in a burst. Most existing analytical models of the CFB scheme have been developed under the assumption of identical stations with saturated traffic loads. In this paper, we present an analytical model for investigating the performance metrics of throughput, end-to-end delay, loss probability, and energy consumption of the CFB scheme in the presence of nonidentical stations with different traffic generation rates. The model is validated against extensive ns2 simulation experiments. Numerical performance results demonstrate the efficiency of the CFB scheme for improving the network performance. Multicluster systems have emerged as a promising infrastructure for provisioning of cost-effective high-performance computing and communications. Analytical models of communication networks in cluster systems have been widely reported. However, for tractability and simplicity, the existing models are based on the assumptions that the network traffic follows the nonbursty Poisson arrival process and the message destinations are uniformly distributed. Recent measurement studies have shown that the traffic generated by real-world applications reveals the bursty nature in both the spatial domain (i.e., nonuniform distribution of message destinations) and temporal domain (i.e., bursty message arrival process). In order to obtain a comprehensive understanding of the system performance, a novel analytical model is developed for communication networks in multicluster systems in the presence of the spatio-temporal bursty traffic. The spatial traffic burstiness is captured by the communication locality and the temporal traffic burstiness is modeled by the Markov-modulated Poisson process. After validating its accuracy through extensive simulation experiments, the model is used to investigate the impact of bursty message arrivals and communication locality on network performance. The analytical results demonstrate that the communication locality can relieve the degrading effects of bursty message arrivals on the network performance. The most known drawback of IEEE 802.11 MAC is the low performance in terms of throughput in case of congested networks. Backoff time is one of the most important factors that govern the collision probability in Ad-Hoc network. Using backoff algorithm could push the performance up and decrease collision rate. This study presents an enhanced fibonacci backoff Algorithm (EFB) for Ad-Hoc Network in which contention window size is selected from two choices: linear and Fibonacci. Simulation results show high performance and less average of end to end delay especially at high number of nodes compared to Binary Exponential Backoff algorithm (BEB). Implementation of differentiated Quality-of-Service (QoS) in next-generation computer networks has received increasing research interests from both academia and industry. The Generalized Processor Sharing (GPS) scheduling strategy has been widely studied as a promising way to provide differentiated QoS due to its service protection feature. Most of the previous studies reported in the literature, however, have focused on the analysis of GPS under either Short Range Dependent (SRD) or Long Range Dependent (LRD) traffic only, neither of which is able to capture the heterogeneous properties of realistic traffic in multi-service networks solely. To fill this gap, this paper develops a new analytical performance model for GPS systems subject to both LRD self-similar traffic and SRD Poisson traffic. More specifically, using an approach based on Large Deviation Principles, this study contributes to performance modelling and evaluation of GPS scheduling by deriving the analytical upper and lower bounds of the aggregate and individual queue length distributions of heterogeneous traffic flows. The comparisons between analytical bounds and extensive simulation results validate the accuracy and merits of the analytical model which can be adopted as a practical and cost-effective evaluation tool for investigating the performance behaviour of GPS systems under heterogeneous network traffic with various parameter settings. Several existing studies have revealed that pipelined circuit switching (or PCS for short) can provide superior performance characteristics over wormhole switching. This paper proposes a new analytical model for PCS in high-radix high-dimensional k-ary n-cubes augmented with virtual channel support. The model uses Random Walk Theory to analyse the backtracking actions of the message header during the path set-up phase, and M/G/1 queueing systems to compute the mean waiting time that a message experiences at a source node before entering the network. Results from simulation experiments confirm that the proposed model exhibits a good degree of accuracy for various network sizes and under different operating conditions. The 2008 Beijing Olympics was an interesting event from a VoD perspective because it involved near real-time video delivery at massive scales over multiple days of a high-profile event. We present some measurement-driven insights into this event through a unique dataset obtained from ChinaCache, the largest CDN in China. The dataset is unique in three respects. First, it gives a \\\"white-box\\\" view into user access patterns which would otherwise be impossible. Second, since the CDN serves different content providers, it allows to compare and contrast the effects of different presentation models on end users. Third, the nature of the content itself is vastly different from traditional VoD systems in terms of the real-time and event-driven nature, which gives rise to unique effects. The dataset allows us to investigate a wide range of interesting issues: (1) how the live nature of the events causes differences in access patterns compared to traditional VoD and User-Generated Content (UGC) systems, (2) how the presentation models affect user behavior, and (3) flash-crowd phenomena. Based on these observations, we discuss implications for future live VoD systems. Recently many decentralized methods for trust management have been proposed. FIRE trust and reputation model presents a modular approach to trust and reputation from different information sources, according to availability, interaction trust, role-based trust, witness reputation, and certified reputation. However, FIRE model does not consider malicious activity and possible collusive behavior in the network nodes and is therefore susceptible to collusion attacks. In this paper we present a trust management approach to detecting collusion in direct and witness interactions among nodes based on history of interactions among nodes. Various interaction policies are defined to detect and prevent collaborative behavior in colluding nodes. Finally a multidimensional trust model FIRE+ is proposed for avoiding collusion attacks in direct and witness based interactions. The complementary features of various wireless access technologies in heterogeneous wireless networks make it attractive and challenging to offer users an always best connected (ABC) service. To achieve this goal, the dynamic network selection has received many research efforts. However, most of the existing work have focused on the network layer performance and ignored the consideration of energy efficiency. To fill this gap, an energy-efficient network selection scheme is proposed in this paper to improve the energy efficiency of wireless network access in heterogeneous wireless networks environment. The dynamics of network selection is formulated as the process of an evolutionary game. The users in different service areas complete for the data rate from different wireless networks (i.e., WMAN, cellular networks, and WLAN), and the network selection made by a user is based on its payoff that is a function of the data rate and power consumption. The addressed problem is then modelled by the replicator dynamics. Simulation results are presented to demonstrate the significant performance improvement compared to the existing scheme. Ensuring the required reliability and energy efficiency is an essential issue in Underwater Acoustic Networks (UANs). Many schemes have been developed to improve reliability and energy efficiency in such networks. However, most of the existing schemes are based on the assumption that the noise is uniformly distributed in the underwater surrounding and the noise attenuation is not considered. This paper takes the noise attenuation into account and proposes a new Asymmetric Multi-path Division Communications (AMDC) mechanism to improve reliability and energy efficiency in UANs. To this end, an Asymmetric Multiple Layer Division (AMLD) scheme is developed to divide the underwater communication space for the purpose of constructing the tree-based multi-paths. The problem of energy efficiency of AMDC is then formulated as a distributed optimization problem and is solved in order to achieve a set of feasible solutions. Finally, simulation experiments are conducted to evaluate the performance of the proposed AMDC. The results reveal that AMDC outperforms the existing multi-path transmission scheme in UANs in terms of energy efficiency and the total Packet Error Rate (PER). Effective media server placement strategies are critical for the quality and cost of multimedia services. Existing studies have primarily focused on optimization-based algorithms to select server locations from a small pool of candidates based on the entire topological information and thus these algorithms are not scalable due to unavailability of the small pool of candidates and low-efficiency of gathering the topological information in large-scale networks. To overcome this limitation, a novel scalable framework called NetClust is proposed in this paper. NetClust takes advantage of the latest network coordinate technique to reduce the workloads when obtaining the global network information for server placement, adopts a new K-means-clustering-based algorithm to select server locations and identify the optimal matching between clients and servers. The key contribution of this paper is that the proposed framework optimizes the trade-off between the service delay performance and the deployment cost under the constraints of client location distribution and the computing/storage/bandwidth capacity of each server simultaneously. To evaluate the performance of the proposed framework, a prototype system is developed and deployed in a real-world large-scale Internet. Experimental results demonstrate that 1) NetClust achieves the lower deployment cost and lower delay compared to the traditional server selection method; and 2) NetClust offers a practical and feasible solution for multimedia service providers. The interconnection network is one of the most critical architectural components in a parallel computer as any interaction between individual processors ultimately depends on its effectiveness. Recent studies have shown that traffic workloads in many parallel computation environments reveal burstiness over a number of time-scales, which has a considerable impact on network performance. Analytical modelling plays an important role in obtaining a clear understanding of the performance of interconnection networks with various design spaces and operating under different traffic conditions. This paper proposes a concise analytical model for the hypercube operating under multiple time-scale bursty traffic. The validity of the model is demonstrated by comparing analytical results to those obtained through simulation experiments of the actual system. The analytical model is then applied to evaluate the impact of multiple time-scale bursty traffic on the performance of hypercubes and investigate the effects of Hurst parameters of the traffic. A performance model of circuit switched k-ary n-cubes with deterministic routing has recently been proposed by Sharma and Varvarigos [IEEE Trans. Parallel Distrib. Syst. 8 (4) (1997) 349]. Many studies have revealed that using adaptive routing along with virtual channels considerably improves network performance over deterministic routing. This paper presents the first analytical model for calculating the message latency in circuit switched k-ary n-cube networks with adaptive routing. The main feature of the proposed model is the use of Markov chains to compute the path set-up time and to capture the effects of using virtual channels to reduce message blocking in the network. The mean waiting time that a message experiences at a source node is calculated using an M/G/1 queueing system. The validity of the model is demonstrated by comparing analytical results with those obtained through simulation experiments. Owing to its superior properties, such as fast identification and relatively long interrogating range over barcode systems, Radio Frequency Identification (RFID) technology has promising application prospects in inventory management. This paper studies the problem of complete identification of missing RFID tag, which is important in practice. Time efficiency is the key performance metric of missing tag identification. However, the existing protocols are ineffective in terms of execution time and can hardly satisfy the requirements of real-time applications. In this paper, a Multi-hashing based Missing Tag Identification (MMTI) protocol is proposed, which achieves better time efficiency by improving the utilization of the time frame used for identification. Specifically, the reader recursively sends bitmaps that reflect the current slot occupation state to guide the slot selection of the next hashing process, thereby changing more empty or collision slots to the expected singleton slots. We investigate the optimal parameter settings to maximize the performance of the MMTI protocol. Furthermore, we discuss the case of channel error and propose the countermeasures to make the MMTI workable in the scenarios with imperfect communication channels. Extensive simulation experiments are conducted to evaluate the performance of MMTI, and the results demonstrate that this new protocol significantly outperforms other related protocols reported in the current literature. An important function of wireless networks is to support mobile computing. Mobile Ad hoc NETworks (MANETs) consist of a collection of mobile stations communicating with each other without the use of any pre-existent infrastructure. The self-organization characteristic of MANETs makes them suitable for many real-world applications where network topology changes frequently. As a result, the development of efficient MAC (Medium Access Control) protocols in MANETs is extremely challenging. Self-similar traffic with scale-invariant burstiness can generate bursty network loads and thus seriously degrade the system performance. This paper presents an adaptive MAC scheme which dynamically adjusts the increasing function and resetting mechanism of contention window based on the status of network loads. The performance of this scheme is investigated in comparison with the legacy DCF (Distributed Coordination Function) under self-similar traffic and different mobility models. The performance results reveal that the proposed scheme is able to achieve the higher throughput and energy efficiency as well as lower end-to-end delay and packet drop probability than the legacy DCF. The topology of interconnection networks plays a key role in the performance of parallel computing systems. A new interconnection network called exchanged crossed cube (ECQ) is proposed and analyzed in this paper. We prove that ECQ has the better properties than other variations of the basic hypercube in terms of the smaller diameter, fewer links, and lower cost factor, which indicates the reduced communication overhead, lower hardware cost, and more balanced consideration among performance and cost. Furthermore, it maintains several attractive advantages including recursive structure, high partitionability, and strong connectivity. Furthermore, the optimal routing and broadcasting algorithms are proposed for this new network topology. Variety and veracity are two distinct characteristics of large-scale and heterogeneous data. It has been a great challenge to efficiently represent and process big data with a unified scheme. In this paper, a unified tensor model is proposed to represent the unstructured, semistructured, and structured data. With tensor extension operator, various types of data are represented as subtensors and then are merged to a unified tensor. In order to extract the core tensor which is small but contains valuable information, an incremental high order singular value decomposition (IHOSVD) method is presented. By recursively applying the incremental matrix decomposition algorithm, IHOSVD is able to update the orthogonal bases and compute the new core tensor. Analyzes in terms of time complexity, memory usage, and approximation accuracy of the proposed method are provided in this paper. A case study illustrates that approximate data reconstructed from the core set containing 18% elements can guarantee 93% accuracy in general. Theoretical analyzes and experimental results demonstrate that the proposed unified tensor model and IHOSVD method are efficient for big data representation and dimensionality reduction. A comprehensive performance model is presented that can obtain the closed-form expressions of the queue length distribution and loss probability of priority queuing systems subject to heterogeneous long-range dependent and short-range dependent traffic. Abstract The secure distribution and maintenance of key information are essential for controlling access to video multicast systems. Unlike the conventional approaches using a dedicated channel independent of the video stream, this paper proposes a novel data embedding scheme, which we refer to as SMDE (statistical modulation data embedding), to support the media-dependent approach that exploits data-dependent channel to transmit and maintain key information. Using embedded data to convey key information is able to achieve added security and reduce bandwidth resource consumption. To this end, we present some new algorithms to embed key messages. By adopting the statistical properties of host video signal, new coding approach for embedded data, and error control algorithm, the proposed SMDE scheme can provide error resilience, transparence for adaptation mechanism, high accuracy of detecting embedded data and real-time processing capability for video applications. These advantages have been demonstrated by the extensive experimental results. Abstract An interconnection network is a crucial component of parallel computers because the overall system performance is very sensitive to the latency of messages delivered by the network to communicate among collaborating processors. This paper presents an analytical performance model to calculate message latency in circuit-switched hypercubic networks in the presence of bursty traffic pattern, which is a typical scenario for multimedia applications. A message in circuit switching may need a number of connection attempts before successfully setting up a path from source to destination. The proposed model uses the approach of superposing infinite bursty traffic streams to capture the effective traffic entering the network from a source node, which includes the traffic generated by the source and those due to many connection attempts. Results obtained from simulation experiments confirm that the proposed model exhibits a good degree of accuracy for various network sizes and under different operating conditions. Greedy and Contention-based forwarding schemes perform data routing hop-by-hop, without discovering the end-to-end route to the destination. Accordingly, the neighboring node that satisfies specific criteria is selected as the next forwarder of the packet. Both schemes require the nodes participating in the selection process to be within the area that confronts the location of the destination. Therefore, the lifetime of links for such schemes is not only dependent on the transmission range, but also on the location parameters (position, speed and direction) of the sending node, the neighboring node and the destination. In this paper, we propose a new link lifetime prediction method for greedy and contention-based routing. The evaluation of the proposed method is conducted by the use of stability-based greedy routing algorithm, which selects the next hop node having the highest link lifetime. a b s t r a c t Continuously identifying a user's location context provides new opportunities to understand daily life and human behavior. Indoor location systems have been mainly based on WiFi infrastructures which consume a great deal of energy mostly due to keeping the user's WiFi device connected to the infrastructure and network communication, limiting the overall time when a user can be tracked. Particularly such tracking systems on battery-limited mobile devices must be energy-efficient to limit the impact on the experience of using a phone. Recently, there have been a lot of studies of energy-efficient positioning systems, but these have focused on outdoor positioning technologies. In this paper, we propose a novel indoor tracking framework that intelligently determines the location sampling rate and the frequency of network communication, to optimize the accuracy of the location data while being energy-efficient at the same time. This framework leverages an accelerometer, widely available on everyday smartphones, to reduce the duty cycle and the network communication frequency when a tracked user is moving slowly or not at all. Our framework can work for 14 h without charging, supporting applications that require this location information without affecting user experience. The efficiency of a large-scale multicomputer is critically dependent on the performance of its interconnection network. Current multicomputers have widely employed the torus as their underlying network topology for efficient interprocessor communication. In order to ensure a successful exploitation of the computational power offered by multicomputers it is essential to obtain a clear understanding of the performance capabilities of their interconnection networks under various system configurations. Analytical modelling plays an important role in achieving this goal. This study proposes a concise performance model for computing communication delay in the torus network with circuit switching in the presence of multiple time-scale correlated traffic which is found in many real-world parallel computation environments and has strong impact on network performance. The tractability and reasonable accuracy of the analytical model demonstrated by extensive simulation experiments make it a practical and cost-effective evaluation tool to investigate network performance with various alternative design solutions and under different operating conditions. Radio Frequency Identification (RFID) technology has been widely used in inventory management in many scenarios, e.g., warehouses, retail stores, hospitals, etc. This paper investigates a challenging problem of complete identification of missing tags in large-scale RFID systems. Although this problem has attracted extensive attention from academy and industry, the existing work can hardly satisfy the stringent real-time requirements. In this paper, a Slot Filter-based Missing Tag Identification (SFMTI) protocol is proposed to reconcile some expected collision slots into singleton slots and filter out the expected empty slots as well as the unreconcilable collision slots, thereby achieving the improved time-efficiency. The theoretical analysis is conducted to minimize the execution time of the proposed SFMTI. We then propose a cost-effective method to extend SFMTI to the multi-reader scenarios. The extensive simulation experiments and performance results demonstrate that the proposed SFMTI protocol outperforms the most promising Iterative ID-free Protocol (IIP) by reducing nearly 45% of the required execution time, and is just within a factor of 1.18 from the lower bound of the minimum execution time. Wireless local area networks (WLANs) have risen in popularity for in-car networking systems that are designed to make driving safer. Wireless mesh networks (WMNs) are widely deployed to expand the coverage of high-speed WLANs and to support last-mile connectivity for mobile users anytime and anywhere at low cost. Many recent measurement studies have shown that the traffic arrival process in wireless networks exhibits the bursty and correlated nature. A new analytical model is developed in this paper as a cost-effective performance tool to investigate the quality-of-service (QoS) of the WMN that interconnects multiple WLANs in the presence of bursty and correlated traffic. After validating its accuracy via extensive simulation experiments, the analytical model is then used to investigate the performance of the hybrid wireless networks. Transmission Opportunity (TXOP) is a promising burst transmission scheme defined in the IEEE 802.11e Medium Access Control (MAC) protocol to achieve differentiated Quality-of-Service (QoS) and improve the utilization of the scarce wireless bandwidth. Although performance modelling of the TXOP scheme has attracted tremendous research efforts from both the academia and industry, most existing analytical models have been developed under the unrealistic assumption that stations in wireless networks generate identical traffic loads. However, this assumption fails to capture the behaviour of wireless stations under the realistic working environment. To fill this gap, this paper proposes a new analytical model for the TXOP service differentiation scheme in single-hop ad hoc networks in the presence of unbalanced stations with different traffic loads. The QoS performance metrics including throughput, end-to-end delay, frame dropping probability, and energy consumption are derived. Extensive NS-2 simulation experiments are conducted to validate the accuracy of the developed model. The analytical results demonstrate the efficiency of the TXOP scheme for QoS differentiation and performance enhancement. Moreover, the analytical model is adopted as a cost-effective tool to investigate the impact of the buffer size and the number of stations on the performance of the TXOP scheme. The performance results show that the desirable throughput differentiation for different stations can be achieved by setting the appropriate TXOP limits. Weak reliability and low energy efficiency are the inherent problems in Underwater Sensor Networks (USNs) characterized by the acoustic channels. Although multiple-path communications coupled by Forward Error Correction (FEC) can achieve high performance for USNs, the low probability of successful recovery of received packets in the destination node significantly affects the overall Packet Error Rate (PER) and the number of multiple paths required, which in turn becomes a critical factor for reliability and energy consumption. In this paper, a novel Multiple-path FEC approach (M-FEC) based on Hamming Coding is proposed for improving reliability and energy efficiency in USNs. A Markovian model is developed to formulate the probability of M-FEC and calculate the overall PER for the proposed decision and feedback scheme, which can reduce the number of the multiple paths and achieve the desirable overall PER in M-FEC. Compared to the existing multipath communication scheme, extensive simulation experiments show that the proposed approach achieves significantly lower packet delay while consuming only 20-30 percent of energy in multiple-path USNs with various Bit Error Rates (BER). Priority scheduling principle plays a crucial role in the Differentiated Services (DiffServ) architecture for the provisioning of Quality-of-Service (QoS) of network-based applications. Analytical modelling and performance evaluation of priority queuing systems have received significant attention and research efforts. However, most existing work has primarily focused on the analysis of priority queuing under either Short Range Dependent (SRD) or Long Range Dependent (LRD) traffic only. Recent studies have shown that realistic traffic reveals heterogeneous nature within modern multi-service networks. With the aim of investigating the impact of heterogeneous traffic on the design and performance of network-based systems, this paper proposes a novel analytical model for priority queuing systems subject to heterogeneous LRD self-similar and SRD Poisson traffic. The key contribution of the paper is to extend the application of the generalized Schilder's theorem (originally a large deviation principle for handling Gaussian processes only) to deal with heterogeneous traffic and further develop the analytical upper and lower bounds of the queue length distributions for individual traffic flows. The validity and accuracy of the model demonstrated through extensive comparisons between analytical bounds and simulation results make it a practical and cost-effective evaluation tool for investigating the performance behaviour of priority queuing systems under heterogeneous traffic with various parameter settings. Wireless mesh networks (WMNs) have been attracting significant attention due to their promising technology. The WMN technology is becoming a major avenue for the fourth generation of wireless mobility. Communication in large-scale wireless networks can create bottlenecks for scalable implementations of computationally intensive applications. A class of crucially important communication patterns that have already received considerable attention in this regard are group communication operations, since these inevitably place a high demand on network bandwidth and have a consequent impact on algorithm execution times. Multicast communication has been among the most primitive group capabilities of any message passing in networks. It is central to many important distributed applications in science and engineering and fundamental to the implementation of higher-level communication operations such as gossip, gather, and barrier synchronisation. Existing solutions offered for providing multicast communications in WMN have severe restriction in terms of almost all performance characteristics. Consequently, there is a need for the design and analysis of new efficient multicast communication schemes for this promising network technology. Hence, the aim of this study is to tackle the challenges posed by the continuously growing need for delivering efficient multicast communication over WMN. In particular, this study presents a new load balancing aware multicast algorithm with the aim of enhancing the QoS in the multicast communication over WMNs. Our simulations experiments show that our proposed multicast algorithm exhibits superior performance in terms of delay, jitter and throughput, compared to the most well known multicast algorithms. The FIRE trust and reputation model is a de-centralized trust model that can be applied for trust management in unstructured Peer-to-Peer (P2P) overlays. The FIRE model does not, however, consider malicious activity and possible collusive behavior in nodes of network and it is therefore susceptible to collusion attacks. This investigation reveals that FIRE is vulnerable to lying and cheating attacks and presents a trust management approach to detect collusion in direct and witness interactions among nodes based on colluding node's history of interactions. A witness ratings based graph building approach is utilized to determine possibly collusive behavior among nodes. Furthermore, various interaction policies are defined to detect and prevent collaborative behavior in colluding nodes. Finally a multidimensional trust model FIRE+ is devised for avoiding collusion attacks in direct and witness based interactions. The credibility of the proposed trust management scheme as an enhancement of the FIRE trust model is verified by extensive simulation experiments. Greedy and contention-based forwarding schemes were proposed for mobile ad hoc networks (MANETs) to perform data routing hop-by-hop, without prior discovery of the end-to-end route to the destination. Accordingly, the neighboring node that satisfies specific criteria is selected as the next forwarder of the packet. Both schemes require the nodes participating in the selection process to be within the area that confronts the location of the destination. Therefore, the lifetime of links for such schemes is not only dependent on the transmission range, but also on the location parameters (position, speed and direction) of the sending node and the neighboring node as well as the destination. In this paper, we propose a new link lifetime prediction method for greedy and contention-based routing which can also be utilized as a new stability metric. The evaluation of the proposed method is conducted by the use of stability-based greedy routing algorithm, which selects the next hop node having the highest link stability. Community Cloud Computing is an emerging and promising computing model for a specific community with common concerns, such as security, compliance and jurisdiction. It utilizes the spare resources of networked computers to provide the facilities so that the community gains services from the cloud. The effective collaboration among the community clouds offers a powerful computing capacity for complex tasks containing the subtasks that need data exchange. Selecting the best group of community clouds that are the most economy-efficient, communication-efficient, secured, and trusted to accomplish a complex task is very challenging. To address this problem, we first formulate a computational model for multi-community-cloud collaboration, namely MG 3 . The proposed model is then optimized from four aspects: minimizing the sum of access cost and monetary cost, maximizing the security-level agreement and trust among the community clouds. Furthermore, an efficient and comprehensive selection algorithm is devised to extract the best group of community clouds in MG 3 . Finally, the extensive simulation experiments and performance analysis of the proposed algorithm are conducted. The results demonstrate that the proposed algorithm outperforms the minimal set coverings based algorithm and the random algorithm. Moreover, the proposed comprehensive community clouds selection algorithm can guarantee good global performance in terms of access cost, monetary cost, security level and trust between user and community clouds. Participatory sensing, a promising sensing paradigm, enables people to collect and share sensor data on phenomena of interest using mobile devices across many applications, such as smart transportation and air quality monitoring. This article presents a framework of participatory sensing and then focuses on a key technical challenge: developing a trajectory-based recruitment strategy of social sensors in order to enable service providers to identify well suited participants for data sensing based on temporal availability, trust, and energy. To devise a basic recruitment strategy, the Dynamic Tensor Analysis algorithm is initially adopted to learn the time-series tensor of trajectory so that the users' trajectory can be predicted. To guarantee reliable sensing data collection and communication, the trust and energy factors are taken into account jointly in our multi-objective recruitment strategy. In particular, friend-like social sensors are also defined to deal with an emergency during participatory sensing. An illustrative example and experiment are conducted on a university campus to evaluate and demonstrate the feasibility and extensibility of the proposed recruitment strategy. Due to the use of acoustic channels with limited available bandwidth, Underwater Sensor Networks (USNs) often suffer from significant performance restrictions such as low reliability, low energy-efficiency, and high end-to-end packet delay. The provisioning of reliable, energy-efficient, and low-delay communication in USNs has become a challenging research issue. In this paper, we take noise attenuation in deep water areas into account and propose a novel layered multipath power control (LMPC) scheme in order to reduce the energy consumption as well as enhance reliable and robust communication in USNs. To this end, we first formalize an optimization problem to manage transmission power and control data rate across the whole network. The objective is to minimize energy consumption and simultaneously guarantee the other performance metrics. After proving that this optimization problem is NP-complete, we solve the key problems of LMPC including establishment of the energy-efficient tree and management of energy distribution and further develop a heuristic algorithm to achieve the feasible solution of the optimization problem. Finally, the extensive simulation experiments are conducted to evaluate the network performance under different working conditions. The results reveal that the proposed LMPC scheme outperforms the existing mechanism significantly. This paper presents the design, implementation, and evaluation of TransCom, a virtual disk (Vdisk) based cloud computing platform that supports heterogeneous services of operating systems (OSes) and their applications in enterprise environments. In TransCom, clients store all data and software, including OS and application software, on Vdisks that correspond to disk images located on centralized servers, while computing tasks are carried out by the clients. Users can choose to boot any client for using the desired OS, including Windows, and access software and data services from Vdisks as usual without consideration of any other tasks, such as installation, maintenance, and management. By centralizing storage yet distributing computing tasks, TransCom can greatly reduce the potential system maintenance and management costs. We have implemented a multi-platform TransCom prototype that supports both Windows and Linux services. The extensive evaluation based on both test-bed experiments and real-usage experiments has demonstrated that TransCom is a feasible, scalable, and efficient solution for successful real-world use. The multifractal-like behaviour (i.e., self-similarity) of network traffic is highly detrimental to the performance of scheduling mechanisms and user-perceived Quality-of-Service. In order to accurately evaluate and predict the effects of self-similar traffic on the performance of the Generalized Processor Sharing (GPS) principle, this paper proposes an original and cost-efficient flow-decomposition approach to modelling the GPS system under homogeneous self-similar traffic (i.e., with the identical Hurst parameters). Based on the comprehensive analysis of the excess service sharing behavior of traffic flows, we decompose the GPS system into a group of single-server single-queue systems and investigate the queue length distributions for individual traffic flows. Extensive simulation experiments are conducted to validate the accuracy of the analytical results for predicting the performance of GPS systems. Radio Frequency Identification (RFID) technology has been widely applied in many scenarios such as inventory control, supply chain management due to its superior properties including fast identification and relatively long interrogating range over barcode systems. It is critical to efficiently identify the unknown tags because these tags can appear when new tagged objects are moved in or wrongly placed. The state-of-the-art Basic Unknown tag Identification Protocol-with Collision-Fresh slot paring (BUIP-CF) protocol can first deactivate all the known tags and then collect all the unknown tags. However, BUIP-CF protocol investigates an ALOHA-like technique and causes too many tag responses, which results in low efficiency. This paper proposes a Fast Unknown tag Identification (FUI) protocol which investigates an indicator vector to label the unknown tags with a given accuracy and removes the time-consuming tag responses in the deactivation phase. FUI also adopts the classical Enhanced Dynamic Framed Slotted ALOHA (EDFSA) protocol to collect the labeled unknown tags. We then investigate the optimal parameter settings to maximize the performance of the proposed FUI protocol. Extensive simulation experiments are conducted to evaluate the performance of the proposed FUI protocol and the experimental results show that it considerably outperforms the state-of-the-art protocol. In peer-to-peer (P2P) networks, trust ratings aggregation and peer ranking are unreliable, time-consuming and space-demanding operations. The swift expansion of emerging P2P techniques towards the domain of mobile computing poses significant challenges for trust and security management. Several trust management schemes have been proposed recently to counter the security threat on P2P systems. However, due to the difficulties caused by system mobility and dynamic network topology, there is an increasing requirement of decentralized and distributed trust management schemes. In this paper, we initially investigate and analyze four typical decentralized and distributed trust management schemes. Based on the findings of this analysis, a robust distributed reputation and trust management scheme, referred to as M-trust, is proposed for mobile P2P networks. The new scheme utilizes confidence in reputation, based on interactions among peers, to reduce the computation complexity. Furthermore, distributed algorithms are presented for accurate and reliable trust ratings aggregation and space management. The performance of M-trust is evaluated in comparison to the existing schemes using extensive simulation experiments. The results demonstrate that M-trust possesses the excellent overall performance in terms of accuracy, reliability, convergence speed, and detection rate under various constraints of mobility, trust threshold and network out-degree. The anonymity and dynamic characters of Peer-to-Peer (P2P) system makes it an ideal medium for selfish and vicious action. In order to solve this problem, P2P reputation systems are proposed to evaluate the trustworthiness of peers and to prevent the selfish, dishonest, and malicious peers' behaviors, which collects local reputation scores and aggregates them into the global reputation. In this paper we propose a DHT trust overlay network (DHTON) to model the network structure and the storage of reputation information. We also design a robust and distributing reputation system, DHTrust, which takes full advantage of the Distributed Hash Table (DHT) to distribute local reputation to trade off the damage of fake reputation information by genuine reputation information. By using the trust evaluation towards two reputation scores, we can also distinguish and evaluate the fundamental behaviors of peers in P2P network, i.e., providing service and issuing reputation scores. Through the simulation experiments, we find our system makes significant performance gains in convergence speed and aggregation accuracy, and the most important, is robust to malicious peers. Wireless Mesh Network (WMN) has been considered as a key emerging technology to construct next generation wireless communication networks. It combines the advantages of both mobile ad-hoc network (MANET) and traditional fixed network, attracting significant industrial and academic attentions. In WMN, the load balancing becomes a hot topic in enhancing the QoS provision as a load balanced WMN exhibits low delay and high quality communications. Although there are a number of proposals on using load-aware routing metrics in WMN, the neighbourhood load has not been considered within the context of load balancing and QoS aware WMNs. In this paper, we propose a Neighbourhood Load Routing scheme to further improve the performance of the existing Routing protocol such as AODV in WMN. We have conducted extensive simulation experiments. Our results confirm the superiority of our proposed scheme over its well-known counterparts, especially in grid topologies. Low duty cycle operation is a well-known mechanism used for achieving energy efficiency in wireless sensor networks (WSNs). Nodes under this concept work only for a small percentage of time and remain largely inactive to conserve energy. S-MAC, is a popular protocol designed specifically for WSNs with low duty cycle operation. At its inception, S-MAC has been designed for low traffic loads. In this paper, we propose an enhanced version of S-MAC, called PS-MAC, that shows to support comparatively higher traffic levels while achieving the better energy efficiency. This is achieved using the parallel transmission concept. Performance evaluation is carried out using the NS2 simulation platform to verify the protocol improvements. In many applications of sensor networks including environmental monitoring and security surveillance, a large volume of sensed data generated by sensors is needed to be either collected at the base station or aggregated within the network to respond to user queries. However, due to the high failure rate of wireless communication, robust query processing is very important for some critical queries in sensor networks by dealing with the messages loss in sensor networks. In this paper, we propose a novel localized and adaptive algorithm for robust top-$k$ query evaluation in wireless sensor networks, which strives the right tradeoff between the energy consumption and the accuracy of obtained results. We finally conduct extensive experiments by simulations to evaluate the performance of the proposed algorithm on real datasets. The experimental results show that the proposed algorithm is energy-efficient when meeting the specified result accuracy requirement. k-means is undoubtedly one of the most popular clustering algorithms owing to its simplicity and efficiency. However, this algorithm is highly sensitive to the chosen initial centers and thus a proper initialization is crucial for obtaining an ideal solution. To address this problem, k-means++ is proposed to sequentially choose the centers so as to achieve a solution that is provably close to the optimal one. However, due to its weak scalability, k-means++ becomes inefficient as the size of data increases. To improve its scalability and efficiency, this paper presents Map Reduce k-means++ method which can drastically reduce the number of Map Reduce jobs by using only one MapReduce job to obtain k centers. The k-means++ initialization algorithm is executed in the Mapper phase and the weighted k-means++ initialization algorithm is run in the Reducer phase. As this new Map Reduce k-means++ method replaces the iterations among multiple machines with a single machine, it can reduce the communication and I/O costs significantly. We also prove that the proposed Map Reduce k-means++ method obtains O(\\u03b1 2 )approximation to the optimal solution of k-means. To reduce the expensive distance computation of the proposed method, we further propose a pruning strategy that can greatly avoid a large number of redundant distance computations. Extensive experiments on real and synthetic data are conducted and the performance results indicate that the proposed Map Reduce k-means++ method is much more efficient and can achieve a good approximation. Participatory sensing is a promising sensing paradigm that enables collection, processing, dissemination and analysis of the phenomena of interest by ordinary citizens through their handheld sensing devices. Participatory sensing has huge potential in many applications, such as smart transportation and air quality monitoring. However, participants may submit low-quality, misleading, inaccurate, or even malicious data if a participatory sensing campaign is not launched effectively. Therefore, it has become a significant issue to establish an efficient participatory sensing campaign for improving the data quality. This article proposes a novel five-tier framework of participatory sensing and addresses several technical challenges in this proposed framework including: (1) optimized deployment of data collection points (DC-points); and (2) efficient recruitment strategy of participants. Toward this end, the deployment of DC-points is formulated as an optimization problem with maximum utilization of sensor and then a Wise-Dynamic DC-points Deployment (WD3) algorithm is designed for high-quality sensing. Furthermore, to guarantee the reliable sensing data collection and communication, a trajectory-based strategy for participant recruitment is proposed to enable campaign organizers to identify well-suited participants for data sensing based on a joint consideration of temporal availability, trust, and energy. Extensive experiments and performance analysis of the proposed framework and associated algorithms are conducted. The results demonstrate that the proposed algorithm can achieve a good sensing coverage with a smaller number of DC-points, and the participants that are termed as social sensors are easily selected, to evaluate the feasibility and extensibility of the proposed recruitment strategies. Distributed Denial-of-Service (DDoS) attacks are critical threats to both network service providers and legitimate network users. DDoS attacks often overwhelm or exhaust the resources of victims and typically result in abnormal bursty traffic passing through victim systems. In this paper, we develop a mechanism for diagnosing traffic anomalies caused by DDoS attacks on the basis of analyzing the behaviour of network traffic. The traffic in communication networks has been shown to exhibit statistical self-similar phenomena that can be characterized by the so-called Hurst parameter. Therefore, in the proposed mechanism the Hurst parameter coupled by variance and autocorrelation are employed as the key performance metrics to spot the anomalies of network traffic. The proposed diagnosis mechanism is validated through experiments where the datasets consist of two groups. The first group is obtained from the MIT Lincoln Laboratory DOS attack dataset. The second group is collected from our DDoS attack simulation experiments, which cover three representative traffic shapes resulting from three different DDoS attack behaviours, namely, constant intensity, ramp-up behaviour and pulse behaviour. The experimental results show that the developed mechanism can alert the DDoS attack schemes within short respond time. The provisioning of differentiated Quality-of-Service (QoS) is a key objective in the design and deployment of convergent multi-service communication networks. Integrated traffic management schemes that combine various fundamental scheduling principles have been proposed for the QoS differentiation in cellular and IEEE 802.16/WiMAX-based broadband wireless networks. In this paper, cost-effective analytical performance models are devised for a new hybrid scheduling scheme based on the integration of the well-known Priority Queueing (PQ) and Weighted Fair Queueing (WFQ) scheduling principles. In these models, traffic arrival processes are represented, respectively, by non-bursty Poisson process and bursty-correlated Markov-Modulate Poisson Process (MMPP) and new closed-form expressions of main performance metrics are derived including system fairness, mean number of packets in each buffer, system throughput, mean queueing delay and packet loss probability of individual traffic flows. The accuracy of the analytical models is validated through extensive comparisons between analytical results and those obtained from simulation experiments. Finally, these models are adopted to investigate the impact of the weights of traffic flows served by WFQ and assess the adverse effects of the traffic burstiness and correlation on the performance of the integrated scheduling scheme. Peer-to-Peer (P2P) has become a popular live streaming delivery technology owing to its scalability and low cost. P2P streaming systems often employ multi-channels to deliver streaming to users simultaneously, which leads to a great challenge of allocating server resources among these channels appropriately. Most existing P2P systems resort to over-allocating server resources to different channels, which results in low-efficiency and high-cost. To allocate server resources to different channels efficiently, we propose a dynamic resource allocation algorithm based on a streaming quality model for P2P live streaming systems. This algorithm can improve the channel streaming quality for multi-channel P2P live streaming system and also guarantees the streaming quality of the channels under extreme Internet conditions. In an experiment, the proposed algorithm is validated by the trace data. Performance modelling of generalized processor sharing (GPS) systems in the presence of self-similar traffic has attracted significant research efforts. To simplify the interdependent relationships between traffic flows in GPS systems, most existing studies have been confined to the unrealistic scenarios where only two traffic flows are taken into account. This paper develops an analytical model for investigating both queue length distributions and loss probabilities of GPS systems subject to multi-class self-similar traffic. To this end, we present an efficient approach to decompose the complicated system based on the notion of feasible ordering of GPS systems and the empty buffer approximation. Through extensive comparisons between analytical and simulation results, we investigate the correctness of the decomposition approach and validate the accuracy of the analytical performance results. The capacity of cognitive radio (CR) systems can be enhanced significantly by deploying relay nodes to exploit the spatial diversity. However, the inevitable imperfect sensing in CR has vital effects on the policy of relay selection, channel access, and power allocation that play pivotal roles in the system capacity. The increase in transmission power can improve the system capacity, but results in high energy consumption, which incurs the increase of carbon emission and network operational cost. Most of the existing schemes for CR systems have not jointly considered the imperfect sensing scenario and the tradeoff between the system capacity and energy consumption. To fill in this gap, this paper proposes an energy-aware centralized relay selection scheme that takes into account the relay selection, channel access, and power allocation jointly in CR with imperfect sensing. Specifically, the CR system is formulated as a partially observable Markov decision process (POMDP) to achieve the goal of balancing the system capacity and energy consumption as well as maximizing the system reward. The optimal policy for relay selection, channel access, and power allocation is then derived by virtue of a dynamic programming approach. A dimension reduction strategy is further applied to reduce its high computation complexity. Extensive simulation experiments and results are presented and analysed to demonstrate the significant performance improvement compared to the existing schemes. The performance results show that the received reward increases more than 50% and the network lifetime increases more than 35%, but the system capacity is reduced less than 6% only. Fast and accurate methods for predicting traffic properties and trend are essential for dynamic network resource management and congestion control. With the aim of performing online and feasible prediction of network traffic, this paper proposes a novel time series model, named adaptive autoregressive (AAR). This model is built upon an adaptive memory-shortening technique and an adaptive-order selection method originally developed by this study. Compared to the conventional one-step ahead prediction using traditional Box\\u2013Jenkins time series models (e.g. AR, MA, ARMA, ARIMA and ARFIMA), performance results obtained from actual Internet traffic traces have demonstrated that the proposed AAR model is able to support online prediction of dynamic network traffic with reasonable accuracy and relatively low computation complexity. Copyright \\u00a9 2005 John Wiley & Sons, Ltd. Distributed contention-based Medium Access Control (MAC) protocols are the fundamental components for IEEE 802.11 type Wireless LANs (WLAN). The deficiency of these types of MAC protocols mainly comes from the idle slots used to contend the channel and from the transmission collisions due to the same backoff slot value being generated. Assigning the same transmission opportunity to various length packets also degrades the system performance. This study takes account of the above issues and presents a new MAC scheme called Maximizing Transmission Time (MTT) to enhance the wireless LAN performance. This scheme allows each station to transmit a burst of packets after winning a transmission opportunity instead of just one packet. This idea can reduce the average number of waiting slots and collision probability in each transmission cycle. Moreover, in order to ensure fairness among stations, a maximum transmission period is assigned to each station for controlling the length of the bursty transmission. An analytical performance model is derived for computing the throughput of the MTT scheme. The extensive simulation experiments reveal that the proposed method can enhance the wireless LANs performance significantly with high throughput, low delay and high degree of fairness. Software rejuvenation is a preventive and proactive fault management technique that is particularly useful for counteracting the phenomenon of software aging, aimed at cleaning up the system internal state to prevent the occurrence of future failure. The increasing interest in combing software rejuvenation with cluster systems has given rise to a prolific research activity in recent years. However, so far there have been few reports on the dependency between nodes in cluster systems when software rejuvenation is applied. This paper investigates the software rejuvenation policy for cluster computing systems with dependency between nodes, and reconstructs an stochastic reward net model of the software rejuvenation in such cluster systems. Simulation experiments and results reveal that the software rejuvenation strategy can decrease the failure rate and increase the availability of the cluster system. It also shows that the dependency between nodes affects software rejuvenation policy. Based on the theoretic analysis of the software rejuvenation model, a prototype is implemented on the Smart Platform cluster computing system. Performance measurement is carried out on this prototype, and experimental results reveal that software rejuvenation can effectively prevent systems from entering into disabled states, and thereby improving the ability of software fault-tolerance and the availability of cluster computing systems. Social recommendation has been popular and successful in various urban sustainable applications such as online sharing, products recommendation and shopping services. These applications allow users to form several implicit social networks through their daily social interactions. The users in such social networks can rate some interesting items and give comments. The majority of the existing studies have investigated the rating prediction and recommendation of items based on user-item bipartite graph and user-user social graph, so called social recommendation. However, the spatial factor was not considered in their recommendation mechanisms. With the rapid development of the service of location-based social networks, the spatial information gradually affects the quality and correlation of rating and recommendation of items. This paper proposes spatial social union (SSU), an approach of similarity measurement between two users that integrates the interconnection among users, items and locations. The SSU-aware location-sensitive recommendation algorithm is then devised. We evaluate and compare the proposed approach with the existing rating prediction and item recommendation algorithms subject to a real-life data set. Experimental results show that the proposed SSU-aware recommendation algorithm is more effective in recommending items with the better consideration of user\\u2019s preference and location. Abstract Reliability and energy effciency are the crucial issues for construction of Underwater Acoustic Networks (UANs). Many schemes have been developed to improve reliability and energy effciency in UANs. However, most of the existing schemes are based on the unrealistic assumptions that the noise in the underwater surrounding is uniformly distributed and the attenuation of noise is not considered. In this paper, we take the noise attenuation into account and propose a new Multi-path Division Transmission (MDT) mechanism to improve reliability and energy effciency in UANs. To this end, a Multiple Layer Division (MLD) scheme is developed to divide the underwater communication space for the purpose of initializing the tree-based multi-paths. The problem of energy effciency of MDT is then formulated as a distributed optimization problem and is solved so as to achieve a set of feasible solutions. Finally, extensive simulation experiments are conducted to evaluate the performance of the proposed MDT. The results reveal that MDT significantly outperforms the current state-of-the-art multi-path transmission schemes in UANs in terms of energy effciency and total Packet Error Rate(PER). Multipath routing can adapt to traffic changes, increase reliability and enhance the Quality of Service support in communication networks. This routing algorithm has been recognized as one of the salient features in Mobile Ad hoc Networks (MANETs). However, existing multipath routing protocols are often practically infeasible due to its security venerability if they suffer from attacks in MANETs, such as repudiation attacks, Denial of Service attacks. In this paper we propose a novel secure multipath routing protocol, referred to as SMRP, and investigate its feasibility and performance. SMRP applies a new heuristic algorithm increasing the number of disjoint paths and a smart authentication mechanism to enhance the security against the attacks in MANETs. The performance results from extensive analysis and experiments show that SMRP can efficiently enhance the security in MANETs while preserving the low overhead of computing and communication. Copyright \\u00a9 2009 John Wiley & Sons, Ltd. Low Rate Wireless Personal Areas Networks (LR-WPANs) have been widely deployed for the purpose of short range, low rate, and low power wireless networking. The IEEE 802.15.4 standard defines a Medium Access Control (MAC) layer protocol for LR-WPANs. The analytical models reported in the literature for 802.15.4 MAC have been mainly developed under the assumptions of ideal channels or uniform error channels, which fail to capture the characteristics of bursty and correlated channel errors in a practical wireless network environment. In this paper, we present an analytical model for 802.15.4 MAC in LR-WPANs in the presence of bursty error channels. This model can be used to obtain the Quality-of-Service (QoS) performance metrics, including throughput, service time, and frame delay. The accuracy of the model is validated through NS-2 simulation experiments. Utilizing the analytical model, we investigate the QoS performance of 802.15.4 MAC under various traffic loads, backoff parameters, numbers of nodes, and channel conditions. The analytical model is demonstrated to be a cost-effective tool for evaluating the QoS performance of 802.15.4 MAC in the presence of bursty error channels. Cognitive Radio Networks have been designed to increase spectrum utilization by utilizing temporarily unused spectrum bands in the primary user's licensed spectrum by secondary (unlicensed) users. In general, when a primary user appears in a band occupied by a secondary user, the secondary user will temporarily cease its transmission and change the operating channel. Unfortunately, successive spectrum handoffs can degrade quality of service (QoS) of the interrupted users. Spectrum handoff mechanism should help cognitive radio users to guarantee QoS. In particular, it should facilitate the channel clearing whilst searching for target vacant channel(s) for completing interrupted secondary users' transmission when the licensed primary user appears at the engaged licensed band. In this paper, a prioritized proactive spectrum handoff decision scheme is proposed to evaluate the total service time for the implemented model. The proposed scheme is modeled using a preemptive resume priority (PRP) M/M/1 queue. It gives a high priority to interrupted users to resume their transmission before any other uninterrupted secondary users. The implemented scheme outperforms the existing spectrum handoff decision schemes in terms of total service time. The cognitive radio (CR) is an emerging technique for increasing the utilisation of communication resources by allowing the unlicensed users to employ the under-utilised spectrum. In this paper, a new analytical performance model is developed to evaluate the QoS of multi-hop CR networks. After validating its accuracy through extensive simulation experiments, the analytical model is adopted as a cost-effective tool to investigate the effects of the primary users' activities on the network performance. Moreover, the model can be used to study the strategy of employing under-utilised spectra so as to maximise the overall resource utilisation and network performance. Mobile ad-hoc network (MANET) is an appealing technology that has attracted lots of research efforts over past years. Although the principle of wireless, structure-less, dynamic networks is attractive, there are still some major flaws that prevent commercial expansion. Security is one of these main barriers; MANETs are known to be particularly vulnerable to security attack. One solution proposed to increase security resilience is to use multipath routing algorithms [Papadimitratos et al., 2002]. However multipath routing also introduces new challenges in terms of security and security overhead. In this paper we consider the problem of secure routing in fully distributed MANETs using multipath routing. After studying the effect of multipath in terms of security, we propose an efficient multipath heuristic and a new approach to protect the route discovery called SDMSR to secure the routing protocol while mitigating security overhead InfiniBand is quickly becoming the choice of interconnection networks for high performance computing (HPC) systems. It defines a system area network (SAN) environment where multiple processor nodes and I/O devices are interconnected using a switched point-to-point network. The unique characteristics of InfiniBand networks, such as no packet dropping, small buffer size and low latency, make the congestion control different from the traditional mechanisms for other high-speed networks and propose more challenges. This paper presents an improved explicit congestion notification (ECN) packet marking mechanism for InfiniBand. In addition, we propose an effective source response function-power increase and power decrease (PIPD), which adopts the rate control with a window, limit to reduce congestion of multiple-class traffic in InfiniBand networks. Simulation experiments have demonstrated that the proposed new resource response function is quite effective in InfiniBand networks and outperforms the existing ECN schemes Convergence of cellular networks and broadcast networks is a long-term research task, because various digital mobile and digital broadcast radio technologies have been developed and specifically been optimized respectively. This paper describes the physical layer convergence of communications and broadcast networks. First, a physical layer downlink frame structure, which contains broadcast sub frame and communication sub frame, is proposed in order to process communication and broadcast services data simultaneously. Then a combined mode of broadcast networks with a big base station and communication networks with many small base stations is presented, and a downlink frame transmission method is proposed. Finally, we simulate the proposed physical layer downlink convergence framework. The simulation results show that the proposed system has close performance with IEEE 802.16e-PUSC system in terms of peak-to-average power ratio, bit error rate and mean square error. Many recent studies have revealed that the traffic generated by multimedia applications exhibits a high degree of burstiness and possesses strong correlation in the number of message arrivals between adjacent time intervals, which can significantly affect network performance. However all existing performance models for multicomputer networks have assumed the Poisson arrival process, which has been shown on many occasions to be unable to capture the nature of multimedia traffic. This paper proposes the first analytical model for wormhole switching in k-ary n-cube networks under multimedia traffic based on the well-known correlated Markov-modulated Poisson process (MMPP). Results obtained from simulation experiments confirm that the proposed model exhibits a good degree of accuracy for various network sizes and under different operating conditions. This paper introduces a parallel algorithm for computing an N=n2/sup n/ point Lagrange interpolation on n-dimensional cube-connected cycles (CCC/sub n/). The algorithm exploits several communication techniques in a novel way which can be adapted for computing similar functions. The performance of the algorithm is also evaluated by means of a speedup measure. It shows a near to optimal speedup for a state-of-the-art implementation technology. A mathematical model for analytical study on complete and partial channel allocation schemes is presented in this paper. Expression for the queue length distribution has been analytically derived. The mathematical model can be used for analysis of multiple call categories like new calls, handoff calls, delay tolerant or delay sensitive calls. This adaptive mathematical model can be utilized to design optimal channel allocation algorithms, or even test the already established resource allocation schemes for provisioning of Quality-of-Service to enable the usage of real time mobile applications. Most existing studies depend upon simulation experiments to investigate the performance of their proposed algorithms, but the mathematical model presented here can really help to evaluate such proposals from an analytical perspective. Since the finding of long-range-dependent (LRD) traffic as a ubiquitous phenomenon in communication networks, analytical modelling of queuing systems subject to such type of traffic has attracted tremendous research interests. Due to the inherent difficulty and complexity of modelling fractal-like LRD traffic, existing studies have usually focused on queuing systems with constant service capacity. Given the fading nature of wireless channels under realistic working environments, it is very necessary to investigate the performance behavior of queuing systems with variable service capacity. To this end, this paper presents a new analytical model for single server queuing systems with LRD input traffic and variable service capacity. Specifically, we derive the close-form expressions for calculating the upper and lower bounds of the tail distribution of queue length in the queuing system. Resorting to extensive comparisons between analytical bounds and simulation results, we validate the effectiveness and accuracy of the model. We further adopt the model to analyze the effects of variable service capacity on the system performance. The enhanced distributed channel access (EDCA) protocol has been proposed to support differentiated quality-of-service (QoS) of wireless local area networks (WLANs). edca introduces a promising scheme, transmission opportunity (TXOP), which enables a station to transmit multiple frames consecutively in a burst upon winning the contention of the channel. The analytical models of the TXOP scheme have been mainly developed under the assumptions that the traffic generated by the stations is saturated or follows a non-bursty Poisson arrival process. These assumptions are unable to capture the characteristics of heterogeneous multimedia traffic. This paper proposes a new analytical model for the TXOP scheme in WLANs comprising heterogeneous classes of stations with multimedia applications. The traffic of such applications is modelled by the non-bursty Poisson, bursty Markov-modulated Poisson process (MMPP), and fractal self-similar traffic. The accuracy of the proposed model is validated through extensive NS-2 simulation experiments. Wireless mesh networks have been attracting significant attention due to its promising technology. It is becoming a major avenue for the fourth generation of wireless mobility. Communication in large-scale wireless networks can create bottlenecks for scalable implementations of computationally intensive applications. A class of crucially important communication patterns that have already received considerable attention in this regard are group communication operations, since these inevitably place a high demand on network bandwidth and have a consequent impact on algorithm execution times. Multicast communication has been among the most primitive group capabilities of any message passing networks. It is central to many important distributed applications in Science and Engineering and fundamental to the implementation of higher-level communication operations such as gossip, gather, and barrier synchronisation. Existing solutions offered for providing multicast communications in WMN have severe restriction in terms of almost all performance characteristics. Consequently, there is a need for the design and analysis of new efficient multicast communication schemes for this promising network technology. Hence, the aim of this study is to tackle the challenges posed by the continuously growing need for delivering efficient multicast communication over WMN. In particular, this study presents a new load balancing aware multicast algorithm with the aim of enhancing the QoS in the multicast communication over WMNs. Many recent studies have convincingly demonstrated that network traffic exhibits a noticeable self-similar nature, which has a considerable impact on queuing performance. However, the networks used in current multicomputers have been primarily designed and analyzed under the assumption of the traditional Poisson arrival process, which is inherently unable to capture traffic self-similarity. Consequently, it is crucial to reexamine the performance properties of multicomputer networks in the context of more realistic traffic models before practical implementations show their potential faults. In an effort toward this end, we propose the first analytical model for wormhole-switched k-ary n-cubes in the presence of self-similar traffic. Simulation experiments demonstrate that the proposed model exhibits a good degree of accuracy for various system sizes and under different operating conditions. The analytical model is then used to investigate the implications of traffic self-similarity on network performance. We reveal that the network suffers considerable performance degradation when subjected to self-similar traffic, stressing the great need for improving network performance to ensure efficient support for this type of traffic. Broadband wireless access for maritime users is a brand-new topic which demands sophisticated transmission technology to support high speed and wide coverage communications. Although the IEEE 802.16j network has been identified as a promising solution to broadband wireless access, the deployment of such networks in a maritime environment faces significant challenges because the natural movement of the ocean surface often causes the motion of stations and further leads to the variation in quality of communication channels. To tackle with these challenges and facilitate channel exploitation for deploy ing 802.16j based maritime communication networks, we first propose a novel scheme to specify the communication channel state between different maritime stations, based on which a new adaptation scheme is developed to exploit the ptimal channel transmission capacity. Moreover, an adaptive traffic scheduling scheme is proposed to fully utilize the channel transmission bandwidth in 802.16j maritime networks. Simulation experiments have shown that the proposed schemes can fully exploit the maritime channels to achieve both satisfactory QoS performance and system throughput for maritime users. Traffic anomalies caused by Distributed Denial-of-Service DDoS attacks are major threats to both network service providers and legitimate customers. The DDoS attacks regularly consume and exhaust the resources of victims and hence result in abnormal bursty traffic through end-user systems. Additionally, malicious traffic aggregated into normal traffic often show dramatic changes in the traffic nature and statistical features. This study focuses on early detection of traffic anomalies caused by DDoS attacks in light of analyzing the network traffic behavior. Key statistical features including variance, autocorrelation, and self-similarity are employed to characterize the network traffic. Further, artificial neural network and support vector machine subject to the performance metrics are employed to predict and classify the abnormal traffic. The proposed diagnosis mechanism is validated through experiments where the datasets consist of two groups. The first group is the Massachusetts Institute of Technology Lincoln Laboratory dataset containing labeled DoS attack. The second group collected from DDoS attack simulation experiments covers three representative traffic shapes resulting from the dynamic attack rate configuration, namely, constant intensity, ramp-up behavior, and pulsing behavior. The experimental results demonstrate that the developed mechanism can effectively and precisely alert the abnormal traffic within short response period. Copyright \\u00a9 2013 John Wiley & Sons, Ltd. The IEEE 802.15.4 standard is uniquely designed to meet the requirements of the low data-rate, low power-consumption applications in the Low Rate Wireless Personal Areas Network (LR-WPAN) with a focus on enabling wireless sensor network applications. Recently, analytical modelling of IEEE 802.15.4 contention-based Medium Access Control (MAC) protocol has received significant research efforts. However, the development of existing models has been mainly based on the use of the bi-dimensional Markov chain. Different from the previous work, this paper presents a new model which adopts the analysis of the average backoff period length rather than the bi-dimensional Markov chain to derive the sensing probability. Using the proposed model, we investigate the impact of the backoff exponent, frame size and number of stations on the performance metrics including sensing failure probability and saturation throughput. Wireless Mesh Networks (WMNs) have emerged as a promising and cost-effective approach to extending the coverage range of wireless local area networks (WLANs) and providing last-mile broadband Internet access. Mesh routers and mesh clients are the key components of WMNs. Mesh routers perform the dual tasks of forwarding packets as well as providing network access to mesh clients. Mesh routers facilitated with the gateway functionality can be connected to the wired infrastructure for Internet access. Traffic streams towards or from the gateway form a tree-based logical structure. In this paper, we develop a new analytical model to investigate the performance metrics including throughput and end-to-end delay in the integrated WLANs and Internet-access WMN with the tree-based logical structure. Extensive simulation experiments are conducted to demonstrate the fact that the proposed analytical model can accurately predict the performance of the integrated network under various working conditions. By virtue of the analytical model, we can tune the system parameters and adjust the density of mesh routers at different locations so as to alleviate the bottleneck problem in the integrated WLANs and WMN. The IEEE 802.15.4 Medium Access Control (MAC) protocol is uniquely designed to meet the requirements of the low data-rate, low power-consumption applications in the Low Rate Wireless Personal Area Network (LR-WPAN). The analytical models of 802.15.4 MAC reported in the literature have been mainly developed under the assumptions of saturated traffic or non-bursty unsaturated traffic conditions. These assumptions are unable to capture the characteristics of bursty multimedia traffic. To fill this gap, this paper proposes a new analytical model for 802.15.4 MAC in LR-WPAN with bursty ON-OFF traffic. Quality-of-Service (QoS) performance metrics in terms of the throughput and total delay are derived. This analytical model is then adopted to investigate the QoS performance of 802.15.4 MAC under various traffic patterns, traffic loads, and number of stations. Numerical results show that the traffic patterns and traffic burstiness have a significant impact on the delay performance of 802.15.4 MAC. Pipelined circuit switching which combines the advantages of both circuit switching and wormhole switching is an efficient method for reconciling the conflicting demands of communication performance and fault-tolerance in interconnection networks of multi-computers. The arrival process and destination distribution of messages generated by real-world parallel applications can markedly affect the performance of communication networks. This paper presents an analytical performance model to calculate the message latency in hypercube interconnection networks with pipelined circuit switching in the presence of bursty and correlated traffic coupled with hot-spot message destinations. The accuracy of the analytical model is validated through extensive simulation experiments. Traffic self-similarity has been discovered to be a ubiquitous phenomenon in communication networks and multimedia systems. Due to its fractal-like nature, performance modelling of self-similar traffic poses greater challenges and exhibits more complexity than that of traditional non-bursty traffic. As a result, most existing studies on analytical modelling of priority queueing systems with self-similar inputs have been restricted to a simplified scenario where only two traffic flows are taken into account. This study contributes to performance modelling of priority queueing systems by proposing a novel queue-decomposition approach, which offers several potential advantages and unique innovations. Specifically, we extend the well-known empty buffer approximation (EBA) method to model priority queueing systems with multiple traffic flows and decompose the original priority queueing system into a group of single-server single-queue systems, which can make the challenging performance modelling problem tractable. We further develop an analytical model to investigate the queue length distributions of individual traffic flows. The validity and accuracy of the model demonstrated through extensive simulation experiments make it a practical and cost-effective evaluation tool for quantitatively evaluating the performance behavior of priority queueing systems with multi-class self-similar traffic under various working conditions. The rapid growth of emerging techniques for mobile resource sharing, content sharing, mobile social networks, and complex cyber-physical systems poses significant challenges for trust and security management. Several trust management schemes have been proposed recently to counter the security threat on Peer-to-Peer (P2P) systems. However, due to the difficulties caused by system mobility, wireless communications, pervasive devices and dynamic network topology, there is an increasing requirement of decentralized and distributed trust management schemes. In this paper, we first investigate and analyze various existing decentralized and distributed trust management schemes. Based on the analytical results, an efficient, accurate, robust and scalable light weight trust ratings aggregation scheme, referred to as M-trust, is proposed for mobile P2P networks. Extensive simulation results show that this proposed scheme can decrease the time required to compute the list of trust ratings and reduce the required storage space. The comparison to other schemes demonstrates that M-trust possesses the excellent overall performance in terms of accuracy, reliability, convergence speed, and detection rate under various constraints of mobility, trust threshold and network out-degree. The design of a network architecture that can efficiently integrate WLAN and cellular networks is a challenging task, particularly when the objective is to make the interoperation between the two networks as seamless and as efficient as possible. To provide end-to-end quality of service (QoS) support is one of the key stages towards such a goal. Due to various constraints, such as the unbalanced capacity of the two systems, handoff from user mobility and unreliable transmission media, end-to-end QoS is difficult to guarantee. In this paper, we propose a generic reservation-based QoS model for the integrated cellular and WLAN networks. It uses an adaptation mechanism to address the above issues and to support end-to-end QoS. The validity of the proposed scheme is demonstrated via simulation experiments. The performance results reveal that this new scheme can considerably improve the system resource utilization and reduce the call blocking probability and handoff dropping probability of the integrated networks while maintaining acceptable QoS to the end users. To support the differentiated Quality-of-Service (QoS) and improve the utilization of the scarce wireless bandwidth, the IEEE 802.11e standard specifies an efficient burst transmission scheme referred to as the Transmission Opportunity (TXOP). Recently, analytical models have been reported to evaluate the performance of the TXOP scheme. However, most of these models were developed under the assumptions of the ideal channels or uniform error channels which fail to capture the characteristics of bursty and correlated channel errors in the practical wireless environment. In this paper, we propose an analytical model for the TXOP scheme in WLANs in the presence of bursty error channels. To this end, the transmission queue of each station is modelled by a two-state continuous time Markov chain. This model can be adopted to obtain the performance metrics including the throughput and buffer overflow probability. The accuracy of the analytical model is validated via NS-2 simulation experiments. Utilizing the proposed model, we investigate the impact of traffic loads, TXOP limit, and the number of stations on the performance of the TXOP scheme under various channel conditions. The Guard Channel Scheme (GCS) and Handoff Queueing Scheme (HQS) are the popular and practical strategies to prioritize handoff calls in wireless cellular networks. A key issue of giving handoff calls the higher priority is how to achieve a tradeoff among the handoff call blocking probability, new call blocking probability and handoff delay. This paper extends GCS and HQS and presents an efficient handoff scheme that dynamically manages the channels reserved for handoff calls depending on the current status of the handoff queue. A three-dimensional Markov model is developed to analyze the performance of this scheme and investigate the desirable performance tradeoff. The Poisson process and Markov-Modulated-Poisson-Process (MMPP) are used to model the arrival processes of new and handoff calls, respectively. The accuracy of this model is evaluated through the extensive comparison of the analytical results to those obtained from discrete-event simulation experiments. Performance measures in terms of the mean number of calls in the system, aggregate response time, aggregate call blocking probability, handoff call blocking probability, new call blocking probability and handoff delay are evaluated. The analytical model is used to investigate the effects of the number of channels originally reserved for handoff calls, the number of dynamic channels, and the ratio of the rate of handover calls to the aggregate arrival rate on the system performance. Information security has been a critical issue in the design and development of reliable distributed communication systems and has attracted significant research efforts. A challenging task is how to maintain information security at a high level for multiple-destination video applications with the huge volume of data and dynamic property of clients. This paper proposes a novel Content-Aware Secure Multicast (CASM) protocol for video distribution that seamlessly integrates three important modules: 1)a scalable light-weight algorithm for group key management; 2) a content-aware key embedding algorithm that can make video quality distortion imperceptible and is reliable for clients to detect embedded keys; and 3) a smart two-level video encryption algorithm that can selectively encrypt a small set of video data only, and yet ensure the video as well as the embedded keys unrecognizable without a genuine key. The implementation of the CASM protocol is independent of the underlying multicast mechanism and is fully compatible with existing coding standards. Performance evaluation studies built upon a CASM prototype have demonstrated that CASM is highly robust and scalable in dynamic multicast environments. Moreover, it ensures secure distribution of key and video data with minimized communication and computation overheads. The proposed content-aware key embedding and encryption algorithms are fast enough to support real-time video multicasting. In this paper we investigate potential benefits that an adaptive delayed channel access algorithm can attain for the next-generation wireless LANs, the IEEE 802.11n. We show that the performance of frame aggregation introduced by the 802.11n adheres due to the priority mechanism of the legacy 802.11e EDCA scheduler, resulting in a poor overall performance. Because high priority flows have low channel utilization, the low priority flows throughputs can be amerced further. By introducing an additional delay at the MAC layer, before the channel access scheduling, it will retain aggregate sizes at higher numbers and consequently a better channel utilization. Also, in order to support both UDP and TCP transport layer protocols, the algorithm's operational conditions are kept adaptive. The simulation results demonstrate that our proposed adaptive delayed channel access outperforms significantly the current 802.1 In specification and non-adaptive delayed channel access. Traffic self-similarity has been discovered to be a ubiquitous phenomenon in modern communication networks and multimedia systems. Due to its scale-invariant bursty nature, performance modelling of self-similar traffic poses greater challenges and exhibits more complexity than that of traditional non-bursty traffic. As a result, most existing studies on analytical modelling of priority queueing systems with self-similar inputs have been restricted to a simplified scenario where only two traffic flows are taken into account. This study contributes to performance modelling and analysis of priority queueing systems by proposing a novel and efficient queue-decomposition approach to handle multi-class self-similar traffic. Specifically, we extend the well-known method of empty buffer approximation in order to decompose the priority queueing system equivalently into a group of single-server single-queue systems. We further obtain the analytical upper and lower bounds of the queue length distributions for individual traffic flows. Interconnection networks are hardware fabrics supporting communications between individual processors in multicomputers. The low-dimensional k-ary n-cubes (or torus) with adaptive wormhole switching have attracted significant research efforts to construct high-performance interconnection networks in contemporary multi-computers. The arrival process and destination distribution of messages have great effects on network performance. With the aim of capturing the characteristics of the realistic traffic pattern and obtaining a deep understanding of the performance behaviour of interconnection networks, this paper presents an analytical model to investigate the message latency in adaptive-routed wormhole-switched torus networks where there exists hot-spot nodes and the message arrivals follow a batch arrival process. Each generated message has a given probability to be directed to the hot-spot node. The average degree of virtual channel multiplexing is computed by the GE/G/1/V queueing system with finite buffer capacity. We compare analytical results of message latency with those obtained through the simulation experiments in order to validate the accuracy of the derived model. Recently there have been considerable interests focusing on the performance evaluation of IEEE 802.11e medium access control (MAC) protocols, which were proposed for supporting quality of services (QoS) in wireless local area networks (WLANs). Different from most existing work, this study has conducted comprehensive performance evaluation and analysis of the IEEE 802.11e enhanced distributed channel access (EDCA) protocol in the presence of heterogeneous network traffic including nonbursty Poisson, bursty ON/OFF, and self-similar traffic generated by wireless multimedia applications. The performance results on throughput, access delay and medium utilization have demonstrated that the protocol is able to achieve satisfying QoS differentiation for heterogeneous multimedia traffic. On the other hand the results have showed that IEEE 802.11e EDCA suffering from the low medium utilization due to the overhead generated by transmission collisions and back-off processes. Active Queue Management (AQM) has been proposed as an efficient policy of congestion control to achieve high system utilization and low packet delay. Many recent high-quality measurement studies have demonstrated that realistic traffic in multimedia communication systems exhibits self-similar nature, which has a significant impact on network performance and user-perceived Quality-of-Service (QoS). Performance modelling of congestion control for self-similar traffic is a challenging and important research issue. In this paper, we present a new analytical model for the AQM congestion control policy based on the well-known Random Early Detection (RED) scheme in the presence of multi-class self-similar traffic. The system buffer is partitioned by a number of thresholds in order to provide differentiated loss priorities to individual traffic classes. We derive the closed-form expressions for calculating the loss probabilities of different traffic classes. By comparing analytical and simulation results, we demonstrate the effectiveness and accuracy of the developed model. Arbitrary inter-frame space (AIFS), contention window (CW) and transmission opportunity (TXOP) are three important quality-of-service (QoS) differentiation schemes specified in the IEEE 802.11e enhanced distributed channel access (EDCA) protocol for wireless local area networks (WLANs). Analytical models of EDCA in the current literature have been mainly developed for the AIFS, CW, and TXOP schemes, separately. This study proposes a comprehensive analytical model to accommodate the combination of these three QoS schemes in WLANs under unsaturated traffic loads. We derive the performance metrics in terms of throughput, end-to- end delay, and frame loss probability. Extensive simulation experiments are conducted to validate the accuracy of the model. Traffic loads have a significant impact on the performance of routing algorithms. Many analytical models for adaptive routing in interconnection networks have been reported. However, most existing studies are based on the assumption that the arrivals of traffic follow a non-bursty Poisson process and the message destinations are uniformly distributed over the network. With the aim of obtaining a deep understanding of network performance under more realistic working conditions, this study develops an analytical performance model for adaptive-routed hypercubic networks under hot-spot and batch arrival traffic. This model adopts the Compound Poisson Process (CPP) to capture the properties of the batch arrival traffic. Extensive simulation experiments are conducted to validate the accuracy of the analytical model. Transmission opportunity (TXOP) is a new scheme specified in the IEEE 802.11e standard which enables a station to transmit multiple frames consecutively within a burst after it gains the channel. Realistic traffic loads in wireless local area networks (WLANs) with multimedia applications often exhibit the bursty and self-similar properties which imply the frequent occurrence of the large bursts of frame arrivals and thus require the TXOP scheme to be dynamically adapted to the traffic characteristics. This paper presents a dynamic TXOP scheme which adjusts the TXOP limits of mobile stations according to the current status of their transmission queues. We further develop an analytical model to evaluate the performance of this scheme under self-similar traffic. QoS performance metrics in terms of throughput, end-to-end delay, and frame loss probability are derived and validated via NS2 simulation experiments. The numerical results reveal that the dynamic TXOP scheme achieves the better QoS than the original one under self-similar traffic. Interconnection network design plays a central role in the design of parallel systems. The paper presents an analytical model to predict communication delay in circuit-switched k-ary n-cube interconnection networks augmented with virtual channel support. The main feature of the proposed model is the use of Markov chains to compute the path set-up time and to capture the effects of using virtual channels to reduce message blocking in the network. The mean waiting time that a message experiences at a source node before entering the network is calculated using an M/G/1 queueing system. The model is validated through flit-level simulation experiments. Summary form only given. The efficiency of a large-scale multicomputer is critically dependent on the performance of its underlying interconnection network. Dimension-ordered routing has been employed to transmit messages in multicomputer networks, as it requires a simple deadlock-avoidance algorithm, resulting in an efficient router implementation. The performance of this routing algorithm has been widely analysed under the assumption of the traditional Poisson arrival process, which is inherently unable to model traffic self-similarity revealed by many real-world applications. In an effort towards providing cost-effective tools that help investigating network performance under more realistic traffic loads, we propose an analytical model for dimension-ordered routing in k-ary n-cube networks when subjected to self-similar traffic. As the service time, blocking probability and waiting time experienced by a message vary from a dimension to another with dimension-ordered routing, the design of this model poses greater challenges. The model validity is demonstrated by performance results obtained from simulation experiments. Voice over IP (VoIP) is a crucial service in the wireless local area network (WLAN) system. The primary medium access control (MAC) technique of the WLAN is IEEE 802.11 distributed coordination function (DCF). Under the situations of various packet sizes, the deficiency of existing MAC protocols mainly comes from the idle slots used to contend the channel for small size packets and from the transmission collisions due to the identical backoff slot value generated for all stations. This study takes account of the above issues and proposes a new MAC scheme to enhance VoIP performance over WLAN. This scheme allows each station to transmit a burst of packets after winning a transmission opportunity instead of just one packet. This reduces the average number of waiting slots and collision probability in each transmission cycle. Moreover, to ensure fairness among stations, a maximum transmission period is assigned to each station in order to control the length of the burst transmission. The simulation results have shown that the proposed method can improve the QoS of VoIP applications significantly by providing lower delay, lower jitter and higher degree of fairness. The hybrid PQ-GPS scheduling mechanism has been identified as a promising policy for the provisioning of differentiated quality-of-service (QoS) in modern communication networks. This paper proposes the first analytical performance model for the PQ-GPS scheduling mechanism subject to long-range dependent (LRD) network traffic. The core contribution is to develop a novel flow-decomposition approach that can equivalently divide the hybrid system into a group of single-server single-queue (SSSQ) systems in order to make the challenging problem of modelling the complex system tractable. The validity and accuracy of the model make it a practical and cost-effective tool for investigating the performance behaviour of the PQ-GPS system The k-ary n-cube which has an n-dimensional grid structure with k nodes in each dimension has been a popular topology for interconnection networks. Analytical models for k- ary n-cubes have been widely reported under the assumptions that the message destinations are uniformly distributed over all network nodes and the message arrivals follow a nonbursty Poisson process. Recent studies have convincingly demonstrated that the traffic pattern in interconnection networks reveals the bursty nature in the both spatial domain (i.e., non-uniform distribution of message destinations) and temporal domain (i.e., bursty message arrival process). With the aim of capturing the characteristics of the realistic traffic pattern and obtaining a comprehensive understanding of the performance behaviour of interconnection networks, this paper presents a new analytical model for k-ary n-cubes in the presence of spatio-temporal bursty traffic. The accuracy of the model is validated through extensive simulation experiments of an actual system. A new analytical model is proposed to investigate the delay and throughput in cognitive mesh networks. The validity of the model is demonstrated via extensive simulation experiments. The model is then used to evaluate the effects of the number of licensed channels and channel utilisation on the network performance. Many measurement studies have shown that telecommunication traffic usually exhibits self-similar nature and the service times of packets often follow heavy-tailed distributions. However, due to the high complexity of modelling the fractal self-similar properties and heavy-tailed distributions, most existing studies on analytical modelling of queuing systems have been confined to investigate the effects of either traffic self-similarity or heavy-tailed service times only. To fill this gap, in this paper we develop a new analytical model for a single server queuing system in the presence of self-similar inputs and heavy-tailed service time distributions. Specifically, we derive the closed-form expressions for three important quality-of-service (QoS) metrics, namely, the tail distributions of queue length, packet loss, and packet delay of the queuing systems where traffic arrivals follow the fractional Brownian motion (fBm) model and the packet sizes have lognormal or Pareto distributions. We validate the accuracy of the developed model through comparing analytical results to those obtained from experimental simulations. Knowledge of end-to-end network distances is essential to many service-oriented applications such as distributed content delivery and overlay network multicast, in which the clients have the flexibility to select their servers from among a set of available ones based on network distance. However, due to the high cost of global measurements in large-scale networks, it is infeasible to actively probe end-to-end network distances for all pairs. In order to address this issue, network distance prediction has been proposed by measuring a few pairs and then predicting the other ones without direct measurements, or splicing the path segments between each pair via observation. It is considered important to improve network performance, and enables service- oriented applications over large-scale networks. In this article, we first illustrate the basic ideas behind network distance prediction, and then categorize the current research work based on different criteria. We illustrate how different protocols work, and discuss their merits and drawbacks. Finally, we summarize our findings, and point out potential issues and future directions for further research. The IEEE 802.15.4 standard defines physical layer and Medium Access Control (MAC) layer protocols for the Low Rate Wireless Personal Areas Network (LR-WPAN). The analytical models of 802.15.4 MAC have been primarily developed under the assumptions of the ideal channels or uniform error channels which fail to capture the characteristics of bursty and correlated channel errors in the practical wireless network environment. In this paper, we propose an analytical model for 802.15.4 MAC in LR-WPAN in the presence of bursty error channels. This model can be adopted to obtain the Quality-of-Service (QoS) performance metrics in terms of throughput, service time, and total delay. Utilizing the analytical model, we investigate the QoS performance of 802.15.4 MAC under various traffic loads, backoff parameters, numbers of stations, and channel conditions. The priority queueing discipline plays a crucial role in the differentiated services (DiffServ) architecture. Analytical modelling and performance evaluation of priority queueing systems have received significant research efforts in the telecommunication community. However, most existing studies have primarily focused on the analysis of such systems under either short range dependent (SRD) or long range dependent (LRD) traffic only. With the aim of investigating the impact of heterogeneous traffic on the design and performance of telecommunication networks, this paper presents an analytical model for priority queueing systems subject to heterogeneous LRD self-similar and SRD Poisson traffic. We extend the application of the generalized Schilder's theorem to deal with heterogeneous traffic and further develop the analytical upper and lower bounds for the queue length distributions of individual traffic flows. Through extensive comparisons between analytical bounds and simulation results, we validate the effectiveness and accuracy of the developed model. The increasing demand for wireless services has led to a severe energy consumption problem with the rising of greenhouse gas emission. While the renewable energy can somehow alleviate this problem, the routing, flow rate, and power still have to be well investigated with the objective of minimizing energy consumption in multi-hop energy renewable wireless mesh networks (ER-WMNs). This paper formulates the problem of network-wide energy consumption minimization under the network throughput constraint as a mixed-integer nonlinear programming problem by jointly optimizing routing, rate control, and power allocation. Moreover, the min-max fairness model is applied to address the fairness issue because the uneven routing problem may incur the sharp reduction of network performance in multi-hop ER-WMNs. Due to the high computational complexity of the formulated mathematical programming problem, an energy-aware multi-path routing algorithm (EARA) is also proposed to deal with the joint control of routing, flow rate, and power allocation in practical multi-hop WMNs. To search the optimal routing, it applies a weighted Dijkstra's shortest path algorithm, where the weight is defined as a function of the power consumption and residual energy of a node. Extensive simulation results are presented to show the performance of the proposed schemes and the effects of energy replenishment rate and network throughput on the network lifetime. With the rapid development of network applications, the Internet has evolved from a content-based communication infrastructure to a social-based community network. The emerging applications require the Internet to preserve not only the existing advantages of simplicity and scalability, but also demand varying amounts of capability, availability, reliability, flexibility, and differentiated quality of service. Therefore, it is of paramount importance to bridge the gap between these applications and the IP networks which were originally designed and developed for supporting one-size-fits-all functionality. An efficient solution is to build a virtual network on top of a generic IP transport layer in order to provide additional functionality and flexibility. The content delivery networks technique is one of the successful virtual networks rapidly developed over the last decade with the specific advantage of optimizing the Internet. Nowadays, the CDN has become one of the most important parts of the Internet architecture for content distribution. In this article we highlight the innovative technologies in CDNs and present their evolution triggered by ever newer emerging applications. By presenting in-depth discussion about the architecture, challenges, and applications of CDNs, we demonstrate their importance for the future Internet. In many applications of sensor networks including environmental monitoring and surveillance, a large volume of sensed data generated by sensors needs to be either collected at the base station or aggregated within the network to respond to user queries. However, due to the unreliable wireless communication, robust query processing in such networks becomes a great challenge in the design of query evaluation algorithms for some mission-critical tasks. In this paper we propose an adaptive, localized algorithm for robust top-k query processing in sensor networks, which trades off between the energy consumption and the accuracy of query results. In the proposed algorithm, whether a sensor is to forward the collected data to the base station is determined in accordance with the calculation of a proposed local function, which is the estimation of the probability of transmitting the data successfully. We also conduct extensive experiments by simulations on real datasets to evaluate the performance of the proposed algorithm. The experimental results demonstrate that the proposed algorithm is energy-efficient while achieving the specified accuracy of the query results. Modelling and performance analysis of Medium Access Control (MAC) protocols in Wireless Local Area Networks (WLANs) has attracted lots of research efforts recently. Although many analytical models for IEEE 802.11 Distributed Coordination Function (DCF) have been reported, most existing studies have not considered traffic dynamics but concentrated on its throughput performance under the saturation condition, assuming that there are always packets available for transmission in each station and thus simplifying the analytical modelling and derivation. The comprehensive performance study under non-saturated traffic situations is still an open problem. In this paper, we propose an analytical performance model for IEEE 802.11 DCF protocol using multidimensional discrete-time Markov chain and Equilibrium Point Analysis (EPA), and present how to model the binary backoff scheme under more flexible traffic sources. We validate the accuracy of the model by comparing the analytical results with those obtained from simulation experiments. Finally, we use the proposed model to investigate the impact of different parameter settings on the performance of this protocol and present some valuable results. Following the two trends of computerization and informatization, another emerging trend is cyberization in which numerous and various cyber entities in cyberspace will exist in cyber-enabled worlds, including the cyber world and cyber-conjugated physical, social, and mental worlds. Computer science and information science, as holistic fields, have, respectively, played important roles in computerization and informatization. Similarly, it is necessary for there to be a corresponding field for cyberization. Cybermatics is proposed as such a holistic field for the systematic study of cyber entities in cyberspace and cyber world, and their properties, functions, and conjugations with entities in conventional spaces/worlds. This paper sets out to explain the necessity and rationale for, and significance of, the proposed field of Cybermatics, what it is and what it encompasses, and how it is related to other fields and areas. In Mobile Social Networks (MSN) users can participate in communication based on their mutual interests. Typical implementations of Social Networks involve client/server architecture where a user needs to subscribe to a server for content sharing with other clients. Popular Online Social Network providers have recently extended the web based service into the mobile domain. However due to mobility and connectivity issues this architecture is in-adequate in providing peer to peer content sharing services to mobile users. Many solutions for Mobile Ad Hoc networks have been proposed however content based communication in partially or intermittently connected MANETs has been a recent research area. In this paper we present an adaptive protocol to facilitate implementation of a Mobile Social Network based on peer to peer content driven communication when end-to-end connectivity is not possible. The proposed protocol takes into account the information about user's interests, content based data storing and forwarding, and host mobility in a disconnected, delay tolerant MANET. In cognitive radio cooperative communication (CR-CC) systems, the achievable data rate can be improved by increasing the transmission power. However, the increase in power consumption may cause the interference with primary users and reduce the network lifetime. Most previous work on CR-CC did not take into account the tradeoff between the achievable data rate and network lifetime. To fill this gap, this paper proposes an energy-efficient joint relay selection and power allocation scheme in which the state of a relay is characterized by the channel condition of all related links and its residual energy. The CR-CC system is formulated as a multi-armed restless bandit problem where the optimal policy is decided in a distributed way. The solution to the restless bandit formulation is obtained through a first-order relaxation method and a primal-dual priority-index heuristic, which can reduce dramatically the on-line computation and implementation complexity. According to the obtained index, each relay can determine whether to provide relaying or not and also can control the corresponding transmission power. Extensive simulation experiments are conducted to investigate the effectiveness of the proposed scheme. The results demonstrate that the power consumption is reduced significantly and the network lifetime is increased more than 40%. The provisioning of fairness among various networking and communication applications is an important Quality-of-Service (QoS) demand and becomes a challenging research topic. Deficit Round Robin (DRR) is a promising fair scheduling mechanism owing to its low complexity and excellent ability of achieving a good degree of fairness in terms of throughput. Although self-similar traffic has been found to exist in multimedia communication networks and has a great impact on the performance of scheduling mechanisms, there has not been any analytical model reported in the open literature for DRR in the presence of self-similar traffic. To fill this gap, we analytically investigate the queueing performance of DRR scheduling mechanism and develop a new analytical model for deriving the upper and lower bounds of the queue length distributions of individual traffic flows in DRR systems under self-similar traffic. Extensive comparison between the simulation and analytical results validates the accuracy of the developed model. In intelligent transportation systems, the cooperation between vehicles and the road side units is essential to bring these systems to fruition. Vehicular ad hoc networks (VANETs) are a promising technology to enable the communications among vehicles on one hand and between vehicles and road side units on the other hand. However, it is a challenging task to develop a reliable routing algorithm for VANETs due to the high mobility and the frequent changes of the network topology. Communication links are highly vulnerable to disconnection in VANETs; hence, the routing reliability of these ever-changing networks needs to be paid special attention. In this paper, we propose a new vehicular reliability model to facilitate the reliable routing in VANETs. The link reliability is defined as the probability that a direct communication link between two vehicles will stay continuously available over a specified time period. Furthermore, the link reliability value is accurately calculated using the location, direction and velocity information of vehicles along the road. We extend the well-known ad hoc on-demand distance vector (AODV) routing protocol to propose our reliable routing protocol AODV-R. Simulation results demonstrate that AODV-R outperforms significantly the AODV routing protocol in terms of better delivery ratio and less link failures while maintaining a reasonable routing control overhead. The growing proliferation of wireless devices in contemporary wireless networks requires more spectrum usage. As a consequence, spectrum bands are pressingly getting congested. However, a number of frequency bands licensed to operators are under utilized for transmission. Cognitive Radio (CR), as a promising technique for improving spectrum utilization, can dynamically allocate spectrum. In this paper, we investigate the performance of a CR network composed of a primary/licensed user and a number of secondary/unlicensed users, which are subject to self-similar traffic flows and contend for a unique channel. An analytical model is developed to isolate the primary and secondary users from the original networks. Further, we address the queueing performance of individual secondary users by employing a decomposition approach. The comparison between analytical and simulation results validates the accuracy of the developed model. The anonymity and dynamic character of a Peer-to-Peer (P2P) network makes it an ideal medium for selfish and vicious action. In order to solve this problem, P2P reputation systems are proposed to evaluate the trustworthiness of peers and to prevent the selfish, dishonest, and malicious peers' behaviors, which collects local reputation scores and aggregates them into the global reputation. In this paper we propose a DHT (Distributed Hash Table) trust overlay network (DHTON) to model the network structure and the storage of reputation information. We also design a robust and distributing reputation system, DHTrust, which takes full advantage of the DHT to distribute local reputation to trade-off the damage of fake reputation information by genuine reputation information. By using the trust evaluation towards two reputation scores, we can also distinguish and evaluate the fundamental behaviors of peers in the P2P network, i.e. providing service and issuing reputation scores. To adapt to the dynamic P2P networks, we take dynamic node mechanism into account. Our scheme can assure convergence effectiveness and robustness, when nodes enter or leave the system. We conduct extensive simulations to evaluate the performance of DHTrust. The results show that our system makes significant improvement in convergence speed and aggregation accuracy. Moreover, it is robust to malicious peers. Copyright \\u00a9 2011 John Wiley & Sons, Ltd. As the access point (AP) is responsible for forwarding all the frames to and from the infrastructure-based WLANs, the network performance is significantly degraded by the unbalanced traffic loads. A potential solution is to assign different transmission opportunities (TXOPs) to AP and other mobile stations, respectively. This study develops an analytical model to investigate the performance of infrastructure-based WLANs with the TXOP differentiation under the unbalanced traffic loads. The analytical model is validated through extensive ns2 simulation experiments. Several recent studies have revealed that pipelined circuit switching (PCS) can provide superior performance characteristics over wormhole routing. This paper proposes an original analytical model of PCS in k-ary n-cube networks augmented with virtual channel support. The model uses random walk theory to analyse the backtracking actions of the header flit during the path setup phase in PCS, and M/G/I queueing systems to compute the mean waiting time that a message experiences at a source before entering the network. Results from simulation experiments show close agreement to those predicted by the model. Generalized processor sharing (GPS) has been widely studied as an important scheme for providing differentiated quality of service. Realistic traffic in multi-service networks exhibits heterogeneous properties and can be categorized into two classes, namely, long range dependent (LRD) and short range dependent (SRD) traffic. Performance modelling and analysis of GPS systems subject to heterogeneous network traffic is an open and challenging problem. This paper develops an analytical performance model for GPS systems under heterogeneous LRD and SRD traffic. To this end, we propose a cost-efficient modelling method for GPS and further develop the analytical upper and lower bounds of the queue length distributions for individual traffic flows. Numerical examples and extensive simulation results are presented to validate the accuracy and merits of the analytical model. Copyright \\u00a9 2007 John Wiley & Sons, Ltd. This paper studies a practically important problem of cloned tag detection in large-scale RFID systems where an attacker compromises genuine tags and produces their replicas, namely cloned tags, to threaten RFID applications. Although many efforts have been made to address this problem, the existing work can hardly satisfy the stringent real-time requirement, and thus cannot catch cloning attacks in a time-efficient way. Moreover, the existing work does not consider energy-efficiency, which is very critical when battery-powered active tags are used. To fill these gaps, this paper proposes a Location Polling-based Cloned tag Detection (LP-CTD) protocol by taking both time-efficiency and energy-efficiency into consideration. LP-CTD reports the existence of cloned tags when the reader finds an expected singleton slot appearing as collision one. To improve the efficiency of detecting cloned tags, only sampled tags in LP-CTD participate in the detection process. Theoretical analysis on the proposed protocol is conducted to minimize their execution time and energy consumption. Extensive simulation experiments are conducted to evaluate the performance of the proposed protocols. The results demonstrate that the proposed LP-CTD protocol considerably outperforms the latest related protocols by reducing more than 80% of the execution time, and more than 90% of the energy consumption. In Mobile Social Networks (MSN) users can participate in communication based on their mutual interests. Many solutions for Mobile Ad Hoc networks have been proposed however content based communication in partially or intermittently connected MANETs has been a recent research area. In this paper we present an adaptive protocol to facilitate implementation of a Mobile Social Network based on peer to peer content driven communication when end-to-end connectivity is not possible. The proposed protocol takes into account the information about user's interests, content based data storing and forwarding, and host mobility in a disconnected, delay tolerant MANET. Admission control is an important mechanism for the provisioning of the user-perceived Quality-of-Service (QoS) in the IEEE 802.11e Wireless Local Area Networks (WLANs). In this paper, we present an efficient admission control scheme based on analytical modelling and non-cooperative game theory where the Access Point (AP) and new users are the players. The decision of admission control is made by virtue of the strategies to maximize the utilities of the players, which are determined by the QoS performance metrics in terms of the end-to-end delay and frame loss probability. To obtain these required performance metrics, we develop a new analytical model incorporating the Contention Window (CW) and Transmission Opportunity (TXOP) differentiation schemes in the IEEE 802.11e protocol under unsaturated working conditions. The efficiency of the proposed admission control scheme is validated via NS-2 simulation experiments. The numerical results demonstrate that the proposed admission control scheme can maintain the system operation at an optimal point where the utility of the AP is maximized subject to the QoS constraints of both the real-time and non-real-time users. Recently a number of studies have indicated that network traffic exhibits noticeable self-similar behaviour, i.e., traffic is bursty over a wide range of time scales. This fractal-like nature of traffic has received significant attention in the networking community as it has a considerable impact on queueing performance. Thus it is very necessary to examine the performance properties of interconnection networks in the presence of self-similar traffic before practical implementations show their potential faults. However, adopting the simulation approach to evaluate system performance under self-similar workloads may be very costly and time-consuming because the convergence of simulations to a steady state is often very slow as burstiness appears over many time scales. This paper proposes the first analytical performance model for k-ary n-cubes with self-similar traffic. The validity of the model is demonstrated by comparing analytical results to those obtained through simulation experiments of the actual system. Position information plays a pivotal role in wireless sensor network (WSN) applications and protocol/algorithm design. In recent years, range-free localization algorithms have drawn much research attention due to their low cost and applicability to large-scale WSNs. However, the application of range-free localization algorithms is restricted because of their dramatic accuracy degradation in practical anisotropic WSNs, which is mainly caused by large error of distance estimation. Distance estimation in the existing range-free algorithms usually relies on a unified per hop length (PHL) metric between nodes. But the PHL between different nodes might be greatly different in anisotropic WSNs, resulting in large error in distance estimation. We find that, although the PHL between different nodes might be greatly different, it exhibits significant locality; that is, nearby nodes share a similar PHL to anchors that know their positions in advance. Based on the locality of the PHL, a novel distance estimation approach is proposed in this article. Theoretical analyses show that the error of distance estimation in the proposed approach is only one-fourth of that in the state-of-the-art pattern-driven scheme (PDS). An anchor selection algorithm is also devised to further improve localization accuracy by mitigating the negative effects from the anchors that are poorly distributed in geometry. By combining the locality-based distance estimation and the anchor selection, a range-free localization algorithm named S elective M ultilateration (SM) is proposed. Simulation results demonstrate that SM achieves localization accuracy higher than 0.3r, where r is the communication radius of nodes. Compared to the state-of-the-art solution, SM improves the distance estimation accuracy by up to 57p and improves localization accuracy by up to 52p consequently. The control/user (C/U) plane decoupled railway wireless network is an innovative architecture recently proposed to meet the communication demands of both train control systems and onboard passengers. The core idea is to completely separate the C-plane and the U-plane into different network nodes operating at different frequency bands. Although the system capacity of this network architecture can be highly increased, the forwarding latency of X3 interfaces to link the C-plane and the U-plane becomes a serious problem, particularly for hybrid automatic repeat request (HARQ) protocols that demand frequent interactions between the C-plane and the U-plane. To address this challenging problem, we propose a low-latency collaborative HARQ scheme in this paper. Specifically, we develop a new collaborative transmission framework where the possible spare resources on lower frequency bands of macrocells excluding those used by C-plane transmissions can be utilized to help small cells relay erroneously received data. Compared with the conventional HARQ scheme, the proposed scheme requires fewer retransmissions to reach the same transmission reliability, thereby mitigating the latency problem caused by HARQ retransmissions. Furthermore, channel mapping is also redesigned to conform to the proposed collaborative transmission framework. Through theoretical analysis, we derive the expression of the average number of retransmissions related to the sum of independent Gamma variables. Finally, the results of simulation experiments show that the proposed scheme can largely decrease the retransmission latency for railway wireless networks. An increasing number of the elderly population wish to live an independent lifestyle, rather than rely on intrusive care programmes. A big data solution is presented using wearable sensors capable of carrying out continuous monitoring of the elderly, alerting the relevant caregivers when necessary and forwarding pertinent information to a big data system for analysis. A challenge for such a solution is the development of context-awareness through the multidimensional, dynamic and nonlinear sensor readings that have a weak correlation with observable human behaviours and health conditions. To address this challenge, a wearable sensor system with an intelligent data forwarder is discussed in this paper. The forwarder adopts a Hidden Markov Model for human behaviour recognition. Locality sensitive hashing is proposed as an efficient mechanism to learn sensor patterns. A prototype solution is implemented to monitor health conditions of dispersed users. It is shown that the intelligent forwarders can provide the remote sensors with context-awareness. They transmit only important information to the big data server for analytics when certain behaviours happen and avoid overwhelming communication and data storage. The system functions unobtrusively, whilst giving the users peace of mind in the knowledge that their safety is being monitored and analysed. Multipath transmission control protocol (MPTCP) allows a TCP connection to operate across multiple paths simultaneously and becomes highly attractive to support the emerging mobile devices with various radio interfaces and to improve resource utilization as well as connection robustness. The existing multipath congestion control algorithms, however, are mainly loss-based and prefer the paths with lower drop rates, leading to severe performance degradation in wireless communication systems, where random packet losses occur frequently. To address this challenge and improve the performance of MPTCP in wireless networks, this paper proposes a new mVeno algorithm, which makes full use of the congestion information of all the subflows belonging to a TCP connection in order to adaptively adjust the transmission rate of each subflow. Specifically, mVeno modifies the additive increase phase of Veno so as to effectively couple all subflows by dynamically varying the congestion window increment based on the receiving ACKs. The weighted parameter of each subflow for tuning the congestion window is determined by distinguishing packet losses caused by random error of wireless links or by network congestion. We implement mVeno in a Linux server and conduct extensive experiments both in test bed and in real WAN to validate its effectiveness. The performance results demonstrate that compared with the existing schemes, mVeno increases the throughput significantly, achieves load balancing, and can keep the fairness with regular TCP. Given the wide range deployment of disconnected delay-tolerant social Internet of Things (SIoT), efficient resource discovery remains a fundamental challenge for large-scale SIoT. The existing search mechanisms over the SIoT do not consider preference similarity and are designed in Cartesian coordinates without sufficient consideration of real-world network deployment environments. In this paper, we propose a novel resource discovery mechanism in a 3-D Cartesian coordinate system with the aim of enhancing the search efficiency over the SIoT. Our scheme is based on both of preference and movement pattern similarity to achieve higher search efficiency and to reduce the system overheads of SIoT. Simulation experiments have been conducted to evaluate this new scheme in a large-scale SIoT environment. The simulation results show that our proposed scheme outperforms the state-of-the-art resource discovery schemes in terms of search efficiency and average delay. Software-Defined Networking (SDN) is an emerging architecture for the next-generation Internet, providing unprecedented network programmability to handle the explosive growth of big data driven by the popularisation of smart mobile devices and the pervasiveness of content-rich multimedia applications. In order to quantitatively investigate the performance characteristics of SDN networks, several research efforts from both simulation experiments and analytical modelling have been reported in the current literature. Among those studies, analytical modelling has demonstrated its superiority in terms of cost-effectiveness in the evaluation of large-scale networks. However, for analytical tractability and simplification, existing analytical models are derived based on the unrealistic assumptions that the network traffic follows the Poisson process, which is suitable to model nonbursty text data, and the data plane of SDN is modelled by one simplified Single-Server Single-Queue (SSSQ) system. Recent measurement studies have shown that, due to the features of heavy volume and high velocity, the multimedia big data generated by real-world multimedia applications reveals the bursty and correlated nature in the network transmission. With the aim of capturing such features of realistic traffic patterns and obtaining a comprehensive and deeper understanding of the performance behaviour of SDN networks, this article presents a new analytical model to investigate the performance of SDN in the presence of the bursty and correlated arrivals modelled by the Markov Modulated Poisson Process (MMPP). The Quality-of-Service performance metrics in terms of the average latency and average network throughput of the SDN networks are derived based on the developed analytical model. To consider a realistic multiqueue system of forwarding elements, a Priority-Queue (PQ) system is adopted to model the SDN data plane. To address the challenging problem of obtaining the key performance metrics, for example, queue-length distribution of a PQ system with a given service capacity, a versatile methodology extending the Empty Buffer Approximation (EBA) method is proposed to facilitate the decomposition of such a PQ system to two SSSQ systems. The validity of the proposed model is demonstrated through extensive simulation experiments. To illustrate its application, the developed model is then utilised to study the strategy of the network configuration and resource allocation in SDN networks. Mobile big data contains vast statistical features in various dimensions, including spatial, temporal, and the underlying social domain. Understanding and exploiting the features of mobile data from a social network perspective will be extremely beneficial to wireless networks, from planning, operation, and maintenance to optimization and marketing. Recently, the charging management for Electric Vehicles (EVs) on-the-move has become an emerging research problem in urban cities. Major technical challenges here involve intelligence for the selection of Charging Stations (CSs) to guide drivers\\u2019 charging plans, as well as the corresponding communication infrastructure for information dissemination between the power grid and EVs. In this article, a Vehicular- Publish/Subscribe (P/S) communication framework, in conjunction with Public Transportation Buses (PTBs) is provisioned to support on-the-move EV charging management. Benefiting from low privacy sensitivity, we propose a fully distributed charging management scheme concerning the driving intention. Results demonstrate a guidance for the provisioning of V P/Scommunication framework, concerning EV drivers\\u2019 experience including charging waiting time and total trip duration. Also, the benefit of V-P/S communication framework is reflected in terms of the communication efficiency. Open research issues of this emerging research area are also presented.\",\n",
            "  \"2117849233\": \"7 Abstract. Vehicle routing problem (VRP) is a combinatorial optimization and integer programming problem seeking to service a number of customers with a fleet of vehicles. Customer characteristics are neglected in traditional VRPs in the past due to the heterogeneity and ambiguousness. This study presents a vehicle route optimization model in consideration of customer characteristics with three major components: (1) A hierarchical analysis structure is developed to convert customers' characteristics into linguistic variables, and fuzzy integration method is used to map the sub-criteria into higher hierarchical criteria based on the trapezoidal fuzzy numbers; (2) A fuzzy clustering algorithm based on Axiomatic Fuzzy Set is proposed to group the customers into multiple clusters; (3) The fuzzy technique for order preference by similarity to ideal solution (TOPSIS) approach is integrated into the dynamic programming approach to optimize vehicle routes in each cluster. A numerical case study in Anshun, China demonstrates the advantages of the proposed method by comparing with the other two prevailing algorithms. In addition, a sensitivity analysis is conducted to capture the impacts of various evaluation criteria weights. The results indicate our approach performs very well to identify similar customer groups and incorporate individual customer's service priority into VRP. 8 9 10 11 12 13 14 15 16 17 18 Dual-loop detector systems are widely deployed in Washington State freeway networks to collect traffic data. The Traffic Systems Management Center (TSMC) aggregates traffic volume, occupancy, speed, and length data from dual-loop detectors into 20-second intervals and stores them to save storage space of archived data and preserve the data format compatible with the control system algorithms of Washington State Department of Transportation (WSDOT). Because of the data aggregation, valuable information of individual vehicle is missing. This makes the detection and in-depth investigation of causes of dual-loop errors complicated or impossible. Since high-resolution event data preserves individual vehicle information, they are an excellent data source for such analyses. For example, our study using event data has found that the main cause of dual-loop malfunction in Washington State is the loop sensitivity discrepancy. Since event data is typically not available from TSMC, a data collection system is desired for collecting loop event data. This paper describes such a system called Advanced Loop Event Data Analyzer (ALEDA). ALEDA is a portable event data collection and real-time analysis system which can be installed on a laptop computer. Besides its capability to collect and record event data at 60 Hz or higher without interfering the operation of controllers, ALEDA analyzes the event data at dual-loop station, displays individual vehicle information, and points out sensitivity problems with solutions based on passing vehicle statistics. Consequently, ALEDA facilitates the on-site real-time identification and correction of loop sensitivity problems to improve data quality. Animal-vehicle collisions (AVCs) have been increasing with increases in both animal populations and motor vehicle miles of travel and have become a major safety concern nationwide. Most previous AVC risk studies have not considered factors related to human behavior or the spatial distribution of animal populations in depth because of missing datasets or the poor quality of data. The two common sources of data\\u2014the Collision Report (CRpt) and Carcass Removal (CR) datasets\\u2014are often found significantly different. To address these data issues, two approaches were followed in this research. In the first approach, a fuzzy logic-based data mapping algorithm was developed to obtain a more complete AVC dataset from the CRpt and CR data. In comparison to the original CR dataset, the combined dataset increased the number of AVC records by 13~22 percent. This combined dataset was used to develop and calibrate a microscopic probability (MP) model that can explicitly consider drivers\\u2019 behaviors and the spatial distributions of animal populations. In the second approach, a Diagonal Inflated Bivariate Poisson (DIBP) regression model was developed to fit the two datasets simultaneously. The DIBP model can effectively identify the overlapping parts of the two datasets and quantify the impacts of road and environmental factors on AVCs. Both proposed models used the CRpt and CR data collected from ten selected study routes in Washington state. The MP model results showed that variables including number of lanes and animal habitat areas are significantly associated with the probability of animals crossing the highway. Two factors, speed limit and truck percentage, have impacts on the probability of a driver\\u2019s ineffective response. A wider median may decrease the probability of an animal failing to avoid a collision. The DIBP results showed that speed limit, restrictive access control, and roadway segment length have an increasing relationship with AVCs. Furthermore, hotspots (high risk roadway segments) were identified for all the study routes on the basis of the modeling and data analysis results. These quantitative results will help WSDOT develop countermeasures to AVCs. The Washington State Department of Transportation (WSDOT) operates thousands of Inductive Loop Detectors (ILDs) on the freeways and highways of Washington State. The collection and disbursement of this data is handled at the regional level, which has led to formatting differences and data fragmentation. The Datamart Project is intended to consolidate data from all regions and store it in one location easily searchable and accessible by all authorized users. Therefore a database and a web site have been created in a computer application called Datamart to accomplish these tasks and demonstrate the idea. Additionally, this research project has been also tasked with creating software applications for traffic sensor data acquisition and developing error detection and correction methodologies for data quality control. Error detection is accomplished in a three-step test proposed by this study to identify loop detectors suffering from severe errors. The default conditions used by this test approach are general enough to be applied in other states. Meanwhile, the approach also has its flexibility to use site specific parameters to optimize its performance at a specific location. These severe errors identifiable by this approach include wrong mode setting, cross chatting, extreme under sensitivity, and incorrect sensitivity level, among others. Loop detectors suffering from incorrect sensitivity levels may be adjusted by the proposed correction methodology. The correction methodology is designed to identify the true length of the loop detector\\u2019s detection zone. Properly calibrated ILDs are generally assumed to have detection zone sizes determined by the size of the loop coil buried in the roadway. The reality is that when ILD sensitivity levels drift from their correct settings, the detection zone size changes. By using the correct detection zone lengths, loop detector measurements can be corrected. This software-based approach is proven effective and can be retroactively applied to archived data as well. The loop error identification and correction algorithms have been implemented in the prototype Datamart system, which is an online database system backed up by Microsoft SQL Server 2008 and the Google Map technologies. It is highly scalable and has the potential to add new data sources from other transportation agencies and online analysis functions for regional transportation planning, traffic management, and analysis purposes. This study is primarily focused on missing traffic sensor data imputation for the purpose of improving the coverage and accuracy of traffic analysis and performance estimation. Missing data, whether attributable to hardware failure or error detection and removal, are a constant problem in loop and other traffic detector data sets. As the rate of missingness increases, the treatment of missing values quickly becomes the controlling factor in overall data quality. Previously, several imputation approaches have been developed for traffic data. However, few studies aim at handling the traffic data with large blocks of missing values for networkwide implementation. A proven predictive mean matching multiple imputation method is introduced; it was applied to loop detector volume data collected on Interstate 5 in Washington State. With the use of the iterative multiple imputation by chained equations approach, the spatial correlation between nearby detectors was considered for prediction, and the presence of mis... With the decreasing price of video cameras and their increased deployment on roadway networks, traffic data collection through video imaging has grown in popularity. Numerous vehicle detection and tracking algorithms have been developed for video sensors. However, most existing algorithms function only within a narrow band of environmental conditions and occlusion-free scenarios. In this study, a novel video-based vehicle detection and tracking algorithm is developed for traffic data collection under a broader range of environmental factors and traffic flow conditions. This algorithm employs a scan-line approach to generate spatio-temporal maps representing vehicle trajectories. Vehicle trajectories are then extracted by determining the Hough lines of the obtained ST-maps and grouping the Hough lines using the connected component analysis method. The algorithm is implemented in C++ using OpenCV and BOOST C++ libraries and is capable of operating in real-time. Over five hours of surveillance video footage was used to test the algorithm. Detection count errors ranged from under 1% in the relatively simple situations to under 15% in highly challenging scenarios. This result is very encouraging because the test video sets were taken under demanding conditions that ordinary video image processing algorithms cannot deal with. This implies that the algorithm is robust and able to produce reasonably accurate vehicle detection results under scenarios with adverse weather conditions and various vehicle occlusions. However, this algorithm requires approximately constant vehicle speed to perform well. Further research is necessary to extend the capabilities of the current algorithm to stop-and-go traffic conditions. Taxi GPS trajectories data contain massive spatial and temporal information of urban human activity and mobility. Taking taxi as mobile sensors, the information derived from taxi trips benefits the city and transportation planning. The original data used in study are collected from more than 1100 taxi drivers in Harbin city. We firstly divide the city area into 400 different transportation districts and analyze the origin and destination distribution in urban area on weekday and weekend. The Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm is used to cluster pick-up and drop-off locations. Furthermore, four spatial interaction models are calibrated and compared based on trajectories in shopping center of Harbin city to study the pick-up location searching behavior. By extracting taxi trips from GPS data, travel distance, time and average speed in occupied and non-occupied status are then used to investigate human mobility. Finally, we use observed OD matrix of center area in Harbin city to model the traffic distribution patterns based on entropy-maximizing method, and the estimation performance verify its effectiveness in case study. Traffic congestion is a major operational problem for freeways in Washington State. Recent studies have estimated that more than 50 percent of freeway congestion is caused by traffic incidents. To help the Washington State Department of Transportation (WSDOT) identify effective countermeasures against such congestion-inducing incidents, a thorough understanding of travel delays caused by incidents is essential. By using traffic data extracted from archived loop detector measurements and incident log data recorded by the WSDOT Incident Response (IR) team, this research project developed a new algorithm for quantifying travel delays produced by different incident categories. The algorithm applies a modified deterministic queuing theory to estimate incident-induced delay by using 1-minute aggregated loop detector data. Incident-induced delay refers to the difference between the total delay and the recurrent travel delay at the time and location influenced by the incident. The uniqueness of the delay calculation in this study is the use of a dynamic traffic-volume-based background profile, which is considered a more accurate representation of prevailing traffic conditions. According to the test results, the proposed algorithm can provide good estimates for incident-induced delay and capture the evolution of freeway traffic flow during incident duration. Because actual traffic data measured by loop detectors were used in this study to compute vehicle arrival and departure rates for delay calculations, the estimated incident-induced delay should be very close to the reality. Additionally, the proposed algorithm was implemented in the Advanced Roadway Incident Analyzer (ARIA) system. ARIA is a database-driven computer system that automates all the computational processes. More accurate incident delay information will help WSDOT improve its understanding of congestion-inducing incidents and select more effective countermeasures against incident-related traffic congestion on freeways. Traffic incidents cause approximately 50 percent of freeway congestion in metropolitan areas, resulting in extra travel time and fuel cost. Quantifying incident-induced delay (IID) will help people better understand the real costs of incidents, maximize the benefit-to-cost-ratio of investments in incident remedy actions, and facilitate the development of active traffic management and integrated corridor management strategies. Currently, a number of algorithms are available for IID quantification. However, these algorithms were developed with certain theoretical assumptions that are difficult to meet in real-world applications. Furthermore, they have only been applied to simulated cases and have not been sufficiently verified with ground-truth data. To quantify IID over a regional freeway network using existing traffic sensor measurements, a new approach for IID estimation was developed in this study. This new approach combines a modified deterministic queuing diagram with short-term traffic flow forecasting techniques to overcome the limitation of the zero vehicle-length assumption in the traditional deterministic queuing theory. A remarkable advantage with this new approach over most other methods is that it uses only volume data from traffic detectors to compute IID and hence is easy to apply. Verification with the video-extracted ground truth IID data found that the IID estimation errors with the new approach were within 6 percent for the two incident cases studied. This implies that the new approach is capable of producing fairly accurate freeway IID estimates using volumes measured by existing traffic sensors. This approach has been implemented on a regional map-based platform to enable quick, convenient, and reliable freeway IID estimates in the Puget Sound region. Pedestrian monitoring is quickly becoming an avid area of interest as information regarding pedestrian flow is needed not only for developing competent access in particular urban corridors, but also system optimization scenarios, such as transit systems and intersection control optimization. In this paper, we present a simple, yet effective method for tracking pedestrian objects in a relatively large surveillance area, using ordinary CCTV cameras. Object extraction is accomplished via background subtraction, while tracking is accomplished through an inherent characteristic cost function. Composite objects are used as a means to deal with occlusions. The algorithm is implemented in a Pedestrian Tracking (PedTrack) system using Microsoft Visual C#. Tested with both synthetic and real video data, this system was proved to be reliable and about 80% pedestrian were assigned with correct IDs. :\\u00a0 This article describes a coordinated ramp metering algorithm for systematically mitigating freeway congestion. A preemptive hierarchical control scheme with a three-priority-layer structure is employed in this algorithm. Ramp metering is formulated as a multiobjective optimization problem to enhance system performance. These optimization objectives include promptly tackling freeway congestion, sufficiently utilizing on-ramp storage capacities, and preventing on-ramp vehicles from overflowing to local streets, balancing on-ramp vehicle equity, and maximizing traffic throughputs for the entire system. Instead of relying heavily on accurate estimates of freeway traffic flow evolvement, this new approach models ramp meter control as a linear program and uses real-time traffic sensor measurements for minimizing the indeterminate impacts from the mainstream flow capacities. VISSIM-based simulation experiments are performed to examine its practicality and effectiveness using geometric and traffic demand data from one real-world freeway segment. The simulation test results show that the proposed ramp metering approach performed well in optimizing overall freeway system operations under various traffic conditions. The system-wide optimal control performance can be achieved to quickly mitigate freeway congestion, prevent traffic from overflowing to local streets, and maximize overall traffic throughputs. The proposed ramp metering approach can dynamically assemble relevant ramp meters to work together and effectively coordinate the individual meter rates to leverage their response strengths for minimizing time to clear the congestion. This study demonstrates that utilization of existing freeway infrastructure can be optimized through the proposed algorithm. Ramp metering has been broadly accepted and deployed as an effective countermeasure against both recurrent and non-recurrent congestion on freeways. However, many current ramp metering algorithms tend to improve only freeway travels using local detectors\\u2019 inputs and overlook the negative impacts on local streets. This may generate two problems: 1) the optimal local settings may not result in a system-wide optimum in terms of traffic operations; and 2) the increased congestion on local streets due to on-ramp overflow may counteract the gains in freeway operations. To address these problems, the authors propose an area-wide ramp metering system to improve the coordination of ramp meters for system-wide optimization and on-ramp overflow minimization. Their experimental results show that their method can offer improved performance in ramp metering operations under tested scenarios and also reduce the likelihood of on-ramp overflow. This novel strategy for active ramp metering is inspired by the principles of a computer network congestion control strategy. In this strategy, certain types of congestion at a targeted freeway location can be significantly reduced by limiting on-ramp vehicle flows to a fraction of ramp demand and then additively increasing rates to avoid ramp queue spillover onto city streets. This approach can be actively used to immediately curb the growth of traffic congestion and therefore shorten travel delays. The effectiveness of this ramp metering approach has been evaluated by microscopic simulation experiments. The authors' experimental results show that their method can reduce system-wide travel delays in the tested scenarios by as much as 28.2% when compared with the Fuzzy Logic ramp metering algorithm, and also reduce the frequency and severity of on-ramp overflow. This indicates that this area-wide algorithm can provide benefit when applied to urban freeway corridors for congestion mitigation. Traffic induction loop simulator (LOOPSIM) software, and a system and method designed for traffic research and education. The system is able to generate output signals in response to previously recorded real traffic events or simulated traffic. If directly connected to a controller cabinet or a traffic event data collection system, LOOPSIM can be used to simulate output signals from loop detectors and to test algorithms executable in the controller, or to test the performance of a traffic data collection/monitoring system. LOOPSIM can simulate different predefined patterns of traffic distributions and replay loop event data recorded by a detector event data collection (DEDAC) system or an advanced loop event data analyzer (ALEDA) at a pace responsive to controller feedback. Using LOOPSIM, many traffic system tests can be conducted for multi-purpose traffic control applications, and traffic arrival patterns can be generated for researchers and educators to test theoretical models. Understanding the relationships between influential factors and incident clearance time is crucial to make effective countermeasures for incident management agencies. Although there have been a certain number of achievements on incident clearance time modeling, limited effort is made to investigate the relative role of incident response time and its self-selection in influencing the clearance time. To fill this gap, this study uses the endogenous switching model to explore the influential factors in incident clearance time, and aims to disentangle causation from self-selection bias caused by response process. Under the joint two-stage model framework, the binary probit model and switching regression model are formulated for both incident response time and clearance time, respectively. Based on the freeway incident data collected in Washington State, full information maximum likelihood (FIML) method is utilized to estimate the endogenous switching model parameters. Significant factors affecting incident response time and clearance time can be identified, including incident, temporal, geographical, environmental, traffic and operational attributes. The estimate results reveal the influential effects of incident, temporal, geographical, environmental, traffic and operational factors on incident response time and clearance time. In addition, the causality of incident response time itself and its self-selection correction on incident clearance time are found to be indispensable. These findings suggest that the causal effect of response time on incident clearance time will be overestimated if the self-selection bias is not considered. Short-term traffic speed forecasting is an important issue for developing Intelligent Transportation Systems applications. So far, a number of short-term speed prediction approaches have been developed. Recently, some multivariate approaches have been proposed to consider the spatial and temporal correlation of traffic data. However, as traffic data often demonstrates periodic patterns, the existing methodologies often fail to take into account spatial and temporal information as well as the periodic features of traffic data simultaneously in the multi-step prediction. This paper comprehensively evaluated the multi-step prediction performance of space time (ST) model, vector autoregression (VAR), and autoregressive integrated moving average (ARIMA) models using the 5 minute freeway speed data collected from five loop detectors located on an eastbound segment of Interstate 394 freeway, in Minnesota. To further consider the cyclical characteristics of freeway speed data, hybrid prediction approaches were pr... Pedestrian and bicycle monitoring is quickly becoming an avid area of interest as information regarding pedestrian and bicycle flow is needed not only for developing competent access to particular urban corridors and trails, but also for system optimization scenarios, such as transit system operations and intersection controls. In this paper, the authors present a simple, yet effective method for tracking pedestrian and bicycle objects in a relatively large surveillance area, using ordinary uncalibrated video images. Object extraction is accomplished via background subtraction, while tracking is accomplished through an inherent characteristic cost function. Composite objects are used as a means of dealing with occlusions. The algorithm is implemented using Microsoft Visual C# and was tested on numerous scenes of varying complexity, resulting in an average count rate of 92.7% at the specified checkpoints. Incident response time is critical for incident management. The sooner an incident is responded to, the lower the negative impact comes from it. There have been some achievements on incident response time modeling. However, most of them were based on empirical observations rather than the mechanism of the system and hence their findings were highly dependent on the proposed hypotheses and study sites. A more general analytical method is needed for response time analysis. To fill up the gap, a mechanism based approach is proposed to model the incident response process and explore the contributing explanatory attributes in this paper. A typical incident response process is mathematically formulated based on the incident response truck (IRT)\\u2019s activity. Response time is considered being comprised of both preparation delay and travel time to the incident site. Both components are modeled using probability distributions to take their stochastic features into account. The response time model is calibrated using the Washington State Incident Tracking System (WITS) data and dual-loop detector data collected in 2009. Seven variables were found to significantly increase the response preparation delay (e.g. injury involved, heavy truck involved, and weekends) and eleven variables were found having a decreasing effect on the preparation delay (e.g. peak hour and average annual daily traffic). The model has the potential to be used for incident response resource optimization and identification of measures for incident response time improvement. The automated fare collection (AFC) system, also known as the transit smart card (SC) system, has gained more and more popularity among transit agencies worldwide. Compared with the conventional manual fare collection system, an AFC system has its inherent advantages in low labor cost and high efficiency for fare collection and transaction data archival. Although it is possible to collect highly valuable data from transit SC transactions, substantial efforts and methodologies are needed for extracting such data because most AFC systems are not initially designed for data collection. This is true especially for the Beijing AFC system, where a passenger\\u2019s boarding stop (origin) on a flat-rate bus is not recorded on the check-in scan. To extract passengers\\u2019 origin data from recorded SC transaction information, a Markov chain based Bayesian decision tree algorithm is developed in this study. Using the time invariance property of the Markov chain, the algorithm is further optimized and simplified to have a linear computational complexity. This algorithm is verified with transit vehicles equipped with global positioning system (GPS) data loggers. Our verification results demonstrated that the proposed algorithm is effective in extracting transit passengers\\u2019 origin information from SC transactions with a relatively high accuracy. Such transit origin data are highly valuable for transit system planning and route optimization. With increasing amounts of data being collected for intelligent transportation systems on arterial networks, the archival, management, and analysis of complex network traffic data have become a challenge. Challenges include inconsistent data connections, data quality control, query performance, traffic prediction, and computational limitations. The web-based RADAR Net system is presented to address these challenges. This system adopts a relational database with link, intersection, and detector entities. Relational data demonstrate its query performance and scalability. The system contains four layers: offline server, online server (middleware), online server (Java Servlet), and online client. This four-layer design successfully distributes the computational burden on the server. To monitor arterial performance, link speeds are calculated directly from loop detector data retrieved in Bellevue, Washington. The system can dynamically predict and smooth real-time loop spot speeds by using an alpha-beta filter... AbstractTo improve customer satisfaction and reduce operation costs, transit authorities have been striving to monitor transit service quality and identify the key factors to enhance it. The recent advent of passive data collection technologies, e.g.,\\u00a0automated fare collection (AFC) and automated vehicle location (AVL), has shifted a data-poor environment to a data-rich environment and offered opportunities for transit agencies to conduct comprehensive transit system performance measures. However, most AFC and AVL systems are not designed for transit performance measures, implying that additional data processing and visualization procedures are needed to improve both data usability and accessibility. This study attempts to develop a data-driven platform for online transit performance monitoring. The primary data sources come from the AFC and AVL systems in Beijing, where a passenger\\u2019s boarding stop (origin) and alighting stop (destination) on a flat-rate bus are not recorded. The individual transit rider\\u2019... Traffic flow prediction is considered a key technology of intelligent transportation systems. This paper presents a hybrid model that combines double exponential smoothing (DES) and a support vector machine (SVM) to predict traffic flow patterns on the basis of weekly similarities in traffic flow. First, in the hybrid model, DES is applied to predict the future data, and its smoothing parameters are determined by the Levenberg-Marquardt algorithm. Second, the SVM is employed to estimate the residual series between the prediction results by the DES model and actual measured data. In the SVM model, the cross-correlation rule is used to optimize its parameters. Third, a case study to test the proposed model with the data at different temporal scales is presented. Furthermore, data-smoothing strategies, including difference and ratio schemes based on weekly similarities, are applied as data processes before prediction. The proposed hybrid model along with the processing scheme demonstrates superiority in pred... In past decades, transportation research has been driven by mathematical equations and has relied on scarce data. With increasing amounts of data being collected from intelligent transportation system sensors, data-driven or data-based research is expected to expand soon. Most online systems are designed to handle one type of data, such as from freeway or arterial sensors. Even if transportation data are ubiquitous, data usability is difficult to improve. A framework is proposed for a regionwide web-based transportation decision system that adopts digital roadway maps as the base and provides data layers for integrating multiple data sources (e.g., traffic sensor, incident, accident, and travel time). This system, called the Digital Roadway Interactive Visualization and Evaluation Network (DRIVE Net), provides a practical method for facilitating data retrieval and integration and enhances data usability. Moreover, DRIVE Net offers a platform for optimizing transportation decisions that also serves as an ideal tool for visualizing historical observations spatially and temporally. Not only can DRIVE Net be used as a practical tool for various transportation analyses, with the use of its online computation engine, DRIVE Net can also help evaluate the benefit of a specific transportation solution. In its current implementation, DRIVE Net demonstrates potential to be used soon as a standard tool to incorporate more data sets from different fields (e.g., health and household data) and offer a platform for real-time decision making. Traveler information delivered through variable message signs and other mobile devices has been proved beneficial for traffic network performance. Understanding travelers\\u2019 en-route responses to real-time traffic conditions is essential for integrated corridor management and strategic investment in Intelligent Transportation Systems (ITS) targeting congestion mitigation. Identifying the availability of spare capacity on alternative routes is the indispensable first step for diversion guidance on congested routes. Otherwise, diversion may not be beneficial for either the travelers who made the decision or the roadway network because diverting to an alternative route without spare capacity will aggravate the congestion and further delay the travel. In order to evaluate traveler\\u2019s diversion and its impact, many studies have tried various approaches, such as survey study and traffic simulations. This paper describes a threshold-based method to replicate different scenarios using loop detector data on freeways and arterials. Unlike the traffic simulation approaches, the threshold-based method can quantitatively capture en-route travelers\\u2019 real-time decision on diversion and the diversion\\u2019s impact on the alternative route without the need for theoretical assumptions. By monitoring the freeway congestion level, the proposed method extracts three types of scenarios, normal congestion, worse-than-expect, and abrupt-traffic-change condition to investigate travelers\\u2019 decisions on diversion. This method was applied to the City of Bellevue in WA and achieved encouraging results. Travel evaluation metrics have been historically biased toward motorized modes, which dominate land transportation choices and are partially responsible for numerous environmental and health issues facing society today. Encouraging active travel solutions is seen as a means of improving the sustainability, health, and cohesiveness of a community. Unfortunately, information about volume, trip origin and destination, travel time, and personal interactions is difficult to obtain because of a lack of sensor infrastructure and unrestricted movement of these modes. Therefore, information is often limited to annual surveys and model estimates that are insufficient to address the increasing needs of sustainable planning and large-scale behavior studies. An automated, cost-effective approach to acquiring pedestrian data is desirable. The emergence of Bluetooth sensors as a means of gathering travel time data for traffic analysis presents an opportunity to use the same technology for pedestrian travel analysis. How... Safety and quality of travel on arterial networks tie closely to the performance of signalized intersections. Measures commonly used for intersection performance evaluations are control delay, queue length, and cycle failure. However, these variables are not directly available from typical configurations of traffic sensors designed for intersection signal control. Collecting vehicle control delay data manually for intersection performance measurement has been a task too time-consuming and labor-intensive to be practical. Video image processors (VIPs) have been increasingly deployed for intersection signal control in recent years. This study aims to use the extra detection capabilities of VIPs for performance monitoring at signalized intersections. Most VIPs can support up to 24 virtual loops, but normally less than half of the virtual loops are used. By properly configuring the spare virtual loops and analyzing the loop measurements, intersection performance can be monitored in real time. In this research... Neural networks have been extensively applied to short-term traffic prediction in the past years. This study proposes a novel architecture of neural networks, Long Short-Term Neural Network (LSTM NN), to capture nonlinear traffic dynamic in an effective manner. The LSTM NN can overcome the issue of back-propagated error decay through memory blocks, and thus exhibits the superior capability for time series prediction with long temporal dependency. In addition, the LSTM NN can automatically determine the optimal time lags. To validate the effectiveness of LSTM NN, travel speed data from traffic microwave detectors in Beijing are used for model training and testing. A comparison with different topologies of dynamic neural networks as well as other prevailing parametric and nonparametric algorithms suggests that LSTM NN can achieve the best prediction performance in terms of both accuracy and stability. With the increasing attention paid to environmental effects and sustainable infrastructure, transportation agencies are now seeking more cost-effective and environmentally friendly countermeasures against traffic congestion than the traditional roadway expansion. Managed lane (ML) systems, as an innovative strategy for managing the roadway conditions in real time, have been gaining increasing popularity in the recent decade. However, the unique characteristics of ML facilities, such as the frictional effects between general purpose lanes (GPLs) and their adjacent ML, are not well studied and modeled. This paper investigates the interaction between GPLs and MLs, as buffer-separated ML facilities are readily affected by congestion in the adjacent GPLs. The frictional effect is quantified through development of speed-flow curves for the ML facility. A traffic flow model is developed on the basis of the cell transmission model incorporating this frictional effect to model the traffic evolution on the ML facil... To mitigate the congestion caused by the ever increasing number of privately owned automobiles, public transit is highly promoted by transportation agencies worldwide. A better understanding of travel patterns and regularity at the \\u201cmagnitude\\u201d level will enable transit authorities to evaluate the services they offer, adjust marketing strategies, retain loyal customers and improve overall transit performance. However, it is fairly challenging to identify travel patterns for individual transit riders in a large dataset. This paper proposes an efficient and effective data-mining procedure that models the travel patterns of transit riders in Beijing, China. Transit riders\\u2019 trip chains are identified based on the temporal and spatial characteristics of their smart card transaction data. The Density-based Spatial Clustering of Applications with Noise (DBSCAN) algorithm then analyzes the identified trip chains to detect transit riders\\u2019 historical travel patterns and the K-Means++ clustering algorithm and the rough-set theory are jointly applied to cluster and classify travel pattern regularities. The performance of the rough-set-based algorithm is compared with those of other prevailing classification algorithms. The results indicate that the proposed rough-set-based algorithm outperforms other commonly used data-mining algorithms in terms of accuracy and efficiency. Traffic speed is one of the most important indicators for traffic control and management. Unfortunately, speed cannot be measured directly from single inductance loops, the most commonly used detectors. To calculate space-mean speed, a constant, g, is often adopted to convert lane occupancy to traffic density. However, as illustrated by data from the present study, such a formula consistently underestimates speed whenever a significant number of trucks or other longer vehicles are present. This is because g is actually not a constant but, rather, a function of vehicle length. To calculate the value of g suitably, one needs to know the percentage of long vehicles or the mean vehicle length in real time. However, such information is not directly available from single-loop outputs. It is shown how the occupancy variance obtained from single-loop data can be used to estimate the percentage of long vehicles and how a log-linear regression model for mean vehicle length estimation based only on single-loop outpu... Roadway agencies face growing challenges to expand freeway capacity. Under the constraints of rising construction costs, right-of-way limitations, and environmental regulations, transportation agencies are seeking solutions for efficiently managing the demand on existing freeway facilities and for providing options for travelers. The concept of managed lanes (MLs) is an increasingly popular countermeasure that aims to make the most efficient use of freeway facilities by restricting access to one or more lanes to certain vehicle classes on a facility that is parallel to existing general purpose (GP) lanes. With different strategies for separating ML and GP lanes, this study examines the interaction between GP lanes and ML operations. It scrutinizes this interaction as a function of different separation types, the number of MLs, and operational ML strategies, including high-occupancy toll (HOT) and high-occupancy vehicle (HOV) lanes. Four freeway sites were selected to explore the effect of this interaction... Abstract Collisions between bicycles and motor vehicles have caused severe life and property losses in many countries. The majority of bicycle\\u2013motor vehicle (BMV) accidents occur at intersections. In order to reduce the number of BMV accidents at intersections, a substantial understanding of the causal factors for the collisions is required. In this study, intersection BMV accidents were classified into three types based on the movements of the involved motor vehicles and bicycles. The three BMV accident classifications were through motor vehicle related collisions, left-turn motor vehicle related collisions, and right-turn motor vehicle related collisions. A methodology for estimating these BMV accident risks was developed based on probability theory. A significant difference between this proposed methodology and most current approaches is that the proposed approach explicitly relates the risk of each specific BMV accident type to its related flows. The methodology was demonstrated using a 4-year (1992\\u20131995) data set collected from 115 signalized intersections in the Tokyo Metropolitan area. This data set contains BMV accident data, bicycle flow data, motor vehicle flow data, traffic control data, and geometric data for each intersection approach. For each BMV risk model, an independent explanatory variable set was chosen according to the characteristics of the accident type. Three negative binomial regression models (one corresponding to each BMV accident type) were estimated using the maximum likelihood method. The coefficient value and its significance level were estimated for each selected variable. The negative binomial dispersion parameters for all the three models were significant at 0.01 levels. This supported the choice of the negative binomial regression over the Poisson regression for the quantitative analyses in this study. This paper presents a method for using a dual roadside seismic sensor to detect moving vehicles on roadway by installing them on a road shoulder. Seismic signals are split into fixed time intervals in recording. In each interval, the time delay of arrival (TDOA) is estimated using a generalized cross-correlation approach with phase transform (GCC-PHAT). Various kinds of vehicle characterization information, including vehicle speed, axle spacing, detection of both vehicle axles and moving direction, can also be extracted from the collected seismic signals as demonstrated in this paper. The error of both vehicle speed and axle spacing detected by this approach has been shown to be less than 20% through the field tests conducted on an urban street in Seattle. Compared to most existing sensors, this new design of dual seismic sensor is cost effective, easy to install, and effective in gathering information for various traffic management applications. Different states have used different tolling strategies for their high-occupancy-toll (HOT) lane facilities, and the performance of an HOT lane system has tied directly to the tolling strategy. However, comparing tolling strategies has been a difficult problem because of the lack of field data as well as missing pieces of information critical to the analysis of HOT lane operations. For example, how single-occupancy vehicles (SOVs) choose the type of lane to use under the effect of tolling and the flow friction between HOT and general purpose lanes remains unclear. This research attempts to analyze the heterogeneity among the SOV users of HOT lanes and to quantify the attractiveness of HOT lanes for SOV drivers. The study uses field data collected by point traffic sensors and transponder toll tags on Washington State Route 167 (SR-167). SOVs are categorized as infrequent users and frequent users, according to the frequency with which their transponder identification tags appear in the database. Their value... Traffic monitoring cameras are widely installed on streets and freeways in U.S. metropolitan areas. Video images captured from these video cameras can be used to extract many valuable traffic parameters through video image processing. A popular way to capture traffic data is to compare the current traffic images with the background image, which contains no vehicles or other moving objects, just background such as pavement. Once the moving vehicle images are separated from the background image, measurements of their number, speed, and so on can be obtained. Typically, background images are extracted from a video stream through image processing because it may be hard to find a frame without any vehicles for normal traffic streams on urban streets. This paper introduces a new method that can quickly extract the background image from traffic video streams for both freeways and intersections in a variety of prevailing traffic conditions. This method has been tested with field data, and the results are promising. This study develops a hybrid model that combines double exponential smoothing (DES) and support vector machine (SVM) to implement a traffic flow predictor. In the hybrid model, DES is used firstly to predict the future data, and the smoothing parameters of the DES are determined by Levenberg-Marquardt algorithm. Then, SVM is employed to fit the residual series between the predicting results of DES model and actual measured data for its powerful no-linear fitting ability. Finally, a practical application is used to testify the proposed model. In the application, data smoothing and wavelet de-noising technology are applied as data pre-treatment before prediction. In addition, the data smoothing contains difference and ratio smoothing strategy. It is demonstrated the superiority of the new hybrid model and the effectiveness of data pre-treatment through the comparison between the prediction results of DES, autoregressive integrated moving average (ARIMA) and DES-SVM model. Classified vehicle volumes are important inputs for traffic operation, pavement design, and transportation planning. However, such data are not available from single-loop detectors, the most widely deployed type of traffic sensor in the existing roadway infrastructure. Several attempts have been made to extract classified vehicle volume data from single-loop measurements in recent years. These studies used estimated speed for length calculation and classified vehicles into bins based on the calculated vehicle lengths. However, because of the stochastic features of traffic flow, deterministic mathematical equations based on certain assumptions for speed calculation typically do not work well for all situations and may result in significant speed estimation errors under certain traffic conditions. Such errors accumulate when estimated speeds are used in vehicle-length calculations and degrade the accuracy of vehicle classification. To solve this problem, an artificial neural network method was developed to ... Traffic congestion is a common phenomenon in our daily lives that greatly costs society. A better understanding of the interaction between freeways and arterial streets may help traffic engineers and researchers improve the operation of existing facilities and deploy feasible traffic diversion plans to improve the usage of existing road capacity within a traffic network. This paper proposes a novel two-step approach to evaluate the interaction between freeways and arterial streets by comparing their performances. The first step is to identify freeway and arterial travel time pattern similarities via template matching techniques commonly used in computer vision. The interaction is quantified in the second step by using conditional probability theory. The result of the two-step process allows analysts to determine whether traffic diversion is possible or likely between freeways and parallel arterials. The city of Bellevue, Washington was selected as a case study site because of the availability of traffic s... The vehicle routing problem with simultaneous deliveries and pickups (VRPSDP) has attracted much research interest because of the potential to provide cost savings to transportation and logistics operators. Several extensions of VRPSDP exist. Of these extensions, the simultaneous deliveries and pickups with split loads problem (SDPSLP) has been proposed to eliminate vehicle capacity constraints, as well as allow the deliveries or pickups for a customer to be split into multiple visits. Although delivery and pickup activities are often constrained by time windows, few studies have considered such constraints when SDPSLP has been addressed. To fill the gap, this paper formulates the vehicle routing problem of simultaneous deliveries and pickups with split loads and time windows (VRPSDPSLTW) as a mixed-integer programming problem. A hybrid heuristic algorithm was developed to solve this problem. Solomon data sets with minor modifications were applied to test the effectiveness of the solution algorithm. The r... Inductive loop detectors (ILDs) form the backbone of many traffic detection networks by providing vehicle detection for freeway and arterial monitoring as well as signal control. Unfortunately, ILD technology generally has limited the available sensitivity settings. Changing roadway conditions and aging equipment can cause ILD settings that had been correct to become under- or oversensitive. ILDs with incorrect sensitivities may result in severe errors in occupancy and volume measurements. Therefore, sensitivity error identification and correction are important for quality data collection from ILDs. In this study, the Gaussian mixture model (GMM) is used to identify ILDs with sensitivity problems. If the sensitivity problem is correctible at the software level, a correction factor is then calculated for the occupancy measurements of the ILD. The correction methodology developed in this study was found effective in correcting occupancy errors caused by the ILD sensitivity problems. Single-loop speed calcul... Managed lane facilities, including high-occupancy vehicle (HOV) lanes, high-occupancy toll lanes, and express lanes, have become attractive tools for managing today's transportation system. Although managed lanes, specifically, HOV lanes, have existed for several decades, there has been little documentation of their traffic flow behaviors. Because these facilities tend to take on a variety of configurations, including different numbers of managed lanes and separation types from abutting general purpose lanes, transportation engineers should understand the traffic flow differences among facility types. An understanding of the interaction between managed lanes and parallel general purpose lanes is also needed for assessing the performance of managed lanes. A study was done to investigate the performance and traffic flow behavior of managed lane facilities at sites across the country. Traffic flow behavior for five facility types, based on separation type and number of lanes, was analyzed. Factors such as fr... Freeway incidents cause major traffic operational problems in the State of Washington. In order to alleviate the severity of incident-induced congestion, a better understanding of incident causes, features, and impacts is indispensable. To accomplish this goal, extensive incident data from the Washington state incident tracking system (WITS) database were statistically analyzed. The queuing analysis, by the use of archived loop detector data, was also applied to evaluate the possible incident impacts on traffic flow during incident duration. Incorporating the statistical and queuing analyses help identify the main categories of incidents on Washington state's freeways and the relationships between incident parameters and incident impacts on freeway traffic. Benefits of reducing incident response times and incident clear times were also discussed based on the queue length information. The analysis results can be applied as the guidance for an incident response team to select most effective countermeasures against incident-induced congestion. Abstract Although various innovative traffic sensing technologies have been widely employed, incomplete sensor data is one of the most major problems to significantly degrade traffic data quality and integrity. In this study, a hybrid approach integrating the Fuzzy C-Means (FCM)-based imputation method with the Genetic Algorithm (GA) is develop for missing traffic volume data estimation based on inductance loop detector outputs. By utilizing the weekly similarity among data, the conventional vector-based data structure is firstly transformed into the matrix-based data pattern. Then, the GA is applied to optimize the membership functions and centroids in the FCM model. The experimental tests are conducted to verify the effectiveness of the proposed approach. The traffic volume data collected at different temporal scales were used as the testing dataset, and three different indicators, including root mean square error, correlation coefficient, and relative accuracy, are utilized to quantify the imputation performance compared with some conventional methods (Historical method, Double Exponential Smoothing, and Autoregressive Integrated Moving Average model). The results show the proposed approach outperforms the conventional methods under prevailing traffic conditions. Abstract In this paper, we firstly use the traffic flow data collected from loop detectors on freeway and measure the complexity of data by Lempel-Ziv algorithm at different temporal scales. Considering each day as a cycle and each cycle as a single node, we then construct complex networks by using the distribution of density and its derivative. In addition, the networks are analyzed in terms of some statistical properties, such as average path length, clustering coefficient, density, and average degree. Finally, we use the correlation coefficient matrix, adjacent matrix and closeness to exploit the periodicity in weekdays and weekends of traffic flow data. Incident clearance time is a major performance measure of the traffic emergency management. A clear understanding of the contributing factors and their effects on incident clearance time is essential for optimal incident management resource allocations. Most previous studies simply considered the average effects of the influential factors. Although the time-varying effects are also important for incident management agencies, they were not sufficiently investigated. To fill up the gap, this study develops a non-proportional hazard-based duration model for analyzing the time-varying effects of influential factors on incident clearance time. This study follows a systematic approach incorporating the following three procedures: proportionality test, model development/estimation, and effectiveness test. Applying the proposed model to the 2009 Washington State Incident Tracking System data, five factors were found to have significant but constant (or time independent) effects on the clearance time, which is similar to the findings from previous studies. However, our model also discovered thirteen variables that have significant time-varying impacts on clearance hazard. These factors cannot be identified through the conventional methods used in most previous studies. The influential factors are investigated from both macroscopic and microscopic perspectives. The population average effect evaluation provides the macroscopic insight and benefits long-term incident management, and the time-dependent pattern identification offers microscopic and time-sequential insight and benefits the specific incident clearance process. Freeway traffic speed and bin volumes for different vehicle categories are typically collected by dual-loop detectors. Good quality dual-loop detector data are crucial for effective real-time traffic management systems and traveler information systems. However, loop detectors are subject to various malfunctions that can result in erroneous measurements. Previous studies indicated that loop sensitivity-level discrepancies between two single loops forming a dual-loop detector and unsuitable sensitivity levels of the single loops are two major causes of quality degradation in dual-loop data. This paper presents an algorithm and its implementation for identifying and correcting such loop sensitivity problems. The algorithm identifies dual-loop sensitivity problems using individual vehicle data extracted from loop event data and corrects dual-loop sensitivities through a two-step procedure: (a) remove the sensitivity discrepancy between the two single loops and (b) adjust their sensitivities to the appropriate... The large-scale implementation of High-Speed Rail (HSR) network in China not only offers a new option for travelers\\u2019 mode choice, but also may influence, or even generate, the redistribution of demographic and economic activities. As has been observed over the past several years in other countries, the impact of HSR spans a wide range. However, few quantitative studies have been conducted to measure this impact. As a new attempt, this study uses accessibility analysis for quantifying the impact of China\\u2019s HSR network. Weighted average travel times and travel costs, contour measures, and potential accessibility are employed as indicators of accessibility at the macro or national level. Forty-nine major cities in the HSR network are used in the accessibility analysis. Accessibility quantification and spatial distribution analysis for the study cities are performed on a Geographical Information System (GIS) platform. Accessibilities associated with varying availabilities of HSR, conventional rail, and airline are estimated and compared. The selected indicators and computational methods are found effective in evaluating the accessibility impacts of HSR from different conceptualization strategies and perspectives. They also offer complementary information on accessibility capacity of the study cities created by the HSR network. In this paper, a self-adaptive tolling strategy (SATS) is developed for dynamically and systematically enhancing highoccupancy toll (HOT) lane system operations. This strategy enhances the overall system performance of both the HOT and general purpose (GP) lanes by better utilizing the HOT lane capacity while maintaining high speed and/or high travel-time reliability for HOT lane traffic when GP lanes are congested. To formulate SATS, the Lighthill-Whitham-Richards kinematic wave model is used to characterize HOT lane traffic flow evolution, and the unilateral Laplace transform is used to convert the system representation from the time domain to the frequency domain. Then, an adaptive tolling controller is designed with both the proportional and integral control components. Real-time traffic measurements, including lane occupancy, average speed, and flow rate, are utilized for toll rate calculations. Following a dual-phase control scheme, the appropriate flow rate for HOT lane utilization is computed, and the corresponding toll is estimated backward. To examine the effectiveness of the proposed tolling strategy, microscopic traffic simulation experiments are conducted using VISSIM. The experiment results demonstrate that the proposed tolling strategy performs reasonably well in improving the overall operations of HOT lane systems under various traffic conditions. A complex network is a powerful tool to research complex systems, traffic flow being one of the most complex systems. In this paper, we use complex network theory to study traffic time series, which provide a new insight into traffic flow analysis. Firstly, the phase space, which describes the evolution of the behavior of a nonlinear system, is reconstructed using the delay embedding theorem. Secondly, in order to convert the new time series into a complex network, the critical threshold is estimated by the characteristics of a complex network, which include degree distribution, cumulative degree distribution, and density and clustering coefficients. We find that the degree distribution of associated complex network can be fitted with a Gaussian function, and the cumulative degree distribution can be fitted with an exponential function. Density and clustering coefficients are then researched to reflect the change of connections between nodes in complex network, and the results are in accordance with the observation of the plot of an adjacent matrix. Consequently, based on complex network analysis, the proper range of the critical threshold is determined. Finally, to mine the nodes with the closest relations in a complex network, the modularity is calculated with the increase of critical threshold and the community structure is detected according to the optimal modularity. The work in our paper provides a new way to understand the dynamics of traffic time series. Understanding how congestion at one location can cause ripples throughout large-scale transportation network is vital for transportation researchers and practitioners to pinpoint traffic bottlenecks for congestion mitigation. Traditional studies rely on either mathematical equations or simulation techniques to model traffic congestion dynamics. However, most of the approaches have limitations, largely due to unrealistic assumptions and cumbersome parameter calibration process. With the development of Intelligent Transportation Systems (ITS) and Internet of Things (IoT), transportation data become more and more ubiquitous. This triggers a series of data-driven research to investigate transportation phenomena. Among them, deep learning theory is considered one of the most promising techniques to tackle tremendous high-dimensional data. This study attempts to extend deep learning theory into large-scale transportation network analysis. A deep Restricted Boltzmann Machine and Recurrent Neural Network architecture is utilized to model and predict traffic congestion evolution based on Global Positioning System (GPS) data from taxi. A numerical study in Ningbo, China is conducted to validate the effectiveness and efficiency of the proposed method. Results show that the prediction accuracy can achieve as high as 88% within less than 6 minutes when the model is implemented in a Graphic Processing Unit (GPU)-based parallel computing environment. The predicted congestion evolution patterns can be visualized temporally and spatially through a map-based platform to identify the vulnerable links for proactive congestion mitigation. Traffic congestion is getting worse and has resulted in increased travel delays and costs. In order to develop effective intelligent transportation systems (ITS) strategies to mitigate traffic congestion on freeways, a good understanding of its causes and impacts is vital but has not been achieved at a satisfactory level. Over the past several decades, deterministic queuing theory (DQT) has been widely used to evaluate freeway travel delays resulted from traffic congestion. However, several studies evaluated the accuracy of its delay estimates and claimed that the DQT method consistently underestimates vehicle delays. The reason for the underestimation, however, had not been clearly identified. This study aims at exploring the main cause of such underestimation problems and proposing a solution to fix it. Based on theoretical analysis and empirical justification, it was found the underestimation resulted primarily from the inappropriate estimates of the time offsets, that is, the travel times between the ... Customer clustering is an essential step to reduce the complexity of large-scale logistics network optimization. By properly grouping those customers with similar characteristics, logistics operators are able to reduce operational costs and improve customer satisfaction levels. However, due to the heterogeneity and high-dimension of customers' characteristics, the customer clustering problem has not been widely studied. This paper presents a fuzzy-based customer clustering algorithm with a hierarchical analysis structure to address this issue. Customers' characteristics are represented using linguistic variables under major and minor criteria, and then, fuzzy integration method is used to map the sub-criteria into the higher hierarchical criteria based on the trapezoidal fuzzy numbers. A fuzzy clustering algorithm based on Axiomatic Fuzzy Set is developed to group the customers into multiple clusters. The clustering validity index is designed to evaluate the effectiveness of the proposed algorithm and find the optimal clustering solution. Results from a case study in Anshun, China reveal that the proposed approach outperforms the other three prevailing algorithms to resolve the customer clustering problem. The proposed approach also demonstrates its capability of capturing the similarity and distinguishing the difference among customers. The tentative clustered regions, determined by five decision makers in Anshun City, are used to evaluate the effectiveness of the proposed approach. The validation results indicate that the clustered results from the proposed method match the actual clustered regions from the real world well. The proposed algorithm can be readily implemented in practice to help the logistics operators reduce operational costs and improve customer satisfaction levels. In addition, the proposed algorithm is potential to apply in other research domains. Animal-Vehicle Collisions (AVCs) have been a major safety problem in the United States over the past decades. Counter measures against AVCs are urgently needed for traffic safety and wildlife conservation. To better understand the AVCs, a variety of data analysis and statistical modeling techniques have been developed. However, these existing models seldom take human factors and animal attributes into account. This paper presents a new probability model which explicitly formulates the interactions between animals and drivers to better capture the relationship among drivers' and animals' attributes, roadway and environmental factors, and AVCs. Findings of this study show that speed limit, rural versus urban, and presence of white-tailed deer habitat have an increasing effect on AVC risk, whereas male animals, high truck percentage, and large number of lanes put a decreasing effect on AVC probability. Language: en Logistics joint distribution network (LJDN) optimization involves vehicle routes scheduling and profit allocation for multiple distribution centers. This is essentially a combinational and cooperative game optimization problem seeking to serve a number of customers with a fleet of vehicles and allocate profit among multiple centers. LJDN routing optimization based on customer clustering units can alleviate the computational complexity and improve the calculation accuracy. In addition, the profit allocation mechanism can be realized based on cooperative game theory through a negotiation procedure by the Logistics Service Provider (LSP). This paper establishes a model to minimize the total cost of the multiple centers joint distribution network when each distribution center is assigned to serve a series of distribution units. An improved particle swarm optimization (PSO) algorithm is presented to tackle the model formulation by assigning distribution centers (DCs) to distribution units. Improved PSO algorithm combines merits of PSO algorithm and genetic algorithm (GA) with global and local search capabilities. Finally, a Shapley value model based on cooperative game theory is proposed to obtain the optimal profit allocation strategy among distribution centers from nonempty coalitions. The computational results from a case study in Guiyang city, China, suggest the optimal sequential coalition of distribution centers can be achieved according to Strictly Monotonic Path (SMP). Length-based vehicle classification data are important inputs for traffic operation, pavement design, and transportation planning. However, such data are not directly measurable by single-loop detectors, the most widely deployed type of traffic sensor in the existing roadway infrastructure. In this study a video-based vehicle detection and classification (VVDC) system was developed for truck data collection using wide-ranging available surveillance cameras. Several computer vision-based algorithms were developed or applied to extract background image from a video sequence, detect presence of vehicles, identify and remove shadows, and calculate pixel-based vehicle lengths for classification. Care was taken to handle robustly negative effects resulting from vehicle occlusions in the horizontal direction and slight camera vibrations. The pixel-represented lengths were exploited to distinguish long vehicles from short vehicles; hence the need for complicated camera calibration can be eliminated. These algorithms were implemented in the prototype VVDC system using Microsoft Visual C#. As a plug-and-play system, the VVDC system is capable of processing both digitized image streams and live video signals in real time. The system was tested at three test locations under different traffic and environmental conditions. The accuracy for vehicle detection was above 97%, and the total truck count error was lower than 9% for all three tests. This indicates that the video image processing method developed for vehicle detection and classification in this study is indeed a viable alternative for truck data collection. The building of managed lanes parallel to general purpose lanes is an increasingly common approach to optimizing freeway capacity. Managed lanes allow agencies to classify customers and assign a portion of the freeway capacity to them. With no methodology in the Highway Capacity Manual (HCM) for analyzing these facilities, analysts rely on more time-consuming simulation analyses. A methodology is presented for estimating the performance of a parallel system of general purpose and managed lane facilities in an HCM context based on NCHRP Project 3-96. The methodology defines new managed lane segment types to use in an HCM analytical framework and is associated with a new set of speed-flow curves. It is sensitive to the number of lanes and the type of separation between managed lanes and general purpose lanes. The method introduces the concept of parallel lane groups of general purpose and managed lanes and thus can account for speed reduction in managed lanes caused by congestion in adjacent general purpose... Traffic speed and length-based vehicle classification data are critical inputs for traffic operations, pavement design and maintenance, and transportation planning. However, they cannot be measured directly by single-loop detectors, the most widely deployed type of traffic sensor in the existing roadway infrastructure. In this study, a Gaussian mixture model (GMM)-based approach is developed to estimate more accurate traffic speeds and classified vehicle volumes using single-loop outputs. The estimation procedure consists of multiple iterations of parameter correction and validation. After the GMM is established to empirically model vehicle on-times measured by single-loop detectors, the optimal solution can be initially sought to separate length-based vehicle volume data. Based on the on-time of the separated short vehicles from the GMM, an iterative process will be conducted to improve traffic speed and classified volume estimation until the estimation results become statistically stable and converge. T... The single-channel detection scheme has been widely used in practice for traffic control. Differing from lane-by-lane detection, single-channel detection has all loop detectors across multiple lanes wired together and provides a single input to the controller. Although single-channel (wired-together) detectors are commonly used by traffic agencies, the traffic volume data collected from these single-channel loops are inaccurate because of high misdetection rates, compared with lane-by-lane detection. This study focuses on developing a probability-based approach for correcting the traffic volume data collected by the single-channel loop detectors at signalized intersections. The proposed probability-based nonlinear model (NM) explicitly describes a potential model compared with a multiple linear regression model. Both models were calibrated and validated using real-life data from seven two-lane intersection approaches and three three-lane intersection approaches under various traffic conditions. The result... The economic vitality of Washington State depends on continuing and even increasing strong foreign trade levels. Improving the mobility of freight traffic on Washington's streets and highways is an important factor in achieving this goal. The Washington State Department of Transportation (Washington State DOT) has devoted significant time and resources to investigating ways to improve freight mobility on its freeway networks. The primary type of sensor used by the department to collect truck volume and speed data on Washington State freeways is a dual-loop detection system. However, recent studies of the Washington DOT classification system found that the quality of dual-loop truck data being collected was questionable because of four main problems: (a) split of loop signals for multiunit trucks, (b) cross talk between adjacent loops, (c) constrained sensitivity adjustments of discrete levels at loop amplifiers, and (d) unsuitable thresholds of on-time differences between the two single loops of a dual-lo... Because of heavy weights and large turning radii, large truck (LT) movements have very different characteristics than those of smaller vehicles, such as passenger cars. This difference makes collection of LT volume data very important for accurate analysis of traffic stream characteristics in transportation planning and engineering. Since LT travel patterns are seasonal, data obtained by surveys conducted for a short period of time every one to three years may not be adequate for safety planning, traffic management, and infrastructure maintenance. Therefore, the ability to collect such data continuously via loop detectors is highly desirable. In this paper, an algorithm for estimating LT volumes using only single-loop outputs is presented. LT volumes estimated by the proposed algorithm were compared with those observed by dual-loop detectors, and the two LT volume series fit each other very well, especially when traffic volume was low. A logistics distribution region partitioning model is developed.This model is to minimize the cost of two-echelon logistics distribution network.A hybrid algorithm with PSO and GA is proposed.The empirical results reveal that EPSO-GA algorithm outperforms other algorithms. Two-echelon logistics distribution region partitioning is a critical step to optimize two or multi-echelon logistics distribution network, and it aims to assign distribution unit to a certain logistics facility (i.e. logistic center and distribution center). Given the partitioned regions, vehicle routing problem can be further developed and solved. This paper established a model to minimize the total cost of the two-echelon logistics distribution network. A hybrid algorithm named as the Extended Particle Swarm Optimization and Genetic Algorithm (EPSO-GA) is proposed to tackle the model formulation. A two-dimensional particle encoding method is adopted to generate the initial population of particles. EPSO-GA combines the merits of Particle Swarm Optimization (PSO) algorithm and Genetic Algorithm (GA) with both global and local search capability. By updating the inertia weight and exchanging best-fit solutions and worst-fit solutions between PSO and GA, EPSO-GA algorithm is able to converge to an optimal solution with a reasonable design of termination and iteration rules. The computation results from a case study in Guiyang city, China, reveal that EPSO-GA algorithm is superior to the other three algorithms, Hybrid Particle Swarm Optimization (HPSO), GA, and Ant Colony Optimization (ACO), in terms of the partitioning schemes, the total cost and number of iterations. By comparing with the exact method, the proposed approach demonstrates its capability to optimize a small scale two-echelon logistics distribution network. The proposed approach can be readily implemented in practice to assist the logistics operators reduce operational costs and improve customer service. In addition, the proposed approach is of great potential to apply in other research domains. High-occupancy toll (HOT) lane operation has been implemented in several urban areas in the United States and is regarded as one of the most effective management strategies against freeway congestion. By allowing single occupancy vehicles (SOVs) to pay a toll for using high-occupancy vehicle (HOV) lanes, the excess capacities of HOV lanes can be used and the overall traffic mobility of the roadway section can be improved. However, research on HOT lane operations is still in its early stage. A series of theoretical and practical issues on optimizing HOT lane system performance particularly require immediate attention for better practice. Simulation-based investigation on HOT lane operations provides a cost-effective, risk-free, and prospective means of exploring and addressing these issues. A microscopic traffic simulation tool, VISSIM, is exploited in this study. By overcoming functional constraints with the VISSIM built-in modules and taking advantage of its component object model interface, we develop a new external module to enable HOT lane simulation. This HOT lane module provides additional flexibility to satisfy any specific demands from particular researchers and practitioners. Based on this external module, HOT lane operations can be simulated and evaluated as demonstrated using the Washington State Route (SR) 167 HOT Lane Pilot Project. The SR-167 simulation results not only quantitatively evaluate the overall system performance but also identify potential problems. Significantly different operational performance was illustrated between HOV lane and HOT lane systems using the developed simulation models. The simulation platform used in this study has the potential to be a cost-effective evaluation tool for HOT lane operations. The analysis of dynamics in traffic flow is an important step to achieve advanced traffic management and control in Intelligent Transportation System (ITS). Complexity and periodicity are definitely two fundamental properties in traffic dynamics. In this study, we first measure the complexity of traffic flow data by Lempel\\u2013Ziv algorithm at different temporal scales, and the data are collected from loop detectors on freeway. Second, to obtain more insight into the complexity and periodicity in traffic time series, we then construct complex networks from traffic time series by considering each day as a cycle and each cycle as a single node. The optimal threshold value of complex networks is estimated by the distribution of density and its derivative. In addition, the complex networks are subsequently analyzed in terms of some statistical properties, such as average path length, clustering coefficient, density, average degree and betweenness. Finally, take 2 min aggregation data as example, we use the correlation coefficient matrix, adjacent matrix and closeness to exploit the periodicity of weekdays and weekends in traffic flow data. The findings in this paper indicate that complex network is a practical tool for exploring dynamics in traffic time series. With the growth in traffic sensing data, the measurement of levels of service such as queue length at intersections in real time has been receiving considerable attention. This paper examines how to estimate real-time cycle-by-cycle queue length at signalized intersections by using the data of an upstream point sensor and the traveling trajectory of a probe vehicle observed by a mobile sensor. Three cases are based on the space relationships between the rear of a queue, the point sensor location, and the vehicle stop or start-up point. When the data of point sensors and mobile sensors are fused, critical break points can be identified to indicate several key states of the queue formation and dissipation. Estimation models of the maximum queue length, based on the Lighthill-Whitham-Richards theory, are proposed for the three cases. The presented algorithm does not impose any restrictions on the traffic condition (i.e., oversaturated and undersaturated conditions). The methodology is applied to ground truth... At signalized intersections, rear-end accidents are frequently the predominant accident type. These accidents result from the combination lead-vehicle deceleration and the ineffective response of the following vehicle\\u2019s driver to this deceleration. This paper mathematically represents this process, by expressing accident probability as the product of the probability of the lead vehicle decelerating and the probability of the driver in the following failing to respond in time to avoid a collision. Using this premise, a model of rear-end accident probabilities is estimated using information on traffic flow, traffic regulations, roadway geometrics, and human factors from four-legged signalized intersections in Tokyo, Japan. Estimation findings provide some important preliminary evidence for the development of countermeasures to reduce the frequency of rear-end accidents at signalized intersections. Traffic safety is one of the most important goals for roadway design and traffic system operations. Collision hotspot identification serves as a major fundamental component for traffic safety performance measurements. With identified collision hotpots, limited resources can be better allocated to improve roadway safety. There have been a significant amount of studies on collision hotspots identification over the past decades. However, most studies only considered crash counts as the sole roadway safety performance indicator. In this study, a new cluster-based method is proposed to quantify roadway safety conditions. This method is able to incorporate more heterogeneous safety-related factors for clustering, such as crash fatality, injuries, and average collision duration. Compared with the prevailing Empirical Bayes methods, our cluster-based method demonstrates its improved accuracy and efficiency, and can be easily implemented in practice. Vehicle headway distribution is fundamental for several important traffic research and simulation issues. Many headway models have been developed over the past decades. Each has its own strength and weakness. Selection of the most suitable model for a certain traffic condition remains an open issue. A comprehensive study of the performance of typical headway distribution models on urban freeways is presented. With the advanced loop event data analyzer system, many accurate headway observations were obtained from I-5 in the area of Seattle, Washington. These headway data were used to calibrate and examine the performance of various headway models. The goodness of fit for several most commonly used headway distribution models was investigated by using headways observed on regular lanes and high-occupancy-vehicle (HOV) lanes from different time periods of day. To evaluate the performance of these headway models, the analytical Kolmogorov-Smirnov test statistic and visualized comparison curves were used to me... This paper explores driver behavior in a paired car-following mode in response to a speed disturbance from a front vehicle. A current state-control action-expected state (SAS) chain is developed to provide a framework for modeling of the hierarchy of expected actions incurred during the need for speed disturbance absorption. Three car-following scenarios and one lane-changing scenario are identified with defined perceptual informative variables to describe the process of speed disturbance absorption. Those variables include dynamic spacing versus the follower's speed, disturbance-effecting and ending spacing, headway, acceleration-deceleration, speed recovery period, speed advantage, and lane-changing duration. A significant improvement in car-following modeling introduced in the paper is the integration of car-following and lane-changing behaviors in the SAS chain. Moreover, critical values of perceptual informative variables are statistically developed as a function of the follower's speed by using obse... Traffic surveillance cameras are becoming a viable replacement for inductive loop detectors. The effectiveness of these cameras, however, depends on video image processing algorithms that can alleviate common problems such as shadows, vehicle occlusion, reflection, and camera shake. Shadows have proved to be a major source of error in the detection and classification of vehicles. Three algorithms of increasing complexity are proposed to address the shadow problem. The algorithms each address the need to remove cast shadows from vehicles while preserving self-shadows, or those areas of a vehicle that are hidden from illumination. They are also geared toward real-time analysis, which requires that they can be implemented efficiently and cannot have complex training or learning requirements. The dual-pass Otsu method of shadow removal was the simplest in application but had the poorest performance. The proposed region growing technique, though showing considerable promise, failed when the pixel intensity var... With the widespread adoption of automated traffic sensors, the increase of computer processing power, and the evolution and growth of the global Internet, our ability to monitor traffic and convey traffic information in real-time has been dramatically advanced. To the authors' knowledge, most of the current traffic information systems mainly provide general traffic flow information. Vehicle classification data, such as truck volumes that should be key inputs for good transportation planning, freight mobility analysis, roadway geometric and structural design and traffic control and operation, are often excluded or only roughly estimated. To address this problem, this paper describes the design and implementation of a traffic monitoring and information system recently developed at the University of Washington that visually conveys speed and vehicle classification information, obtained by processing real-time single-loop measurements, to the general public through the client/server computer architecture. The development of this system makes real-time monitoring of truck volume data on a freeway network possible. Due to the marked difference in many characteristics between trucks and smaller vehicles, accurate and timely truck data are of significant importance. Unfortunately, few frequent and wide-area truck data are collected with the systems currently in place. Furthermore, the systems that are capable of truck data collection are typically expensive and limited in application. For this reason, wide-area truck data are typically collected every few years, although more timely truck data are desired. There is no doubt that continuous collection of truck data is beneficial to a variety of purposes. This work presents an image processing algorithm for length-based vehicle classification using an image stream captured by an uncalibrated video camera. Although the current implementation separates vehicles based only upon length, the ultimate goal is to develop a system based upon the highway performance monitoring system guidelines. The basis of the algorithm is to relatively compare vehicle lengths to each other to estimate truck volumes and eliminate the need for complicated system calibration. The algorithm was implemented in C#, a new programming language platform developed by the Microsoft Corporation. The system test revealed that the vehicle length classifications estimated by the algorithm do indeed satisfactorily resemble the actual observations. The proposed algorithm may enable the widely installed surveillance video cameras to count classified vehicles including trucks. Optimization of signal control at isolated intersections has been an important research focus in traffic engineering over the past few years. Due to its flexibility and practicality, fully actuated control has been extensively deployed. In the conventional actuated control scheme, two important parameters, i.e., minimum and maximum green times, are arbitrarily prespecified, although it is widely recognized that they can significantly impact system operations. Previous studies have concentrated on computing these parameters using deterministic models. Due to the stochastic features of traffic arrival, such statically designated green time boundaries cannot sufficiently handle various traffic demands. To solve this problem, a stochastic model is established to dynamically optimize the minimum and maximum green times using real-time queue lengths and traffic arrival characteristics for each phase. Multiple criteria are fused and exploited as control objectives, such as avoiding cycle failures, minimizing control delays, and maximizing total traffic throughputs. Performance of the proposed algorithms is examined using a microscopic traffic simulation program, i.e., VISSIM 4.30, under various scenarios. The results show that the control system operated by the proposed algorithm produces promising improvements in system operation efficiency and fairness under various traffic demands. There is an increasing demand for use of the Global Positioning System (GPS) to navigate or track objects in the forest. However, objects near a GPS receiver antenna, such as tree leaves and branches, can reflect GPS signals and result in large position errors. Canopies in the forest will also block satellite signals and cause the GPS receiver to stop updating data. This is of practical significance for evaluating the performance of GPS in the forest environment. A field test was conducted to understand how large the position errors are and how long the position updates may be deferred under different levels of canopy densities. A digital camera was used to record the canopies over the test site. Image processing techniques, especially Otsu's algorithm, were used and the canopy density was classified into three levels. The ANOVA was used to analyze the effect of canopy density on the GPS position errors. The result shows that the GPS position errors are significantly different under different canopy density levels. The GPS data-update frequency was also analyzed, and the result indicates that the scheduled position update intervals are lengthened due to the existence of forest canopies. Although trucks move larger volumes of goods than other modes of transportation, public agencies know little about their travel patterns and how the roadway network performs for trucks. Trucking companies use data from the Global Positioning System (GPS) provided by commercial vendors to dispatch and track their equipment. This research collected GPS data from approximately 2,500 trucks in the Puget Sound, Washington, region and evaluated the feasibility of processing these data to support a statewide network performance measures program. The program monitors truck travel time and system reliability and will guide freight investment decisions by public agencies. While other studies have used a limited number of project-specific GPS devices to collect frequent location readings, which permit a fine-grained analysis of specific roadway segments, this study used data that involved less frequent readings but that were collected from a larger number of trucks for more than a year. Automated processing was used... Systems and methods for detecting and tracking objects, such as motor vehicles, within video data. The systems and method analyze video data, for example, to count objects, determine object speeds, and track the path of objects without relying on the detection and identification of background data within the captured video data. The detection system uses one or more scan lines to generate a spatio-temporal map. A spatio-temporal map is a time progression of a slice of video data representing a history of pixel data corresponding to a scan line. The detection system detects objects in the video data based on intersections of lines within the spatio-temporal map. Once the detection system has detected an object, the detection system may record the detection for counting purposes, display an indication of the object in association with the video data, determine the speed of the object, etc. A microscopic model of freeway rear-end crash risk is developed based on a modified negative binomial regression and estimated using Washington State data. Compared with most existing models, this model has two major advantages: (1) It directly considers a driver's response time distribution; and (2) it applies a new dual-impact structure accounting for the probability of both a vehicle becoming an obstacle (Po) and the following vehicle's reaction failure (Pf). The results show for example that truck percentage-mile-per-lane has a dual impact, it increases Po and decreases Pf, yielding a net decrease in rear-end crash probabilities. Urban area, curvature, off-ramp and merge, shoulder width, and merge section are factors found to increase rear-end crash probabilities. Daily vehicle miles traveled (VMT) per lane has a dual impact; it decreases Po and increases Pf, yielding a net increase, indicating for example that focusing VMT related safety improvement efforts on reducing drivers' failure to avoid crashes, such as crash-avoidance systems, is of key importance. Understanding such dual impacts is important for selecting and evaluating safety improvement plans for freeways. This article describes a LOOP detector SIMulator (LOOPSIM) system designed for traffic research and education. LOOPSIM is capable of simulating calls from loop detectors and testing algorithms executable in the controller. It can also be used to simulate different patterns of traffic streams and replay collected loop event data. With the aid of LOOPSIM, researchers and educators can designate traffic characteristics in line with the purpose of the testing or teaching. \\u00a9 2008 Wiley Periodicals, Inc. Comput Appl Eng Educ 16: 45\\u201354, 2008; Published online in Wiley InterScience (www.interscience.wiley.com); DOI 10.1002/cae.20117 A detector event data collection (DEDAC) system is proposed. This system is able to sample loop actuations with sampling rates of 60 Hz or higher and then save, process and present the collected event data in real time without interfering with the detector controllers normal operation. The authors have developed a stand-alone Windows program for performing real-time high-frequency loop event data collection. A system reliability test and a field application indicate that the system has the capability of collecting real-time detector event data at a high sampling rate (60 Hz or higher). Additionally, this system makes real-time loop data quality evaluation, loop malfunction identification, and loop error correction feasible Two types of animal-vehicle collision (AVC) data are commonly adopted for AVC-related risk analysis research: reported AVC data and carcass removal data. One issue with these two data sets is that they were found to have significant discrepancies by previous studies. In order to model these two types of data together and provide a better understanding of highway AVCs, this study adopts a diagonal inflated bivariate Poisson regression method, an inflated version of bivariate Poisson regression model, to fit the reported AVC and carcass removal data sets collected in Washington State during 2002-2006. The diagonal inflated bivariate Poisson model not only can model paired data with correlation, but also handle under- or over-dispersed data sets as well. Compared with three other types of models, double Poisson, bivariate Poisson, and zero-inflated double Poisson, the diagonal inflated bivariate Poisson model demonstrates its capability of fitting two data sets with remarkable overlapping portions resulting from the same stochastic process. Therefore, the diagonal inflated bivariate Poisson model provides researchers a new approach to investigating AVCs from a different perspective involving the three distribution parameters (\\u03bb(1), \\u03bb(2) and \\u03bb(3)). The modeling results show the impacts of traffic elements, geometric design and geographic characteristics on the occurrences of both reported AVC and carcass removal data. It is found that the increase of some associated factors, such as speed limit, annual average daily traffic, and shoulder width, will increase the numbers of reported AVCs and carcass removals. Conversely, the presence of some geometric factors, such as rolling and mountainous terrain, will decrease the number of reported AVCs. Language: en To improve the level of service for Community Transit (CT) buses, the South Snohomish Regional Transit Signal Priority (SS-RTSP) project has been launched. To understand the overall benefit of this project, the SS-RTSP system was tested and evaluated after the completion of the hardware and software installations on the 164th Street SW street corridor (phase-one) and the SR-99 corridor (phase-two) in Snohomish County, Washington State. In this study, impacts of the SS-RTSP system on both transit and local traffic operations were quantitatively evaluated based on field observed data. Simulation models were also built and calibrated to compute measures of effectiveness that cannot be obtained from field-observed data. With simulation models and field observed data, the impacts of the SS-RTSP system on both transit and local traffic operations were quantitatively evaluated. Our evaluation results showed that the SS-RTSP system introduced remarkable benefits to transit vehicles, with insignificant negative impacts to local traffic on cross streets. The overall impact of the SS-RTSP system on local traffic of each entire intersection was not statistically significant at the p=0.05 level. To improve the performance of the current SS-RTSP system, more transit vehicles can be made TSP eligible. The average number of granted TSP trips was only 16.96 per day per intersection during the phase-one test and 14.40 during phase-two test. Considering that negative impacts of the SS-RTSP on local traffic were not significant, more transit trips can be granted with proper TSP treatments to generate more benefits from the SS-RTSP system. Also, near-side bus stops were found to introduce extra transit delays when TSP was provided under certain conditions. Our recommendation is that the TSP treatment of extended green be disabled at intersections with near-side bus stops to avoid introducing negative impacts on transit vehicles. Real-time speed and truck data are important inputs for modern freeway traffic control and management systems. However, these data are not directly measurable by single-loop detectors. Although dual-loop detectors provide speeds and classified vehicle volumes, there are too few them on our current freeway systems to meet the practical needs of advanced traffic management systems. This makes it extremely desirable to develop appropriate algorithms to calculate speed and truck volume from single-loop outputs or from video data. To obtain quality estimates of traffic speed and truck volume data, several algorithms were developed and implemented in this study. These algorithms are (1) a speed estimation algorithm based on the region growing mechanism and single-loop measurements; (2) a set of computer vision-based algorithms for extracting background images from a video sequence, detecting the presence of vehicles, identifying and removing shadows, and calculating pixel-based vehicle lengths for classification; and (3) a speed estimation algorithm that uses paired video and single- loop sensor inputs. These algorithms were implemented in three distinct computer applications. Field-collected video and loop detector data were used to test the algorithms. Our test results indicated that quality speed and truck volume data can be estimated with the proposed algorithms by using single-loop data, video data, or both video and single-loop data. The Video-based Vehicle Detection and Classification (VVDC) system, based on proposed video image processing algorithms, provides a cost-effective solution for automatic traffic data collection with surveillance video cameras. For locations with both video and single- loop sensors, speed estimates can be improved by combining video data with single-loop data. A generalized nonlinear model (GNM)-based approach for modeling highway rear-end crash risk is formulated using Washington State traffic safety data. Previous studies majorly focused on causal factor identification and crash risk modeling using Generalized linear Models (GLMs), such as Poisson regression, Logistic regression, etc. However, their basic assumption of a generalized linear relationship between the dependent variable (for example, crash rate) and independent variables (for example, contribute factors to crashes) established via a link function can be often violated in reality. Consequently, the GLM-based modeling results could provide biased findings and conclusions. In this research, a GNM-based approach is developed to utilize a nonlinear regression function to better elaborate non-monotonic relationships between the independent and dependent variables using the rear end accident data collected from 10 highway routes from 2002 through 2006. The results show for example that truck percentage and grade have a parabolic impact: they increase crash risks initially, but decrease them after the certain thresholds. Such non-monotonic relationships cannot be captured by regular GLMs which further demonstrate the flexibility of GNM-based approaches in the nonlinear relationship among data and providing more reasonable explanations. The superior GNM-based model interpretations help better understand the parabolic impacts of some specific contributing factors for selecting and evaluating rear-end crash safety improvement plans. Freeway incidents not only threaten travelers\\u2019 safety but also cause severe congestion. Incident-induced delay (IID) refers to the extra travel delay resulting from incidents on top of the recurrent congestion. Quantifying IID would help people better understand the real cost of incidents, maximize the benefit-cost ratio of investment on incident remedy actions, and develop active traffic management and integrated corridor management strategies. By combining a modified queuing diagram and short-term traffic flow forecasting techniques, this study proposes an approach to estimate the temporal IID for a roadway section, given that the incidents occurs between two traffic flow detectors. The approach separates IID from the total travel delay, estimates IID for each individual incident, and only takes volume as input for IID quantification, avoiding using speed data that are widely involved in previous algorithms yet are less available or prone to poor data quality. Therefore, this approach can be easily deployed to broader ranges where only volume data are available. To verify its estimation accuracy, this study captures two incident videos and extracts ground-truth IID data, which is rarely done by previous studies. The verification shows that the IID estimation errors of the proposed approach are within 6% for both cases. The approach has been implemented in a Web-based system, which enables quick, convenient, and reliable freeway IID estimation in the Puget Sound region in the state of Washington. Pedestrian and cyclist crossing characteristics are important for the design of urban intersections and signalized crossings. Parameters such as waiting time, crossing time, and arrival rate are key variables for describing pedestrian characteristics and improving crossing designs and signal timing plans. Manually collecting such data is often extremely labor intensive. Therefore, an automated computer-vision-based approach is introduced for collecting these parameters in real time with ordinary video cameras. Broadly defined pedestrian objects, including bicyclists and other nonmotorized modes, are extracted by means of the background subtraction technique and tracked through an inherent cost characteristic function in conjunction with an \\u03b1-\\u03b2-filter. The waiting-zone concept introduced helps provide robust pedestrian tracking initialization and parameter extraction. The proposed approach is implemented in a pedestrian tracking (PedTrack) system by using Microsoft Visual C++. Tested with real video data f... Though there have been extensive studies on AS and ISP level topologies, BGP path stability and overall path variations and churn, there is comparatively little data on the behavior of overall reachability in the Internet. We can explain reachability as a measure of path robustness over time, and thus as a significant measure of the general quality of the infrastructure. Our results show that BGP views across different collector route servers, through our period of study, are very homogeneous, and that variations in overall reachability are relatively small. We interpret these observations as indications that overall reachability is robust, and found it to be particularly good in G7 countries, perhaps due to a better communications infrastructure. Web-based mapping technologies have been utilized for traffic information systems. However, most such systems are for freeways and very few of them focus on arterials or urban streets. This paper presents a real-time Google-map-based Arterial Traffic Information (GATI) system for urban streets in the City of Bellevue, Washington State. Open source web tools and emerging web technologies, such as Ajax, are used in implementing the system to ensure its performance and minimize its cost. Convenient administrative functions are enabled through advanced database design and the Model-View-Controller (MVC) application. This GATI system, though presented and demonstrated by using Bellevue's data, is a general technology that can be applied to any arterial network. This study evaluated the accuracy of truck data collected by dual-loop detectors on Seattle area freeways. The objectives of the study were to 1) quantitatively evaluate the accuracy of a representative sample of dual-loop measurements of vehicle volumes and vehicle classifications on the Seattle FLOW system, 2) identify the types and causes of dual-loop data inaccuracies, and 3) recommend appropriate methods for improving the quality of real-time dual-loop measurements. One representative station with four dual-loop detectors was selected for an extended error analysis. The detector measurements were compared to ground truth data collected via a video image system. Data for 20-second intervals were compared for peak and off-peak periods. Since there was no documentation on the settings of the dual-loop system clock, an independent analysis was conducted to synchronize the dual-loop and video image systems clocks before analysis. In general, three dual-loop error types were identified through comparison of dual-loop and video ground-truth data: (1) underestimation of vehicle volumes, (2) incorrect assignment of Bin 3 vehicles to Bin 4, (3) incorrect assignment of Bin 2 vehicles to Bins 1 and 3. Dual-loop measurement errors such as those described may be due to defects in system hardware, software, or the underlying measurement algorithm. Because hardware errors were virtually eliminated as a factor in the current study, the serious errors that still occurred were most likely due to defects in the underlying dual-loop algorithm or in the implementation program. Therefore, to radically improve the quality of dual-loop data, and thus the quality of real-time truck data on the FLOW system, the Washington State Department of Transportation dual-loop algorithm and its corresponding implementation code should be the emphasis of future research. :\\u00a0Signal cycle failure (or overflow) is an interrupted traffic condition in which a number of queued vehicles are unable to depart due to insufficient capacity during a signal cycle. Cycle failure detection is essential for identifying signal control problems at intersections. However, typical traffic sensors do not have the capability of capturing cycle failures. In this article, we introduce an algorithm for traffic signal cycle failure detection using video image processing. A cycle failure for a particular movement occurs when at least one vehicle must wait through more than one red light to complete the intended movement. The proposed cycle failure algorithm was implemented using Microsoft Visual C#. The system was tested with field data at different locations and time periods. The test results show that the algorithm works favorably: the system captured all the cycle failures and generated only three false alarms, which is approximately 0.9% of the total cycles tested. The Washington State Department of Transportation (WSDOT) has a loop detection system on its Greater Seattle freeway network to provide real-time traffic data. The dual-loop detectors installed in the system are used to measure vehicle lengths and then classify each detected vehicle into one of four categories according to its length. The dual loop's capability of measuring vehicle length makes the loop detection system a potential real-time truck data source for freight movement studies because truck volume estimates by basic length category can be developed from the vehicle length measurements produced by the dual-loop detectors. However, a previous study found that the dual-loop detectors were consistently underreporting truck volumes, whereas the single-loop detectors were consistently overcounting vehicle volumes. As an extension of the previous study, the research project described here investigated possible causes of loop errors under nonforced-flow traffic conditions. A new dual-loop algorithm tha... The capability of measuring vehicle lengths makes dual-loop detectors a potential real-time truck data source for freight movement studies. However, a previous study found that the dual-loop detection system of the Washington State Department of Transportation (WSDOT) was not consistently reporting accurate truck volumes because of its sensitivity setting problems. Specifically, the sensitivity problems found were (1) sensitivity discrepancies between the two single loops that form a dual-loop detector; and (2) unsuitable sensitivity level settings for both single loops even when discrepancies weren't significant. Both problems can result in erroneous vehicle length estimates and, consequently, inaccurate truck counts. As an extension of the previous study, this research project developed an algorithm for the identification and correction of such loop sensitivity problems. The algorithm identifies dual-loop sensitivity problems using individual vehicle information extracted from high-resolution loop event data and corrects dual-loop sensitivities through a two-step procedure: 1) remove the sensitivity discrepancy between the two single loops and 2) adjust their sensitivities to the appropriate level. The algorithm was also implemented in a computer application named the Advanced Loop Event Data Analyzer (ALEDA) system for convenient usage. Elimination of dual-loop sensitivity problems significantly enhances the reliability of the dual-loop detection system and improves the quality of truck volume data. The findings and products from this study will help the WSDOT to obtain more accurate speed and truck volume data from the existing dual-loop detectors. Dramatically increasing travel demands and insufficient traffic facilities have induced severe traffic congestion. High-occupancy toll (HOT) lane operation has been proposed as one of the most applicable and acceptable countermeasures against freeway congestion. With balanced pricing and vehicle occupancy constraints, HOT lane operations can realize the optimal traffic allocation and enhance overall infrastructure efficiency. However, few previous studies have concentrated on optimal tolling strategies. Two major problems with inferior tolling strategies degrade HOT lane system performance. First, an undersensitive tolling algorithm is incapable of handling the hysteretic properties of traffic systems and may cause severe response delays. Second, oversensitive characteristics of imperfect tolling strategies may cause unfavorable flow fluctuations in HOT and general-purpose lanes that disrupt traffic operations. A new feedback-based tolling algorithm to optimize HOT lane operations addresses these problems... Traffic accidents cause loss of life and property. Proper identification of accident causal factors is essential for composing countermeasures against traffic accidents and reducing related costs. However, two-lane rural roads have distinctive roadway characteristics compared with other types of roads. In order to find cost-effective countermeasures and prioritize roadway safety improvement plans for two-lane rural roadways, a better understanding of the relationship between accident risk and respective characteristics is necessary. This study focuses on accident analysis of two-lane rural roads in Washington State. Six representative state routes (SRs), SR-2, SR-12, SR-20, SR-21, SR-97 and SR-101, are selected as study routes based on their location, length, and geometric characteristics. Along with the six-year (1999~2004) accident data from the Highway Safety Information System (HSIS), roadway video image data and geographical information system data retrieved from Washington State Department of Transportation are employed in this study. Econometric modeling methods are utilized to identify accident causal factors and evaluate their impacts on accident risk at roadway segments and intersections, respectively. Results from the statistical analyses and accident risk models not only help identify accident causal factors, but also provide valuable insights for developing countermeasures against two-lane rural road traffic accidents. AbstractAnimal-vehicle collisions (AVCs) cause hundreds of human and wildlife animal fatalities and tens of thousands of human and wildlife animal injuries in North America. It is estimated that AVCs cause more than $1 billion in property damage each year in the United States. Further research efforts are needed to identify effective countermeasures against AVCs. Two types of data have been widely used in AVC-related research: collision reported (CRpt) data and carcass removal (CR) data. However, previous studies showed that these two data set are significantly different, implying the incompleteness in either set of the data. Hence, this study aims at developing an algorithm to combine these two types of data to improve the completeness of data for AVC studies. A fuzzy logic\\u2013based data mapping algorithm is proposed to identify matching data from the two data sets so that data are not overcounted when combining the two data sets. The membership functions of the fuzzy logic algorithm are determined by a sur... ABSTRACTA number of approaches have been developed for analysing incident clearance time data and investigating the effects of different explanatory variables on clearance time. Among these methods, hazard-based duration models (i.e. proportional hazard and accelerated failure time (AFT) models) have been extensively used. The finite mixture model is an alternative approach in survival data analysis, and offers greater flexibility in describing different shapes of the hazard function. Additionally, the finite mixture model assumes that the incident clearance time data set contains distinct subpopulations, and it allows the effects of explanatory variables to vary between different subpopulations. In this study, a g-component mixture model is applied to analyse incident clearance time. To demonstrate advantages of the proposed finite mixture model framework, incident clearance time data collected on freeway sections in Seattle, Washington State are analysed. Estimation and prediction results from the propo... A novel method for detecting the average speed of traffic from non-stationary aerial video is presented. The method first extracts interest points from a pair of frames and performs interest point tracking with an optical flow algorithm. The output of the optical flow is a set of motion vectors which are k-means clustered in velocity space. The centers of the clusters correspond to the average velocities of traffic and the background, and are used to determine the speed of traffic relative to the background. The proposed method is tested on a 70-frame test sequence of UAV aerial video, and achieves an average error for speed estimates of less than 12%. Expedient incident detection and understanding are important in traffic management and control. Social media as important information venues have immense value for increasing an awareness of traffic incidents. In this paper, an attempt is made to assess the potential of using harvested social media for traffic incident detection. Twitter in Seattle, Washington, was chosen as a representative sample environment for this work. A hybrid mechanism based on latent Dirichlet allocation and document clustering was proposed to model incident-level semantic information, while spatial point pattern analysis was applied to explore the spatial patterns and to assess the spatial dependence between incident-topic tweets and traffic incidents. A global Monte Carlo K-test indicated that the incident-topic tweets were significantly clustered at different scales up to 600 m. The nearest neighbor clutter removal method was used to separate feature tweet points from clutter; then a density-based algorithm successfully detect... This paper proposes a two-stage algorithm to simultaneously estimate origin-destination (OD) matrix, link choice proportion, and dispersion parameter using partial traffic counts in a congested network. A non-linear optimization model is developed which incorporates a dynamic dispersion parameter, followed by a two-stage algorithm in which Generalized Least Squares (GLS) estimation and a Stochastic User Equilibrium (SUE) assignment model are iteratively applied until the convergence is reached. To evaluate the performance of the algorithm, the proposed approach is implemented in a hypothetical network using input data with high error, and tested under a range of variation coefficients. The root mean squared error (RMSE) of the estimated OD demand and link flows are used to evaluate the model estimation results. The results indicate that the estimated dispersion parameter theta is insensitive to the choice of variation coefficients. The proposed approach is shown to outperform two established OD estimation methods and produce parameter estimates that are close to the ground truth. In addition, the proposed approach is applied to an empirical network in Seattle, WA to validate the robustness and practicality of this methodology. In summary, this study proposes and evaluates an innovative computational approach to accurately estimate OD matrices using link-level traffic flow data, and provides useful insight for optimal parameter selection in modeling travelers\\u2019 route choice behavior. Recently, a number of short-term speed prediction approaches have been developed, in which most algorithms are based on machine learning and statistical theory. This paper examined the multistep ahead prediction performance of eight different models using the 2-minute travel speed data collected from three Remote Traffic Microwave Sensors located on a southbound segment of 4th ring road in Beijing City. Specifically, we consider five machine learning methods: Back Propagation Neural Network (BPNN), nonlinear autoregressive model with exogenous inputs neural network (NARXNN), support vector machine with radial basis function as kernel function (SVM-RBF), Support Vector Machine with Linear Function (SVM-LIN), and Multilinear Regression (MLR) as candidate. Three statistical models are also selected: Autoregressive Integrated Moving Average (ARIMA), Vector Autoregression (VAR), and Space-Time (ST) model. From the prediction results, we find the following meaningful results: () the prediction accuracy of speed deteriorates as the prediction time steps increase for all models; () the BPNN, NARXNN, and SVM-RBF can clearly outperform two traditional statistical models: ARIMA and VAR; () the prediction performance of ANN is superior to that of SVM and MLR; () as time step increases, the ST model can consistently provide the lowest MAE comparing with ARIMA and VAR. Travel time is an important measurement used to evaluate the extent of congestion within road networks. This paper presents a new method to estimate the travel time based on an evolving fuzzy neural inference system. The input variables in the system are traffic flow data (volume, occupancy, and speed) collected from loop detectors located at points both upstream and downstream of a given link, and the output variable is the link travel time. A first order Takagi-Sugeno fuzzy rule set is used to complete the inference. For training the evolving fuzzy neural network (EFNN), two learning processes are proposed: (1) a K-means method is employed to partition input samples into different clusters, and a Gaussian fuzzy membership function is designed for each cluster to measure the membership degree of samples to the cluster centers. As the number of input samples increases, the cluster centers are modified and membership functions are also updated; (2) a weighted recursive least squares estimator is used to optimize the parameters of the linear functions in the Takagi-Sugeno type fuzzy rules. Testing datasets consisting of actual and simulated data are used to test the proposed method. Three common criteria including mean absolute error (MAE), root mean square error (RMSE), and mean absolute relative error (MARE) are utilized to evaluate the estimation performance. Estimation results demonstrate the accuracy and effectiveness of the EFNN method through comparison with existing methods including: multiple linear regression (MLR), instantaneous model (IM), linear model (LM), neural network (NN), and cumulative plots (CP). A new method based on complex network theory is proposed to analyze traffic flow time series in different states. We use the data collected from loop detectors on freeway to establish traffic flow model and classify the flow into three states based on K-means method. We then introduced two widely used methods to convert time series into networks: phase space reconstruction and visibility graph. Furthermore, in phase space reconstruction, we discuss how to determine delay time constant and embedding dimension and how to select optimal critical threshold in terms of cumulative degree distribution. In the visibility graph, we design a method to construct network from multi-variables time series based on logical OR. Finally, we study and compare the statistic features of the networks converted from original traffic time series in three states based on phase space and visibility by using the degree distribution, network structure, correlation of the cluster coefficient to betweenness and degree\\u2013degree correlation. Intersection control delay is one of the most important performance indicators for evaluating the traffic level of service and intersection capacities. In current traffic data detection infrastructure, control delay is not directly measurable. Although video-based detection approaches have been applied, their detection accuracy and reliability are constrained by application conditions and detection environments. Manual control delay data collection is labour-intensive, tedious, and time-consuming. High-resolution global positioning system (GPS) data provide an effective means of estimating control delays at intersections, but computationally intensive algorithms and hardware support are needed to handle a large network and impede their wide applications. In this study, a computationally cost-effective control delay estimation algorithm is developed based on low-resolution GPS-based transit bus trajectory data. Transit bus travelling behaviour is formulated to facilitate delay estimation. The effectiveness of the proposed algorithm is examined and verified by the field data and the results indicate that the proposed algorithm provides accurate and reliable control delay estimation at intersections under various conditions. A facility comprising methods and systems for discovering travel patterns based at least in part on data collected from a plurality of dynamic and/or mobile sensor devices is disclosed. The mobile sensor devices sense or detect the presence of nearby devices and record information about each detection, such as the location, the date and time, and an identifier of the detected device and provide this information to the facility. Thus, the facility can leverage the communication capabilities of existing mobile devices and networks to provide a broad, wide-ranging, scalable sensor network that detects the presence of individual devices. By recording and analyzing the encounter data received from various mobile sensor devices, the facility can ascertain or estimate the path of devices and their associated users. Given the availability of mobile devices and mobile networks, the disclosed techniques can be deployed without a substantial investment in infrastructure and maintenance. This paper proposes a two-layer decision framework to model taxi drivers' customer-search behaviors within urban areas. The first layer models taxi drivers' pickup location choice decisions, and a Huff model is used to describe the attractiveness of pickup locations. Then, a path size logit (PSL) model is used in the second layer to analyze route choice behaviors considering information such as path size, path distance, travel time, and intersection delay. Global Positioning System data are collected from more than 36\\u2009000 taxis in Beijing, China, at the interval of 30 s during six months. The Xidan district with a large shopping center is selected to validate the proposed model. Path travel time is estimated based on probe taxi vehicles on the network. The validation results show that the proposed Huff model achieved high accuracy to estimate drivers' pickup location choices. The PSL outperforms traditional multinomial logit in modeling drivers' route choice behaviors. The findings of this paper can help understand taxi drivers' customer searching decisions and provide strategies to improve the system services. This study explores urban mobility from a network-based perspective. The data samples used in study were collected from more than 1100 taxi drivers during a half year period in the city of Harbin in China. We extract trips from the original dataset and analyze operational efficiency. Then, by constructing travel networks based on occupied and vacant taxi trips, we calculate some statistical properties of the network such as degree, strength, edge weight, betweenness, clustering coefficient and network structure entropy. Analysis of such properties allows for a deep exploration of travel mobility. We also analyze the correlation between strength and betweenness to evaluate the importance of nodes in the network. Furthermore, two traditional community detection algorithms: the Louvain method and the visualization of similarities (VOS) method are applied to divide traffic zones in the mainland area of Harbin city. Two indices, the Rand index (RI) and the Fowkles\\u2013Mallows index (FMI) are adopted to evaluate recognition performance, which shows the similarity between administrative division and results from the algorithms. Finally, a dilatation index based on the weighted average distance among trips is applied to analyze the spatial structure of an urban city. Furthermore, hotspots are identified from local density of locations with different thresholds as determined by the Lorenz curve. Abstract Maritime accidents have the potential to cause significant financial loss, injury, and damage to the environment. One approach to investigating maritime safety is to focus on near misses, that is, situations which did not lead to an accident but where an accident was narrowly avoided. Based on the principles of the traffic conflict technique, which ranks traffic encounters through a conflict severity hierarchy, this paper proposes a novel model for screening maritime traffic data for near miss ship-ship encounters, particularly for open sea and coastal restricted sea areas. Compared to previous methods, the proposed method has a greater specificity, leaving fewer possible near miss cases to be assessed by navigational experts in a contextualised traffic setting. This is achieved by including the effect of ship size through a ship domain, and by better accounting for the criticality of the encounter direction through the Minimum Distance To Collision concept compared to earlier proposed models. The factors included in the model and their relation are based on expert judgments and using knowledge from previous studies. Model parameters are derived from AIS data points from a reference encounter situation dataset. The developed model has been applied to traffic data from the Northern Baltic Sea. The model is subjected to a number of validity tests, the results of which suggest that the model is adequate for ranking and prioritizing encounters for further assessment in an expert judgment phase to identify near misses. Thus, it establishes a method to enable subsequent research into the validity of near miss information to make statements of maritime safety in relation to collision accidents.\",\n",
            "  \"499017155\": \"This paper describes work that explores how tools can help change agents in software organizations incorporate new software engineering tools and methods more effectively. A prototype expert system was built to assist change agents in gathering information and planning tasks in a software engineering adoption and implementation effort. The knowledge engineering process for this system helped to integrate and translate research and practice from software engineering technology adoption and implementation within the field of technology transfer and diffusion of innovation for software change agents. Engineering disciplines are characterised by the use of mature knowledge by means of which they can achieve predictable results. Unfortunately, the type of knowledge used in software engineering can be considered to be of a relatively low maturity, and developers are guided by intuition, fashion or market-speak rather than by facts or undisputed statements proper to an engineering discipline. Testing techniques determine different criteria for selecting the test cases that will be used as input to the system under examination, which means that an effective and efficient selection of test cases conditions the success of the tests. The knowledge for selecting testing techniques should come from studies that empirically justify the benefits and application conditions of the different techniques. This paper analyses the maturity level of the knowledge about testing techniques by examining existing empirical studies about these techniques. For this purpose, we classify testing technique knowledge according to four categories. Context: The majority of the empirical studies on Test-driven development (TDD) are concerned with verifying or refuting the effectiveness of the technique over a traditional approach, and they tend to neglect whether the subjects possess the necessary skills to apply TDD, though they argue such skills are necessary.Objective: We evaluate a set of minimal, a priori and in process skills necessary to apply TDD. We determine whether variations in external quality (i.e., number of defects) and productivity (i.e., number of features implemented) can be associated with different clusters of the TDD skills' set.Method: We executed a quasi-experiment involving 30 practitioners from industry. We first grouped the participants according to their TDD skills' set (consisting of a priori experience on programming and testing as well as in-process TDD conformance) into three levels (Low-Medium-High) using k-means clustering. We then applied ANOVA to compare the clusters in terms of external quality and productivity, and conducted post-hoc pairwise analysis.Results: We did not observe a statistically significant difference between the clusters either for external software quality ( F ( 2 , 27 = 1.44 , p = . 260 ), or productivity ( F ( 2 , 27 ) = 3.02 , p = . 065 ). However, the analysis of the effect sizes and their confidence intervals shows that the TDD skills' set is a factor that could account for up to 28% of the external quality, and 38% for productivity.Conclusion: We have reason to conclude that focusing on the improvement of TDD skills' set investigated in this study could benefit software developers in improving their baseline productivity and the external quality of the code they produce. However, replications are needed to overcome the issues related with the statistical power of this study. We suggest practical insights for future work to investigate the phenomenon further. Like any other quality attribute, usability imposes specific constraints on software components. We have empirically corroborated that software design and usability are related. Therefore usability features impacting design need to be considered from a functional viewpoint at requirements stage. But discovering and documenting usability features is likely to be beyond the usability knowledge of most requirements engineers, developers and users. We propose an approach based on developing specific guidelines that capitalize upon key elements recurrently intervening in the usability features elicitation and specification. Developers can use these guidelines to ask the appropriate questions and capture usability requirements information. This practice should lead to build software with higher usability. Software usability is a quality attribute found in a number of classifications (IEEE, 1998), (ISO9126, 1991), (Boehm, 1978). Nielsen gave one of the most well-known descriptions related to software system learnability and memorability, efficiency of use, ability to avoid and manage user errors, and user satisfaction (Nielsen, 1993). In spite of the relevance of usability in software development it is still insufficient in most software systems (Seffah, 2004) (Bias, 2005). Techniques from the HCI (Human-Computer Interaction) field have been used for the development of usable software products for a long time, but their use is often not integrated with software engineering practices. In this work we describe an approach for bridging the gap between software engineering and HCI, by offering orientation to software practitioners on the application of HCI techniques and activities. For this purpose, we have carried out a survey in HCI literature to define the activities in a user-centered development process, and to select the HCI techniques that are more appropriate for integration into software engineering practice. Software development organizations wanting to introduce usability practices into their defined software process have to undertake laborious efforts for that purpose, since, for the time being, there exists a lack of reference model or framework which indicates where and how in the software process usability needs to be considered. They also have to overcome the important differences between HCI (Human-Computer Interaction) and SE (Software Engineering) in terminology and approach to process definition. We offer developers who have the objective of integrating usability practices into their software process, a framework that characterizes 35 selected HCI techniques in relation to six relevant criteria from a SE viewpoint, and organizes them according to the kind of activities in the development process where they may be applied, and to the best moment of application in an iterative life cycle. The only requirement for the existing software process is to be based on an iterative approach. User-centered design is not just about building nice-looking and usable interfaces, and software development is not just about implementing functionality that supports user tasks. This paper aims to build a tighter fit between human-computer interaction and software engineering practices and research by addressing what software and usability engineering practitioners can learn from each other regarding the impact of usability on software development. More specifically we aim to support usability people in helping developers to elicit requirements that can incorporate usability functions into software development. The paper shows what type of impact usability has on software models and suggests how this impact can be dealt with at the requirements elicitation and specification stages of the development cycle. Controlled experiments in laboratory settings are relatively commonplace in software engineering, but experiments in industry are thin on the ground. Of the few existing cases, most are 1-1 (running one experiment at one company), just a few are n-1 (running n experiments at one company) and still fewer are 1-n (running one and the same experiment at n companies). In this paper we report the difficulties that we experienced running the same experiment at several companies. We ran the same experiment in five different settings at three companies, and the results were transferred to each company so that they could exploit the resulting evidence in their decision-making process. We have found that: 1) it was relatively easy to get companies involved; 2) they did not cooperate as much as they had agreed to in the project proposal; 3) our industrial environments imposed many more constraints on the experimental design than laboratory environments; 4) professionals were less motivated than students; 5) the reliability of the results could be compromised by subject characteristics and behaviour; and 6) experiment findings could not be transferred using just the standard reporting guidelines that are used for scientific articles. Testing technique-related empirical studies have been performed for 25 years. We have managed to accumulate a fair number of experiments in this time, which might lead us to think that we now could have a sizeable empirically backed body of knowledge (BoK) on testing techniques. However, the experiments in this field have some flaws, and, consequently, the empirical BoK we have on testing techniques is far from solid. In this paper, we use the results of a survey that we did on empirical testing techniques studies to identify and discuss solutions that could lead to the formation of a solid empirical BoK. The solutions are related to two fundamental experimental issues: (1) the rigorousness of the experimental design and analysis, and (2) the need for a series of community-wide agreements to coordinate empirical research and assure that studies ratify and complement each other. Conventional software (CS) and knowledge-based software (KBS) validation and verification have been traditionally accepted to be radically different disciplines with nothing in common. However, the differences between CS and KBS should not stop efforts being made to develop a general framework for evaluation, covering validation and verification of both types of software. A review of the concepts of validation, verification and testing in both software engineering and knowledge engineering may lead to a better understanding of the objectives of software evaluation during software development and enable a framework for the evaluation of both CS and KBS to be proposed. This does not mean that the same techniques should be applied to both types of software. An evaluation framework should establish the types of evaluation to be conducted, when they are to be conducted and what techniques are to be used. So, even if it is common to both types of software, the framework may contain techniques usable in both cases and techniques peculiar to one type of software. All meta-analyses should include a heterogeneity analysis. Even so, it is not easy to decide whether a set of studies are homogeneous or heterogeneous because of the low statistical power of the statistics used (usually the Q test). Objective: Determine a set of rules enabling SE researchers to find out, based on the characteristics of the experiments to be aggregated, whether or not it is feasible to accurately detect heterogeneity. Method: Evaluate the statistical power of heterogeneity detection methods using a Monte Carlo simulation process. Results: The Q test is not powerful when the meta-analysis contains up to a total of about 200 experimental subjects and the effect size difference is less than 1. Conclusions: The Q test cannot be used as a decision-making criterion for meta-analysis in small sample settings like SE. Random effects models should be used instead of fixed effects models. Caution should be exercised when applying Q test-mediated decomposition into subgroups. Acuna, Silvia T.; Gomez, Marta N.; Hannay, Jo Erskine; Juristo, Natalia; Pfahl, Dietmar. Are team personality and climate related to satisfaction and software quality? Aggregating results from a twice replicated experiment. Information and Software Technology 2015 ;Volum 57. s. 141-156 Abstract -- Background: Most of the experiments in software engineering (SE) employ students as subjects. This raises concerns about the realism of the results acquired through students and adaptability of the results to software industry. Aim: We compare students and professionals to understand how well students represent professionals as experimental subjects in SE research. Method: The comparison was made in the context of two test-driven development experiments conducted with students in an academic setting and with professionals in a software organization. We measured the code quality of several tasks implemented by both subject groups and checked whether students and professionals perform similarly in terms of code quality metrics. Results: Except for minor differences, neither of the subject groups is better than the other. Professionals produce larger, yet less complex, methods when they use their traditional development approach, whereas both subject groups perform similarly when they apply a new approach for the first time. Conclusion: Given a carefully scoped experiment on a development approach that is new to both students and professionals, similar performances are observed. Further investigation is necessary to analyze the effects of subject demographics and level of experience on the results of SE experiments. This tutorial examines the relationship between usability and the user interface and discusses how the usability process follows a design-evaluate-redesign cycle. It also discusses some management issues an organization must face when applying usability techniques. One of the main problems in software testing is the development of a suitable set of test cases so that the effectiveness of the test is maximised with a minimum number of test cases. A lot of testing techniques are now available for developing test cases. However, some of them are misused, others are never used and only a few are applied again and again. When developers have to decide what testing techniques(s) they should use in a project, they have little (if any) experiential information about the available testing techniques, their usefulness and, in general, how suited they are to the project. This paper presents the results of developing a characterization scheme for test technique selection. When instantiated for different techniques, the scheme should provide developers with enough information for choosing the best suited to their project. Thus, their decisions would be based on sound knowledge of the techniques, instead of perceptions, suppositions and assumptions. Gathering evidence in any discipline is a lengthy procedure, requiring experimentation and empirical confirmation to transform information from mere opinion to undisputed fact. Software engineering is a relatively young field and experimental SE is even younger, so undisputed facts are few and far between. Nevertheless, ESE's relevance is growing because experimental results can help practitioners make better decisions. We have aggregated results from unit-testing experiments with the aim of identifying information with some experimental basis that might help practitioners make decisions. Most of the experiments focus on two important characteristics of testing techniques: effectiveness and efficiency. Some other experiments study the quality of test-case sets according to different criteria In no science or engineering discipline does it make sense to speak of isolated experiments. The results of a single experiment cannot be viewed as representative of the underlying reality. Experiment replication is the repetition of an experiment to double-check its results. Multiple replications of an experiment increase the confidence in its results. Software engineering has tried its hand at the identical (exact) replication of experiments in the way of the natural sciences (physics, chemistry, etc.). After numerous attempts over the years, apart from experiments replicated by the same researchers at the same site, no exact replications have yet been achieved. One key reason for this is the complexity of the software development setting, which prevents the many experimental conditions from being identically reproduced. This paper reports research into whether non-exact replications can be of any use. We propose a process aimed at researchers running non-exact replications. Researchers enacting this process will be able to identify new variables that are possibly having an effect on experiment results. The process consists of four phases: replication definition and planning, replication operation and analysis, replication interpretation, and analysis of the replication's contribution. To test the effectiveness of the proposed process, we have conducted a multiple-case study, revealing the variables learned from two different replications of an experiment. Software and knowledge engineering are increasingly converging into a single life-cycle, as the two engineering disciplines are studied in more depth, and increasingly larger systems are developed in the two fields. In this article, the authors advocate a conical-spiral type life cycle, arranged in two dimensions (the spiral) for the development of software engineering (SE) systems and three dimensions (the conical-spiral) for the knowledge engineering (KE) life cycle. A conventional example for the overall personnel management of a company is also presented, showing how the two branches of engineering are essential and complementary in solving important problems. Context: The software engineering community is becoming more aware of the need for experimental replications. In spite of the importance of this topic, there is still much inconsistency in the terminology used to describe replications. Objective: Understand the perspectives of empirical researchers about various terms used to characterize replications and propose a consistent taxonomy of terms. Method: A survey followed by plenary discussion during the 2013 International Software Engineering Research Network meeting. Results: We propose a taxonomy which consolidates the disparate terminology. This taxonomy had a high level of agreement among workshop attendees. Conclusion: Consistent terminology is important for any field to progress. This work is the first step in that direction. Additional study and discussion is still necessary. One of object orientation's main limitations is the object-oriented analysis process's immaturity. This article proposes an approach that is based on using linguistic information from information specifications to apply during this process. Our method helps to analyze this information semantically and syntactically and employs a semi-formal procedure to extract an object-oriented system's components. Abstract Context Nowadays, there are sound methods and tools which implement the Model-Driven Development approach (MDD) satisfactorily. However, MDD approaches focus on representing and generating code that represents functionality, behaviour and persistence, putting the interaction, and more specifically the usability, in a second place. If we aim to include usability features in a system developed with a MDD tool, we need to extend manually the generated code. Objective This paper tackles how to include functional usability features (usability recommendations strongly related to system functionality) in MDD through conceptual primitives. Method The approach consists of studying usability guidelines to identify usability properties that can be represented in a conceptual model. Next, these new primitives are the input for a model compiler that generates the code according to the characteristics expressed in them. An empirical study with 66 subjects was conducted to study the effect of including functional usability features regarding end users\\u2019 satisfaction and time to complete tasks. Moreover, we have compared the workload of two MDD analysts including usability features by hand in the generated code versus including them through conceptual primitives according to our approach. Results Results of the empirical study shows that after including usability features, end users\\u2019 satisfaction improves while spent time does not change significantly. This justifies the use of usability features in the software development process. Results of the comparison show that the workload required to adapt the MDD method to support usability features through conceptual primitives is heavy. However, once MDD supports these features, MDD analysts working with primitives are more efficient than MDD analysts implementing these features manually. Conclusion This approach brings us a step closer to conceptual models where models represent not only functionality, behaviour or persistence, but also usability features. Although the aim of empirical software engineering is to provide evidence for selecting the appropriate technology, it appears that there is a lack of recognition of this work in industry. Results from empirical research only rarely seem to find their way to company decision makers. If information relevant for software managers is provided in reports on experiments, such reports can be considered as a source of information for them when they are faced with making decisions about the selection of software engineering technologies. To bridge this communication gap between researchers and professionals, we propose characterizing the information needs of software managers in order to show empirical software engineering researchers which information is relevant for decision-making and thus enable them to make this information available. We empirically investigated decision makers' information needs to identify which information they need to judge the appropriateness and impact of a software technology. We empirically developed a model that characterizes these needs. To ensure that researchers provide relevant information when reporting results from experiments, we extended existing reporting guidelines accordingly. We performed an experiment to evaluate our model with regard to its effectiveness. Software managers who read an experiment report according to the proposed model judged the technology's appropriateness significantly better than those reading a report about the same experiment that did not explicitly address their information needs. Our research shows that information regarding a technology, the context in which it is supposed to work, and most importantly, the impact of this technology on development costs and schedule as well as on product quality is crucial for decision makers. This paper analyses what implications usability has for software development, paying special attention to the impact of this quality attribute on design. In this context, the aim is twofold. On the one hand, we intend to empirically corroborate that software design and usability are really related. This would mean that this, like other quality attributes, would need to be dealt with no later than at design time to develop usable software at a reasonable cost. On the other hand, we present a possible quantification, calculated from a number of real applications, of the effect of incorporating certain usability features at design time. Abstract Context An accepted fact in software engineering is that software must undergo verification and validation process during development to ascertain and improve its quality level. But there are too many techniques than a single developer could master, yet, it is impossible to be certain that software is free of defects. So, it is crucial for developers to be able to choose from available evaluation techniques, the one most suitable and likely to yield optimum quality results for different products. Though, some knowledge is available on the strengths and weaknesses of the available software quality assurance techniques but not much is known yet on the relationship between different techniques and contextual behavior of the techniques. Objective This research investigates the effectiveness of two testing techniques \\u2013 equivalence class partitioning and decision coverage and one review technique \\u2013 code review by abstraction, in terms of their fault detection capability. This will be used to strengthen the practical knowledge available on these techniques. Method The results of eight experiments conducted over 5\\u00a0years to investigate the effectiveness of three techniques \\u2013 code reading by stepwise abstraction, equivalence class partitioning and decision (branch) coverage were aggregated using a less rigorous aggregation process proposed during the course of this work. Results It was discovered that the equivalence class partitioning and the decision coverage techniques behaved similarly in terms of fault detection capacity (and type of faults caught) based on the programs and fault classification used in the experiments. They both behaved better than the code reading by stepwise abstraction technique. Conclusion Overall, it can be deducted from the aggregation results that the equivalence class partitioning and the decision coverage techniques used are actually equally capable in terms of the type and number of faults detected. Nevertheless, more experiments is still required in this field so that this result can be verified using a rigorous aggregation technique. People are a critical software development issue, and the human dimension can be even more important than the technical. An important part of human resources management is assigning people to development roles. This process isn't just crucial for generating productive teams; it can also help software organizations develop systematic long-term competence. Despite the importance of identifying the right people for roles, little is known about doing this properly. Integrating managerial experience with a procedure for identifying the person best suited for each role can help improve human resources management and long-term career development. We've defined a human capability-based procedure to supplement managerial activities for supporting personnel development and human resources management. Along with occupational psychologists and software managers, we've applied our procedure in small and medium-sized enterprises (SMEs). Background: There is no globally accepted open source software development process to define how open source software is developed in practice. A process description is important for coordinating all the software development activities involving both people and technology. Aim: The research question that this study sets out to answer is: What activities do open source software process models contain? The activity groups on which it focuses are Concept Exploration, Software Requirements, Design, Maintenance and Evaluation. Method: We conduct a systematic mapping study (SMS). A SMS is a form of systematic literature review that aims to identify and classify available research papers concerning a particular issue. Results: We located a total of 29 primary studies, which we categorized by the open source software project that they examine and by activity types (Concept Exploration, Software Requirements, Design, Maintenance and Evaluation). The activities present in most of the open source software development processes were Execute Tests and Conduct Reviews, which belong to the Evaluation activities group. Maintenance is the only group that has primary studies addressing all the activities that it contains. Conclusions: The primary studies located by the SMS are the starting point for analyzing the open source software development process and proposing a process model for this community. The papers in our paper pool that describe a specific open source software project provide more regarding our research question than the papers that talk about open source software development without referring to a specific open source software project. Several authors have pointed out that current conceptual models have two main shortcomings. First, they are clearly oriented to a specific development paradigm (structured, objects, etc.). Second, once the conceptual models have been obtained, it is really difficult to switch to another development paradigm, because the model orientation to a specific development approach. This fact induces problems during development, since practitioners are encouraged to think in terms of a solution before the problem at hand is well understood, thus anticipating perhaps bad design decisions.An appropriate analysis task requires models that are independent of any implementation issues. In concrete, models should support developers to understand the problem and its constraints before any solution is identified. This paper proposes such an alternative approach to conceptual modelling, called \\\"problem-oriented analysis method\\\". The replication of experiments is a key undertaking in SE. Successful replications enable a discipline's body of knowledge to grow, as the results are added to those of earlier replications. However, replication is extremely difficult in SE, primarily because it is difficult to get a setting that is exactly the same as in the original experiment. Consequently, changes have to be made to the experiment to adapt it to the new site. To be able to replicate an experiment, information also has to be transmitted (usually orally and in writing) between the researchers who ran the experiment earlier and the ones who are going to replicate the experiment. This article examines the influence of the type of communication there is between experimenters on how successful a replication is. We have studied three replications of the same experiment in which different types of communication were used. Context: Measurement is crucial and important to empirical software engineering. Although reliability and validity are two important properties warranting consideration in measurement processes, they may be influenced by random or systematic error (bias) depending on which metric is used. Aim: Check whether, the simple subjective metrics used in empirical software engineering studies are prone to bias. Method: Comparison of the reliability of a family of empirical studies on requirements elicitation that explore the same phenomenon using different design types and objective and subjective metrics. Results: The objectively measured variables (experience and knowledge) tend to achieve more reliable results, whereas subjective metrics using Likert scales (expertise and familiarity) tend to be influenced by systematic error or bias. Conclusions: Studies that predominantly use variables measured subjectively, like opinion polls or expert opinion acquisition, must take every care to prevent bias that can result in incorrect results. Our goal is to analyze the optimality of search strategies for use in systematic reviews of software engineering experiments. Studies retrieval is an important problem in any evidence-based discipline. This question has not been examined for evidence-based software engineering as yet. We have run several searches exercising different terms denoting experiments to evaluate their recall and precision. Based on our evaluation, we propose using a high recall strategy when there are plenty of resources or the results need to be exhaustive. For any other case, we propose optimal, or even acceptable, search strategies. As a secondary goal, we have analysed trends and weaknesses in terminology used in articles reporting software engineering experiments. We have found that it is impossible for a search strategy to retrieve 100% of the experiments of interest (as happens in other experimental disciplines), because of the shortage of reporting standards in the community. This paper describes an empirical study that examined the work climate within software development teams. The question was whether the team climate in software developer teams has any relation to software product quality. We define team climate as the shared perceptions of the team's work procedures and practices. The team climate factors examined were West and Anderson's participative safety, support for innovation, team vision and task orientation. These four factors were measured before the project using the Team Selection Inventory (TSI) test to establish subject climate preferences, as well as during and after the project using the Team Climate Inventory (TCI) test, which establishes the subject's perceptions of the climate. In this quasi-experimental study, data were collected from a sample of 35 three-member developer teams in an academic setting. These teams were formed at random and their members were blind to the quasi-experimental conditions and hypotheses. All teams used an adaptation of extreme programming (XP) to the students' environment to develop the same software system. We found that high team vision preferences and high participative safety perceptions of the team were significantly related to better software. Additionally, the results show that there is a positive relationship between the categorization of better than preferred, as preferred and worse than preferred climate and software quality for two of the teamwork climate factors: participative safety and team vision. So it seems important to track team climate in an organization and team as one (of many) indicators of the quality of the software to be delivered. When software engineers set out to build a system, they usually have the informal idea that there is a relation between the linguistic world and the conceptual world. In this paper, we present a formalisation of this empirical relation, defining an intermediate mapping of the components of the linguistic and conceptual worlds to their mathematical representations. This process outputs a justified correspondence between natural language, used as a means of communication between users and software engineers, and conceptual models, employed by software engineers as a first step towards building a system. In other words, our aim is to show how the equivalence of the linguistic and conceptual representations of a requirement can be established in a formal and justified manner. Context: A replication is the repetition of an experiment. Several efforts have been made to adopt replication as a common practice in software engineering. There are different types of replications, depending on their purpose. Similar replications keep the experimental conditions as alike as possible to the original ones. External similar replications, where the replicating experimenters are not the same people as the original experimenters, have been a stumbling block. Several attempts at combining the results of replications have resulted in failure. Software engineering does not appear to be well suited to such replications, because it works with complex experimentally immature contexts. Software engineering settings have a large number of variables, and the role that many of them play is unknown. A successful (or useful) similar replication helps to better understand the phenomenon under study by verifying results and/or identifying contextual variables that could influence (or not) the results, through the combination of experimental results. Objective: To be able to get successful similar replications, there needs to be interaction between original and replicating experimenters. In this paper, we propose an interaction process for achieving successful similar replications. Method: This process consists of: an adaptation meeting, where experimenters tailor the experiment to the new setting; querying, to settle occasional inquiries while the experiment is being run; and a combination meeting, where experimenters meet to discuss the combination of replication outcomes with previous results. To check its effectiveness, the process has been tested on three different replications of the same experiment. Results: The proposed interaction process has helped to identify new contextual variables that could potentially influence (or not) the experimental results in the three replications run. Additionally, the interaction process has helped to uncover certain problems and deviations that occurred during some of the replications that we would have not been aware of otherwise. Conclusions: There are signs that suggest that it is possible to get successful similar replications in software engineering experimentation, when there is appropriate interaction among experimenters. Usability is increasingly recognized as a quality attribute that one has to design for. The conventional alternative is to measure usability on a finished system and improve it. The disadvantage of this approach is, obviously, that the cost associated with implementing usability improvements in a fully implemented system are typically very high and prohibit improvements with architectural impact. In this tutorial, we present the insights gained, techniques developed and lessons learned in the EU-IST project STATUS (SofTware Architectures That supports USability). These include a forward-engineering perspective on usability, a technique for specifying usability requirements, a method for assessing software architectures for usability and, finally, for improving software architectures for usability. The topics are extensively illustrated by examples and experiences from many industrial cases. Software process assessment is an essential activity for improving software development in an organization. It is very difficult to put together an efficient and effective improvement plan unless it is based on the results of a preparatory assessment. This will determine the current status of the organization's software process and will identify the areas or points that need improvement. There is a need for a rigorous method of assessment that encompasses the factors that affect software production. By rigorous we mean an evaluation based on Evaluation Theory. This theoretical foundation must assure that the evaluation carried out is comprehensive and reliable. Evaluation Theory, defined by Scriven and other authors, describes the components for each type of evaluation method. Six guidelines can be deduced by generalising these components, according to which any evaluation method should be developed: target; criteria; yardstick; assessment techniques; synthesis techniques, and evaluation process. In this paper, we present a software process assessment method based on Evaluation Theory. This theoretical foundation was one of the things that compelled us to reflect on the factors to be included in the evaluation. As a result, the proposed method jointly assesses four essential factors in software production: processes; technology resources; human resources, and organizational design. The aim behind this method is to provide a remedy for the main shortcomings of current software process assessment methods: partiality of the evaluated factors, because most centre on the assessment of management processes; and non-rigorousness of the evaluation processes. The applicability of the proposed method has been corroborated by means of an experimentation conducted at small and medium-sized Spanish businesses. Copyright \\u00a9 2000 John Wiley & Sons Ltd Some verification and validation techniques have been evaluated both theoretically and empirically. Most empirical studies have been conducted without subjects, passing over any effect testers have when they apply the techniques. We have run an experiment with students to evaluate the effectiveness of three verification and validation techniques (equivalence partitioning, branch testing and code reading by stepwise abstraction). We have studied how well able the techniques are to reveal defects in three programs. We have replicated the experiment eight times at different sites. Our results show that equivalence partitioning and branch testing are equally effective and better than code reading by stepwise abstraction. The effectiveness of code reading by stepwise abstraction varies significantly from program to program. Finally, we have identified project contextual variables that should be considered when applying any verification and validation technique or to choose one particular technique. Abstract The objective of this introductory paper is twofold. On one hand, it shows the guest editors's view about the complex field of Conceptual Modelling. To do that, we discuss some concepts related to this topic, as well as the relation that nowadays exists between this process and the other parts of software development. On the other hand, this introductory paper describes the papers that are included in this special issue, connecting each of these papers with the view that the guest editors have about the field. Abstract The requirements analysis process is essential to software development. The success or failure of a software system can be said to largely depend on the quality of this activity. A formal and disciplined process is therefore necessary for requirements analysis. In this paper, we present an approach that is based on the formal definition of relations between linguistic and OO conceptual structures as a basis for a formal and disciplined problem analysis process. This process is based on two components, conceptual model formalization and OO model construction. The first provides formal rules to identify the key components of conceptual models, and the second, provides a set of definite steps to guide the analyst in model construction. We also present some conclusions concerning the application of our approach versus the standard OMT approach by a group of students at our university. Mature knowledge allows engineering disciplines the achievement of predictable results. Unfortunately, the type of knowledge used in software engineering can be considered to be of a relatively low maturity, and developers are guided by intuition, fashion or market-speak rather than by facts or undisputed statements proper to an engineering discipline. Testing techniques determine different criteria for selecting the test cases that will be used as input to the system under examination, which means that an effective and efficient selection of test cases conditions the success of the tests. The knowledge for selecting testing techniques should come from studies that empirically justify the benefits and application conditions of the different techniques. This paper analyzes the maturity level of the knowledge about testing techniques by examining existing empirical studies about these techniques. We have analyzed their results, and obtained a testing technique knowledge classification based on their factuality and objectivity, according to four parameters. Context: To develop usable software we need to understand the users that will interact with the system. Personas is a HCI technique that gathers information about users in order to comprehend their characteristics. This information is used to define fictitious persons on which development should focus. Personas provides an understanding of the user, often overlooked in SE developments. Objective: The goal of our research is to modify Personas to readily build the technique into the requirements stage of regular SE developments. Method: We tried to apply Cooper's version of the Personas technique and we found shortcomings in both the definition of the procedure to be enacted and the formalization of the product resulting from the execution of each step of the Personas technique. For each of these limitations (up to a total of 11), we devised an improvement to be built into Personas. We have incorporated these improvements into a SE version of Personas. The improved Personas avoid the weaknesses encountered by an average software developer unfamiliar with HCI techniques applying the original Personas. Results: We aim to improve requirements elicitation through the use of Personas. We have systematized and formalized Personas in the SE tradition in order to build this new version of the technique into the requirements stage. We have applied our proposal in an application example. Conclusion: The integration of Personas into the SE requirements stage might improves the understanding of what the software product should do and how it should behave. We have modified the HCI Personas technique to comply with the levels of systematization required by SE. We have enriched the SE requirements process by incorporating Personas activities into requirements activities. Requirements elicitation and requirements analysis are the RE activities most affected by incorporating Personas. Abstract In this paper, a methodology is proposed for designing and constructing a knowledge-based system (KBS) in order to generate and evaluate administrative grants. The methodology is applied to the Common Agricultural Policy regulations covering Spanish farmers for validation. The proposed methodology facilitates the development of applications of this type from preliminary problem analysis to application implementation and maintenance. This methodology is well adapted to the characteristics of the knowledge-based software under development: open requirements and diverse computational models. It constitutes a complete guide for the knowledge engineer in producing and maintaining an automated solution to a real-world problem, as the methodology is not only declarative (i.e. indicates what to do) but also procedural (i.e. indicates how to do it). The KBS demonstrator built is a query system for use by specialists from the Agricultural Extension Services. The prototype implemented by means of the Kappa-PC V. 2.3 was developed jointly by the Departments of Artificial Intelligence, Rural Projects and Planning and Mathematics Applied to Agronomics of the Polytechnic University of Madrid and the Castile-Leon Provincial Office of Agriculture in Segovia. \\u00a9 Abstract In this paper, we discuss all of the steps needed to build an expert system (ES) in the domain of technology transfer into an organization. Firstly, there is an introduction about the domain and the problems involved in software technology transfer (TT) into an organization, followed by a description of the usefulness of an advisory and planning ES to guide an inexperienced change agent in such activities. Secondly, we discuss the methodology-based construction of the ES and show the different phases and stages of the process. We detail a knowledge acquisition method used to both elicit knowledge and organize that knowledge as a task model. The conceptualization phase, closely linked with the above knowledge structuring step, establishes the concepts, relations and functions handled by TT. The result is a set of individualized tasks that make up a complete transfer plan. As regards formalization, system representation is frame-oriented, whereby the tasks and their properties are encapsulated in frames and later the system is implemented in the Kappa tool environment. Finally, the system is executed and evaluated by users. Abstract Context Replication plays an important role in experimental disciplines. There are still many uncertainties about how to proceed with replications of SE experiments. Should replicators reuse the baseline experiment materials? How much liaison should there be among the original and replicating experimenters, if any? What elements of the experimental configuration can be changed for the experiment to be considered a replication rather than a new experiment? Objective To improve our understanding of SE experiment replication, in this work we propose a classification which is intend to provide experimenters with guidance about what types of replication they can perform. Method The research approach followed is structured according to the following activities: (1) a literature review of experiment replication in SE and in other disciplines, (2) identification of typical elements that compose an experimental configuration, (3) identification of different replications purposes and (4) development of a classification of experiment replications for SE. Results We propose a classification of replications which provides experimenters in SE with guidance about what changes can they make in a replication and, based on these, what verification purposes such a replication can serve. The proposed classification helped to accommodate opposing views within a broader framework, it is capable of accounting for less similar replications to more similar ones regarding the baseline experiment. Conclusion The aim of replication is to verify results, but different types of replication serve special verification purposes and afford different degrees of change. Each replication type helps to discover particular experimental conditions that might influence the results. The proposed classification can be used to identify changes in a replication and, based on these, understand the level of verification. Experiment replication is a key component of the scientific paradigm. The purpose of replication is to verify previously observed findings. Although some Software Engineering (SE) experiments have been replicated, yet, there is still disagreement about how replications should be run in our field. With the aim of gaining a better understanding of how replications are carried out, this paper examines different replication types in other scientific disciplines. We believe that by analysing the replication types proposed in other disciplines it is possible to clarify some of the question marks still hanging over experimental SE replication. SUMMARY This paper is based on the premise that people\\u2019s behavioural competencies or characteristics of professional conduct influence the effectiveness and efficiency with which they perform a predetermined role in the software process. We propose a capabilities-oriented process model that includes traditional elements of the software process (activities, products, techniques, people and roles) and the original element of this paper (capabilities). With the aim of adding behavioural competencies to the process model, we define the capability\\u2013person and capability\\u2013role relationships involved in software development. Additionally, we propose two procedures that are based on each of these relationships: a procedure that can be used to determine the capabilities of the members of a development team; and a procedure that can be used to assign people to perform roles depending on their capabilities and the capabilities demanded by the roles. Finally, the person\\u2013capabilities\\u2013role relationship has been empirically validated. The results yielded by this experiment confirm the hypothesis that assigning people to roles according to their capabilities and the capabilities demanded by the role improves software development. Copyright c Background: There is no specialized survey of experiments conducted in the software industry. Goal: Identify the major features of software industry experiments, such as time distribution, independent and dependent variables, subject types, design types and challenges. Method: Systematic literature review, taking the form of a scoping study. Results: We have identified 10 experiments and five quasi-experiments up to July 2012. Most were run as of 2003. The main features of these studies are that they test technologies related to quality and management and analyse outcomes related to effectiveness and effort. Most experiments have a factorial design. The major challenges faced by experimenters are to minimize the cost of running the experiment for the company and to schedule the experiment so as not to interfere with production processes. Conclusion: Companies appear to be disinclined to run experiments because they are not perceived to have direct benefits. We believe that researchers staging a field experiment in a company should adopt a business-aligned stance and plan an experiment that clearly benefits managers and professionals. This article analyses the relationships between personality, team processes, task characteristics, product quality and satisfaction in software development teams. The data analysed here were gathered from a sample of 35 teams of students (105 participants). These teams applied an adaptation of an agile methodology, eXtreme Programming (XP), to develop a software product. We found that the teams with the highest job satisfaction are precisely the ones whose members score highest for the personality factors agreeableness and conscientiousness. The satisfaction levels are also higher when the members can decide how to develop and organize their work. On the other hand, the level of satisfaction and cohesion drops the more conflict there is between the team members. Finally, the teams exhibit a significant positive correlation between the personality factor extraversion and software product quality. In no science or engineering discipline does it make sense to speak of isolated experiments. The results of a single experiment cannot be viewed as representative of the underlying reality. The concept of experiment is closely related to replication. Experiment replication is the repetition of an experiment to double-check its results. Multiple replications of an experiment increase the credibility of its results. Software engineering has tried its hand at the identical repetition of experiments in the way of the natural sciences (physics, chemistry, etc.). After numerous attempts over the years, excepting experiments repeated by the same researchers at the same site, no exact replications have yet been achieved. One key reason for this is the complexity of the software development setting. This complexity prevents the many experimental conditions from being reproduced identically. This paper reports research into whether non-exact replications can be of any use. We propose a process that allows researchers to generate new knowledge when running non-exact replications. To illustrate the advantages of the proposed process, two different replications of an experiment are shown. Experimentation has played a major role in scientific advancement. Replication is one of the essentials of the experimental methods. In replications, experiments are repeated aiming to check their results. Successful replication increases the validity and reliability of the outcomes observed in an experiment.  There is debate about the best way of running replications of Software Engineering (SE) experiments. Some of the questions that have cropped up in this debate are, \\\"Should replicators reuse the baseline experiment materials? Which is the adequate sort of communication among experimenters and replicators if any? What elements of the experimental structure can be changed and still be considered a replication instead of a new experiment?\\\". A deeper understanding of the concept of replication should help to clarify these issues as well as increase and improve replications in SE experimental practices.  In this chapter, we study the concept of replication in order to gain insight. The chapter starts with an introduction to the importance of replication and the state of replication in ESE. Then we discuss replication from both the statistical and scientific viewpoint. Based on a review of the diverse types of replication used in other scientific disciplines, we identify the different types of replication that are feasible to be run in our discipline. Finally, we present the different purposes that replication can serve in Experimental Software Engineering (ESE). Like any other quality attribute, usability imposes specific constraints on software components. Features that raise the software system's usability have to be considered from the earliest development stages. But, discovering and documenting usability features is likely to be beyond the usability knowledge of most requirements engineers, developers, and users. We propose an approach based on developing specific guidelines that capitalize upon key elements recurrently intervening in the usability features elicitation and specification process. The use of these guidelines provides requirements analysts with a knowledge repository. They can use this repository to ask the right questions and capture precise usability requirements information. Software architects have techniques to deal with many quality attributes such as performance, reliability, and maintainability. Usability, however, has traditionally been concerned primarily with presentation and not been a concern of software architects beyond separating the user interface from the remainder of the application. In this paper, we present usability-supporting architectural patterns. Each pattern describes a usability concern that is not supported by separation alone. For each concern, a usability-supporting architectural pattern provides the forces from the characteristics of the task and environment, the human, and the state of the software to motivate an implementation independent solution cast in terms of the responsibilities that must be fulfilled to satisfy the forces. Furthermore, each pattern includes a sample solution implemented in the context of an overriding separation based pattern such as J2EE Model View Controller. This paper reports a systematic review of empirical studies concerning the effectiveness of elicitation techniques, and the subsequent aggregation of empirical evidence gathered from those studies. The most significant results of the aggregation process are as follows: (1) Interviews, preferentially structured, appear to be one of the most effective elicitation techniques; (2) Many techniques often cited in the literature, like card sorting, ranking or thinking aloud, tend to be less effective than interviews; (3) Analyst experience does not appear to be a relevant factor; and (4) The studies conducted have not found the use of intermediate representations during elicitation to have significant positive effects. It should be noted that, as a general rule, the studies from which these results were aggregated have not been replicated, and therefore the above claims cannot be said to be absolutely certain. However, they can be used by researchers as pieces of knowledge to be further investigated and by practitioners in development projects, always taking into account that they are preliminary findings. Is 25 years enough time to build up a coherent body of knowledge that can help point to useful principles? As a testbed for helping us answer this question, software testing techniques are a good place to start. Few software practices are as important as testing, and testing techniques are amenable to measurement and reasoning about their effectiveness. Because they're aimed at removing faults, measuring the number and type of such removed faults seems like a natural part of applying these techniques. To make sense of this data, Universidad Politecnica de Madrid researchers have spent some time worrying about how to put 25 years' worth of work together usefully. Getting the requirements right is one of the most important activities in software development. Making a crucial misstep at this phase can easily lead to large amounts of rework when the customer simply can't accept a system the way it was developed. When used correctly, approaches such as incremental development or agile methods can mitigate the risks of getting the requirements wrong by making sure that systems are developed in smaller chunks and that each chunk can be shown to the customer for approval. However, the best way to develop a high-quality system with minimal effort is still to get the requirements right the first time. Replications play a key role in Empirical Software Engineering by allowing the community to build knowledge about which results or observations hold under which conditions. Therefore, not only can a replication that produces similar results as the original experiment be viewed as successful, but a replication that produce results different from those of the original experiment can also be viewed as successful. In this paper we identify two types of replications: exact replications, in which the procedures of an experiment are followed as closely as possible; and conceptual replications, in which the same research question is evaluated by using a different experimental procedure. The focus of this paper is on exact replications. We further explore them to identify two sub-categories: dependent replications, where researchers attempt to keep all the conditions of the experiment the same or very similar and independent replications, where researchers deliberately vary one or more major aspects of the conditions of the experiment. We then discuss the role played by each type of replication in terms of its goals, benefits, and limitations. Finally, we highlight the importance of producing adequate documentation for an experiment (original or replication) to allow for replication. A properly documented replication provides the details necessary to gain a sufficient understanding of the study being replicated without requiring the replicator to slavishly follow the given procedures. This article gives an informal overview of the situation of European companies regarding requirements engineering. It reviews what progress the industry has made with respect to the requirements-related problems identified years ago and calls attention to the gap between RE practice and the solutions provided by research. The authors aim to determine whether the same problems still exist today, even though potential solutions have been provided in the literature for some time. The RESER workshop provides a venue in which empirical software engineering researchers can discuss the theoretical foundations and methods of replication, as well as present the results of specific replicated studies. In 2011, the workshop co-located with the International Symposium on Empirical Software Engineering and Measurement (ESEM) in Banff, Alberta, Canada. In addition to several outstanding paper sessions, highlights of the 2011 workshop included a keynote address by Dr. Victor R. Basili, in which he addressed the question, \\\"What's so hard about replication of software engineering experiments?\\\" The workshop also featured a joint replication panel session discussing the first cooperative joint replication ever conducted in empirical software engineering research and a planning session for next year's joint replication project addressing Conway's Law. Quality assessment is one of the activities performed as part of systematic literature reviews. It is commonly accepted that a good quality experiment is bias free. Bias is considered to be related to internal validity (e.g., how adequately the experiment is planned, executed and analysed). Quality assessment is usually conducted using checklists and quality scales. It has not yet been proven, however, that quality is related to experimental bias. Aim: Identify whether there is a relationship between internal validity and bias in software engineering experiments. Method: We built a quality scale to determine the quality of the studies, which we applied to 28 experiments included in two systematic literature reviews. We proposed an objective indicator of experimental bias, which we applied to the same 28 experiments. Finally, we analysed the correlations between the quality scores and the proposed measure of bias. Results: We failed to find a relationship between the global quality score (resulting from the quality scale) and bias, however, we did identify interesting correlations between bias and some particular aspects of internal validity measured by the instrument. Conclusions: There is an empirically provable relationship between internal validity and bias. It is feasible to apply quality assessment in systematic literature reviews, subject to limits on the internal validity aspects for consideration. Engineering disciplines are characterised by the use of mature knowledge by means of which they can achieve predictable results. Unfortunately, the type of knowledge used in software engineering can be considered to be of a relatively low maturity, and developers are guided by reasoning based on intuition, fashion or market-speak rather than by facts or undisputed statements proper to an engineering discipline. This paper analyses the maturity level of the knowledge about testing techniques by examining existing empirical studies about these techniques. For this purpose, three categories of knowledge of increasing maturity have been presented and the results of these empirical studies have been placed in these three categories. Background: Several meta-analysis methods can be used to quantitatively combine the results of a group of experiments, including the weighted mean difference, statistical vote counting, the parametric response ratio and the non-parametric response ratio. The software engineering community has focused on the weighted mean difference method. However, other meta-analysis methods have distinct strengths, such as being able to be used when variances are not reported. There are as yet no guidelines to indicate which method is best for use in each case Aim: Compile a set of rules that SE researchers can use to ascertain which aggregation method is best for use in the synthesis phase of a systematic review. Method: Monte Carlo simulation varying the number of experiments in the meta analyses, the number of subjects that they include, their variance and effect size. We empirically calculated the reliability and statistical power in each case Results: WMD is generally reliable if the variance is low, whereas its power depends on the effect size and number of subjects per meta-analysis; the reliability of RR is generally unaffected by changes in variance, but it does require more subjects than WMD to be powerful; NPRR is the most reliable method, but it is not very powerful; SVC behaves well when the effect size is moderate, but is less reliable with other effect sizes. Detailed tables of results are annexed. Conclusions: Before undertaking statistical aggregation in software engineering, it is worthwhile checking whether there is any appreciable difference in the reliability and power of the methods. If there is, software engineers should select the method that optimizes both parameters. Replication of software engineering experiments is crucial for dealing with validity threats to experiments in this area. Even though the empirical software engineering community is aware of the importance of replication, the replication rate is still very low. The RESER'11 Joint Replication Project aims to tackle this problem by simultaneously running a series of several replications of the same experiment. In this article, we report the results of the replication run at the Universidad Politecnica de Madrid. Our results are inconsistent with the original experiment. However, we have identified possible causes for them. We also discuss our experiences (in terms of pros and cons) during the replication. The field of knowledge engineering has been one of the most visible successes of AI to date. Knowledge acquisition is the main bottleneck in the knowledge engineer's work. Machine-learning tools have contributed positively to the process of trying to eliminate or open up this bottleneck, but how do we know whether the field is progressing? How can we determine the progress made in any of its branches? How can we be sure of an advance and take advantage of it? This article proposes a benchmark as a classificatory, comparative, and metric criterion for machine-learning tools. The benchmark centers on the knowledge engineering viewpoint, covering some of the characteristics the knowledge engineer wants to find in a machine-learning tool. The proposed model has been applied to a set of machine-learning tools, comparing expected and obtained results. Experimentation validated the model and led to interesting results. Usability is increasingly recognized as a quality attribute that one has to explicitly deal with during development. Nevertheless, usability techniques, when applied, are decoupled from the software development process. The host of techniques offered by the HCI (Human-Computer Interaction) field make the task of selecting the most appropriate ones for a given project and organization a difficult task. Project managers and developers aiming to integrate usability practices into their software process have to face important challenges, as the techniques are not described in the frame of a software process as it is understood in SE (Software Engineering). Even when HCI experts (either in-house or from an external organization) are involved in the integration process, it is also a tough endeavour due to the strong differences in terminology and overall approach to software development between HCI and SE. In this tutorial we will present, from a SE viewpoint, which usability techniques can be most valuable to development teams with little or no previous usability experience, how a particular set of techniques can be selected according to the specific characteristics of the organization and project, and how usability techniques match with the activity groups in the development process. Classification makes a significant contribution to advancing knowledge in both science and engineering. It is a way of investigating the relationships between the objects to be classified and identifies gaps in knowledge. Classification in engineering also has a practical application; it supports object selection. They can help mature software engineering knowledge, as classifications constitute an organized structure of knowledge items. Till date, there have been few attempts at classifying in software engineering. In this research, we examine how useful classifications in software engineering are for advancing knowledge by trying to classify testing techniques. The paper presents a preliminary classification of a set of unit testing techniques. To obtain this classification, we enacted a generic process for developing useful software engineering classifications. The proposed classification has been proven useful for maturing knowledge about testing techniques, and therefore, SE, as it helps to: 1) provide a systematic description of the techniques, 2) understand testing techniques by studying the relationships among techniques (measured in terms of differences and similarities), 3) identify potentially useful techniques that do not yet exist by analyzing gaps in the classification, and 4) support practitioners in testing technique selection by matching technique characteristics to project characteristics. This paper shares our experience with initial negotiation and topic elicitation process for conducting industry experiments in six software development organizations in Finland. The process involved interaction with company representatives in the form of both multiple group discussions and separate face-to-face meetings. Fitness criteria developed by researchers were applied to the list of generated topics to decide on a common topic. The challenges we faced include diversity of proposed topics, communication gaps, skepticism about research methods, initial disconnect between research and industry needs, and lack of prior work relationship. Lessons learned include having enough time to establish trust with partners, importance of leveraging the benefits of training and skill development that are inherent in the experimental approach, uniquely positioning the experimental approach within the landscape of other validation approaches more familiar to industrial partners, and introducing the fitness criteria early in the process. Software engineering (SE) and knowledge engineering (KE) develop software systems using different construction process models. Because of the growing complexity of the problems to be solved by computers, the conventional systems (CS) and knowledge-based systems (KBS) software process is at present passing through a period of integration. In this paper, we propose a software process model applicable to both CS and KBS. The model designed is declarative, that is, it indicates what is done to build a software system. Its goal is to provide software and knowledge engineers with a techno-conceptual tool to develop systems comprising both traditional and knowledge-based software. To develop usable systems, it is necessary to understand the users that interact with the system. The personas technique from the human-computer interaction discipline (HCI) gathers data about users, gains an understanding of their characteristics, defines fictitious personas based on this understanding and focuses on these personas throughout the software development process. The aim of our research is to build personas into routine software development following software engineering (SE) guidelines. The intention is to improve the usability of the resulting software. To achieve this aim, we first present a modification of the personas technique, called Personas*. This new technique complies with SE systematization standards. Second, we incorporate the proposed Personas* technique into the software process requirements analysis activity. The benefits to be gained from the Personas* technique are: i) it provides an understanding of the user, which SE traditionally has tended to overlook, and ii) it enriches the requirements analysis activities enabling the software engineer to focus on the people involved in software system use. The RESER 2010 workshop provides a venue in which empirical Software Engineering researchers may present and discuss theoretical foundations and methods of replication, as well as the results of replicated studies. We have located the results of empirical studies on elicitation techniques and aggregated these results to gather empirically grounded evidence. Our chosen surveying methodology was systematic review, whereas we used an adaptation of comparative analysis for aggregation because meta-analysis techniques could not be applied. The review identified 564 publications from the SCOPUS, IEEEXPLORE, and ACM DL databases, as well as Google. We selected and extracted data from 26 of those publications. The selected publications contain 30 empirical studies. These studies were designed to test 43 elicitation techniques and 50 different response variables. We got 100 separate results from the experiments. The aggregation generated 17 pieces of knowledge about the interviewing, laddering, sorting, and protocol analysis elicitation techniques. We provide a set of guidelines based on the gathered pieces of knowledge. We have conducted a qualitative investigation on test-driven development (TDD) with focus groups to develop insights on the opinions of developers using TDD regarding the unintuitive process involved, its claimed effects, as well as the context factors that can facilitate (or hinder) its application. In particular, we conducted two focus group sessions: one with professionals and another with Master students in Computer Science. We used thematic analysis template (TAT) method for identifying patterns, themes, and interpretations in gathered data. We obtained a number of results that can be summarized as follows: (i) applying TDD without knowing advanced unit testing techniques can be difficult; (ii) refactoring (one of the phases of TDD) is not done as often as the process requires; (iii) there is a need for live feedback to let developers understand if TDD is being applied correctly; and (iv) the usefulness of TDD hinges on task and domain to which it is applied to. In experiments with crossover design subjects apply more than one treatment. Crossover designs are widespread in software engineering experimentation: they require fewer subjects and control the variability among subjects. However, some researchers disapprove of crossover designs. The main criticisms are: the carryover threat and its troublesome analysis. Carryover is the persistence of the effect of one treatment when another treatment is applied later. It may invalidate the results of an experiment. Additionally, crossover designs are often not properly designed and/or analysed, limiting the validity of the results. In this paper, we aim to make SE researchers aware of the perils of crossover experiments and provide risk avoidance good practices. We study how another discipline (medicine) runs crossover experiments. We review the SE literature and discuss which good practices tend not to be adhered to, giving advice on how they should be applied in SE experiments. We illustrate the concepts discussed analysing a crossover experiment that we have run. We conclude that crossover experiments can yield valid results, provided they are properly designed and analysed, and that, if correctly addressed, carryover is no worse than other validity threats. Background: Test-driven development (TDD) is an iterative software development technique where unit tests are defined before production code. Previous studies fail to analyze the values, beliefs, and assumptions that inform and shape TDD. Aim: We designed and conducted a qualitative study to understand the values, beliefs, and assumptions of TDD. In particular, we sought to understand how novice and professional software developers, arranged in pairs (a driver and a pointer), perceive and apply TDD. Method: 14 novice software developers, i.e., graduate students in Computer Science at the University of Basilicata, and six professional software developers (with one to 10 years work experience) participated in our ethnographically informed study. We asked the participants to implement a new feature for an existing software written in Java. We immersed ourselves in the context of the study, and collected data by means of contemporaneous field notes, audio recordings, and other artifacts. Results: A number of insights emerge from our analysis of the collected data, the main ones being: (i) refactoring (one of the phases of TDD) is not performed as often as the process requires and it is considered less important than other phases, (ii) the most important phase is implementation, (iii) unit tests are almost never up-to-date, (iv) participants first build a sort of mental model of the source code to be implemented and only then write test cases on the basis of this model; and (v) apart from minor differences, professional developers and students applied TDD in a similar fashion. Conclusions: Developers write quick-and-dirty production code to pass the tests and ignore refactoring. Context: Test-driven development (TDD) is an agile practice claimed to improve the quality of a software product, as well as the productivity of its developers. A previous study (i.e., baseline experiment) at the University of Oulu (Finland) compared TDD to a test-last development (TLD) approach through a randomized controlled trial. The results failed to support the claims. Goal: We want to validate the original study results by replicating it at the University of Basilicata (Italy), using a different design. Method: We replicated the baseline experiment, using a crossover design, with 21 graduate students. We kept the settings and context as close as possible to the baseline experiment. In order to limit researchers bias, we involved two other sites (UPM, Spain, and Brunel, UK) to conduct blind analysis of the data. Results: The Kruskal-Wallis tests did not show any significant difference between TDD and TLD in terms of testing effort (p-value = .27), external code quality (p-value = .82), and developers' productivity (p-value = .83). Nevertheless, our data revealed a difference based on the order in which TDD and TLD were applied, though no carry over effect. Conclusions: We verify the baseline study results, yet our results raises concerns regarding the selection of experimental objects, particularly with respect to their interaction with the order in which of treatments are applied. We recommend future studies to survey the tasks used in experiments evaluating TDD. Finally, to lower the cost of replication studies and reduce researchers' bias, we encourage other research groups to adopt similar multi-site blind analysis approach described in this paper. Requirements engineering research has been conducted for over 40 years. It is important to recognize the plethora of results accumulated to date to: (a) improve researchers' understanding of the historical roots of our field in the real-world and the problems that they are trying to solve, (b) expose researchers to the breadth and depth of solutions that have been proposed, (c) provide a synergistic basis for improving those solutions or building new ones to solve real-world problems facing the industry today, and d) increase practitioner awareness of available solutions. A detailed meta-analysis of the requirements engineering literature will provide an objective overview of the advances and current state of the discipline. This paper represents the first step in a planned multi-year analysis. It presents the results of a demographic analysis by date, type, outlet, author, and author affiliation for an existing database of over 4,000 requirements engineering publications. Experimentation has played a major role in scientific advancement. Replication is one of the essentials of the experimental methods. In replications, experiments are repeated aiming to check their results. Successful replication increases the validity and reliability of the outcomes observed in an experiment. There is debate about the best way of running replications of Soft- ware Engineering (SE) experiments. Some of the questions that have cropped up in this debate are, \\\"Should replicators reuse the baseline ex- periment materials? Which is the adequate sort of communication among experimenters and replicators if any? What elements of the experimental structure can be changed and still be considered a replication instead of a new experiment?\\\". A deeper understanding of the concept of replica- tion should help to clarify these issues as well as increase and improve replications in SE experimental practices. In this chapter, we study the concept of replication in order to gain insight. The chapter starts with an introduction to the importance of replication and the state of replication in ESE. Then we discuss replica- tion from both the statistical and scientific viewpoint. Based on a review of the diverse types of replication used in other scientific disciplines, we identify the different types of replication that are feasible to be run in our discipline. Finally, we present the different purposes that replication can serve in Experimental Software Engineering (ESE).\",\n",
            "  \"51535807\": \"In crisis situations it may be necessary to re-establish communications via ad-hoc networks of communicating way-stations. Crisis, defence, or surveillance scenarios may require the distribution of sensor units over some region of interest. In both cases the use of communicating and sensing, autonomous, mobile robots will become prevalent in the near future. Existing literature describes various control rules for forming teams of robots into optimally distributed communicating or sensing grids. We show how these robot behaviors exhibit advantages and disadvantages related to the range between robots. Further we describe a new approach which enables several different behaviors to be combined, utilizing the best behavior for any particular inter-robot range. This method is demonstrated using computer model simulations and the improved performance of the combined behaviors, relative to that for individual behaviors, is graphed over important situational parameters A fundamental (and popular) task in computer and robot vision is the tracking of an object which moves relative to the camera, essentially segmenting the object region of each successive frame. There are a great many published approaches, which are often variations, combinations or advances on well known techniques such as background subtraction, image differencing, predictive filtering and Bayesian estimation. Generally, these techniques rely on simple models of the tracked object and/or models of the background. Many techniques in computer vision derive from ideas previously established in the pattern recognition community, where it is usual to learn models offline from historical training data sets. Hence these models, once learned, typically remain static during the online tracking process. Such static models are ultimately of limited robustness in real world computer vision tracking scenarios where the appearance of both the background and the tracked object may change significantly and frequently due to camera motion (resulting in background change), object motion or deformation, introduction and removal of additional objects and clutter (e.g. passing traffic on a road) and changes in lighting and visibility conditions (either changes in ambient conditions or, for example, spotlights mounted on and moving with an underwater robot). In contrast, this chapter will discuss a variety of tracking algorithms and techniques which are highly adaptable. These techniques have in common that they incorporate models which are continuously relearned from new input image frames while simultaneously performing tracking on those frames. These techniques are powerful, in that they offer a way of successfully adapting to a changing environment. However, the price paid for adaptability can be a tendency towards certain kinds of instability. In simple terms, any system that continuously relearns (e.g. models of the tracked object and the background), has a risk of relearning incorrectly (e.g. relearning that background looks like object). Therefore, this chapter will also discuss various techniques for automatically detecting and correcting such errors as they occur, and survey techniques by which algorithms might continuously monitor their own performance. It is also useful to consider continuous machine learning techniques in vision in terms of the rate of relearning. Firstly we will consider well established algorithms which incrementally re-learn models, very gradually, over many frames. Later we will look at very recent work, A method and apparatus is disclosed for tracking an arbitrarily moving object in a sequence of images where the background may be changing. The tracking is based on visual features, such as color or texture, where regions of images (such as those which represent the object being tracked or the background) can be characterized by statistical distributions of feature values. The method improves on the prior art by incorporating a means whereby characterizations of the background can be rapidly re-learned for each successive image frame. This makes the method robust against the scene changes that occur when the image capturing device moves. It also provides robustness in difficult tracking situations, such as when the tracked object passes in front of backgrounds with which it shares similar colors or other features. Furthermore, a method is disclosed for automatically detecting and correcting certain kinds of errors which may occur when employing this or other tracking methods. This paper addresses the problem of learning about the interactions of rigid bodies. A probabilistic framework is presented for predicting the motion of one rigid body following contact with another. We describe an algorithm for learning these predictions from observations, which does not make use of physics and is not restricted to domains with particular physics. We demonstrate the method in a scenario where a robot arm applies pushes to objects. The probabilistic nature of the algorithm enables it to generalize from learned examples, to successfully predict the resulting object motion for previously unseen object poses, push directions and new objects with novel shape. We evaluate the method with empirical experiments in a physics simulator. This paper presents a method for single target tracking of arbitrary objects in challenging video sequences. Targets are modeled at three different levels of granularity (pixel level, parts-based level and bounding box level), which are cross-constrained to enable robust model relearning. The main contribution is an adaptive clustered decision tree method which dynamically selects the minimum combination of features necessary to sufficiently represent each target part at each frame, thereby providing robustness with computational efficiency. The adaptive clustered decision tree is implemented in two separate parts of the tracking algorithm: firstly to enable robust matching at the parts-based level between successive frames; and secondly to select the best superpixels for learning new parts of the target. We have tested the tracker using two different tracking benchmarks (VOT2013-2014 and CVPR2013 tracking challenges), based on two different test methodologies, and show it to be significantly more robust than the best state-of-the-art methods from both of those tracking challenges, while also offering competitive tracking precision. Stevens Institute of Technology is performing research aimed at determining the acoustical parameters that are necessary for detecting and classifying underwater threats. This paper specifically addresses the problems of passive acoustic detection of small targets in noisy urban river and harbor environments. We describe experiments to determine the acoustic signatures of these threats and the background acoustic noise. Based on these measurements, we present an algorithm for robustly discriminating threat presence from severe acoustic background noise. Measurements of the target's acoustic radiation signal were conducted in the Hudson River. The acoustic noise in the Hudson River was also recorded for various environmental conditions. A useful discriminating feature can be extracted from the acoustic signal of the threat, calculated by detecting packets of multi-spectral high frequency sound which occur repetitively at low frequency intervals. We use experimental data to show how the feature varies with range between the sensor and the detected underwater threat. We also estimate the effective detection range by evaluating this feature for hydrophone signals, recorded in the river both with and without threat presence. This paper presents a method for optimally combining pixel information from an infra-red thermal imaging camera, and a conventional visible spectrum colour camera, for tracking a moving target. The tracking algorithm rapidly re-learns its background models for each camera modality from scratch at every frame. This enables, firstly, automatic adjustment of the relative importance of thermal and visible information in decision making, and, secondly, a degree of \\u201ccamouflage target\\u201d tracking by continuously re-weighting the importance of those parts of the target model that are most distinct from the present background at each frame. Furthermore, this very rapid background adaptation ensures robustness to large, sudden and arbitrary camera motion, and thus makes this method a useful tool for robotics, for example visual servoing of a pan-tilt turret mounted on a moving robot vehicle. The method can be used to track any kind of arbitrarily shaped or deforming object, however the combination of thermal and visible information proves particularly useful for enabling robots to track people. The method is also important in that it can be readily extended for data fusion of an arbitrary number of statistically independent features from one or arbitrarily many imaging modalities. : The capacitated arc routing problem (CARP) has attracted considerable attention from researchers due to its broad potential for social applications. This paper builds on, and develops beyond, the cooperative coevolutionary algorithm based on route distance grouping (RDG-MAENS), recently proposed by Mei et al. Although Mei's method has proved superior to previous algorithms, we discuss several remaining drawbacks and propose solutions to overcome them. First, although RDG is used in searching for potential better solutions, the solution generated from the decomposed problem at each generation is not the best one, and the best solution found so far is not used for solving the current generation. Second, to determine which sub-population the individual belongs to simply according to the distance can lead to an imbalance in the number of the individuals among different sub-populations and the allocation of resources. Third, the method of Mei et al. was only used to solve single-objective CARP. To overcome the above issues, this paper proposes improving RDG-MAENS by updating the solutions immediately and applying them to solve the current solution through areas shared, and then according to the magnitude of the vector of the route direction, and a fast and simple allocation scheme is proposed to determine which decomposed problem the route belongs to. Finally, we combine the improved algorithm with an improved decomposition-based memetic algorithm to solve the multiobjective large scale CARP (LSCARP). Experimental results suggest that the proposed improved algorithm can achieve better results on both single-objective LSCARP and multiobjective LSCARP. Abstract This paper addresses the problem of unsupervised change detection in Synthetic Aperture Radar (SAR) images. Previous approaches have used evolutionary clustering optimization methods, which can suffer from reduced accuracy, because they often use only a single objective function and can easily become trapped at locally optimal values. To overcome these difficulties, we propose a new approach which combines the artificial immune system (AIS) theory with a multi-objective optimization algorithm. First, the self-adaptive artificial immune multi-objective algorithm is adopted to pre-sort the difference image. During this procedure, the difference image is categorized into three classes \\u2013 changed class, unchanged class and uncertain samples. Second, based on wavelet decomposition to extract features from the difference image, the immune clonal multi-objective clustering algorithm is used to search for the optimal clustering centers of uncertain samples, labeling them as changed or unchanged. Experimental comparisons with four state-of-the-art approaches show that the proposed algorithm can obtain a higher accuracy, is more robust to noise, and finds solutions which are more globally optimal. Additionally, the proposed algorithm can improve the local search ability for the optimal solutions and produces better cluster centers. The existence of infeasible solutions makes it very difficult to handle constrained optimization problems (COPs) in a way that ensures efficient, optimal and constraint-satisfying convergence. Although further optimization from feasible solutions will typically lead in a direction that generates further feasible solutions, certain infeasible solutions can also provide useful information about the optimal direction of improvement for the objective function. How well an algorithm makes use of these two solutions determines its performance on COPs. This paper proposes a novel selection evolutionary strategy (NSES) for constrained optimization. A self-adaptive selection method is introduced to exploit both informative infeasible and feasible solutions from a perspective of combining feasibility with multi-objective problem (MOP) techniques. Since the global optimal solution of a COP is a feasible non-dominated solution, both non-dominated solutions with low constraint violation and feasible ones with low objective values are beneficial to an evolution process. Thus, the exploration and exploitation of both of these two kinds of solutions are preferred during the selection procedure. Several theorems and properties are given to prove the above assertion. Furthermore, the performance of our method is evaluated using 22 well-known benchmark functions. Experimental results show that the proposed method outperforms state-of-the-art algorithms in terms of the speed of finding feasible solutions and the stability of converging to global optimal solutions. In particular, when dealing with problems that have zero feasibility ratios and more than one active constraint, our method provides feasible solutions within fewer fitness evaluations (FES) and converges to the optimal solutions more reliably than other popular methods from the literature. This paper addresses the problem of finding sparse solutions to linear systems. Although this problem involves two competing cost function terms (measurement error and a sparsity-inducing term), previous approaches combine these into a single cost term and solve the problem using conventional numerical optimization methods. In contrast, the main contribution of this paper is to use a multiobjective approach. The paper begins by investigating the sparse reconstruction problem, and presents data to show that knee regions do exist on the Pareto front (PF) for this problem and that optimal solutions can be found in these knee regions. Another contribution of the paper, a new soft-thresholding evolutionary multiobjective algorithm (StEMO), is then presented, which uses a soft-thresholding technique to incorporate two additional heuristics: one with greater chance to increase speed of convergence toward the PF, and another with higher probability to improve the spread of solutions along the PF, enabling an optimal solution to be found in the knee region. Experiments are presented, which show that StEMO significantly outperforms five other well known techniques that are commonly used for sparse reconstruction. Practical applications are also demonstrated to fundamental problems of recovering signals and images from noisy data. This paper describes an algorithm for the automatic detection of a particular class of underwater sounds, using a single hydrophone. It is observed that many life-forms, systems or mechanisms emit distinctive acoustic signatures which are characterized by packets of relatively high frequency sound that are repeated at regular, low frequency intervals. These types of sounds are commonly produced by biological (e.g. fishes and invertebrates) and anthropogenic (e.g. scuba diver) sources. The algorithm exploits a simple feature, extracted from the raw hydrophone signal, which enables robust detection even in conditions of severe background noise. In order to demonstrate how the algorithm can be used, trial applications are presented for the detection of two different kinds of underwater sound source. First, the algorithm is applied to the problem of detecting soniferous fish sounds, showing that it is possible to robustly automate the detection of instances of cusk-eel presence in hydrophone recordings, thereby simplifying the arduous task of human monitoring of long sound recordings in marine biological research. Second, the algorithm is applied to the problem of automatic diver detection in a noisy urban estuary, demonstrating its potential for harbor security and fleet protection. Our previous work describes a simple algorithm for automated detection of diver presence using a single passive hydrophone. This technique is based on extracting a single feature, the \\\"Swimmer Number\\\", from the hydrophone signal, which correlates with diver presence. At any point in time, diver presence can be automatically determined by thresholding the incoming Swimmer Number at an appropriate level. For this system (and other threshold based detection systems) this paper explains how to calculate the probability of detecting a diver at various ranges from the hydrophone. This function is then used to evaluate the probability of detecting a diver at any point in a region, given an arbitrary number of hydrophones which are scattered in arbitrary positions over the region. We next show how non-linear optimization techniques can be used to find the optimal set of sensor positions, which maximize the detection probability over a region of interest, for a given number of sensors. Lastly, we show how this theory can be incorporated into tracking systems, which estimate the location of a moving diver at any point in time, given the outputs of an arbitrarily positioned set of hydrophones. Abstract This paper presents a method for improved ensemble learning, by treating the optimization of an ensemble of classifiers as a compressed sensing problem. Ensemble learning methods improve the performance of a learned predictor by integrating a weighted combination of multiple predictive models. Ideally, the number of models needed in the ensemble should be minimized, while optimizing the weights associated with each included model. We solve this problem by treating it as an example of the compressed sensing problem, in which a sparse solution must be reconstructed from an under-determined linear system. Compressed sensing techniques are then employed to find an ensemble which is both small and effective. An additional contribution of this paper, is to present a new performance evaluation method (a new pairwise diversity measurement) called the roulette-wheel kappa-error. This method takes into account the different weightings of the classifiers, and also reduces the total number of pairs of classifiers needed in the kappa-error diagram, by selecting pairs through a roulette-wheel selection method according to the weightings of the classifiers. This approach can greatly improve the clarity and informativeness of the kappa-error diagram, especially when the number of classifiers in the ensemble is large. We use 25 different public data sets to evaluate and compare the performance of compressed sensing ensembles using four different sparse reconstruction algorithms, combined with two different classifier learning algorithms and two different training data manipulation techniques. We also give the comparison experiments of our method against another five state-of-the-art pruning methods. These experiments show that our method produces comparable or better accuracy, while being significantly faster than the compared methods. This paper presents a method for tracking a moving target by fusing bi-modal visual information from a deep infra-red thermal imaging camera and a conventional visible spectrum color camera. The tracking method builds on well-known methods for color-based tracking using particle filtering, but it extends these to handle fusion of color and thermal information when evaluating each particle. The key innovation is a method for continuously relearning local background models for each particle in each imaging modality, comparing these against a model of the foreground object being tracked, and thereby adaptively weighting the data fusion process in favor of whichever imaging modality is currently the most discriminating at each successive frame. The method is evaluated by testing on a variety of extremely challenging video sequences, in which people and other targets are tracked past occlusion, clutter, and distracters causing severe and sustained camouflage conditions in one or both imaging modalities. In existing metaheuristics for solving the capacitated arc routing problem, traversal local search operators are often used to explore neighbors of the current solutions. This mechanism is beneficial for finding high-quality solutions; however, it entails a large number of function evaluations, causing high computational complexity. Hence, there is a need to further enhance the efficiency of such algorithms. This paper proposes a high-efficiency immune clonal selection algorithm for capacitated arc routing instances within a limited number of function evaluations. First, an improved constructive heuristic is used to initialize the antibody population. The initial antibodies generated by this heuristic help accelerate the algorithm's convergence. Second, we show how an immune clonal selection algorithm can select in favor of these high-quality antibodies. By adopting a variety of different strategies for different clones of the same antibody, it not only promotes cooperation and information exchanging among antibodies, but also increases diversity and speeds up convergence. Third, two different antibody repair operations are proposed for repairing various kinds of infeasible solutions. These operations cause infeasible solutions to move towards global optima. Experimental studies demonstrate improved performance over state-of-art algorithms, especially on medium-scale instances. Community detection plays an important role in research on network characteristics and in the mining of network information. A variety of algorithms have previously been proposed, but with the continuous growth of network scale, few of them can detect community structure efficiently. Additionally, most of these algorithms only consider non-overlapping community structures in networks. This paper addresses these problems by proposing a new algorithm, based on node membership grade and sub-communities integration, to detect community structure in large-scale networks. The proposed algorithm firstly introduces two functions based on the local information of each node in networks, namely neighboring inter-nodes membership function fMS\\u2212NN and node-to-community membership function fMS\\u2212NC. Firstly, local potential\\u2019s complete sub-graphs are efficiently mined using the function fMS\\u2212NN, and then these small graphs are merged into larger ones in light of local modularity. Secondly, incorrectly divided nodes are modified according to function fMS\\u2212NN. Additionally, by adjusting the parameters in fMS\\u2212NC, we can accurately obtain both non-overlapping communities and overlapping communities. Furthermore, the proposed algorithm employs a framework resembling label propagation, which has low time complexity and is suitable for detecting communities in large-scale networks. Experimental results on both artificial networks and real networks indicate the accuracy and efficiency of the proposed algorithm. We consider the task of monocular visual motion estimation from video image sequences. We hypothesise that performance on the task can be improved by incorporating an understanding of physically likely and feasible object dynamics. We test this hypothesis by incorporating a physical simulator into a least-squares estimation procedure. We initialise a full trajectory estimate using RANSAC followed by gradient descent refinement. We present results for 2D image sequences consisting of single ambiguous, visible or occluded balls, as well as results for 3D computer-generated sequences of objects in free-flight with added noise. Results suggest that restricting the estimation to allow only motions that are feasible according to the physics simulator can produce marked improvement when the observed object motion is within the limits of the physics simulator and its world model. Conversely, merely penalising deviations from feasible physical dynamics produces a consistent but incremental improvement over more common dynamics models. This paper addresses the problems of tracking targets which undergo rapid and significant appearance changes. Our starting point is a successful, state-of-the-art tracker based on an adaptive coupled-layer visual model [10]. In this paper, we identify four important cases when the original tracker often fails: significant scale changes, environment clutter, and failures due to occlusion and rapid disordered movement. We suggest four new enhancements to solve these problems: we adapt the scale of the patches in addition to adapting the bounding box, marginal patch distributions are used to solve patch drifting in environment clutter, a memory is added and used to assist recovery from occlusion, situations where the tracker may lose the target are automatically detected, and a particle filter is substituted for the Kalman filter to help recover the target. We demonstrate the advantages of the enhanced tracker over the original tracker using a test toolkit [17]. We demonstrate the advantages of the enhanced tracker over the original tracker, as well as several other state-of-the art trackers from the literature. Sensors used in protective applications are typically placed on perimeters or over areas in an evenly distributed pattern. However, such patterns may actually be sub-optimal, since environmental factors may make some forms of attack more or less likely than others. We describe a protective application of sensors for detecting underwater threats in an urban estuary environment. We demonstrate that environmental information, derived from a computational river current model, can be utilized to optimize sensor placement, increasing detection rates and decreasing the number of required sensors. Simulation results show a significant improvement in detection for a given number of sensors; alternatively, fewer sensors can be used while still maintaining the detection rate of a conventional approach This paper addresses the problem of constrained motion for a manipulator performing a task while in contact with the environment, and proposes a solution based on projected operational space dynamics. The main advantages of this control technique are: 1) it exploits the environment contact constraint itself, so as to minimise the joint torques needed to perform the task; 2) it enables full decoupling of motion and force control; 3) force feedback from a force sensor mounted at the end effector or other contact points is not needed. This work is a step towards a robot control strategy which mimics the human behaviour of exploiting contacts with the environment to help perform tasks. We present an experimental implementation of the control method in which a KUKA LWR IV manipulator uses an eraser to wipe a whiteboard, and we show that this controller can effectively exploit contact with the whiteboard in order to reduce joint torques while still performing the desired wiping motion. This paper presents a method for tracking a moving target by fusing bi-modal visual information from a deep infrared thermal imaging camera, and a conventional visible spectrum colour camera. The tracking method builds on well-known methods for colour-based tracking using particle filtering, but extends these to handle fusion of colour and thermal information when evaluating each particle. The key innovation is a method for continuously relearning local background models for each particle in each imaging modality, comparing these against a model of the foreground object being tracked, and thereby adaptively weighting the data fusion process in favour of whichever imaging modality is currently the most discriminating at each successive frame. The method is evaluated by testing on a variety of extremely challenging video sequences, in which people and other targets are tracked past occlusion, clutter and distracters causing severe and sustained camouflage conditions in one or both imaging modalities. This paper addresses the problems of automatically planning autonomous underwater vehicle (AUV) paths which best exploit complex current data, from computational estuarine model forecasts, while also avoiding obstacles. In particular we examine the possibilities for a novel type of AUV mission deployment in fast flowing tidal river regions which experience bi-directional current flow. These environments are interesting in that, by choosing an appropriate path in space and time, an AUV may both bypass adverse currents which are too fast to be overcome by the vehicle's motors and also exploit favorable currents to achieve far greater speeds than the motors could otherwise provide, while substantially saving energy. The AUV can \\\"ride\\\" currents both up and down the river, enabling extended monitoring of otherwise energy-exhausting, fast flow environments. The paper discusses suitable path parameterizations, cost functions and optimization techniques which enable optimal AUV paths to be efficiently generated. These paths take maximum advantage of the river currents in order to minimize energy expenditure, journey time and other cost parameters. The resulting path planner can automatically suggest useful alternative mission start and end times and locations to those specified by the user. Examples are presented for navigation in a simple simulation of the fast flowing Hudson River waters around Manhattan. This paper addresses the problems of visual tracking in conditions of extremely poor visibility. The human visual system can often correctly interpret images that are of such poor quality that they contain insufficient explicit information to do so. We assert that such systems must therefore make use of prior knowledge in several forms. A tracking algorithm is presented which combines observed data (the current image) with predicted data derived from prior knowledge of the object being viewed and an estimate of the camera's motion. During image segmentation, a predicted image is used to estimate class conditional distribution models and an Extended-Markov Random Field technique is used to combine observed image data with expectations of that data within a probabilistic framework. Interpretations of scene content and camera position are then mutually improved using Expectation Maximisation. Models of background and tracked object are continually relearned and adapt iteratively with each new image frame. The algorithm is tested using real video sequences, filmed in poor visibility conditions with complete pre-measured ground-truth data. Dexterous grasping of objects with uncertain pose is a hard unsolved problem in robotics. This paper solves this problem using information gain re-planning. First we show how tactile information, acquired during a failed attempt to grasp an object can be used to refine the estimate of that object's pose. Second, we show how this information can be used to replan new reach to grasp trajectories for successive grasp attempts. Finally we show how reach-to-grasp trajectories can be modified, so that they maximise the expected tactile information gain, while simultaneously delivering the hand to the grasp configuration that is most likely to succeed. Our main novel outcome is thus to enable tactile information gain planning for Dexterous, high degree of freedom (DoFs) manipulators. We achieve this using a combination of information gain planning, hierarchical probabilistic roadmap planning, and belief updating from tactile sensors for objects with non-Gaussian pose uncertainty in 6 dimensions. The method is demonstrated in trials with simulated robots. Sequential replanning is shown to achieve a greater success rate than single grasp attempts, and trajectories that maximise information gain require fewer re-planning iterations than conventional planning methods before a grasp is achieved. This paper presents an algorithm for planning sequences of pushes, by which a robotic arm equipped with a single rigid finger can move a manipulated object (or manipulandum) towards a desired goal pose. Pushing is perhaps the most basic kind of manipulation, however it presents difficult challenges for planning, because of the complex relationship between manipulative pushing actions and resulting manipulandum motions. The motion planning literature has well developed paradigms for solving e.g. the piano-mover's problem, where the search occurs directly in the configuration space of the manipulandum object being moved. In contrast, in pushing manipulation, a plan must be built in the action space of the robot, which is only indirectly linked to the motion space of the manipulandum through a complex interaction for which inverse models may not be known. In this paper, we present a two stage approach to planning pushing operations. A global RRT path planner is used to explore the space of possible manipulandum configurations, while a local push planner makes use of predictive models of pushing interactions, to plan sequences of pushes to move the manipulandum from one RRT node to the next. The effectiveness of the algorithm is demonstrated in simulation experiments in which a robot must move a rigid body through complex 3D transformations by applying only a sequence of simple single finger pushes. This paper proposes a dynamic-context cooperative quantum-behaved particle swarm optimization algorithm. The proposed algorithm incorporates a new method for dynamically updating the context vector each time it completes a cooperation operation with other particles. We first explain how this leads to enhanced search ability and improved optimization over previous methods, and demonstrate this empirically with comparative experiments using benchmark test functions. We then demonstrate a practical application of the proposed method, by showing how it can be applied to optimize the parameters for Otsu image segmentation for processing medical images. Comparative experimental results show that the proposed method outperforms other state-of-the-art methods from the literature. Under water robotics projects offer an excellent medium for discovery based engineering and science learning. The challenge of building underwater robotic vehicles and manipulators engages and stimulates students while encompassing a very broad spectrum of engineering disciplines and scientific concepts. This paper describes the successful design and implementation of student projects, building wire guided remotely operated underwater vehicles (ROVs) with motorized grabbers. This work is part of an ongoing effort to incorporate innovative, hands on projects into our freshman engineering curriculum. These projects help expose students to practical design issues in the freshman year, foster creative problem solving skills and may aid student retention on engineering programs. These projects have also been successfully piloted in pre-college programs, aimed at generating interest in engineering careers among high school students. We describe ongoing work to extend these projects to include computer control and sensory feedback, allowing students to develop autonomous underwater vehicles (AUVs). Further, we outline ongoing work to assess the effectiveness of these modules An important task in robot vision is that of determining the position, orientation and trajectory of a moving camera relative to an observed object or scene. Many such visual tracking algorithms have been proposed in the computer vision, artificial intelligence and robotics literature over the past 30 years. However, it is seldom possible to explicitly measure the accuracy of these algorithms, since the ground-truth camera positions and orientations at each frame in a video sequence are not available for comparison with the outputs of the proposed vision systems. A method is presented for generating real visual test data with complete underlying ground truth. The method enables the production of long video sequences, filmed along complicated six-degree-of-freedom trajectories, featuring a variety of objects and scenes, for which complete ground-truth data are known including the camera position and orientation at every image frame, intrinsic camera calibration data, a lens distortion model and models of the viewed objects. This work encounters a fundamental measurement problem\\u2014how to evaluate the accuracy of measured ground truth data, which is itself intended for validation of other estimated data. Several approaches for reasoning about these accuracies are described. An important problem in robotic manipulation is the ability to predict how objects behave under manipulative actions. This ability is necessary to allow planning of object manipulations. Physics simulators can be used to do this, but they model many kinds of object interaction poorly. An alternative is to learn a motion model for objects by interacting with them. In this paper we address the problem of learning to predict the interactions of rigid bodies in a probabilistic framework, and demonstrate the results in the domain of robotic push manipulation. A robot arm applies random pushes to various objects and observes the resulting motion with a vision system. The relationship between push actions and object motions is learned, and enables the robot to predict the motions that will result from new pushes. The learning does not make explicit use of physics knowledge, or any pre-coded physical constraints, nor is it even restricted to domains which obey any particular rules of physics. We use regression to learn efficiently how to predict the gross motion of a particular object. We further show how different density functions can encode different kinds of information about the behaviour of interacting objects. By combining these as a product of densities, we show how learned predictors can cope with a degree of generalisation to previously unencountered object shapes, subjected to previously unencountered push directions. Performance is evaluated through a combination of virtual experiments in a physics simulator, and real experiments with a 5-axis arm equipped with a simple, rigid finger. Sensors used in protective applications are conventionally placed on perimeters or over areas in an evenly distributed pattern. However, such patterns may actually be suboptimal, since environmental factors may make some forms of attack more or less likely than others. We describe a protective application of sensors for detecting underwater threats in an urban estuary environment. We demonstrate that environmental information, derived from a computational river current model, can be utilized to optimize sensor placement, increasing detection rates and decreasing the number of required sensors. Simulation results show a significant improvement in detection likelihood for a given number of sensors; alternatively, fewer sensors can be used while still maintaining the detection rate of a conventional approach The problem of model-based object tracking in three dimensions is addressed. Most previous work on tracking assumes simple motion models, and consequently tracking typically fails in a variety of situations. Our insight is that incorporating physics models of object behaviour improves tracking performance in these cases. In particular it allows us to handle tracking in the face of rigid body interactions where there is also occlusion and fast object motion. We show how to incorporate rigid body physics simulation into a particle filter. We present two methods for this based on pose and force noise. The improvements are tested on four videos of a robot pushing an object, and results indicate that our approach performs considerably better than a plain particle filter tracker, with the force noise method producing the best results over the range of test videos. This paper provides a probabilistic analysis of simple detection systems which are based on thresholding feature values extracted from a sensor signal. For such systems, this paper explains how to calculate the probability of detection as a function of range from the sensor to the object of interest. This function is important in that it enables optimal positioning of a group of sensors, either maximizing detection rates for a given number of sensors or informing the minimum number of sensors necessary to achieve a desired probability of detection throughout an area. An example case study is presented, based on a novel approach to passive acoustic diver detection in noisy environments. Visual tracking of an object can provide a powerful source of feedback information during complex robotic manipulation operations, especially those in which there may be uncertainty about which new object pose may result from a planned manipulative action. At the same time, robotic manipulation can provide a challenging environment for visual tracking, with occlusions of the object by other objects or by the robot itself, and sudden changes in object pose that may be accompanied by motion blur. Recursive filtering techniques use motion models for predictor-corrector tracking, but the simple models typically used often fail to adequately predict the complex motions of manipulated objects. We show how statistical machine learning techniques can be used to train sophisticated motion predictors, which incorporate additional information by being conditioned on the planned manipulative action being executed. We then show how these learned predictors can be used to propagate the particles of a particle filter from one predictor-corrector step to the next, enabling a visual tracking algorithm to maintain plausible hypotheses about the location of an object, even during severe occlusion and other difficult conditions. We demonstrate the approach in the context of robotic push manipulation, where a 5-axis robot arm equipped with a rigid finger applies a series of pushes to an object, while it is tracked by a vision algorithm using a single camera. Visual tracking algorithms have important robotic applications such as mobile robot guidance and servoed wide area surveillance systems. These applications ideally require vision algorithms which are robust to camera motion and scene change but are cheap and fast enough to run on small, low power embedded systems. Unfortunately most robust visual tracking algorithms are either computationally expensive or are restricted to a stationary camera. This paper describes a new color based tracking algorithm, the Adaptive Background CAMSHIFT (ABCshift) tracker and an associated technique, mean shift servoing, for efficient pan-tilt servoing of a motorized camera platform. ABCshift achieves robustness against camera motion and other scene changes by continuously relearning its background model at every frame. This also enables robustness in difficult scenes where the tracked object moves past backgrounds with which it shares significant colors. Despite this continuous machine learning, ABCshift needs minimal training and is remarkably computationally cheap. We first demonstrate how ABCshift tracks robustly in situations where related algorithms fail, and then show how it can be used for real time tracking with pan-tilt servo control using only a small embedded microcontroller. This letter presents a feature weighting method for polarimetric synthetic aperture radar (PolSAR) image classification. Appropriate feature weighting is essential for obtaining accurate classifications but so far has remained an open research problem. We propose in this letter a supervised three-component feature weighting method based on the Fisher linear discriminant. Fisher linear discriminant method is used to calculate a coefficient for each feature. Then, these coefficients are modified according to a three-component scattering power decomposition model, combining both physical and statistical scattering characteristics to adapt them for the particular scattering mechanisms inherent in PolSAR data and assigned to the coherency matrix to enhance the discriminating ability of the features. Freeman decomposition and Wishart classifier are used to classify the PolSAR image. The effectiveness of the proposed method is demonstrated by experiments NASA/JPL AIRSAR L-band and CSA Radarsat-2 C-band PolSAR images of the San Francisco area. Safety critical and demanding tasks (e.g. Search and rescue or hazardous environments inspection), can benefit from robotic systems that offer a spectrum of control modes. These can range from direct teleoperation to full autonomy. This paper describes a pilot-study experiment in which a variable autonomy robot completes a navigation task. It explores the comparative performances of the human-robot system at different autonomy levels under different sets of conditions. This is done from a Mixed-Initiative system investigation perspective. Sensor noise was added to degrade robot performance, while a secondary task induced varying degrees of additional workload on the human operator. Carrying out these experiments and analyzing the initial results, has highlighted the profound complexities of designing tasks, conditions, and performance metrics which are: principled, eliminate confounding factors, and yield scientifically rigorous insights into the intricacies of a collaborative system that combines both human and robot intelligences. A key contribution of this paper is to describe the lessons learned from attempting these experiments, and to suggest a variety of guidelines for other researchers to consider when designing experiments in this context. Community detection plays an important role in reflecting and understanding the topological structure of complex networks, and can be used to help mine the potential information in networks. This paper presents a Multiobjective Evolutionary Algorithm based on Affinity Propagation (APMOEA) which improves the accuracy of community detection. Firstly, APMOEA takes the method of affinity propagation (AP) to initially divide the network. To accelerate its convergence, the multiobjective evolutionary algorithm selects nondominated solutions from the preliminary partitioning results as its initial population. Secondly, the multiobjective evolutionary algorithm finds solutions approximating the true Pareto optimal front through constantly selecting nondominated solutions from the population after crossover and mutation in iterations, which overcomes the tendency of data clustering methods to fall into local optima. Finally, APMOEA uses an elitist strategy, called \\u201cexternal archive\\u201d, to prevent degeneration during the process of searching using the multiobjective evolutionary algorithm. According to this strategy, the preliminary partitioning results obtained by AP will be archived and participate in the final selection of Pareto-optimal solutions. Experiments on benchmark test data, including both computer-generated networks and eight real-world networks, show that the proposed algorithm achieves more accurate results and has faster convergence speed compared with seven other state-of-art algorithms. The fuzzy c-means (FCM) clustering algorithm has been widely used in image segmentation. However, FCM exhibits poor robustness to noise, often leading to unsatisfactory segmentations on noisy images. Additionally, the FCM algorithm is sensitive to the choice of initial cluster centers. In order to solve these problems, this paper proposes clone kernel spatial FCM (CKS_FCM), which improves segmentation performance in several ways. First, in CKS_FCM, an immune clone algorithm is used to generate the initial cluster centers, which helps prevent the algorithm from converging on local optima. Second, CKS_FCM improves the robustness to noise by incorporating spatial information into the objective function of FCM. Third, CKS_FCM uses a non-Euclidean distance based on a kernels metric, instead of the Euclidean distance conventionally used in FCM, to enhance the segmentation accuracy (SA). We present experimental results on both real and synthetic SAR images, which suggest that the proposed method can generate higher accuracy, and obtain more robustness to noise, as compared against six state-of-the-art methods from the literatures. This paper presents a new method for object tracking in a camera sensor with particle filters. The method enables multiple target and background models, arbitrarily spanning many features or imaging modalities, to be adaptively fused to provide optimal discriminating ability against changing backgrounds, which may present varying degrees of clutter and camouflage for different kinds of features at different times. Furthermore, we show how to continuously and robustly relearn all models for all feature modalities online during tracking and for targets whose appearance may be continually changing. Both the data fusion weightings and model relearning parameters are robustly adapted at each frame, by extracting contextual information to inform the saliency assessments of each part of each model. In addition, we propose a two-step estimation method for improving robustness, by preventing excessive drifting of particles during tracking past challenging, cluttered background scenes. We demonstrate the method by implementing a version of the tracker, which combines both shape and color models, and testing it on a publicly available benchmark data set. Results suggest that the proposed method outperforms a number of well-known state-of-the-art trackers from the literature. This paper presents a method for one-shot learning of dexterous grasps and grasp generation for novel objects. A model of each grasp type is learned from a single kinesthetic demonstration and several types are taught. These models are used to select and generate grasps for unfamiliar objects. Both the learning and generation stages use an incomplete point cloud from a depth camera, so no prior model of an object shape is used. The learned model is a product of experts, in which experts are of two types. The first type is a contact model and is a density over the pose of a single hand link relative to the local object surface. The second type is the hand-configuration model and is a density over the whole-hand configuration. Grasp generation for an unfamiliar object optimizes the product of these two model types, generating thousands of grasp candidates in under 30 seconds. The method is robust to incomplete data at both training and testing stages. When several grasp types are considered the method selects the highest-likelihood grasp across all the types. In an experiment, the training set consisted of five different grasps and the test set of 45 previously unseen objects. The success rate of the first-choice grasp is 84.4% or 77.7% if seven views or a single view of the test object are taken, respectively. Learning can be of great use when dealing with problems in various fields. Inspired by locally linear embedding from manifold, we propose a novel automatic change-detection method through an offline learning approach. The proposed method comprises three steps. First, two coupled dictionaries of the difference image (DI) patches and change-detection map patches are generated from known image pairs. Second, we approximately represent each patch of the input DI with respect to the DI dictionary by using the matching the pursuit algorithm. Third, the coefficients of this representation are applied with the change-detection map dictionary to generate the output change-detection map. This way, we exploit the relationship between the DI patches and the corresponding change-detection map patches based on two coupled dictionaries. In addition, the relationship guides us to construct the change-detection map for any given input DI. Experimental results on real synthetic aperture radar databases show that the proposed method is superior to its counterparts. Our method can obtain promising results, even though the dictionaries are prepared by simple random sampling from fixed training images. We propose a new method for online tracking of articulated human body poses.Our method offers online sequential tracking from one frame to the next.Many other methods mutually optimize poses offline over all frames of a sequence.We propose a novel cross-coupled global-local model of articulated human body pose.We propose an adaptive penalty function for optimizing the pose estimates. This paper addresses the problem of online tracking of articulated human body poses in dynamic environments. Many previous approaches perform poorly in realistic applications: often future frames or entire sequences are used anticausally to mutually refine the poses in each individual frame, making online tracking impossible; tracking often relies on strong assumptions about e.g. clothing styles, body-part colours and constraints on body-part motion ranges, limiting such algorithms to a particular dataset; the use of holistic feature models limits the ability of optimisation-based matching to distinguish between pose errors of different body parts. We overcome these problems by proposing a coupled-layer framework, which uses the previous notions of deformable structure (DS) puppet models. The underlying idea is to decompose the global pose candidate in any particular frame into several local parts to obtain a refined pose. We introduce an adaptive penalty with our model to improve the searching scope for a local part pose, and also to overcome the problem of using fixed constraints. Since the pose is computed using only current and previous frames, our method is suitable for online sequential tracking. We have carried out empirical experiments using three different public benchmark datasets, comparing two variants of our algorithm against four recent state-of-the-art (SOA) methods from the literature. The results suggest comparatively strong performance of our method, regardless of weaker constraints and fewer assumptions about the scene, and despite the fact that our algorithm is performing online sequential tracking, whereas the comparison methods perform mutual optimisation backwards and forwards over all frames of the entire video sequence. Single image super-resolution (SR) reconstruction is an ill-posed inverse problem because the high-resolution (HR) image, obtained from the low-resolution (LR) image, is non-unique or unstable. In this paper, single image SR reconstruction is treated as an optimization problem, and a new single image SR method, based on a genetic algorithm and regularization prior model, is proposed. In the proposed method, the optimization problem is constructed with a regularization prior model which consists of the non-local means (NLMs) filter, total variation (TV) and adaptive sparse domain selection (ASDS) scheme for sparse representation. In order to avoid local optimization, we combine the genetic algorithm and the iterative shrinkage algorithm to deal with the regularization prior model. Compared with several other state-of-the-art algorithms, the proposed method demonstrates better performances in terms of both numerical analysis and visual effect. This paper presents a novel method for single target tracking in RGB images under conditions of extreme clutter and camouflage, including frequent occlusions by objects with similar appearance as the target. In contrast to conventional single target trackers, which only maintain the estimated target status, we propose a multi-level clustering-based robust estimation for online detection and learning of multiple target-like regions, called distractors, when they appear near to the true target. To distinguish the target from these distractors, we exploit a global dynamic constraint (derived from the target and the distractors) in a feedback loop to improve single target tracking performance in situations where the target is camouflaged in highly cluttered scenes. Our proposed method successfully prevents the estimated target location from erroneously jumping to a distractor during occlusion or extreme camouflage interactions. To gain an insightful understanding of the evaluated trackers, we have augmented publicly available benchmark videos, by proposing a new set of clutter and camouflage sub-attributes, and annotating these sub-attributes for all frames in all sequences. Using this dataset, we first evaluate the effect of each key component of the tracker on the overall performance. Then, the proposed tracker is compared to other highly ranked single target tracking algorithms in the literature. The experimental results show that applying the proposed global dynamic constraint in a feedback loop can improve single target tracker performance, and demonstrate that the overall algorithm significantly outperforms other state-of-the-art single target trackers in highly cluttered scenes. In recent years, a variety of feature selection algorithms based on subspace learning have been proposed. However, such methods typically do not exploit information about the underlying geometry of the data. To overcome this shortcoming, we propose a novel algorithm called subspace learning-based graph regularized feature selection (SGFS). SGFS builds on the feature selection framework of subspace learning, but extends it by incorporating the idea of graph regularization, in which a feature map is constructed on the feature space in order to preserve geometric structure information on the feature manifold. Additionally, the L2,1-norm is used to constrain the feature selection matrix to ensure the sparsity of the feature array and avoid trivial solutions. The resulting method can provide more accurate discrimination information for feature selection. We evaluate SGFS by comparing it against five other state-of-the-art algorithms from the literature, on twelve publicly available benchmark data sets. Empirical results suggest that SGFS is more effective than the other five feature selection algorithms. Underwater robotics projects offer an excellent medium for discovery based engineering and science learning. The challenge of building underwater robotic vehicles and manipulators engages and stimulates students while encompassing a broad spectrum of engineering disciplines and scientific concepts. This paper describes an ongoing effort, at Stevens Institute of Technology, to incorporate such projects into the engineering curriculum. We report the successful design and implementation of student projects, building wire guided remotely operated underwater vehicles (ROVs) with motorized grabbers. We also describe ongoing work to extend these projects to include computer control and sensory feedback, allowing students to develop autonomous underwater vehicles (AUVs). The effectiveness of these modules for teaching fundamental engineering skills will be independently assessed in accordance with established educational theory. This work addresses the problem of planning the reach-to-grasp trajectory for a robotic arm and hand, when there is uncertainty in the pose of the object being grasped. If the object is not in its expected location, then the robot may still gain additional information about the object pose by making tactile or haptic observations if a finger or other part of the hand collides with part of the object during the reach-to-grasp operation. Therefore, it is desirable to plan the reach-to-grasp trajectory in such a way that it takes into account and exploits knowledge about the size and shape of the pose distribution associated with the target pose uncertainty. Here we propose a reach-to-grasp trajectory planning algorithm which addresses this exploration-action problem by trading off a smoothness constraint against likelihood of making haptic observations. Visual tracking has attracted a significant attention in the last few decades. The recent surge in the number of publications on tracking-related problems have made it almost impossible to follow the developments in the field. One of the reasons is that there is a lack of commonly accepted annotated data-sets and standardized evaluation protocols that would allow objective comparison of different tracking methods. To address this issue, the Visual Object Tracking (VOT) workshop was organized in conjunction with ICCV2013. Researchers from academia as well as industry were invited to participate in the first VOT2013 challenge which aimed at single-object visual trackers that do not apply pre-learned models of object appearance (model-free). Presented here is the VOT2013 benchmark dataset for evaluation of single-object visual trackers as well as the results obtained by the trackers competing in the challenge. In contrast to related attempts in tracker benchmarking, the dataset is labeled per-frame by visual attributes that indicate occlusion, illumination change, motion change, size change and camera motion, offering a more systematic comparison of the trackers. Furthermore, we have designed an automated system for performing and evaluating the experiments. We present the evaluation protocol of the VOT2013 challenge and the results of a comparison of 27 trackers on the benchmark dataset. The dataset, the evaluation tools and the tracker rankings are publicly available from the challenge website (http://votchallenge.net). The Visual Object Tracking challenge VOT2016 aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 70 trackers are presented, with a large number of trackers being published at major computer vision conferences and journals in the recent years. The number of tested state-of-the-art trackers makes the VOT 2016 the largest and most challenging benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the Appendix. The VOT2016 goes beyond its predecessors by (i) introducing a new semi-automatic ground truth bounding box annotation methodology and (ii) extending the evaluation system with the no-reset experiment. The dataset, the evaluation kit as well as the results are publicly available at the challenge website (http://votchallenge.net).\",\n",
            "  \"35480463\": \"The concept of the ambiguity function (AF) is extended to linear signal spaces. Some properties of the AF of a linear signal space, its relations with other space representations, and the results obtained for some specific signal spaces are discussed. It is shown that the AF of a signal space describes the performance of the maximum likelihood (ML) range/Doppler estimator for a slowly fluctuating point target when a series of orthogonal signal pulses are transmitted. This multipulse estimator is more accurate than a conventional signal-pulse estimator since the AF of a linear signal space may have arbitrarily good thumbtack shape. > The performance of coherent MIMO-OFDM systems critically depends on the availability of accurate channel estimates. In wireless communications, the MIMO channel is time-varying and thus has to be tracked by the receiver. In this paper, we develop an extended Kalman filter technique for pilot symbol assisted MIMO-OFDM channel tracking. Our tracking scheme is able to exploit spatial correlations of the channel. It includes on-line estimation of the channel's state-space parameters and hence does not require any prior knowledge. Simulations using measured channels demonstrate the excellent performance of our channel tracking scheme. It has been shown that continuous-time orthonormal Wilson bases with good time-frequency localization can be constructed. We introduce and discuss discrete-time Wilson function sets and frames, and we show that Wilson sets and frames (potentially oversampled) can be derived from Weyl-Heisenberg sets and frames. We also show that discrete-time Wilson expansions correspond to a new class of cosine-modulated filter banks. We propose a distributed sequential estimation scheme for wireless sensor networks with asynchronous measurements. Our scheme combines the prediction and update steps of a Bayesian filter (for time alignment and recursive state estimation) with a fusion rule (for intersensor fusion using local communication). We also propose a reduced-complexity implementation using particle filtering and Gaussian mixture approximations, and an estimator of the delays resulting from processing and communication. Simulations for a target tracking problem demonstrate the good performance of our scheme. A time-frequency analysis of linear, time-varying (LTV) systems may be based on two new time-frequency representations of LTV systems called the input Wigner distribution (WD) and the output WD. The two WD definitions coincide in the case of normal systems. The proposed WD representation can also be applied to the optimum design of LTV systems. > Various methods for time-varying filtering with prescribed time-frequency (TF) pass region are compared analytically and numerically. The Weyl correspondence permits a unified TF analysis of apparently unrelated concepts of linear TF filtering, including STFT-based filters, Weyl filters, and TF projection filters. The linear TF filters are compared to nonlinear TF analysis/masking/synthesis methods based on the Wigner distribution (WD) or the smoothed WD. The study of TF (signal separation problems permits the objective evaluation of TF filter methods. Linear methods yield improved performance and had reduced cost as compared to nonlinear methods. > We introduce a framework and methodology of cooperative simultaneous localization and tracking (CoSLAT) in decentralized mobile agent networks. CoSLAT provides a con- sistent combination of cooperative self-localization (CS L) and distributed target tracking (DTT). Multiple mobile targets and mobile agents are tracked using pairwise measurements between agents and targets and between agents. We propose a distributed CoSLAT algorithm that combines particle-based belief propa- gation with the likelihood consensus scheme and performs a bidirectional probabilistic information transfer between CSL and DTT. Simulation results demonstrate significant improveme nts in both self-localization and target tracking performance compared to separate CSL and DTT. A method is presented for time-frequency filtering, i.e. filtering with pass and stop regions directly defined in the time-frequency plane. The filtering is performed by projecting the input signal on a time-frequency signal subspace corresponding to the time-frequency pass region prescribed. This projection takes the form of a linear time-varying filter where the filter's impulse response is constructed using Wigner distribution signal synthesis techniques. > In the tradition of Gabor's 1946 landmark paper [1], we advocate a time-frequency (TF) approach to communications. TF methods for communications have been proposed very early (see the box History). While several tutorial papers and book chapters on the topic are available (see, e.g., [2]-[4] and references therein), the goal of this paper is to present the fundamental aspects in a coherent and easily accessible manner. Specifically, we establish the role of TF methods in communications across a range of subject areas including TF dispersive channels, orthogonal frequency division multiplexing (OFDM), information-theoretic limits, and system identification and channel estimation. Furthermore, we present fundamental results that are stated in the literature for the continuous-time case in simple linear algebra terms. The nonstationary Wiener filter (WF) is the optimum linear system for estimating a nonstationary signal contaminated by nonstationary noise. We propose a time-frequency (TF) formulation of nonstationary WFs for the practically important case of underspread processes. This TF formulation extends the spectral representation of stationary WFs to the nonstationary case, and it allows an approximate TF design of nonstationary WFs. For underspread processes, the performance obtained with the approximate TF design is close to that of the exact WF. The nth power class (PC/sub n/) of quadratic time-frequency representations (QTFRs) is specifically suited for the multiresolution analysis of signals passing through systems whose dispersion characteristic is approximately f/sup /spl kappa//. This paper considers several aspects of PC analysis important in applications. We discuss the geometry of PC auto terms and cross terms, and we propose new PC QTFRs that attenuate cross terms via smoothing. We describe the implementation of PC QTFRs via warping techniques. Finally, simulation results demonstrate the advantages of PC analysis. We apply tools from free probability theory to the asymptotic analysis of MIMO systems with spatial correlation at the transmitter or receiver side. The MIMO channel is modeled by a one-sided Kronecker model with exponential transmitter or receiver correlation matrix. We compute the asymptotic channel capacity based on a calculation of the asymptotic eigenvalue pdf. The dependence of the asymptotic channel capacity on the SNK, dimension ratio, and correlation parameter is studied. Finally, it is demonstrated that the asymptotic capacity closely approximates the simulated capacity of finite-dimensional MIMO channels already for moderate system dimensions. We propose time-frequency (TF) formulations, efficient and stable TF design procedures, and an efficient TF implementation for quadratic detectors. Conventional and TF detectors are applied to knock detection and compared with regard to performance and required a priori knowledge. It is shown that TF detectors are advantageous under realistic conditions where robustness and stability are important. The pseudo-Wigner distribution (PWD) is a time-frequency signal representation particularly suited for analyzing and processing 'long' signals. Signal processing by means of PWD involves a signal synthesis step. Two signal synthesis algorithms for PWD are presented. These are the pseudopower method which allows optimal signal synthesis, but is computationally expensive for longer signals, and the partial sum method, which is suboptimal but suited for the synthesis of signals with arbitrary length. The performance of the two algorithms is demonstrated by simple synthesis experiments. > Channel estimation is an important and challenging task in MIMO communications. The minimum mean-square-error (MMSE) channel estimator is able to exploit spatial correlation of the MIMO channel but requires prior estimation of the channel correlation matrix. In this paper, we investigate pilot-based MMSE channel estimation including channel correlation estimation. We propose an MMSE channel estimator using a structured correlation estimator and demonstrate its advantages over conventional MMSE estimators. Simulation results show that the proposed channel estimator outperforms conventional channel estimators in the case of strong spatial correlation and at low SNR. We propose an unbiased, computationally efficient estimator of the scattering function that is specifically suited to underspread WSSUS channels. The novel estimator allows for nonideal sounding signals, fast time-variation, and on-line operation during data transmission. Its good performance is assessed by a bias/variance analysis and simulation results. We develop a distributed particle filter for sequential estimation of a global state in a decentralized wireless sensor network. A global state estimate that takes into account the measurements of all sensors is computed in a distributed manner, using only local calculations at the individual sensors and local communication between neighboring sensors. The paper presents two main contributions. First, the likelihood consensus scheme for distributed calculation of the joint likelihood function (used by the local particle filters) is generalized to arbitrary local likelihood functions. This generalization overcomes the restriction to exponential-family likelihood functions that limited the applicability of the original likelihood consensus (Hlinka , \\u201cLikelihood consensus and its application to distributed particle filtering,\\u201d IEEE Trans. Signal Process., vol. 60, pp. 4334\\u20134349, Aug. 2012). The second contribution is a consensus-based distributed method for adapting the proposal densities used by the local particle filters. This adaptation takes into account the measurements of all sensors, and it can yield a significant performance improvement or, alternatively, a significant reduction of the number of particles required for a given level of accuracy. The performance of the proposed distributed particle filter is demonstrated for a target tracking problem. We present time-frequency (TF) formulations of optimal detectors for various nonstationary detection scenarios involving underspread processes. These TF formulations yield a better understanding of optimal detectors and simple TF design procedures. A recently developed method for separating time-frequency disjoint signal components is based on the Wigner distribution (WD) and uses a time-frequency masking procedure followed by a signal synthesis step. A problem with this method is that the positioning of the time-frequency masks is done manually and can be difficult due to WD interference terms. Also, masking is ineffective if a WD interference term overlaps with the WD signal term to be isolated. The authors present an approach to time-frequency signal decomposition which uses an extended version of signal synthesis and does not require a masking procedure. > When a signal passes through a linear, time-varying (LTV) system various signal components are weighted (attenuated or amplified) and displaced in the time-frequency (TF) plane. A gross description of TF displacement effects is provided by centroids and spreads whose definition is based on the Wigner distribution. The classes of normal, time-invariant, frequency-invariant, and unitary systems are considered, and the class of systems minimizing FF displacement (for fixed TF weighting) is derived. The polar decomposition of LTV systems is shown to provide a separation of weighting and displacement effects. > Signal synthesis is an indispensable part of signal processing schemes based on bilinear time-frequency representations like Wigner distribution, ambiguity function or spectrogram. We present a comprehensive theory of signal synthesis in which a signal subspace constraint imparts flexibility to the synthesis process. Representing the signal subspace by a projection operator or an explicit basis results in two different types of synthesis algorithms. Subspace synthesis is particularly suited for Wigner distribution and ambiguity function. Presents a theory of linear and quadratic time-frequency representations (TFRs) that are covariant to time-frequency displacement operators. The theory unifies important TFR classes (short-time Fourier transform, wavelet transform; Cohen's, affine, hyperbolic, and power classes), and it allows the systematic construction of new TFRs that are covariant to a given operator. > Localization and synchronization in wireless networks are strongly related when they are based on internode time measurements. We leverage this relation by presenting a message passing algorithm for cooperative simultaneous localization and synchronization (CoSLAS). The proposed algorithm jointly estimates the locations and clock parameters of the network nodes in a fully decentralized manner while requiring time measurements and communications only between neighboring nodes and making only minimal assumptions about the network topology. Low computation and communication requirements are achieved by a hybrid use of sample-based and Gaussian belief propagation. Our simulations demonstrate performance advantages of the proposed CoSLAS algorithm over separate state-of-the-art localization and synchronization algorithms. We propose low-complexity detectors for large MIMO systems with BPSK or QAM constellations. These detectors work at the bit level and consist of three stages. In the first stage, maximum likelihood decisions on certain bits are made in an efficient way. In the second stage, soft values for the remaining bits are calculated. In the third stage, these remaining bits are detected by means of a heuristic programming method for high-dimensional optimization that uses the soft values (\\u201csoft-heuristic\\u201d algorithm). We propose two soft-heuristic algorithms with different performance and complexity. We also consider a feedback of the results of the third stage for computing improved soft values in the second stage. Simulation results demonstrate that, for large MIMO systems, our detectors can outperform state-of-the-art detectors based on nulling and canceling, semidefinite relaxation, and likelihood ascent search. We consider scale-covariant quadratic time-frequency representations (QTFRs) specifically suited for the analysis of signals passing through dispersive systems. These QTFRs satisfy a scale covariance property that is equal to the scale covariance property satisfied by the continuous wavelet transform and a covariance property with respect to generalized time shifts. We derive an existence/representation theorem that shows the exceptional role of time shifts corresponding to group delay functions that are proportional to powers of frequency. This motivates the definition of the power classes (PCs) of QTFR's. The PCs contain the affine QTFR class as a special case, and thus, they extend the affine class. We show that the PCs can be defined axiomatically by the two covariance properties they satisfy, or they can be obtained from the affine class through a warping transformation. We discuss signal transformations related to the PCs, the description of the PCs by kernel functions, desirable properties and kernel constraints, and specific PC members. Furthermore, we consider three important PC subclasses, one of which contains the Bertrand (1992) P/sub k/ distributions. Finally, we comment on the discrete-time implementation of PC QTFRs, and we present simulation results that demonstrate the potential advantage of PC QTFRs. The evolutionary spectrum (ES) is a \\\"time-varying power spectrum\\\" of nonstationary random processes. Starting from an innovations system interpretation of the ES, we introduce the generalized evolutionary spectrum (GES) as a novel family of time-varying power spectra. The GES contains the ES and the transitory evolutionary spectrum as special cases. We consider the problem of finding an innovations system for a process characterized by its correlation function, and we discuss the connection between GES analysis and the class of underspread processes. Furthermore, we show that another special case of the GES-a novel time-varying power spectrum that we call the Weyl spectrum-has substantial advantages over all other members of the GES family. The properties of the Weyl spectrum are discussed, and its superior performance is verified experimentally for synthetic and real-data processes. We propose a low-complexity detector for multiple-input multiple-output (MIMO) systems using BPSK or QAM constellations. The detector operates at the bit level and is especially advantageous for large MIMO systems. It consists of three stages performing partial ML detection, generation of soft values, and soft-input genetic optimization. For the last stage, we present a genetic programming algorithm that uses the soft values computed by the second stage. Simulation results demonstrate that for large systems, our detector can outperform state-of-the-art methods, and its complexity scales roughly cubically with the system dimension. We introduce two methods for quantization noise reduction in oversampled filter banks. These methods are based on predictive quantization (noise shaping or linear prediction). It is demonstrated that oversampled noise shaping or linear predictive subband coders are well suited for subband coding applications where, for technological or other reasons, low-resolution quantizers have to be used. In this case, oversampling combined with noise shaping or linear prediction improves the effective resolution of the subband coder at the expense of increased rate. Simulation results are provided to assess the achievable quantization noise reduction and resolution enhancement, and to investigate the rate-distortion properties of the proposed methods. For ECG interpretation, the detection and delineation of P and T waves are challenging tasks. This paper proposes sequential Bayesian methods for simultaneous detection, threshold-free delineation, and waveform estimation of P and T waves on a beat-to-beat basis. By contrast to state-of-the-art methods that process multiple-beat signal blocks, the proposed Bayesian methods account for beat-to-beat waveform variations by sequentially estimating the waveforms for each beat. Our methods are based on Bayesian signal models that take into account previous beats as prior information. To estimate the unknown parameters of these Bayesian models, we first propose a block Gibbs sampler that exhibits fast convergence in spite of the strong local dependencies in the ECG signal. Then, in order to take into account all the information contained in the past rather than considering only one previous beat, a sequential Monte Carlo method is presented, with a marginalized particle filter that efficiently estimates the unknown parameters of the dynamic model. Both methods are evaluated on the annotated QT database and observed to achieve significant improvements in detection rate and delineation accuracy compared to state-of-the-art methods, thus providing promising approaches for sequential P and T wave analysis. We introduce a generalized concept of underspread linear time-varying systems (linear operators) which contains a previous definition as a special case. We show that an existing approximate transfer function calculus (symbolic calculus) can be extended to this wider and practically more relevant class of underspread operators. As a mathematical underpinning of this calculus, we establish explicit bounds on various error quantities associated with it. The transfer function calculus provides a theoretical basis for various methods recently proposed for nonstationary signal processing, and it has important implications in the theory of time-varying power spectra. We consider the estimation of a sparse parameter vector from measurements corrupted by white Gaussian noise. Our focus is on unbiased estimation as a setting under which the difficulty of the problem can be quantified analytically. We show that there are infinitely many unbiased estimators but none of them has uniformly minimum mean-squared error. We then provide lower and upper bounds on the Barankin bound, which describes the performance achievable by unbiased estimators. These bounds are used to predict the threshold region of practical estimators. The problem studied in this paper is unbiased estimation of a sparse nonrandom vector corrupted by additive white Gaussian noise. It is shown that while there are infinitely many unbiased estimators for this problem, none of them has uniformly minimum variance. Therefore, the focus is placed on locally minimum variance unbiased (LMVU) estimators. Simple closed-form lower and upper bounds on the variance of LMVU estimators or, equivalently, on the Barankin bound (BB) are derived. These bounds allow an estimation of the threshold region separating the low-signal-to-noise ratio (SNR) and high-SNR regimes, and they indicate the asymptotic behavior of the BB at high SNR. In addition, numerical lower and upper bounds are derived; these are tighter than the closed-form bounds and thus characterize the BB more accurately. Numerical studies compare the proposed characterizations of the BB with established biased estimation schemes, and demonstrate that while unbiased estimators perform poorly at low SNR, they may perform better than biased estimators at high SNR. An interesting conclusion of this analysis is that the high-SNR behavior of the BB depends solely on the value of the smallest nonzero entry of the sparse vector, and that this type of dependence is also exhibited by the performance of certain practical estimators. We propose a sequential likelihood consensus (SLC) for a distributed, sequential computation of the joint (all-sensors) likelihood function (JLF) in a wireless sensor network. The SLC is based on a novel dynamic consensus algorithm, of which only a single iteration is performed per time step. We demonstrate the application of the SLC in a distributed particle filter with low communication requirements and low latency. Because the JLF is available at each sensor, the local particle filters at the individual sensors take into account the measurements of all sensors. The performance of the proposed distributed particle filter is assessed for a target tracking problem. Abstract We propose the generalized class of quadratic time-frequency representations (QTFRs) that satisfy the scale covariance property, which is important in multiresolution analysis, and the generalized time-shift covariance property, which is important in the analysis of signals propagating through systems with specific dispersive characteristics. We discuss a formulation of the generalized class QTFRs in terms of two-dimensional kernel functions, a generalized signal expansion related to the generalized class time-frequency geometry, an important member of the generalized class, a set of desirable QTFR properties and their corresponding kernel constraints, and a \\u201clocalized-kernel\\u201d generalized subclass that is characterized by one-dimensional kernels. Special cases of the generalized QTFR class include theaffine classand the newhyperbolic classandpower classes.All these QTFR classes satisfy the scale covariance property. In addition, the affine QTFRs are covariant to constant time shifts, the hyperbolic QTFRs are covariant to hyperbolic time shifts, and the power QTFRs are covariant to power time shifts. We present a detailed study of these classes that includes their definition and formulation, an associated generalized signal expansion, important class members, desirable QTFR properties and corresponding kernel constraints, and localized-kernel subclasses. Also, we investigate the subclasses formed by the intersection between the affine and hyperbolic classes, the affine and power classes, and the hyperbolic and power classes. These subclasses are important since their members satisfy additional desirable properties. We show that the hyperbolic class is obtained from Cohen's QTFR class using a \\u201chyperbolic time-frequency warping\\u201d and that the power classes are obtained similarly by applying a \\u201cpower time-frequency warping\\u201d to the affine class. The affine class is a special case of the power classes. Furthermore, we generalize the time-frequency warping so that when applied either to Cohen's class or to the affine class, it yields QTFRs that are always generalized time-shift covariant but not necessarily scale covariant. We consider Bayesian detection/classification of discrete random parameters that are strongly dependent locally due to some deterministic local constraint. Based on the recently introduced partially collapsed Gibbs sampler (PCGS) principle, we develop a Markov chain Monte Carlo method that tolerates and even exploits the challenging probabilistic structure imposed by deterministic local constraints. We study the application of our method to the practically relevant case of nonuniformly spaced binary pulses with a known minimum distance. Simulation results demonstrate significant performance gains of our method compared to a recently proposed PCGS that is not specifically designed for the local constraint. We propose classes of quadratic time-frequency distributions that retain the inner structure of Cohen's (see IEEE Trans. Signal Processing, vol.41, no.12, p.3275-3292, 1993) class. Each of these classes is based on a pair of \\\"conjugate\\\" unitary operators producing time-frequency displacements. The classes satisfy covariance and marginal properties corresponding to these operators. For each class, we define a \\\"central member\\\" generalizing the Wigner distribution and the Q-distribution, and we specify a transformation by which the class can be derived from Cohen's class. An overlapping method (OM) for signal synthesis from modified pseudo-Wigner distributions (PWD) has been suggested. The authors present a suboptimal PWD signal synthesis algorithm which, like the OM, has a recursive structure and thus allows signal synthesis without an inherent restriction of signal length. This algorithm is called the simplified partial sum method (SPSM). While the SPSM appears to be similar to the OM in its formulation, its results are generally quite different. OM and SPSM are compared in a simple synthesis experiment and it is shown that while OM results are clearly improper, SPSM yields a satisfactory performance. > We consider minimum variance estimation within the sparse linear Gaussian model (SLGM). A sparse vector is to be estimated from a linearly transformed version embedded in Gaussian noise. Our analysis is based on the theory of reproducing kernel Hilbert spaces (RKHS). After a characterization of the RKHS associated with the SLGM, we derive a lower bound on the minimum variance achievable by estimators with a prescribed bias function, including the important special case of unbiased estimation. This bound is obtained via an orthogonal projection of the prescribed mean function onto a subspace of the RKHS associated with the SLGM. It provides an approximation to the minimum achievable variance (Barankin bound) that is tighter than any known bound. Our bound holds for an arbitrary system matrix, including the overdetermined and underdetermined cases. We specialize it to compressed sensing measurement matrices and express it in terms of the restricted isometry constant. For the special case of the SLGM given by the sparse signal in noise model, we derive closed-form expressions of the Barankin bound and of the corresponding locally minimum variance estimator. Finally, we compare our bound with the variance of several well-known estimators, namely, the maximum-likelihood estimator, the hard-thresholding estimator, and compressive reconstruction using orthogonal matching pursuit and approximate message passing. We propose a Bayesian method for detecting multiple events in signals under the practically relevant assumption that successive events may not be arbitrarily close and distant events are effectively independent. Our detector has low complexity since it involves only the (Monte Carlo approximation to the) one-dimensional marginal posteriors. However, its performance is good since the metric it minimizes depends on the entire event sequence. We also describe an efficient sequential implementation of our detector that is based on a tree representation and a recursive metric computation. The authors introduce the Wigner distribution (WD) of a linear signal space and show how this concept can be used for the time-frequency analysis and synthesis of linear signal spaces and the optimal design of time-frequency projection filters. The WD of a linear signal space describes the space's energy distribution over the time-frequency plane and possesses many interesting properties. It can be expressed as the Weyl symbol of the space's projection operator or, alternatively, as the sum of WDs of all basis signals. As an analysis tool, the WD of a signal space characterizes the space's time-frequency localization. From the WD, parameters can be derived which allow a global quantitative characterization of localization and concentration properties, and inequalities bounding these parameters can be shown to exist. > Recently connections between the wavelet transform and filter banks have been established. We show that similar relations exist between the Gabor expansion and DFT filter banks. We introduce the \\\"z-Zak transform\\\" by suitably extending the discrete-time Zak transform and show its equivalence to the polyphase representation. A systematic discussion of parallels between DFT filter banks and Weyl-Heisenberg frames (Gabor expansion theory) is then given. Among other results, it is shown that tight Weyl-Heisenberg frames correspond to paraunitary DFT filter banks. The wavelet transform, filter banks, and multiresolution signal analysis have recently been unified within a single theory.'6 This led to new results and deeper insights in both areas. In this paper, we show that an important linear time-frequency representation known as the Gabor expansion79 and the computationally efficient DFT filter banks'\\u00b0'4'6 can be unified in a similar manner.15 We show that the theory of Weyl-Heisenberg frames ( WHFs) ,16-18 which is a fundamental concept in Gabor expansion theory, allows to establish known and new results on DFT filter banks. We also extend the Zak transfoin,'92' a transformation particularly useful for the Gabor expansion, to the complex plane (z-plane), and we show that the resulting \\\"z-Zak transform\\\" is equivalent to the polyphase representation used in filter bank theory.22\\\"\\u00b0'6\\\"2\\\"3 Current signal post processing in spectrally encoded frequency domain (FD) optical coherence microscopy (OCM) and optical coherence tomography (OCT) uses Fourier transforms in combination with non-uniform resampling strategies to map the k-space data acquired by the spectrometer to spatial domain signals which are necessary for tomogram generation. We propose to use a filter bank (FB) framework for the remapping process. With our new approach, the spectrometer is modeled as a critically sampled analysis FB, whose outputs are quantized subband signals that constitute the k-space spectroscopic data. The optimal procedure to map this data to the spatial domain is via a suitably designed synthesis FB which has low complexity. FB theory additionally states that 1) it is possible to find a synthesis FB such that the overall system has the perfect reconstruction (PR) property; 2) any processing on critically sampled subband signals (as done in current schemes) results in aliasing artifacts. These perspectives are evaluated both theoretically and experimentally. We determine the analysis FB corresponding to our FD-OCM system by using a tunable laser and show that for our grating-based spectrometer - employing a CCD-line camera - the non-uniform resampling together with FFT indeed causes aliasing terms and depth dependent signal attenuation. Furthermore, we compute a finite impulse response based synthesis FB and assess the desired PR property by means of layered samples. The resulting images exhibit higher resolution and improved SNR compared to the common FFT-based approach. The potential of the proposed FB approach opens a new perspective also for other spectroscopic applications. We propose a compressive estimator for the discrete Rihaczek spectrum (RS) of a time-frequency sparse, underspread, nonstationary random process. The new estimator uses a compressed sensing technique to achieve a reduction of the number of measurements. The measurements are randomly located samples of the ambiguity function of the observed signal. We provide a bound on the mean-square estimation error and demonstrate the performance of the estimator by means of simulation results. The proposed RS estimator can also be used for estimating the Wigner-Ville spectrum (WVS) since for an underspread process the RS and WVS are almost equal. Cooperative sensor self-localization (CSL) in wireless networks usually requires the nodes to be equipped with specific ranging hardware including ultra-wideband or ultrasonic distance sensors. Such designs are not suitable for application in low-cost, low-power sensor networks. Here, we demonstrate how low-cost, low-power, asynchronous sensor nodes can be used to perform CSL (and, simultaneously, distributed synchronization) by means of time-stamped communication without additional ranging hardware. Our method combines a belief propagation message passing algorithm for cooperative simultaneous localization and synchronization (CoSLAS) with a MAC-layer time stamping scheme.We validate the models underlying the CoSLAS algorithm by means of measurements, and we demonstrate that the localization accuracy achieved by our hardware implementation is far better than that corresponding to the time resolution and measurement errors of the hardware. The authors discuss the signal synthesis problem in the general framework of bilinear signal representations (BSRs), thereby obtaining a unified treatment which encompasses, e.g., the Wigner distribution and ambiguity function as special cases. The inclusion of a signal space constraint serves to impart flexibility to the signal synthesis process and to relax mathematical requirements. The characterization of signal spaces either by orthogonal projection operators or by orthonormal bases leads to two different signal synthesis methods. Both methods assume the BSR to be unitary (i.e., satisfy Moyal's formula) on the signal space on which signal synthesis is performed. As an application of the general signal synthesis methods, band-limited signal synthesis in the case of the Wigner distribution is considered. > Hitherto communication theory was based on two alternative methods of signal analysis. One is the description of the signal as a function of time; the other is Fourier analysis. Both are idealizations, as the first method operates with sharply defined instants of time, the second with infinite wave-trains of rigorously defined frequencies. But our everyday experiences?especially our auditory sensations?insist on a description in terms of both time and frequency.? A tutorial review of both linear and quadratic representations is given. The linear representations discussed are the short-time Fourier transform and the wavelet transform. The discussion of quadratic representations concentrates on the Wigner distribution, the ambiguity function, smoothed versions of the Wigner distribution, and various classes of quadratic time-frequency representations. Examples of the application of these representations to typical problems encountered in time-varying signal processing are provided. > Distributed particle filter (DPF) algorithms are sequential state estimation algorithms that are executed by a set of agents. Some or all of the agents perform local particle filtering and interact with other agents to calculate a global state estimate. DPF algorithms are attractive for large-scale, nonlinear, and non-Gaussian distributed estimation problems that often occur in applications involving agent networks (ANs). In this article, we present a survey, classification, and comparison of various DPF approaches and algorithms available to date. Our emphasis is on decentralized ANs that do not include a central processing or control unit. The authors present two methods for the design of perfect-reconstruction filter banks corresponding to a partition of the time-frequency plane into nonoverlapping pass regions. The first method is an extension of the concept of optimal time-frequency projection filters. The second method is an approximate filter bank design with reduced computational cost. > The mathematical theory of reproducing kernel Hilbert spaces (RKHSs) provides powerful tools for minimum variance estimation (MVE) problems. Here, we extend the classical RKHS-based analysis of MVE in several directions. We develop a geometric formulation of five known lower bounds on the estimator variance (Barankin bound, Cramer-Rao bound, constrained Cramer-Rao bound, Bhattacharyya bound, and Hammersley-Chapman\\u2013Robbins bound) in terms of orthogonal projections onto a subspace of the RKHS associated with a given MVE problem. We show that, under mild conditions, the Barankin bound (the tightest possible lower bound on the estimator variance) is a lower semicontinuous function of the parameter vector. We also show that the RKHS associated with an MVE problem remains unchanged if the observation is replaced by a sufficient statistic. Finally, for MVE problems conforming to an exponential family of distributions, we derive novel closed-form lower bounds on the estimator variance and show that a reduction of the parameter set leaves the minimum achievable variance unchanged. The time-frequency (TF) version of the wavelet transform and the \\\"affine\\\" quadratic/bilinear TF representations can be used for a TF analysis with constant-Q characteristic. The paper considers a new approach to constant-Q TF analysis. A specific TF warping transform is applied to Cohen's class of quadratic TF representations, which results in a new class of quadratic TF representations with constant-Q characteristic. The new class is related to a \\\"hyperbolic TF geometry\\\" and is thus called the hyperbolic class (HC). Two prominent TF representations previously considered in the literature, the Bertrand P/sub 0/ distribution and the Altes-Marinovic Q-distribution, are members of the new HC. The authors show that any hyperbolic TF representation is related to both the wideband ambiguity function and a \\\"hyperbolic ambiguity function\\\". It is also shown that the HC is the class of all quadratic TF representations which are invariant to \\\"hyperbolic time-shifts\\\" and TF scalings, operations which are important in the analysis of Doppler-invariant signals and self-similar random processes. The paper discusses the definition of the HC via constant-Q warping, some signal-theoretic fundamentals of the \\\"hyperbolic TF geometry\\\", and the description of the HC by 2D kernel functions. Several members of the HC are considered, and a list of desirable properties of hyperbolic TF representations is given together with the associated kernel constraints. > The recently introduced framework of cooperative simultaneous localization and tracking (CoSLAT) combines Bayesian cooperative agent self-localization with distributed target tracking. The original CoSLAT algorithm suffers from high computation and communication costs because it uses a particle-based message representation. Here, we propose an advanced hybrid particle-based and parametric message passing algorithm for CoSLAT in which both costs are significantly reduced. Simulation results show that the localization/tracking performance is not affected. The Wigner distribution (WD) is perhaps the most prominent quadratic time-frequency signal representation. In this paper, which has mainly tutorial character but also contains some new results, we describe extensions of the WD concept to multidimensional vector signals, nonstationary random processes, linear time-varying systems (deterministic and random), linear signal spaces, and frames. We discuss the interpretation and properties of these WD extensions and various relations connecting them. Some application examples are also provided. Interleave-division multiple access (IDMA) has recently been proposed as a promising alternative to code-division multiple access (CDMA). In this paper, we consider the use of IDMA within a multiple-input multiple-output orthogonal frequency-division multiplexing (MIMO-OFDM) multiuser system employing higher-order modulation and transmitting over frequency-selective MIMO channels. Based on a factor graph/message passing framework and the sum-product algorithm, we devise an iterative receiver that jointly performs pilot-aided channel estimation, multiuser detection, and channel decoding. The use of Gaussian message approximations results in a receiver complexity that scales only linearly with the number of users. A further complexity reduction is obtained by a novel selective message updating scheme. We also present a simulation-based comparison of the maximum rate achievable with our MIMO-OFDM-IDMA receiver with the information-theoretic capacity of the multiple-access channel. Finally, we provide simulation results illustrating the bit error rate (BER) performance of our receiver. It is observed that the proposed turbo-like integration of channel estimation in the iterative multiuser detection and channel decoding scheme yields a dramatic BER reduction, and that the proposed selective message updating scheme results in a significant reduction of complexity. We propose advanced compressive estimators of doubly dispersive channels within multicarrier communication systems (including classical OFDM systems). The performance of compressive channel estimation has been shown to be limited by leakage components impairing the channel's effective delay-Doppler sparsity. We demonstrate a group sparse structure of these leakage components and apply recently proposed recovery techniques for group sparse signals. We also present a basis optimization method for enhancing group sparsity. Statistical knowledge about the channel can be incorporated in the basis optimization if available. The proposed estimators outperform existing compressive estimators with respect to estimation accuracy and, in one instance, also computational complexity. The delineation of P and T waves is important for the interpretation of ECG signals. We propose a Bayesian detection-estimation algorithm for simultaneous detection, delineation, and estimation of P and T waves. A block Gibbs sampler exploits the strong local dependencies in ECG signals by imposing block constraints on the P and T wave locations. The proposed algorithm is evaluated on the annotated QT database and compared with two classical algorithms. The Bertrand (1991) distribution, which is a member of the affine class of time-frequency distributions (TFDs), and the Altes (1970, 1990) distribution, which does not belong to any known class of TFDs, are studied. It is shown that both TFDs are closely related to a hyperbolic time-frequency geometry. Based on this geometry, a new hyperbolic class of TFDs which contains both the Bertrand distribution and the Altes distribution is defined and studied. It is shown that the hyperbolic class can be derived from the Cohen's (1966) class of TFDs by a frequency-warping procedure that potentially results in a constant-Q time-frequency analysis. > Interleave-division multiple access (IDMA) has recently been introduced as an attractive alternative to CDMA. IDMA employs user-specific interleavers combined with low-rate channel coding for user separation. In this paper, we consider a MIMO-IDMA system with increased spectral efficiency due to the use of higher-order symbol constellations. Based on a factor graph framework and the sum-product algorithm, we develop an iterative turbo multiuser receiver. Gaussian approximations for certain messages propagated through the factor graph lead to a complexity that scales only linearly with the number of users. To further reduce complexity, we introduce a selective message update scheme. Numerical simulations demonstrate the performance of the proposed receiver algorithms. The Wigner distribution (WD) and WD-based sig- nal synthesis can be used for designing and processing signals in a joint time-frequency domain. Unfortunately, the perfor- mance of this method is adversely affected by the occurrence of interference terms (IT'S) in the WD. This paper provides an analysis of IT effects in WD-based signal synthesis, and shows that IT effects can be substantially reduced by using a smoothed Wigner distribution (SWD) instead of the WD. An iterative al- gorithm for SWD-based signal synthesis is presented, and the improvement over WD-based signal synthesis is verified via computer simulation. We present two POCS (projections onto convex sets) algorithms for deterministic blind equalization of linear time-varying (LTV) channels. Our approach is based on a multichannel LTI representation of LTV channels. We prove a theorem on unique reconstruction/equalization, formulate a POCS algorithm for the resolution of a matrix ambiguity, and show how to cope with unequal subchannel lengths. We also present an alternative POCS algorithm for equalization that avoids a singular value decomposition. Both algorithms are guaranteed to converge to the desired solution. We define two broad classes of unitary operator families termed modulation and warping operators. Some fundamental relations connecting these operators are shown. Application of the characteristic function method and the covariance method to modulation and warping operators yields joint (a, b), (/spl alpha/, /spl beta/), and time-frequency signal representations that satisfy marginal and covariance properties. We introduce the time-frequency-moving-average (TFMA) model as a highly parsimonious time-varying MA model formulated in terms of time-frequency (TF) shifts. For estimation of the TFMA model parameters, we develop a computationally efficient nonlinear technique based on a novel complex TF cepstrum, TF cepstral recursions, and an underspread approximation. Simulation results demonstrate significant performance advantages of the proposed TFMA model and parameter estimation technique over an existing method for time-varying MA modeling and estimation. We propose a compressive estimator of doubly selective channels within pulse-shaping multicarrier MIMO systems (including MIMO-OFDM as a special case). The use of multichannel compressed sensing exploits the joint sparsity of the MIMO channel for improved performance. We also propose a multichannel basis optimization for enhancing joint sparsity. Simulation results demonstrate significant advantages over channel-by-channel compressive estimation. Interleave-division multiple access (IDMA) has recently been proposed as an alternative to CDMA. IDMA employs user-specific interleavers combined with low-rate channel coding for user separation. It can outperform coded CDMA when iterative receivers are used, and it allows the design of multiuser detectors with moderate complexity. In this paper, we extend IDMA to MIMO multiuser systems employing spatial multiplexing. We also develop an iterative receiver for MIMO-IDMA that incorporates an efficient soft multiuser detector whose complexity is linear in the number of users. Both flat-fading and frequency-selective MIMO channels are considered. The performance of the proposed MIMO-IDMA system is assessed through simulation results. We present a unified framework for time-varying or time-frequency (TF) spectra of nonstationary random processes in terms of TF operator symbols. We provide axiomatic definitions and TF operator symbol formulations for two broad classes of TF spectra, one of which is new. These classes contain all major existing TF spectra such as the Wigner-Ville, evolutionary, instantaneous power, and physical spectrum. Our subsequent analysis focuses on the practically important case of nonstationary processes with negligible high-lag TF correlations (so-called underspread processes). We demonstrate that for underspread processes all TF spectra yield effectively identical results and satisfy several desirable properties at least approximately. We also show that Gabor frames provide approximate Karhunen-Loeve (KL) functions of underspread processes and TF spectra provide a corresponding approximate KL spectrum. Finally, we formulate simple approximate input-output relations for the TF spectra of underspread processes that are passed through underspread linear time-varying systems. All approximations are substantiated mathematically by upper bounds on the associated approximation errors. Our results establish a TF calculus for the second-order analysis and time-varying filtering of underspread processes that is as simple as the conventional spectral calculus for stationary processes Two structural properties of bilinear time-frequency representations (BTFRs) of signals are introduced and studied. The definition of these properties is based on a linear-operator description of BTFRs. The first property, termed regularity, has important implications with respect to the recovery of signals from the BTFR outcome, the derivation of other bilinear signal representations from a BTFR, the BTFRs reaction to linear signal transformations, and the construction of bases of induced BTFR-domain spaces. The second property, called unitarity, is equivalent to validity of Moyal's formula (1949). Unitarity is thus necessary and sufficient for a closed-form solution of optimal signal synthesis and for a BTFR formulation or optimal detection/estimation methods. Unitarity also allows the systematic construction of BTFR product relations like the Wigner distribution's interference formula and the ambiguity function's self-transformation property. Unitarity permits the construction of induced orthogonal projection operators and guarantees the orthonormality of induced basis functions. > We consider a pilot-assisted interleave-division multiple access (IDMA) system transmitting over block-fading channels. We describe this system in terms of a factor graph and use the sum-product algorithm to develop a receiver that performs joint data detection and channel estimation. Suitable approximations to the messages passed by the sum-product algorithm yield an implementation with a complexity that scales linearly with the number of users. Simulation results demonstrate large performance gains compared to classical receivers performing separate channel estimation and data detection. In wireless OFDM systems, the time-varying channel is often estimated by a Wiener filter-type MMSE estimator based on pilot symbols. Such an estimator, however, requires statistical prior knowledge that is not easily obtained. Here, we propose adaptive Wiener filters for channel estimation that do not require statistical prior knowledge. We also calculate the performance limits of finite-length and infinite-length MMSE estimation. Simulation results demonstrate the good performance of our adaptive estimators. We consider estimation of a sparse parameter vector that determines the covariance matrix of a Gaussian random vector via a sparse expansion into known \\u201cbasis matrices.\\u201d Using the theory of reproducing kernel Hilbert spaces, we derive lower bounds on the variance of estimators with a given mean function. This includes unbiased estimation as a special case. We also present a numerical comparison of our lower bounds with the variance of two standard estimators (hard-thresholding estimator and maximum likelihood estimator). Signal processing by means of discrete-time Wigner distribution requires a signal synthesis step which contains a troublesome phase ambiguity. The authors analyze the problem of phase ambiguities for unconstrained and halfband-constrained signal synthesis and discuss various strategies for coping with it. After a review of the phase-matching algorithm using a reference signal, the authors present autonomous phase-matching algorithms which do not require a reference signal. Online versions of both reference-based and autonomous phase-matching algorithms which feature a short-time or causal mode of operation and are thus suited for the online processing of signals with arbitrary length are derived. The performance of the algorithms is assessed by computer simulations. > Lattice reduction (LR) is a powerful technique for improving the performance of suboptimum MIMO data detection methods. For LR-assisted data detection, the LLL algorithm has been considered almost exclusively so far. In this paper, we propose and develop the application of Seysen's algorithm (SA) to LR-assisted MIMO detection, and we show that the SA is a promising alternative to the LLL algorithm. Specifically, the SA outperforms the LLL algorithm in that it finds better lattice bases for MIMO systems of practical interest, which is reflected by an improved performance of SA-assisted detectors relative to their LLL-assisted counterparts. We present an efficient implementation of the SA whose per-iteration complexity is linear in the number of antennas, and we demonstrate that the SA requires significantly fewer iterations than the LLL algorithm. We propose a distributed implementation of the Gaussian particle filter (GPF) for use in a wireless sensor network. Each sensor runs a local GPF that computes a global state estimate. The updating of the particle weights at each sensor uses the joint likelihood function, which is calculated in a distributed way, using only local communications, via the recently proposed likelihood consensus scheme. A significant reduction of the number of particles can be achieved by means of another consensus algorithm. The performance of the proposed distributed GPF is demonstrated for a target tracking problem. The authors consider the class of bilinear time-frequency representations (BTFR's) that are invariant (or covariant) to time shifts, frequency shifts, and time-frequency scalings. This \\\"shift-scale invariant\\\" class is the intersection of the classical shift-invariant (Cohen) class and the recently defined affine class. The mathematical description of shift-scale invariant BTFR's is in terms of a 1-D kernel and is thus particularly simple. The paper concentrates on the time-frequency localization properties of shift-scale invariant BTFR's. Since any shift-scale invariant BTFR is a superposition of generalized Wigner distributions, the time-frequency localization of the family of generalized Wigner distributions is studied first. For those shift-scale invariant BTFR's that may be interpreted as smoothed versions of the Wigner distribution (e.g., the Choi-Williams distribution), an analysis in the Fourier transform domain shows interesting peculiarities regarding time-frequency concentration and interference geometry properties. > It is known that conventional nulling-and-canceling (NC) detection for multiple-input/multiple-output (MIMO) systems cannot exploit all of the available diversity, and, thus, its performance is significantly inferior to that of maximum likelihood (ML) detection. Conventional NC employs the layerwise postequalization signal-to-noise ratios (SNRs) as reliability measures for layer sorting. These SNRs are average quantities that do not depend on the received vector. In this paper, we propose the novel dynamic nulling-and-canceling (DNC) technique that uses approximate a posteriori probabilities as measures of layer reliability. The DNC technique is a minimum mean-square error (MMSE) nulling scheme combined with an improved \\\"dynamic\\\" layer sorting rule that exploits the information contained in the current received vector. We calculate the error probability of DNC for a simple special case and show that it is upper bounded by the error probability of conventional NC. Simulation results are presented for spatial multiplexing systems and for systems using linear dispersion codes. It is demonstrated that the DNC technique can yield near-ML performance for a wide range of system sizes and channel SNRs at a fraction of the computational complexity of the sphere-decoding algorithm for ML detection We introduce a distributed cooperative framework and method for Bayesian estimation and control in decentralized agent networks. Our framework combines joint estimation of time-varying global and local states with information-seeking control optimizing the behavior of the agents. It is suited to nonlinear and non-Gaussian problems and, in particular, to location-aware networks. For cooperative estimation, a combination of belief propagation message passing and consensus is used. For cooperative control, the negative posterior joint entropy of all states is maximized via a gradient ascent. The estimation layer provides the control layer with probabilistic information in the form of sample representations of probability distributions. Simulation results demonstrate intelligent behavior of the agents and excellent estimation performance for a simultaneous self-localization and target tracking problem. In a cooperative localization scenario with only one anchor, mobile agents can localize themselves after a short time with an accuracy that is higher than the accuracy of the performed distance measurements. We propose a Monte Carlo method for determining the parameters of multipath components (MPCs) for ultra-wideband channels. A partially collapsed Gibbs sampler is used for jointly estimating the number, times-of-arrival, angles-of-arrival, and amplitudes of the MPCs as well as the sounding pulse from signals received by a 2D antenna array. Our system model accounts for propagation delays between the receive antennas. Temporal-angular sparsity of the detected MPCs is ensured by a 2D minimum distance constraint. Numerical results for synthetic and real signals demonstrate the excellent performance and fast convergence of our method. We provide a frame-theoretic analysis of oversampled and critically sampled, FIR and IIR, uniform filter banks (FBs). Our analysis is based on a relation between the polyphase matrices and the frame operator. For a given oversampled analysis FB, we parameterize all synthesis FBs providing perfect reconstruction, and we discuss the minimum norm synthesis FB and its approximative construction. We find conditions for a FB to provide a frame expansion. Paraunitary and biorthogonal FBs are shown to correspond to tight and exact frames, respectively. A new procedure for the design of paraunitary FBs is formulated. We show that the frame bounds are related with the eigenvalues of the polyphase matrices and the oversampling factor, and that they determine important numerical properties of the FB. We consider distributed state estimation in a wireless sensor network without a fusion center. Each sensor performs a global estimation task-based on the past and current measurements of all sensors-using only local processing and local communications with its neighbors. In this estimation task, the joint (all-sensors) likelihood function (JLF) plays a central role as it epitomizes the measurements of all sensors. We propose a distributed method for computing, at each sensor, an approximation of the JLF by means of consensus algorithms. This \\u201clikelihood consensus\\u201d method is applicable if the local likelihood functions of the various sensors (viewed as conditional probability density functions of the local measurements) belong to the exponential family of distributions. We then use the likelihood consensus method to implement a distributed particle filter and a distributed Gaussian particle filter. Each sensor runs a local particle filter, or a local Gaussian particle filter, that computes a global state estimate. The weight update in each local (Gaussian) particle filter employs the JLF, which is obtained through the likelihood consensus scheme. For the distributed Gaussian particle filter, the number of particles can be significantly reduced by means of an additional consensus scheme. Simulation results are presented to assess the performance of the proposed distributed particle filters for a multiple target tracking problem. Oversampled filter banks (FBs) offer more design freedom and better noise immunity than critically sampled FBs. Due to the increased computational complexity caused by oversampling, oversampled FBs allowing an efficient implementation, such as cosine modulated filter banks (CMFBs), are of particular interest. So far, only critically sampled CMFBs have been considered. In this paper, we introduce oversampled CMFBs with perfect reconstruction (PR). Extending a classification of CMFBs recently proposed by Gopinath, we consider two types of oversampled CMFBs with PR. One of these types allows linear phase filters in all channels, and comprises CMFBs recently introduced by Lin and Vaidyanathan as well as Wilson-type CMFBs. For both types of oversampled CMFBs, we formulate PR conditions in the time, frequency, and polyphase domains. It is shown that any PR CMFB corresponds to a PR DFT FB with twice the oversampling factor and that (under a specific condition) the same PR prototype can be used for both CMFB types. We also show that the frame-theoretic properties of a CMFB and of the corresponding DFT FB are closely related. In particular, it is demonstrated that the minimum-norm synthesis prototype in an oversampled PR CMFB equals that in the corresponding DFT FB. Finally, we briefly address design methods and the efficient DCT/DST-based implementation of oversampled CMFBs. The author discusses a fundamental duality principle of BTFRs (bilinear time-frequency representations) and presents a systematic classification of BTFRs which is consistent with BTFR duality. BTFRs are first grouped into two basic domains, namely, the energy density domain (E-domain) with energetic interpretation, and the correlation domain (C-domain) with correlative interpretation. It is shown that these domains are related by a Fourier transform duality: to any BTFR, BTFR relation, or BTFR property of the E-domain, there corresponds a dual BTFR, BTFR relation, or BTFR properties of the C-domain, and vice versa. With this duality principle as a background, a classification of BTFRs is given. This classification is based on two dual shift-invariance properties and a self-dual scale-invariance property of BTFRs. The mathematical description of BTFRs by means of kernel functions is simplified, inside the respective BTFR classes. It is shown that BTFRs which are both shift and scale invariant can be represented as superpositions of generalized Wigner distributions (E-domain) or generalized ambiguity function (C-domain). > We consider three different versions of the Zak (1967) transform (ZT) for discrete-time signals, namely, the discrete-time ZT, the polyphase transform, and a cyclic discrete ZT. In particular, we show that the extension of the discrete-time ZT to the complex z-plane results in the polyphase transform, an important and well-known concept in multirate signal processing and filter bank theory. We discuss fundamental properties, relations, and transform pairs of the three discrete ZT versions, and we summarize applications of these transforms. In particular, the discrete-time ZT and the cyclic discrete ZT are important for discrete-time Gabor (1946) expansion (Weyl-Heisenberg frame) theory since they diagonalize the Weyl-Heisenberg frame operator for critical sampling and integer oversampling. The polyphase representation plays a fundamental role in the theory of filter banks, especially DFT filter banks. Simulation results are presented to demonstrate the application of the discrete ZT to the efficient calculation of dual Gabor windows, tight Gabor windows, and frame bounds. Parsimonious parametric models for nonstationary random processes are useful in many applications. Here, we consider a nonstationary extension of the classical autoregressive moving-average (ARMA) model that we term the time-frequency autoregressive moving-average (TFARMA) model. This model uses frequency shifts in addition to time shifts (delays) for modeling nonstationary process dynamics. The TFARMA model and its special cases, the TFAR and TFMA models, are shown to be specific types of time-varying ARMA (AR, MA) models. They are attractive because of their parsimony for underspread processes, that is, nonstationary processes with a limited time-frequency correlation structure. We develop computationally efficient order-recursive estimators for the TFARMA, TFAR, and TFMA model parameters which are based on linear time-frequency Yule-Walker equations or on a new time-frequency cepstrum. Simulation results demonstrate that the proposed parameter estimators outperform existing estimators for time-varying ARMA (AR, MA) models with respect to accuracy and/or numerical efficiency. An application to the time-varying spectral analysis of a natural signal is also discussed. We present multiuser space-time receiver algorithms for synchronization, channel estimation, and data detection in the downlink of a universal mobile telecommunications system (UMTS)/time-division duplex (TDD) cellular communication system with multiple receive antennas. These algorithms were designed for use in a network monitoring device that analyzes the interference situation present, thereby allowing the operators to improve their networks. For interference analysis, we decode the broadcast channels (BCHs) of surrounding base stations. To cope with the widely differing power levels of signals received from different base stations, we combine multiuser space-time signal processing techniques with reestimation and successive cancellation schemes. Simulation results demonstrate that our algorithms enable reliable BCH data detection even at low SINR. We consider the estimation of doubly selective wireless channels within pulse-shaping multicarrier systems (which include OFDM systems as a special case). A new channel estimation technique using the recent methodology of compressed sensing (CS) is proposed. CS-based channel estimation exploits a channel's delay-Doppler sparsity to reduce the number of pilots and, hence, increase spectral efficiency. Simulation results demonstrate a significant reduction of the number of pilots relative to least-squares channel estimation. We provide a frame-theoretic analysis of oversampled finite impulse response (FIR) and infinite impulse response (FIR) uniform filter banks (FBs). Our analysis is based on a new relationship between the FBs polyphase matrices and the frame operator corresponding to an FB. For a given oversampled analysis FB, we present a parameterization of all synthesis FBs providing perfect reconstruction. We find necessary and sufficient conditions for an oversampled FB to provide a frame expansion. A new frame-theoretic procedure for the design of paraunitary FBs from given nonparaunitary FBs is formulated. We show that the frame bounds of an FB can be obtained by an eigen-analysis of the polyphase matrices. The relevance of the frame bounds as a characterization of important numerical properties of an FB is assessed by means of a stochastic sensitivity analysis. We consider special cases in which the calculation of the frame bounds and synthesis filters is simplified. Finally, simulation results are presented. We show that measurements of time-varying mobile radio channels obtained with uncalibrated correlative channel sounders are affected by four different types of systematic errors (commutation, pulse-compression, aliasing, and misinterpretation error). We analyze these errors and provide upper error bounds that are formulated in terms of channel and sounder parameters. Based on these error bounds, we provide guidelines for a judicious choice of important sounder parameters. Computer simulations using a simple two-path channel illustrate our theoretical results. Finally, we show how our results can be used to assess the accuracy of measured channel data. In bit-interleaved coded modulation (BICM) systems employing maximum-likelihood decoding, a demodulator (demapper) calculates a log-likelihood ratio (LLR) for each coded bit, which is then used as a bit metric for Viterbi decoding. In the MIMO case, the computational complexity of LLR calculation tends to be excessively high, even if the log-sum approximation is used. Thus, there is a strong demand for efficient suboptimum MIMO-BICM demodulation algorithms with near-optimum performance. We propose an efficient MIMO-BICM demodulator that is derived by means of a Gaussian approximation for the post-detection interference. Our derivation results in an MMSE equalizer followed by per-layer LLR calculation (i.e., LLRs are calculated separately for each layer). The novel demodulator can be interpreted as an MMSE analogue of a recently proposed ZF-equalization based demodulator, as well as an extension of ZF-equalization based demodulation to correlated post-detection interference. Because it performs per-layer LLR calculation, it has the same (low) computational complexity as the ZF-equalization based demodulator. Simulation results demonstrate that the performance of our demodulator is close to that of LLR calculation using all layers jointly, and significantly better than that of the ZF-equalization based demodulator. The affine and hyperbolic classes of quadratic time-frequency representations (QTFRs) provide frameworks for multiresolution or constant-Q time-frequency analysis. The authors study the QTFR properties of regularity (QTFR reversibility) and unitarity (preservation of inner products, Moyal's formula) in the context of affine and hyperbolic QTFRs. They develop the calculus of inverse kernels and discuss important implications of regularity and unitarity, such as signal recovery, the derivation of other quadratic signal representations, optimum detection, least-squares signal synthesis, the effect of linear signal transforms, and the construction of QTFR basis systems. > It is well known that suboptimal detection schemes for multiple-input multiple-output (MIMO) spatial multiplexing systems (equalization-based schemes as well as nulling-and-cancelling schemes) are unable to exploit all of the available diversity, and thus, their performance is inferior to ML detection. Motivated by experimental evidence that this inferior performance is primarily caused by the inability of suboptimal schemes to deal with \\\"bad\\\" (i.e., poorly conditioned) channel realizations, we study the decision regions of suboptimal schemes for bad channels. Based on a simplified model for bad channels, we then develop two computationally efficient detection algorithms that are robust to bad channels. In particular, the novel sphere-projection algorithm (SPA) is a simple add-on to standard suboptimal detectors that is able to achieve near-ML performance and significantly increased diversity gains. The SPA's computational complexity is comparable with that of nulling-and-cancelling detectors and only a fraction of that of the Fincke-Phost sphere-decoding algorithm for ML detection. We present a distributed particle filtering scheme for time-space-sequential Bayesian state estimation in wireless sensor networks. Low-rate inter-sensor communications between neighboring sensors are achieved by transmitting Gaussian mixture (GM) representations instead of particles. The GM representations are calculated using a clustering algorithm. We also propose a \\u201clook-ahead\\u201d technique for designing the proposal density used for importance sampling. Simulation results for a target tracking application demonstrate the performance of our distributed particle filter and, specifically, the advantage of the look-ahead proposal design over a conventional design. For blind deconvolution of an unknown sparse sequence convolved with an unknown pulse, a powerful Bayesian method employs the Gibbs sampler in combination with a Bernoulli-Gaussian prior modeling sparsity. In this paper, we extend this method by introducing a minimum distance constraint for the pulses in the sequence. This is physically relevant in applications including layer detection, medical imaging, seismology, and multipath parameter estimation. We propose a Bayesian method for blind deconvolution that is based on a modified Bernoulli-Gaussian prior including a minimum distance constraint factor. The core of our method is a partially collapsed Gibbs sampler (PCGS) that tolerates and even exploits the strong local dependencies introduced by the minimum distance constraint. Simulation results demonstrate significant performance gains compared to a recently proposed PCGS. The main advantages of the minimum distance constraint are a substantial reduction of computational complexity and of the number of spurious components in the deconvolution result. We propose a compressive method for tracking doubly selective channels within multicarrier systems, including OFDM systems. Using the recently introduced concept of modified compressed sensing (MOD-CS), the sequential delay-Doppler sparsity of the channel is exploited to improve estimation performance through a recursive estimation mode. The proposed compressive channel tracking algorithm uses a MOD-CS version of OMP with reduced complexity. Simulation results demonstrate substantial performance gains over conventional compressive channel estimation. We propose adaptive channel predictors for orthogonal frequency division multiplexing (OFDM) communications over time-varying channels. Successful application of the normalized least-mean-square (NLMS) and recursive least-squares (RLS) algorithms is demonstrated. We also consider the use of adaptive channel predictors for delay-free equalization, thereby avoiding the need for regular transmission of pilot symbols. Simulation results demonstrate the good performance of the proposed techniques. The coherence function is extended to nonstationary random processes through the introduction and investigation of a coherence operator and time-frequency (TF) coherence functions. For underspread nonstationary processes, it is shown that TF coherence functions are a meaningful tool for nonstationary coherence analysis and that they provide approximate TF formulations of the coherence operator. We consider the parametric analysis of frequency-domain optical coherence tomography (OCT) signals. A Monte Carlo (Gibbs sampler) detection-estimation method for determining the depths and reflection coefficients of tissue interfaces (reflective sites in the tissue) is proposed. Our method is blind since it estimates the instrumentation-dependent \\u201cfringe\\u201d function along with the tissue parameters. Sparsity of the detected interfaces is enforced by an impulse detector and a modified Bernoulli-Gaussian prior with a minimum distance constraint. Numerical results using synthetic and real signals demonstrate the excellent performance and fast convergence of our method. We present a factor graph based design of a receiver for pilot-assisted OFDM-IDMA systems transmitting over frequency-selective channels. The receiver performs joint iterative multiuser data detection and channel estimation with a complexity that is linear in the number of users, and it includes estimation of the channel length. Simulation results demonstrate large performance gains compared to OFDM-IDMA receivers using separate MMSE channel estimation. We present a consensus-based distributed particle filter (PF) for wireless sensor networks. Each sensor runs a local PF to compute a global state estimate that takes into account the measurements of all sensors. The local PFs use the joint (all-sensors) likelihood function, which is calculated in a distributed way by a novel generalization of the likelihood consensus scheme. A performance improvement (or a reduction of the required number of particles) is achieved by a novel distributed, consensus-based method for adapting the proposal densities of the local PFs. The performance of the proposed distributed PF is demonstrated for a target tracking problem. MIMO-OFDM using bit-interleaved coded modulation (BICM) is an attractive scheme for wireless communications in frequency-selective fading channels. The BICM decoder requires log-likelihood ratios (LLRs) whose exact computation is extremely costly. In this paper, we present a novel method termed soft sphere-projection algorithm (SSPA) that provides approximate LLRs for the case of constant modulus symbol alphabets. The SSPA has very low computational complexity and performs nearly as well as the list sphere decoder. These properties are demonstrated by numerical simulations using synthetic and measured MIMO channels. We propose a low-complexity intercarrier interference/intersymbol interference (ICI/ISI) equalizer for multicarrier transmissions over doubly dispersive channels. Decision-feedback (or interference cancellation) is used with respect to both time and frequency. The ICI stage employs an extension of the iterative LSQR algorithm using groupwise interference cancellation with reliability-based sorting of sets of subcarriers and a band approximation of the frequency-domain channel matrix. The LSQR algorithm is attractive because of its excellent numerical properties and low complexity. Optimal pulse design is optionally considered for shaping the ICI/ISI. Simulation results demonstrate the excellent performance of the proposed ICI/ISI equalizer. We propose sliding-window multiedge detectors and reflectivity estimators for complex SAR images. The novel detectors and estimators allow to take into account additive observation noise and colored signal (speckle) and noise processes; furthermore, they employ an exponential data weighting to improve spatial resolution. In the multiedge case, simulation results demonstrate a substantial performance improvement over existing methods when the speckle is colored and additive noise is present. The application of Wigner distribution (WD)-based signal synthesis to signal separation problems is often adversely affected by WD interference terms. We present a modified signal synthesis method where the use of a masked WD allows the definition of don't-care regions. In the don't-care regions, detrimental interference terms (whose time-frequency location is assumed to be known) are ignored. The synthesis result is calculated using a modified version of the quasi power algorithm previously proposed for smoothed WD's. The new synthesis method is shown to possess a desirable time-frequency extrapolation capability as well as a potential tendency to produce spurious signal components in the don't-care regions. The occurrence of spurious signal components can be avoided by the inclusion of an \\\"energy penalty.\\\". > We propose two methods for deterministic blind equalization of linear time-varying (LTV) channels with multiple users. Our approach is based on an LTI (linear time invariant) multichannel representation of LTV channels. With the first method, the multiuser channel is equalized up to an instantaneous mixture that is subsequently resolved by means of a POCS (projections onto convex sets) algorithm. The second method performs the entire equalization (including resolution of the instantaneous mixture) by means of a POCS algorithm. Simulation results demonstrate the good performance of our methods. This paper considers pulse-shaping multicarrier (MC) systems that transmit over doubly dispersive fading channels. We provide exact and approximate expressions for the intersymbol and intercarrier interference occurring, in such systems. This analysis reveals that the time and frequency concentration of the transmit and receive pulses is of paramount importance for low interference. We prove the (nonobvious) existence of such jointly concentrated pulse pairs by adapting recent mathematical results on Weyl-Heisenberg frames to the MC context. Furthermore, pulse optimization procedures are proposed that aim at low interference and capitalize on the design freedom existing for redundant MC systems. Finally, we present efficient FFT-based modulator and demodulator implementations. Our numerical results demonstrate that for realistic system and channel parameters, optimized pulse-shaping MC systems can outperform conventional cyclic-prefix OFDM systems We propose an online time-frequency (TF) projection filter with reasonable computational complexity for signals of arbitrary length. The filter uses local projections and TF subspace tracking based on an efficient iterative eigendecomposition. The high TF selectivity of the online TF projection filter is shown to be advantageous for interference excision in spread-spectrum communications. It is well known that suboptimal detection schemes for MIMO spatial multiplexing systems (equalization-based as well as nulling-and-cancelling detectors) cannot exploit all of the available diversity. We show that this inferior performance is primarily caused by poorly conditioned channel realizations. We then present the novel sphere-projection algorithm (SPA) that is robust to poorly conditioned channels. The SPA is a computationally efficient add-on to standard suboptimal detectors. Simulation results show that the SPA is able to achieve near-ML performance and significantly increased diversity gains. The SPA's computational complexity is comparable to that of nulling-and-cancelling detectors and only a fraction of that of the Fincke-Phost sphere-decoding algorithm for ML detection (Fincke, U. and Phost, M., Math. of Comp., vol.44, p.463-71, 1985). We propose a frequency-domain method for equalizing intercarrier interference (ICI) and intersymbol interference (ISI) in multicarrier transmissions over rapidly time-varying and strongly delay-spread channels. Postcursor ISI is cancelled by a decision-feedback structure, and ICI is equalized by a sequential version of the recently proposed LSQR equalizer, based on a band approximation for the frequency-domain channel matrix. The sequential LSQR equalizer uses an interference cancellation scheme with reliability-based sorting of sets of subcarriers. This approach is shown to yield excellent performance at moderate complexity. A pulse-shaped multicarrier system is considered because of its generality and advantages. This framework includes cyclic-prefix OFDM as a special case. Abstract The smoothed pseudo-Wigner distribution, the Choi-Williams distribution, and the cone-kernel representation are three time-frequency representations (TFRs) which feature an attenuation of cross (interference) terms as compared with the Wigner distribution. In this paper, we use an analysis of ambiguity-domain weighting functions for comparing the interference attenuation and time-frequency concentration properties of the three TFRs. These properties are then further investigated by studying the results obtained for a set of simple two-component signals. This analysis shows important effects and performance limitations whose understanding is essential for a practical application of the TFRs. We consider distributed estimation of a time-dependent, random state vector based on a generally nonlinear/non-Gaussian state-space model. The current state is sensed by a serial sensor network without a fusion center. We present an optimal distributed Bayesian estimation algorithm that is sequential both in time and in space (i.e., across sensors) and requires only local communication between neighboring sensors. For the linear/Gaussian case, the algorithm reduces to a time-space-sequential, distributed form of the Kalman filter. We also demonstrate the application of our state estimator to a target tracking problem, using a dynamically defined \\u201clocal sensor chain\\u201d around the current target position. For OFDM transmission over rapidly varying channels, intercarrier interference (ICI) constitutes a major source of performance degradation. We propose a low-complexity ICI equalization technique that uses the iterative LSQR algorithm for regularized inversion of a triple-band approximation to the frequency-domain channel matrix. The LSQR algorithm achieves regularization of the (typically ill-conditioned) channel inversion via early termination of the iteration process, and thus yields good results at low complexity. We also consider pulse shaping as a means of reducing the bandwidth of the matrix approximation. Simulation results demonstrate the excellent performance of the proposed ICI equalizer even for strongly dispersive and rapidly varying channels, as well as the potential of pulse shaping for reducing the \\\"ICI bandwidth.\\\" We propose a \\u201ccompressive\\u201d estimator of the Wigner-Ville spectrum (WVS) for time-frequency sparse, underspread, nonstationary random processes. A novel WVS estimator involving the signal's Gabor coefficients on an undersampled time-frequency grid is combined with a compressed sensing transformation in order to reduce the number of measurements required. The performance of the compressive WVS estimator is analyzed via a bound on the mean square error and through simulations. We also propose an efficient implementation using a special construction of the measurement matrix. A time-frequency representation of linear signal spaces, called its Wigner distribution (WD), is introduced. Similar to the WD of a signal, the WD of a linear signal space describes the space's energy distribution over the time-frequency plane. It is shown that the WD of a signal space can be defined both in a deterministic and in a stochastic framework, and it can be expressed in a simple way in terms of the space's projection operator and the bases. It is shown to satisfy many interesting properties which are often analogous to corresponding properties of the WD of a signal. The results obtained for some specific signal spaces are found to be intuitively satisfactory. The cross-WD of two signal spaces, a discrete-time WD version, and the extension of the WD definition to arbitrary quadratic signal representation are also discussed. > We present a technique for simulating time-varying mobile radio channels. This technique is specifically suited to the small relative Doppler bandwidths of wideband channels encountered in CDMA and OFDM communications. A \\\"subsampled\\\" ARMA innovations filter and multistage interpolation are used to achieve an accurate and computationally efficient approximation of specified or measured Doppler spectra (scattering functions). We discuss the calculation of the ARMA coefficients and the optimal design of the multistage interpolator. Simulation results demonstrate the excellent performance of the proposed channel simulator. The expected ambiguity function (EAF) is shown to provide a generalization of stationary correlation analysis to nonstationary random processes. Important properties of the EAF are discussed, and the EAFs of special processes are considered. Based on the EAF, a fundamental classification (underspread/overspread) of nonstationary processes is introduced and shown to be relevant to time-varying spectral analysis. > We consider the application of compressed sensing (CS) to the estimation of doubly selective channels within pulse-shaping multicarrier systems (which include orthogonal frequency-division multiplexing (OFDM) systems as a special case). By exploiting sparsity in the delay-Doppler domain, CS-based channel estimation allows for an increase in spectral efficiency through a reduction of the number of pilot symbols. For combating leakage effects that limit the delay-Doppler sparsity, we propose a sparsity-enhancing basis expansion and a method for optimizing the basis with or without prior statistical information about the channel. We also present an alternative CS-based channel estimator for (potentially) strongly time-frequency dispersive channels, which is capable of estimating the ?off-diagonal? channel coefficients characterizing intersymbol and intercarrier interference (ISI/ICI). For this estimator, we propose a basis construction combining Fourier (exponential) and prolate spheroidal sequences. Simulation results assess the performance gains achieved by the proposed sparsity-enhancing processing techniques and by explicit estimation of ISI/ICI channel coefficients. We propose a Bayesian method for distributed sequential localization of mobile networks composed of both cooperative agents and noncooperative objects. Our method provides a consistent combination of cooperative self-localization (CS) and distributed tracking (DT). Multiple mobile agents and objects are localized and tracked using measurements between agents and objects and between agents. For a distributed operation and low complexity, we combine particle-based belief propagation with a consensus or gossip scheme. High localization accuracy is achieved through a probabilistic information transfer between the CS and DT parts of the underlying factor graph. Simulation results demonstrate significant improvements in both agent self-localization and object localization performance compared to separate CS and DT, and very good scaling properties with respect to the numbers of agents and objects. We propose a Bayesian method for distributed sequential localization of mobile networks composed of both cooperative agents and noncooperative objects. Our method provides a consistent combination of cooperative self-localization (CS) and distributed tracking (DT). Multiple mobile agents and objects are localized and tracked using measurements between agents and objects and between agents. For a distributed operation and low complexity, we combine particle-based belief propagation with a consensus or gossip scheme. High localization accuracy is achieved through a probabilistic information transfer between the CS and DT parts of the underlying factor graph. Simulation results demonstrate significant improvements in both agent self-localization and object localization performance compared to separate CS and DT, and very good scaling properties with respect to the numbers of agents and objects. We apply joint probabilistic data association (JPDA) to multipath-assisted indoor navigation and tracking (MINT). In MINT, position-related information in multipath components (MPCs) is exploited to increase the accuracy and robustness of indoor tracking. Conventional MINT algorithms are based on deterministic data association and perform a global nearest-neighbor \\u201chard\\u201d association of MPC-related delays with the room geometry. In such a setup, incorrect associations may lead to severe tracking errors and to divergence of the Bayesian filter. Here, we propose a JPDA-MINT algorithm that is able to handle difficult situations where MPC delays overlap and data association is ambiguous. The algorithm is based on a recently introduced loopy belief propagation scheme that performs probabilistic data association jointly with agent state estimation, scales well in all relevant systems parameters, and has a very low computational complexity. Using data from an ultra-wideband indoor measurement campaign, we demonstrate that the proposed JPDA-MINT algorithm is highly accurate and more robust than the conventional MINT algorithms based on deterministic data association. We propose a multisensor method for tracking an unknown number of targets. Low computational complexity and very good scalability in the number of targets, number of sensors, and number of measurements per sensor are achieved by running a belief propagation (BP) message passing scheme on a suitably devised factor graph. Using a redundant formulation of data association uncertainty and \\u201caugmented target states\\u201d including target indicators allows the proposed BP method to leverage statistical independencies for a drastic reduction of complexity. The proposed method is shown to outperform previously proposed multisensor methods for multitarget tracking, including methods with a less favorable scaling behavior. We propose a distributed method for computing the joint (all-sensors) likelihood function (JLF) in a wireless sensor network. A consensus algorithm is used for a decentralized, iterative calculation of a sufficient statistic that describes an approximation to the JLF. After convergence of the consensus algorithm, the approximate JLF\\u2014which epitomizes the measurements of all sensors\\u2014is available at each sensor. This \\u201clikelihood consensus\\u201d method requires only communications between neighboring sensors. We implement the likelihood consensus method in a distributed particle filtering scheme. Each sensor runs a local particle filter that computes a global state estimate. The updating of the particle weights of each local particle filter uses the JLF. The performance of this distributed particle filter is demonstrated on a target tracking problem. We present a consensus-based distributed particle filter (PF) for wireless sensor networks. Each sensor runs a local PF to compute a global state estimate that takes into account the measurements of all sensors. The local PFs use the joint (all-sensors) likelihood function, which is calculated in a distributed way by a novel generalization of the likelihood consensus scheme. A performance improvement (or a reduction of the required number of particles) is achieved by a novel distributed, consensus-based method for adapting the proposal densities of the local PFs. The performance of the proposed distributed PF is demonstrated for a target tracking problem. We study the performance of estimators of a sparse nonrandom vector based on an observation which is linearly transformed and corrupted by white Gaussian noise. Using the framework of reproducing kernel Hilbert spaces, we derive a new lower bound on the estimator variance for a given differentiable bias function (including the unbiased case) and an almost arbitrary transformation matrix (including the underdetermined case considered in compressed sensing theory). For the special case of a sparse vector corrupted by white Gaussian noise\\u2014i.e., without a linear transformation\\u2014and unbiased estimation, our lower bound improves on a previously proposed bound.\",\n",
            "  \"61618545\": \"Any information about people such as their gender may be useful in some secure places; however, in some occasions, it is more appropriate to obtain such information in an unobtrusive manner such as using gait. In this study, we propose a novel method for gender classification using gait template, which is based on Radon Transform of Mean Gait Energy Image (RTMGEI). Robustness against image noises and reducing data dimensionality can be achieved by using Radon Transformation, as well as capturing variations of Mean Gait Energy Images (MGEIs) over their centers. Feature extraction is done by applying Zernike moments to RTMGEIs. Orthogonal property of Zernike moment basis functions guarantee the statistically independence of coefficients in extracted feature vectors. The obtained feature vectors are used to train a Support Vector Machine (SVM). Our method is evaluated on the CASIA database. The maximum Correct Classification Rate (CCR) of 98.94% was achieved for gender classification. Results show that our method outperforms the recently presented works due to its high performance. Human face recognition is one of the most popular biometric approaches. In last decade 3D face recognition attracted much attention. In this paper, we present an automatic face recognition algorithm and demonstrate its performance on the Bosphorus 3D face database. A novel Dynamic mask is used to segment automatically the regions of face which are less sensitive to expressions. We applied a multilayer perceptron (MLP) to compute maskable region (MR). MR shows which percentage of face image pixels must be masked to produce the expression insensitive binary mask for 3D faces. We applied a modified nearest neighbor classifier for identification. We only used one neutral frontal face of each subject as gallery images and tested our algorithm with emotional expression images. The identification rate obtained is 85.36 percent in non-neutral expression. We propose a novel method for real-world pose-invariant face recognition.Proposed method used from a single image in gallery with any facial expressions.We generate a collaborative dictionary matrix for each people.Promising results were obtained to handle pose on the FERET, LFW and video databases. In this paper, a novel method is proposed for unconstrained pose-invariant face recognition from only an image in a gallery. A 3D face is initially reconstructed using only a 2D frontal image. Then, for each person in the gallery, a Triplet Collaborative Dictionary Matrix (TCDM) is created from all face poses by rotating the 3D reconstructed models and extracting features in rotated face. Each TCDM is subsequently rendered based on triplet angles of face poses. Finally, the classification is performed by Collaborative Representation Classification (CRC) with Regularized Least Square (RLS). Promising results were acquired to handle pose changes on the FERET, LFW and video face databases compared to state-of-the-art methods in pose-invariant face recognition. In this paper, selecting line segments as matching features in stereo vision, the orientation difference (O.D.) of the line segments is more deeply evaluated than the previous studies and two new constraints i.e. ordering and collinearity are proposed. The O.D. was supposed to be used for an indoor application, reducing candidates without elimination of the actual matches, especially if the line is horizontal or vertical with respect to the floor. The findings of this paper are as follows: 1) Applying a threshold on O.D. would result in a missing probability for the actual matches and this probability can be calculated for any given threshold, 2) The upper limit of O.D. for horizontal and vertical lines is a function of geometric parameters of the system, 3) An optimal tilt angle, whose results is the minimum upper limit for O.D., can be computed, 4) for disambiguation process, ordering and collinearity constraints are proposed. These are applied in a matching algorithm and their effectiveness is investigated on real stereo images. IP routers need lookup tables to forward packets. They also classify packets to determine which flow they belong to and to decide what quality of service they should receive. Increasing rate of communication links is in contrast with practical processing power of routers and switches. We propose a few neural network algorithms to solve the IP lookup problem. Some of these algorithms, gives promising results, however, they have problems in training time. Parallel processing of neural networks provide a huge processing power to do IP lookup. The algorithm can be implemented in hardware on a single chip. Our method can perform an IP lookup in 4.5 nanoseconds, which implies supporting 60 Gbps link rate. Pipelining and parallel processing can be used to increase the link rate up to 400 Gbps and decrease the learning time. Personal identification based on the ear structure is an emerging biometrics. Clearly ear segmentation plays a vital role in automated ear recognition methods. In this paper, a new segmentation method for ear recognition is proposed. We apply the Canny edge detector to an ear image. Then the longest path in the edge image is extracted and selected as the outer boundary of the ear. By selecting the top, bottom, and left points of the detected boundary, we form a triangle with the selected points as its vertices. Further we calculate the barycenter of the triangle and select it as a reference point in all images. Then the ear region is extracted from the entire image using a predefined window centered at the reference point. Experimental results show the effectiveness of our proposed method. The performance of pattern recognition systems that use statistical features depends on a specific feature extraction technique. This technique is used to represent an image by a set of features and to reduce the dimension of the image space by removing redundant data. This study investigates a variety of moment-based feature extraction techniques, including Zernike, pseudo Zernike and orthogonal Fourier\\u2013Mellin, for the recognition of human faces. In this study, the authors have concerned with both values and orders of moments in the sense of accuracy and efficiency. Two public large face databases, FERET and CAS-PEAL-R1, have been exploited in the experiments. The authors have also employed two typical classifiers, radial basis function neural network and support vector machine, in order to ensure the reliability and consistency of the results from the classification point of view. The extensive experiments in variations of illumination, expression, aging and different accessories have shown that Zernike moments achieve the best overall performance in terms of both classification accuracy and execution time. In this research paper, we proposed a new method for image encryption through using chaotic function and graph theory. In this technique, we used a graph for making the coding algorithm more complicated with higher security and changing the gray value of the original image. Experimental results denote that the method benefit from high efficiency against prevalent attacks; for example, the obtained entropy value is 7.9911 that is very close to ideal value of 8. Controlling chaos in a passive biped robot with an artificial neural network is investigated in this paper. The dynamical model is based on the compass-like biped robot proposed by Garcia et al. (1998) with a point-mass at the hip and infinitesimal point-masses at the feet ignoring the scuffing situation. The governing dynamics and chaotic behavior of the system is explored and the bifurcation diagram is drawn with respect to the ramp slope. Controlling chaos is based on stabilizing the unstable periodic orbits in the chaotic attractor. The UPOs are detected using an iterated algorithm. The artificial neural network is constructed using the information of seven previous steps and the control parameters in each one. The network is trained to find the appropriate control parameter in order to put the next step on the unstable periodic orbit. The control parameter is the toe-off impulse at the heel strike. In this paper a system is developed for face recognition processes. After preprocessing of face images, for omitting the redundant information such as background and hair, the oval shape of face is approximated by an ellipse using shape information. Then the parameters (orientation and center coordinates) of this ellipse are optimized using Genetic Algorithm (GA). High order Pseudo Zernike Moment Invariant (PZMI) which has useful properties is utilized to produce feature vectors. We use GAs in combination with nearest neighbor classifier to select the optimal feature set for classification. Also, Support Vector Machines (SVMs) which has very good generalization ability has been used as a classifier with ERBF kernel function. Proposed approach has been applied on ORL and Yale databases and has shown a high classification rate with small number of feature elements. The bandwidth-delay-constrained least-cost multicast routing is a challenging problem in high-speed multimedia networks. Computing such a constrained Steiner tree is an NP-complete problem. In this paper, we propose a novel QoS-based multicast routing algorithm based on the genetic algorithms (GA). In the proposed method, the degree-based permutation encoding is used for genotype representation. Some novel heuristic algorithms are also proposed for mutation, crossover, and creation of random individuals. We evaluate the performance and efficiency of the proposed GA-based algorthims in comparison with other existing heuristic and GA-based algorithms by the result of simulation. This proposed algorithm has overcome all of the previous algorithms in the literatures. In this paper, we introduce an efficient feature extraction method for character recognition. The EA strategy is used to maximize the Fisher linear discriminant function (FLD) over a high order Pseudo-Zernike moment. The argument, which maximizes the FLD criteria, is selected as the proposed weight function. To evaluate the performance of the proposed feature, experimental studies are carried out on the historic Middle-Age Persian characters. The numerical results show 96.8% recognition rate on the selected database with the weighted Pseudo-Zernike feature (with order 10) and 65, 111,16 neurons for the input, hidden, and output layers while this amount for the original Pseudo-Zernike is 93%. SUMMARY Finding corresponding edges is considered being the most difficult part of edge-based stereo matching algorithms. Usually, correspondence for a feature point in the first image is obtained by searching in a predefined region of the second image, based on epipolar line and maximum disparity. Reduction of search region can increase performances of the matching process, in the context of execution time and accuracy. Traditionally, hierarchical multiresolution techniques, as the fastest methods are used to decrease the search space and therefore increase the processing speed. Considering maximum of directional derivative of disparity in real scenes, we formulated some relations between maximum search space in the second images with respect to relative displacement of connected edges (as the feature points), in successive scan lines of the first images. Then we proposed a new matching strategy to reduce the search space for edge-based stereo matching algorithms. Afterward, we developed some fast stereo matching algorithms based on the proposed matching strategy and the hierarchical multiresolution techniques. The proposed algorithms have two stages: feature extraction and feature matching. We applied these new algorithms on some stereo images and compared their results with those of some hierarchical multiresolution ones. The execution times of our proposed methods are decreased between 30% to 55%, in the feature matching stage. Moreover, the execution time of the overall algorithms (including the feature extraction and the feature matching) is decreased between 15% to 40% in real scenes. Meanwhile in some cases, the accuracy is increased too. Theoretical investigation and experimental results show that our algorithms have a very good performance with real complex scenes, therefore these new algorithms are very suitable for fast edge-based stereo applications in real scenes like robotic applications. In this paper, recognition of ancient middle Persian documents is studied. Our major attention has been focused on feature extraction and classification. A set of invariant moments has been selected as the features and the minimum mean distance (three versions of which that is called MMD1, MMD2, MMD3), KNN and Parzen as the classifier. Preprocessing is also considered in this paper which allows, the effects of under sampling (resolution pyramids), smoothing, and thinning be investigated. The algorithm has been tested not only on the original and smoothed images but also on the skeletonized and under sampled version of the text under test. The results show an acceptable recognition rate with the selected features with the proposed processing for the middle age Persian. The best-achieved classification rates are 95% and 90.5% for smoothed and original character images respectively. It was interesting to note that KNN and MMD2 classifiers yielded better recognition rate. In this paper, we propose a new image hiding method by using LSB substitution for improving stego-image quality. In this method, we try to transform the secret image into meaningless picture by using a bijective mapping function so that the difference of embedded secret image bits and LSB bits of host image pixels is of minimum possible value. This operation results in the encryption of the secret image. Thus, if grabbers detect existence of the secret image into stego-image, they won't be able to recognize the secret image exactly. As a result, the security of our method will be increased. We use genetic algorithm for setting parameters of bijective mapping function to obtain the best condition in distribution of the pixels. We compare the our proposed method with LSB substitution method and Chang et al.'s method. The experimental results show that our proposed method has enhanced both the quality and the security of stego-image by using LSB substitution method. In addition, our method results are approximately close to the results of Chang et al.'s method. The bandwidth-delay-constrained least-cost multicast routing is a challenging problem in high-speed multimedia networks. Computing such a constrained Steiner tree is an NP-complete problem. In this paper, we propose a novel QoS-based multicast routing algorithm based on the genetic algorithms (GA). In the proposed method, Prufer number is used for genotype representation. Some novel heuristic algorithms are also proposed for mutation, crossover, and creation of random individuals. We evaluate the performance and efficiency of the proposed GA-based algorithm in comparison with other existing heuristic and GA-based algorithms by the result of simulation. This proposed algorithm has overcome all of the previous algorithms in the literatures. This paper examines application of various feature domains for recognition of human face images to introduce an efficient feature extraction method. The proposed feature extraction method comprised of two steps. In the first step, a human face localization technique with defining a new parameter to eliminate the effect of irrelevant data is applied to the facial images. In the next step three different feature domains are applied to localized faces to generate the feature vector. These include Pseudo Zernike Moments (PZM), Principle Component Analysis (PCA) and Discrete Cosine Transform (DCT). We have compared the effectiveness of each of the above feature domains through the proposed feature extraction for human face recognition. The Radial Basis Function (RBF) neural network has been utilized as classifier. Simulation results on the ORL database indicate the effectiveness of the proposed feature extraction with the PZM for human face recognition. This paper aims to provide a document restoration and segmentation algorithm for the Historic Middle Persian or Pahlavi manuscripts. The proposed algorithm uses the mathematical morphology and connected component concept to segment the line, word, and character overlapped in the Middle-age Persian documents in preparation for OCR application. To evaluate the performance of the restoration algorithm, 200 pages of the Pahlavi documents are used as experimental data in our test. Numerical results indicate that the proposed algorithm can remove the noise and destructive effects. The results also show 99.14% accuracy on the baseline detection, 97.35% accuracy on the text line extraction and removing other lines overlaps, and 99.5% accuracy for segmenting the extracted text lines to their components. In this paper, a morphological based method for recognition of handwritten middle Persian characters is presented. After pre-processing and noise cancellation, morphological erosion operator with many structure elements is applied. The structure elements are with variable length lines at directions 0, 45, 90, 135 degrees. A five element feature set has been defined so: (1) relative energy of eroded version with respect to the original image energy (REL/spl I.bar/ENG),(2) displacement of the center of mass (CM/spl I.bar//spl I.bar/DIS), (3) minimum eigenvalue (EIG/spl I.bar/MIN), (4) maximum eigenvalue (EIG/spl I.bar/MAX) and (5) its direction (EIG-DIR). These features are used to design a feedforward neural network with one hidden layer. The best classification error is about 2.39% (97.61% recognition rate), and is achieved with 150 neurons for the hidden layer. In this study, a novel method is proposed for gender classification by adding facial depth features to texture features. Accordingly, the three-dimensional (3D) generic elastic model is used to reconstruct the 3D model from human face using only a single 2D frontal image. Then, the texture and depth are extracted from the reconstructed face model. Afterwards, the local Gabor binary pattern (LGBP) is applied to both facial texture and reconstructed depth to extract the feature vectors from both texture and reconstructed depth images. Finally, by combining 2D and 3D feature vectors, the final LGBP histogram bins are generated and classified by the support vector machine. Favourable outcomes are acquired for gender classification on the labelled faces in the wild and FERET databases based on the proposed method compared to several state-of-the-arts in gender classification. Feature Selection (FS) and reduction of pattern dimensionality is a most important step in pattern recognition systems. One approach in the feature selection area is employing population-based optimization algorithms such as Genetic Algorithm (GA)-based method and Ant Colony Optimization (ACO)- based method. This paper presents a novel feature selection method that is based on Ant Colony Optimization (ACO). ACO algorithm is inspired of ant's social behavior in their search for the shortest paths to food sources. Most common techniques for ACO-Based feature selection use the priori information of features. However, in the proposed algorithm, classifier performance and the length of selected feature vector are adopted as heuristic information for ACO. So, we can select the optimal feature subset without the priori information of features. This approach is easily implemented and because of using one simple classifier in it, its computational complexity is very low. Simulation results on face recognition system and ORL database show the superiority of the proposed algorithm. Localization of texts in natural images could be an important stage in many applications such as content-based image retrieval, visual impairment assistance systems, automatic robot navigation in urban environments and tourist assistance systems. However due to the variations of font, script, scale, orientations, color, shadow and lighting conditions, robust scene text localization is still a challenging task. In this paper, we propose a novel method to localize not only Farsi/Arabic and Latin texts with different sizes, fonts and orientations but also low luminance contrast and poor quality ones in the natural images taken with uneven illumination conditions. Firstly, fast weighted median filtering as a nonlinear edge-preserving smoothing filter and then color contrast preserving decolorization are exploited to make the text localization system more robust for low luminance contrast and poor quality texts. In order to extract the Farsi/Arabic and Latin scene texts and also filter the nontext ones, a unified framework is proposed incorporating the maximally stable extremal regions and a novel proposed region detector called Stable Width Stroke Regions which is based on closed boundary regions. Phase congruency and Laplacian operators are exploited to extract the closed boundary regions. Finally, to extract the single text lines, the Meanshift clustering and radon transform were used. Experimental results show that the proposed method localize low luminance contrast and low quality scene texts for both Farsi/Arabic and Latin scripts encouragingly. In this paper, we proposed a new method for spatially registered multi-focus images fusion. Image fusion based on wavelet transform is the most commonly fusion method, which fuses the source images information in wavelet domain according to some fusion rules. There are some disadvantages in Discrete Wavelet Transform, such as shift variance and poor directionality. Also, because of the uncertainty about the source images contributions to the fused image, designing a good fusion rule to integrate as much information as possible into the fused image becomes one of the most important problem. In order to solve these problems, we proposed a fusion method based on double-density dual-tree discrete wavelet transform, which is approximately shift invariant and has more sub-bands per scale for finer frequency decomposition, and fuzzy inference system for fusing wavelet coefficients. This new method provides improved subjective and objectives results compared to the previous wavelet fusion methods. This paper introduces an experimental evaluation of the effectiveness of utilizing various moments as pattern features in recognition of the handwritten Farsi characters. The moments that have been used are Zernike moments, pseudo Zernike moments, and Legendre moments. We have used an unsupervised neural network (ART2) for this application, so that the clusters are formed only based on inherent properties of pattern features. The performance of classification is dependent on the moment order as well as the type of the moment invariant, but the classification error rate was below 10% in all cases. The pseudo Zernike moments of order 5 had the best performance among all the moment invariants. Its error rate and discrimination factor were 3.06% and 96.92% respectively. The reduction of the search region in stereo correspondence can increase the performance of the matching process, in the context of execution time and accuracy. For edge-based stereo matching, we establish the relationship between the search space and parameters like relative displacement of the edges, the disparity under consideration, the image resolution, the CCD dimensions and the focal length of the stereo system. Then, we propose a novel matching strategy for the edge-based stereo. Afterward, we develop a fast algorithm for edge based-stereo with combination of the obtained matching strategy and the multiresolution technique using the Haar wavelet. Considering conventional multiresolution techniques, we show that the execution time of our algorithm is decreased more than 36%. Moreover, the matching rate and the accuracy are increased. Theoretical investigation and experimental results show that our algorithm has a very good performance, therefore this new algorithm is very suitable for fast edge-based stereo applications like stereo robot vision. This paper introduced an experimental evolution of the effectiveness of utilizing various moments as pattern features in human face technology. In this paper, we apply pseudo Zernike moments (PZM) for recognition of human faces in two-dimensional images, and we compare their performance with other type of moments. The moments that we have used are Zernike moments (ZM), pseudo Zernike moments (PZM) and Legendre moments (LM). We have used shape information for human face localization, also we have used a radial basis function (RBF) neural network as a classifier for this application. The performance of classification is dependent on the moment order as well as the type of moment invariant, but the classification error rate was below %10 in all cases. Simulation results on the face database of Olivetti Research Laboratory (ORL) indicate that high order degree pseudo Zernike moments contain very useful information about face recognition process, while low order degree moments contain information about face expression. This paper presents a face detection framework based on up to the third-order two-dimensional central geometrical moments (CGMs) of face components and their horizontal and vertical gradients. To detect faces in an image an exhaustive search over space and scale is carried out by using a multistage classifier which quickly discards background regions and spends more computation on promising face-like regions. A new method for fast computation of up to the third-order local geometrical moments, suitable for sliding window applications is presented whose computational complexity is invariant to scale and is much faster compared to previous methods for PC-based applications. The presented results show that the proposed system yields good performance in terms of detection and false positive rates. In this paper, the problem of designing a power-aware medium access control (MAC) algorithm for Ad hoc wireless networks is considered. Based on the insights obtained from analyzing the problem in optimization framework, we formulate it as a random scheduling MAC in the game theory framework. Defining a payoff for each link as a function of its persistence probability and power, the objective of the proposed non-cooperative static power-aware MAC game (PAMG) is to find the appropriate strategy for the link in its 2D strategy space. The game theoretic aspects of PAMG including existence, uniqueness, and convergence to the Nash equilibrium are investigated analytically under some mild conditions. Based on PAMG, a message passing totally asynchronous distributed power-aware MAC (PAM) algorithm is presented. In the proposed algorithm, at each active time slot the link broadcasts a message simultaneous to its transmission. At each inactive time slot it listens to the channel to capture the other active links messages and updates its cost factor. Simulation results are provided to evaluate the convergence and performance of the algorithm and are compared to the optimal solution. Coverage, connectivity, and network lifetime are important issues in wireless sensor networks (WSNs). Balancing the energy consumption in the network, reducing the transmission range of nodes, and density control of active nodes are approaches to extend the network lifetime. However, transmission range reduction and smaller number of active nodes can affect the network topology and may cause the network to be disconnected. So, there exist conflicts among lifetime, coverage, and connectivity. In this paper, these conflicting issues are considered and an evolutionary multiobjective optimization approach based on nondominated sorting genetic algorithm-II (NSGA-II) is proposed to optimize them. Simulation results demonstrate that the proposed algorithm can improve the network lifetime and coverage while maintaining the network connectivity. Unlike many other languages, 18 out of 32 Farsi characters have dots appearing in groups of one, two or three. Some of these letters share common primary shapes, differing only in the number of dots and whether the dots are above or below the primary shape. In this paper, a new concept of using dots in a cursively handwritten Farsi/Arabic word is introduced for lexicon reduction and a fast method for extracting dots is presented. The technique involves extraction and representation of number and position of dots from off-line handwritten words to eliminate unlikely candidates. Experimental results on a set of 12,000 handwritten word images yield a lexicon reduction of 93% with accuracy of 85%. The proposed lexicon reduction algorithm achieves the speedup factor of 2 as well as 13% improvement in recognition rate. This paper presents a new wavelet-based algorithm for the fusion of spatially registered infrared and visible images. Wavelet-based image fusion is the most common fusion method, which fuses the information from the source images in the wavelet transform domain according to some fusion rules. We specifically propose new fusion rules for fusion of low and high frequency wavelet coefficients of the source images in the second step of the wavelet-based image fusion algorithm. First, the source images are decomposed using dual-tree discrete wavelet transform (DT-DWT). Then, a fuzzy-based approach is used to fuse high frequency wavelet coefficients of the IR and visible images. Particularly, fuzzy logic is used to integrate the outputs of three different fusion rules (weighted averaging, selection using pixel-based decision map (PDM), and selection using region-based decision map (RDM)), based on a dissimilarity measure of the source images. The objective is to utilize the advantages of previous pixel- and region-based methods in a single scheme. The PDM is obtained based on local activity measurement in the DT-DWT domain of the source images. A new segmentation-based algorithm is also proposed to generate the RDM using the PDM. In addition, a new optimization-based approach using population-based optimization is proposed for the low frequency fusion rule instead of simple averaging. After fusing low and high frequency wavelet coefficients of the source images, the final fused image is obtained using the inverse DT-DWT. This new method provides improved subjective and objectives results as compared to previous image fusion methods. A recognition system based on template matching for identifying handwritten Farsi/Arabic numerals has been developed in this paper. Template matching is a fundamental method of detecting the presence of objects and identifying them in an image. In the proposed method, templates have been chosen so that they represent the features of FARSI/Arabic prescribed form of writing as possible. Experimental results show that the performance of proposed language-based method has been achieved more than the other usual common feature extraction approaches. NM-MLP is used as a classifier and trained with 6000 samples. Test set includes 4000 samples. The recognition rate of 97.65% was obtained, which is 0.64% more than Zernike moment approach. Here we propose bi-orthogonal and Gabor wavelet-based region covariance matrices as a novel method, which is robust to changes in illumination and pose variations, for ear recognition. In this method we construct a region covariance matrix by using bi-orthogonal and Gabor wavelet features and illumination intensity and pixel location and use it as an efficient and robust ear descriptor. We performed our experimental studies comparing various ear recognition methods, including our proposed method, with the PCA\\u00a0+\\u00a0RBFN method, the ICA\\u00a0+\\u00a0RBFN method, the Hmax\\u00a0+\\u00a0SVM method, the LSBP method, conventional RCM-based method, and the GRCM method. Superiority of our new method has been successfully tested on ear recognition using 488 images corresponding to 137 subjects from two databases 1 and 2 in USTB database. Our proposed method achieves the average accuracy of 96.6% and 93.5%, respectively, on the databases 1 and 2 for ear recognition. Spatial LSB\\u00b11 steganography changes smooth characteristics between adjoining pixels of the raw image. We present a novel steganalysis method for LSB\\u00b11 steganography based on feature vectors derived from the co-occurrence matrix in the spatial domain. We investigate how LSB\\u00b11 steganography affects the bit planes of an image and show that it changes more least significant bit (LSB) planes of it. The co-occurrence matrix is derived from an image in which some of its most significant bit planes are clipped. By this preprocessing, in addition to reducing the dimensions of the feature vector, the effects of embedding were also preserved. We compute the co-occurrence matrix in different directions and with different dependency and use the elements of the resulting co-occurrence matrix as features. This method is sensitive to the data embedding process. We use a Fisher linear discrimination (FLD) classifier and test our algorithm on different databases and embedding rates. We compare our scheme with the current LSB\\u00b11 steganalysis methods. It is shown that the proposed scheme outperforms the state-of-the-art methods in detecting the LSB\\u00b11 steganographic method for grayscale images. In this paper, we describe four important indirect methods which be used to extract the fetal Electrocardiogram (FECG) signal from an ECG recorded on the mother's abdomen. These methods include the following ones: singular value decomposition (SVD) method, independent component analysis (ICA) method, wavelet based methods and adaptive filtering method. The mentioned methods use signal processing techniques for extracting FECG from abdominal electrocardiogram (AECG). We have explained advantages and disadvantages of each method. The methods have also applied on both synthetic and real ECG signals. Efficiencies of the methods compared together based on three important criterions and results are stated and best method based on three criterions is selected. This paper presents a new wavelet-based method for fusion of spatially registered multi-focus images. We have formulated the image fusion process as a two-class classification problem: in and out-of-focus classes. First, a 12---dimensional feature vector using dual-tree discrete wavelet transform (DT-DWT) sub-bands of the source images are extracted, and then a trained two-class fisher classifier projects it to the class labels. The classifier output is used as a decision map for fusing high-frequency wavelet coefficients of multi-focus source images in different directions and decomposition levels of the DT-DWT. In addition, there is an uncertainty for selecting high-frequency wavelet coefficients in smooth regions of source images, which causes some misclassified pixels in the classification output or the decision map. In order to solve this uncertainty and integrate as much information as possible from the source images into the fused image, we propose an algorithm based on fuzzy logic, which combines outputs of two different fusion rules based on a dissimilarity measure from the source images: Selection based on the decision map and weighted averaging. An estimation of the decision map is also used for fusing low-frequency wavelet coefficients of the source images instead of simple averaging. After fusing low- and high-frequency wavelet coefficients of the source images, the final fused image is obtained using the inverse DT-DWT. This new method provides improved subjective and objectives results (more than 4.5 dB on average) as compared to previous fusion methods. In this paper, we propose a new method of spatially registered multi-focus images fusion. Image fusion based on wavelet transform is the most commonly fusion method, which fuses the source images information in wavelet domain according to some fusion rules. We formulate image fusion process as a two class problem: in focus and out of focus classes. Two-class fisher classifier is used for this purpose and six dimensional feature vectors, which is obtained via dual-tree discrete wavelet transform sub-bands are used for training classifier. We use classifier output as a decision map for selecting wavelet coefficients between two images in the different directions and level of decomposition, equally. Also there is an uncertainty about selecting wavelet coefficients in the smooth regions of two images, which causes some misclassified regions. In order to solve this uncertainty and integrate as much information of each source image as possible into the fused image, we propose an algorithm based on fuzzy logic, which combines output of three fusion rules. This new method provides improved subjective and objectives results compared to the previous fusion methods. Abstract In this paper, we introduce homogeneous cosimulation method, which is suitable for modeling and simulation of systems comprising hardware (HW) and software (SW) components. In this method, a hardware description language with embedded programming capabilities (like VHDL) is used to model both HW/SW parts at any desired level of abstraction. Because of platform homogeneity, no sophisticated coordinator interface between HW/SW models is required, and also the final system can be constructed from these models easily and rapidly using CAD tools. We have applied this method to ISDN user\\u2013network interface and discussed about its benefits and drawbacks. In this paper, we proposed a new method for Steganography based on deceiving \\u03a7 2 algorithm. Since the cover image coefficients and stego image coefficients histograms have sensible difference for purposes of statistical properties, statistical analysis of \\u03a7 2-test reveals the existence of hidden messages inside stego image .We are introducing the idea for hiding messages in the cover image. It causes that DCT (Discrete Cosine Transforms) coefficient histogram not having remarkable modification before and after embedding message. As a result, the identifying of hidden message inside an image is impossible for an eavesdropper through \\u03a7 2-test. In fact, the proposed method increases the Steganography security against \\u03a7 2-test, but the capacity of embedding messages decreases to half. In this paper, a novel script-independent block-based text line extraction technique is proposed for multi-skewed document images. Three parameters are defined to adopt the method with various writings. Extensive experiments on different datasets demonstrate that the proposed algorithm outperforms previous methods In this paper, a Bayesian framework is introduced for pattern modeling and multiple point statistics simulation. The method presented here is a generalized clustering-based method where the patterns can live on a hyper-plane of very low dimensionality in each cluster. The provided generalizationallows a remarkable increase in variability of the model and a significant reduction in the number of necessary clusters for pattern modeling which leads to more computational efficiency compared with clustering-based methods. The Bayesian model employed here is a nonlinear model which is composed of a mixture of linear models. Therefore, the model is stronger than linear models for data modeling and computationally more effective than nonlinear models. Furthermore, the model allows us to extract features from incomplete patterns and to compare patterns in feature space instead of spatial domain. Due to the lower dimensionality of feature space, comparison in feature space results in more computational efficiency as well. Despite most of the previously employed methods, the feature extraction filters employed here are customized for each training image (TI). This causes the features to be more informative and useful. Using a fully Bayesian model, the method does not require extensive parameter setting and tunes its parameters itself in a principled manner. Extensive experiments on different TIs (either continuous or categorical) show that the proposed method is capable of better reproduction of complex geostatistical patterns compared with other clustering-based methods using a very limited number of clusters. Abstract Recent neurophysiologic findings have shown that astrocytes (the most abundant type of glial cells) are active partners in neural information processing and regulate the synaptic transmission dynamically. Motivated by these findings, in the present research, a digital neuromorphic circuit to implement the astrocyte dynamics is developed. To model the dynamics of the intracellular Ca 2+ waves produced by astrocytes, we utilize a simplified model which considers the main physiological pathways of neuron\\u2013astrocyte interactions. Next, a digital circuit for the astrocyte dynamic is proposed which is simulated using ModelSim and finally, it is implemented in hardware on the ZedBoard. The results of hardware synthesis, FPGA implementations are in agreement with MATLAB and ModelSim simulations and confirm that the proposed digital astrocyte is suitable for applications in reconfigurable neuromorphic devices which implement biologically brain circuits. Though numerous approaches have been proposed for face recognition, little attention is given to the moment-based face recognition techniques. In this paper we propose a novel face recognition approach based on adaptively weighted patch pseudo Zernike moment array (AWPPZMA) when only one exemplar image per person is available. In this approach, a face image is represented as an array of patch pseudo Zernike moments (PPZM) extracted from a partitioned face image containing moment information of local areas instead of global information of a face. An adaptively weighting scheme is used to assign proper weights to each PPZM to adjust the contribution of each local area of a face in terms of the quantity of identity information that a patch contains and the likelihood of a patch is occluded. An extensive experimental investigation is conducted using AR and Yale face databases covering face recognition under controlled/ideal conditions, different illumination conditions, different facial expressions and partial occlusion. The system performance is compared with the performance of four benchmark approaches. The encouraging experimental results demonstrate that moments can be used for face recognition and patch-based moment array provides a novel way for face representation and recognition in single model databases. In this study, an illumination-tolerant face recognition algorithm is proposed. This work highlights the significance of matrix polar decomposition for illumination-invariant face recognition. The proposed algorithm has two stages. In the first stage, the authors reduce the effect of illumination changes by weakening the discrete cosine transform coefficients of block intensities using a new designed quantisation table. In the second stage, the unitary factor of polar decomposition of the reconstructed image is used as a feature matrix. In the recognition phase, a novel indirect method for measuring the similarities in feature matrices is proposed. The nearest-neighbour rule is applied to the matching. The authors have performed some experiments on several databases to evaluate the proposed method in its different aspects. Experimental results on recognition demonstrate that this approach provides a suitable representation for illumination invariant face recognition. One of the newest biometric identifier, which is recently used for personal identity authentication, is Finger-Knuckle-Print (FKP). In this paper, we present a novel method for personal identification and identity verification which includes Gabor filter bank, combination of PCA and LDA algorithms and Euclidean distance measure. These three steps are used for feature extraction, dimensionality reduction and the classification stage, respectively. Also the information fusion at feature level is used for different combination of fingers to improve the recognition rate. In the other hand, here this algorithm works as a kind of multi-modal method with a single biometric characteristic but multiple units. Poly-U Finger-Knuckle-Print database is used to examine the performance of the proposed method. The result of identification and verification experiments by combining the features of four fingers are obtained, 98.79% and 91.8%, respectively, which demonstrate the efficiency and effectiveness of this new biometric characteristic. This study presents a new adaptive data-hiding method based on least-significant-bit (LSB) substitution and pixel-value differencing (PVD) for grey-scale images. The proposed method partition the cover image into some non-overlapping blocks having three consecutive pixels and select the second pixel of each block as the central pixel (called base-pixel). Then, k-bits of secret data are embedded in the base pixel by using LSB substitution and optimal pixel adjustment process (OPAP). The difference between the base-pixel value and other pixel values in the block are utilised to determine how many secret bits can be embedded in the two pixels. Also, the method divides all possible differences into lower level and higher level with a number of ranges. Then, it obtains the number of the secret bits that will be embedded into each block depending on the range which the difference values belong to. The experimental results show that the proposed method can embed a large amount of secret data while maintaining a high visual quality of the stego-images. The peak signal-to-noise ratio (PSNR) values and the embedding capacity of our method are higher than those of three other data-hiding methods which are investigated in this study. We proposed a new method for isolated handwritten Farsi/Arabic characters and numerals recognition using fractal codes and Haar wavelet transform. Fractal codes represent affine transformations which when iteratively applied to the range-domain pairs in an arbitrary initial image, the result is close to the given image. Each fractal code consists of six parameters such as corresponding domain coordinates for each range block, brightness offset and an affine transformation, in this system, The support vector machine (SVM) which is based on statistical learning theory, with good generalization ability is used as the classifier. This method is robust to scale and frame size changes. 32 Farsi 's characters are categorized to 8 different classes in which the characters are very similar to each other. There are ten digits in Farsi/Arabic language and since two of them are not used in the postal codes in Iran, therefore 8 more classes are needed for digits. According to experimental results, classification rates of 92.71% and 92% were obtained for digits and characters respectively on the test sets gathered from various people with different educational background and different ages. This paper uses a method for extracting the Fetal Electrocardiogram (FECG) signal from two ECG signals recorded at thoracic and abdominal areas of mother. The thoracic ECG is assumed to be completely maternal ECG (MECG) while the abdominal ECG is assumed to be a combination of mother's and fetus's ECG signals and random noise. The maternal component of the abdominal ECG is a nonlinearly transformed version of MECG. The method uses Adaptive Neuro-Fuzzy Inference System (ANFIS) structure to identify the nonlinear transformation. We have used Genetic Algorithm (GA) as a tool for training the ANFIS structure. By identifying the nonlinear transformation, we have extracted FECG by subtracting the aligned version of the MECG signal from the abdominal ECG (AECG) signal. We validate the method on both real and synthetic ECG signals. The results show improvement in extraction of FECG signal with the method in this study. In this paper, we propose a new wavelet shrinkage algorithm based on fuzzy logic. In particular, intra-scale dependency within wavelet coefficients is modeled using a fuzzy feature. This feature space distinguishes between important coefficients, which belong to image discontinuity and noisy coefficients. We use this fuzzy feature for enhancing wavelet coefficients' information in the shrinkage step. Then a fuzzy membership function shrinks wavelet coefficients based on the fuzzy feature. In addition, we extend our noise reduction algorithm for multi-channel images. We use inter-relation between different channels as a fuzzy feature for improving the denoising performance compared to denoising each channel, separately. We examine our image denoising algorithm in the dual-tree discrete wavelet transform, which is the new shiftable and modified version of discrete wavelet transform. Extensive comparisons with the state-of-the-art image denoising algorithm indicate that our image denoising algorithm has a better performance in noise suppression and edge preservation. In this paper, the performance of several moment invariant features combined with various classification methods for the recognition of middle age Persian manuscripts is presented. Specifically, Legendre moments (order 2 to 12), Zernike and pseudo-Zernike moments (order 2 to 15), and the set of invariant moments (\\u03d51, \\u03d52, ..., \\u03d57) are used as features. These features are computed from four versions of character images; (1) grayscale character images (Set A), (2) semithresholded character images (Set B), (3) binarized character images (Set C), (4) character skeleton (Set D). For classification, we have used the minimum Mean Distance (MMD), k-nearest neighbor (KNN), and Parzen methods. The experiment yielded a 2.86% error rate (97.14% classification rate) with pseudo-Zernike moments on the semithresholded character images (set B). Error-prone patterns have been extensively studied for low-density parity-check codes yet they have never been fully explored for generator-based \\u2018Fountain codes\\u2019. It is shown here that these phenomena are related to certain combinatorial structures within the Tanner graph (TG) representation of the code, previously termed absorbing sets. The authors systematically define the \\u2018absorbing sets\\u2019 in the generator-based TG of a code. They then demonstrate how these substructures are damaging to the \\u2018realised rate, delay\\u2019 and \\u2018decoding cost\\u2019 of Fountain codes particularly at low error rates. They further analyse the existence probability of certain absorbing sets and propose a new encoder/decoder pair forming a new family of Fountain codes. The authors experimental results show that these new codecs lead to improvements in all system features. Typical gains for Luby-transform codes include 20% reduction in the decoding complexity and simultaneous coding gains of 0.6 and 0.9 dB at bit error rates of 10 \\u2212 5 and 10 \\u2212 6, respectively. As such, this work takes a step towards better rateless code design and construction. Abstract In this paper, a novel method is proposed for real-world pose-invariant face recognition from only a single image in a gallery. A 3D Facial Expression Generic Elastic Model (3D FE-GEM) is proposed to reconstruct a 3D model of each human face using only a single 2D frontal image. Then, for each person in the database, a Sparse Dictionary Matrix (SDM) is created from all face poses by rotating the 3D reconstructed models and extracting features in the rotated face. Each SDM is subsequently rendered based on triplet angles of face poses. Before matching to SDM, an initial estimate of triplet angles of face poses is obtained in the probe face image using an automatic head pose estimation approach. Then, an array of the SDM is selected based on the estimated triplet angles for each subject. Finally, the selected arrays from SDMs are compared with the probe image by sparse representation classification. Convincing results were acquired to handle pose changes on the FERET, CMU PIE, LFW and video face databases based on the proposed method compared to several state-of-the-art in pose-invariant face recognition. In this paper, a novel method is proposed for person-independent pose-invariant facial expression recognition based on 3D face reconstruction from only 2D frontal images in a training set. A 3D Facial Expression Generic Elastic Model (3D FE-GEM) is proposed to reconstruct an expression-invariant 3D model of each human face in the present database using only a single 2D frontal image with/without facial expressions. Then, for each 7-class of facial expressions in the database, a Feature Library Matrix (FLM) is created from yaw face poses by the rotating the 3D reconstructed models and extracting features in rotated face. Each FLM is subsequently rendered based on yaw angles of face poses. Before matching to the FLM, an initial estimate of yaw angles of face poses is obtained in the test face image using an automatic head pose estimation approach. Then, an array of the FLM is selected based on the estimated yaw angles for each class of facial expressions. Finally, the selected arrays from FLMs are compared with target image features by Support Vector Machine (SVM) classification. Favorable outcomes were acquired to handle pose in facial expression recognition on the available image based on the proposed method compared to several state-of-the-arts in pose-invariant facial expression recognition. Information fusion of various biometrics has attracted much attention in recent years. So in this paper we fused the information of biometrics in two different aspects. At the first, we investigate the information fusion in single modality, that is, the Finger-Knuckle-Print (FKP) biometric. FKP is one of the newest biometrics identifier which is recently used for personal identity authentication. For fusing the information of each FKP, two different representations of each image is used (Gray-Level intensity and its Gabor transform). On the other hand, two different subsets of feature vectors are extracted from each image. At the second stage, the information of each finger at two different fusion levels is fused: feature and matching score level. In fact this algorithm works as a kind of multi-modal method with a single biometric characteristic but multiple units. By fusing the information at different levels, the recognition rate can improve significantly. For example, by combining the information of four fingers, the recognition rate will be obtained 96.56% and 95.4% at feature and matching score levels, respectively. Poly-U Finger-Knuckle-Print database was used to examine the performance of the proposed method and the experimental results demonstrated the efficiency and effectiveness of this new biometric characteristic. One of the key problems in automated face recognition system is that of handling the face image variation in terms of scale, rotation (in plane) and translation. One approach is fixing mentioned problems in recognition processes by extracting one linear transformation invariant feature. This paper presents a novel method for face recognition. Pseudo Zernike moment invariant (PZMI) which has linear transformation invariance properties and is robust in the presence of noise utilized to produce feature vectors. For decreasing computational complexity of feature extraction step, we use genetic algorithm (GA) to select the optimal feature set which contains optimal PZMI orders and corresponding repetitions. In addition, we have investigated the effect of PZMI orders on recognition rate in noisy images. Proposed scheme has been tested on the FERET database. Experimental results prove the advantages of the proposed method when compared with other PZMI-based face recognition systems. A preprocessing stage in every audio application including music/speech separation, speech or speaker recognition and audio transcription task is inevitable to determine each frame belongs to which classes, namely: speech only, music only and finally mixture. Such classification can significantly lower the computational complexity due to factorial search commonly used in many model-based systems including monaural separation systems as well as music transcription scenarios. In this paper, employing sinusoidal parameters obtained by a fixed dimension modified sinusoidal model (FDMSM) already proposed in [7] a new classification approach is proposed to separate mixed type audio frames based on Support Vector Machine (SVM) and Relevance Vector Machine (RVM). The challenging problem in this work is seeking the most appropriate features to discriminate the underlying classes. As a result, we employ some unsupervised feature selection procedure to determine which feature to select to get the best results. The experimental results show that the proposed system presents acceptable classification result and outperforms other classification systems including k Nearest Neighbor (k-NN), Multi-Layer Perceptron (MLP). Traffic signal controllers and route guidance systems are two major subsystems of Intelligent Transportation Systems that influence each other directly. Due to correlation between these subsystems, some research has been done to combine them into an integrated system. In this paper, PersianGulf - an autonomous combined traffic signal controller and route guidance system - will be proposed which has two unique features as compared to similar research. First, it is totally distributed because calculation of optimal routes are done independently in traffic signal controllers of intersections and second, it is totally autonomous because there is no need to communicate/cooperate with either traffic supervisors such as traffic management centers or the vehicles/drivers. Also, Simulation results show that PersianGulf improve the average speed of vehicles significantly in the simulated scenarios. Recent neurophysiologic findings have shown that astrocytes play important roles in information processing and modulation of neuronal activity. Motivated by these findings, in the present research, a digital neuromorphic circuit to study neuron-astrocyte interaction is proposed. In this digital circuit, the firing dynamics of the neuron is described by Izhikevich model and the calcium dynamics of a single astrocyte is explained by a functional model introduced by Postnov and colleagues. For digital implementation of the neuron-astrocyte signaling, Single Constant Multiply (SCM) technique and several linear approximations are used for efficient low-cost hardware implementation on digital platforms. Using the proposed neuron-astrocyte circuit and based on the results of MATLAB simulations, hardware synthesis and FPGA implementation, it is demonstrated that the proposed digital astrocyte is able to change the firing patterns of the neuron through bidirectional communication. Utilizing the proposed digital circuit, it will be illustrated that information processing in synaptic clefts is strongly regulated by astrocyte. Moreover, our results suggest that the digital circuit of neuron-astrocyte crosstalk produces diverse neural responses and therefore enhances the information processing capabilities of the neuromorphic circuits. This is suitable for applications in reconfigurable neuromorphic devices which implement biologically brain circuits. The quality of the image formed by an optical system is reduced by aberrations. This paper points out and attempts to correct blind of lens aberration. To this end Zernike moments introduced for presenting lens aberration model within which their coefficients are estimated through polyspectral analysis. The model parameters are divided into asymmetric and symmetric which are estimated through bicoherence and tricoherence respectively. The obtained precision compares favorably to the aberration given by state of the art ployspectral analysis and reaches a RMSE of 0.1 pixels. Experimental observations have shown that astrocytes are active partner in neural information processing which are mainly carried out by neurons. In this research, we investigate this issue and provide a computationally efficient digital circuit for neuron-astrocyte interactions. The firing dynamics of the neuron is described by Izhikevich model (spiking and bursting activities) and the calcium dynamics of a single astrocyte explained by a proposed model adapted from a functional model introduced by Postnov and colleagues. The new linear model makes it possible to design an efficient multiplier-less hardware architecture for digital implementation of neuron-astrocyte interaction on field-programmable gate array (FPGA). Using the proposed neuron-astrocyte circuit and based on the results of MATLAB simulation, hardware synthesis and FPGA implementation, it is demonstrated that the proposed digital astrocyte is able to change the neuron spiking frequency through bidirectional signaling. This circuit may represent a fundamental computational unit for the development of artificial neuron-astrocyte networks, opening new perspectives in pattern recognition tasks. Information fusion of various biometrics for its high performance in biometric recognition system, has recently attracted much attention. So in this paper the information of biometrics in different aspects is considered. At the first, the information fusion in single modal is investigated. In this stage, two different subsets of feature vectors are extracted from each image and are serially combined. With this work a new vector with higher dimension is considered as feature vector of each image. The characteristic biometric used in this paper is Finger-Knuckle-Print (FKP) which is unique and is recently used for personal identity authentication. The database used in our experiment contains the images of four different fingers. Therefore at the second stage, the information fusion of each finger at feature level is investigated. In fact, this fusion works as a kind of multi-modal method with a single biometric characteristic but multiple units. Poly-U FKP database was used to examine the performance of the proposed method and the experimental results proved that combining the features of four fingers is able to increase the recognition rate compared to that produced by each finger, separability. This paper introduces an efficient method for face localization and recognition in color images. The proposed method uses the location of eyes for computation and extraction of a face's bounding ellipse. In this way, parameters of a face's ellipse (center, orientation, major and minor axis), is computed by the location of eyes in a face image. In the next step, we apply Pseudo Zernike Moments (PZM), Zernike Moments (ZM) and Principal Component Analysis (PCA) for feature extraction. For classification of these feature vectors a new structure of RBF neural networks with a novel distance function is introduced and a new method for determination of RBF unit parameters is proposed. Finally, we compare the efficiency of the proposed system for three types of feature vectors (PZM, ZM and PCA). Results emphasize the high accuracy and efficiency of the PZM features proportion to other features (ZM and PCA) for use in the proposed recognition system. Speech recognition technology is a technology that allows a computer to recognize the speech and the words that express through the microphone or speaks by phone. A Fuzzy Neural Network (FNN) is a learning approach that finds the parameters of a fuzzy system by exploiting approximation techniques from neural network. But FNN has some difficulty about how to automatically generate and adapt the membership function and fuzzy rules. To overcome the shortage of FNN, in this paper, we use SEFNN (Structure Equivalent Fuzzy Neural Network) and optimized its parameters with Firefly algorithm. Firefly Algorithm (FA), which is usually used in optimization problems is a stochastic population-based algorithm inspired by intelligent collective behavior of fireflies in the nature. The parameters of SEFNN trained by FA were used in speech recognition system to improve the ability of generalization of FNN. Results shows that the SEFNN optimized by FA for speech recognition system have higher recognition rate in compare of FNN trained by PSO method. This paper introduces a novel method for the recognition of human faces in two-dimensional digital images using a new feature extraction method and Radial Basis Function (RBF) neural network with a Hybrid Learning Algorithm (HLA) as classifier. The proposed feature extraction method includes human face localization derived from the shape information using a proposed distance measure as Facial Candidate Threshold (FCT) as well as Pseudo Zernike Moment Invariant (PZMI) with a newly defined parameter named Correct Information Ratio (CIR) of images for disregarding irrelevant information of face images. In this paper, the effect of these parameters in disregarding irrelevant information in recognition rate improvement is studied. Also we evaluate the effect of orders of PZMI in recognition rate of the proposed technique as well as learning speed. Simulation results on the face database of Olivetti Research Laboratory (ORL) indicate that high order PZMI together with the derived face localization technique for extraction of feature data yielded a recognition rate of 99.3%. This paper presents a fuzzy hybrid learning algorithm (FHLA) for the radial basis function neural network (RBFNN). The method determines the number of hidden neurons in the RBFNN structure by using cluster validity indices with majority rule while the characteristics of the hidden neurons are initialized based on advanced fuzzy clustering. The FHLA combines the gradient method and the linear least-squared method for adjusting the RBF parameters and the neural network connection weights. The RBFNN with the proposed FHLA is used as a classifier in a face recognition system. The inputs to the RBFNN are the feature vectors obtained by combining shape information and principal component analysis. The designed RBFNN with the proposed FHLA, while providing a faster convergence in the training phase, requires a hidden layer with fewer neurons and less sensitivity to the training and testing patterns. The efficiency of the proposed method is demonstrated on the ORL and Yale face databases, and comparison with other algorithms indicates that the FHLA yields excellent recognition rate in human face recognition. The selection of threshold is one the general methods in image segmentation, but often the selection of the optimal value for threshold is a challenge for researchers. In this paper we proposed a fast and optimal method for selection of good enough threshold value based on Particle Swarm Optimization algorithms (PSOa). To achieve the fast speed in the proposed method, five types of PSO algorithms have been evaluated. The brief Introduction of the principle OTSU, as the fitness function of PSO algorithm is given. Moreover, the proposed method has been applied in various experiments in comparison with famous methods based on several standard test Images. Experimental results demonstrated that the proposed method outperformed better in comparison of other methods. Biometrics-based authentication is an effective approach which is used for automatically recognizing a person's identity. Recently, it has been found that the finger-knuckle-print (FKP), which refe... In this paper, a novel approach based on multiobjective particle swarm optimization (MOPSO) is presented for panchromatic (Pan) sharpening of a multispectral (MS) image. This new method could transfer spatial details of the pan image into a high-resolution version of the MS image, while color information from the low-resolution MS image is well preserved. The pan and MS images are locally different because of different resolutions, and therefore we cannot directly combine them in the spatial domain. For this reason, we generate two initial results, which are more appropriate for a weighted combination. First, the pan and the MS images are histogram matched. Then we use the shiftable contourlet transform (SCT) to decompose the histogram-matched pan and MS images. The SCT is a new shiftable and modified version of the contourlet transform. In this step, an algorithm based on the SCT is used to generate two initial results of the high-resolution MS images. Our objective is to produce two modified high-resolution MS images, in which one has high spatial similarity to the pan image and the other one has high radiometric quality in each band. Therefore, we have used two different fusion rules to integrate the high-frequency contourlet coefficients of the pan and MS images to generate two initial results of high-resolution MS image or the pan-sharpened (PS) image. Finally, we can find the optimal PS image by applying the MOPSO algorithm and using the two initial PS results. Specifically, the PS image is obtained via a weighted combination of the two initial results, in which the weights are locally estimated via a multiobjective particle swarm optimization algorithm to generate a PS image with high spatial and radiometric qualities. Based on experimental results obtained, the produced pan-sharpened image also has good spectral quality. The efficiency of the proposed method is tested by performing pan-sharpening of high-resolution (Quickbird and Wordview2) and medium-resolution (Landsat-7 ETM +) datasets. Extensive comparisons with the state-of-the-art pan-sharpening algorithms indicate that our new method provides improved subjective and objective results. Although error-prone patterns have been extensively studied for low-density parity-check (LDPC) codes, to the best of our knowledge, they have never been fully explored for Fountain codes. It is shown that dominant trapping sets of Fountain codes are the absorbing sets. They happen in the so-called error floor, corresponding to a significant flattening in the error probability curves. In this paper, we introduce the properties of these dominant trapping sets for Fountain codes. This definition of absorbing sets leads to better design of practical finite-length Fountain encoders and decoders. Biometrics based personal identification is regarded as an effective method for automatic identification, with a high confidence coefficient. A multimodal biometric system consolidates the evidence presented by multiple biometric sources and typically provides better recognition performance compared to systems based on a single biometric modality. So in this paper we use combination of Face and Ear characteristic to individual's authentication. In our approach, features extracted using HMAX model are translation and scale-invariant. Then we applied Support vector machine (SVM) and K-nearest neighbor (KNN) classifiers to distinguish the classes. In fusion stage we use matching-score level. Experimental results showed 96% accuracy rate on ORL Face database and 94% accuracy rate on USTB Ear database; however we achieve 98% accuracy rate on Face and Ear multimodal biometric. Multiple-point statistics simulation has recently attracted significant attention for the simulation of complex geological structures. In this paper, a fast direct sampling (FDS) algorithm is presented based on a fast gradient descent pattern matching strategy. The match is directly extracted from the training image (TI) and so the method does not require intensive preprocessing and database storage. The initial node of the search path is selected randomly but the following nodes are selected in a principled manner so that the path is conducted to the right match. Each node is selected based on the matching accuracy and the behavior of the TI in the previous node. A simple initialization strategy is presented in this paper which significantly accelerates the matching process at the expense of a very naive preprocessing stage. The proposed simulation algorithm has several outstanding advantages: it needs no (or very limited) preprocessing, does not need any database storage, searches for the match directly in the TI, is not limited to fixed size patterns (the pattern size can be easily changed during simulation), is capable of handling both continuous and categorical data, is capable of handling multivariate data, and finally and more importantly, is a fast method while maintaining high standards for the matching quality. Experiments on different TIs reveal that the simulation results of FDS and DS are comparable in terms of pattern reproduction and connectivity while FDS is far faster than DS. The novel Imperialist Competitive Algorithm (ICA) that was recently introduced has a good performance in some optimization problems. The ICA inspired by sociopolitical process of imperialistic competition of human being in the real world. In this paper, a new Adaptive Imperialist Competitive Algorithm (AICA) is proposed. In the proposed algorithm, for an effective search, the Absorption Policy changed dynamically to adapt the angle of colonies movement towards imperialist's position. The ICA is easily stuck into a local optimum when solving high-dimensional multi-model numerical optimization problems. To overcome this shortcoming, we use probabilistic model that utilize the information of colonies positions to balance the exploration and exploitation abilities of the imperialistic competitive algorithm. Using this mechanism, ICA exploration capability will enhance. Some famous unconstraint benchmark functions used to test the AICA performance. Also, we use the AICA Algorithm to adjust the weights of a three-layered Perceptron neural network to predict the maximum worth of the stocks change in Tehran's Bourse Market. Simulation results show this strategy can improve the performance of the ICA algorithm significantly. Fountain codes are a new class of codes originally designed for robust and scalable transmission of data over lossy computer networks. Binary Fountain codes such as Luby transform codes are a class of erasure codes which have demonstrated an asymptotic performance close to the Shannon limit when decoded with the belief propagation algorithm. Although structures have been extensively studied for low-density parity-check codes, to the best of our knowledge, they have never been fully explored for Fountain codes and there is no survey for them. Thus, we will introduce the $$G$$ G -based tanner graph and the properties of Fountain codes as rateless low-density generator-matrix codes in this survey. Most of the work carried out during the previous years has been presented. Computing the bandwidth-delay-constrained least-cost multicast routing tree is an NP-complete problem. In this paper, we propose a novel QoS-based multicast routing algorithm based on the genetic algorithms (GA). In the proposed algorithm, the connectivity matrix of edges is used for genotype representation. Some novel heuristics are also proposed for mutation, crossover, and creation of random individuals. We evaluate the performance and efficiency of the proposed GA-based algorithm in comparison with other existing heuristic and GA-based algorithms by the result of simulation. The proposed algorithm has overcome all of the previous algorithms in the literature. In this paper, we propose adaptive multi-objective optimization framework based on non-dominated sorting genetic algorithm-II and learning automata (LA) for coverage and topology control in heterogeneous wireless sensor networks. The multi-objective optimization approach of the proposed framework, called MOOCTC (multi-objective optimization coverage and topology control), can simultaneously optimize several conflicting issues such as number of active sensor nodes, coverage rate of the monitoring area and balanced energy consumption while maintaining the network connectivity. This approach incorporates problem-specific knowledge in its operators to find high-quality solutions. In addition, this approach uses LA to dynamically adapt the crossover and mutation rates without any external control to improve the behavior of the optimization algorithm. Simulation results demonstrate the efficiency of the proposed multi-objective optimization approach in terms of lifetime, coverage and connectivity. In this paper, we propose a new approach for face representation and recognition based on Adaptively Weighted Sub-Gabor Array (AWSGA) when only one sample image per enrolled subject is available. Instead of using holistic representation of face images which is not effective under different facial expressions and partial occlusions, the proposed algorithm utilizes a local Gabor array to represent faces partitioned into sub-patterns. Especially, in order to perform matching in the sense of the richness of identity information rather than the size of a local area and to handle the partial occlusion problem, the proposed method employs an adaptively weighting scheme to weight the Sub-Gabor features extracted from local areas based on the importance of the information they contain and their similarities to the corresponding local areas in the general face image. An extensive experimental investigation is conducted using AR and Yale face databases covering face recognition under controlled/ideal condition, different illumination condition, different facial expression and partial occlusion. The system performance is compared with the performance of four benchmark approaches. The promising experimental results indicate that the proposed method can greatly improve the recognition rates under different conditions. The American Cancer Society (ACS) recommends women aged 40 and above have a mammogram every year as a Gold Standard for breast cancer detection. Multiple Classifier Technique, which is a hybrid intelligent system, aims to improve the Classification accuracy rate over single classifiers. In this paper, we present an effective approach to breast mammogram analysis to modify the classification accuracy of ensemble neural networkin which we utilize BI-RADS features that were combined with patient's age and subtlety value, which has been tested on a widely available Digital Database of Screening Mammography (DDSM). In our proposed method, we created an ensemble cluster by using Bagging, AdaBoost, Rotation Forest and reached 92% overall classification accuracy. Meeting quality of service(QoS) is a challenging task in Wireless Multimedia Sensor Networks(WMSNs). In this paper, a new power-aware QoS routing protocol for WMSNs is proposed, which uses local information in choosing the next hop. For real-time communication, we consider both delay at the sender node and queuing delay at the receiver, and try to reach the latency requirements by minimizing the hop counts for a packet needed to be transferred towards the sink. In order to achieve reliability requirements and energy efficiency, each node dynamically adjusts it's transmission power and chooses nodes which have less remaining hops towards the sink. We consider the history of the packet in order to update these requirements at each hop. A load balancing approach is used to increase lifetime and avoid congestion. Simulation results shows that our protocol can support QoS with less energy consumption. Abstract Fusion of multiple instances within a modality for improving the performance of biometric verification has attracted much attention in recent years. In this letter, we present an efficient Finger-Knuckle-Print (FKP) recognition algorithm based on multi-instance fusion, which combines the left index/middle and right index/middle fingers of an individual at the matching score level. Before fusing, a novel normalization strategy is applied on each score and a fused score is generated for the final decision by summing the normalized scores. The experimental results on Poly-U FKP database show that the proposed method has an obvious performance improvement compared with the single-instance method and different normalization strategies. In order to attack to a network, an attacker first must find vulnerability points of the target network. This task is done through scanning. There are many methods of scan detection. Most of these methods are based on thresholding. Setting a proper threshold value is crucial and depends on many parameters such as network structure and time window. In this study we proposed a new scan detection method based on genetic algorithm (GA). This method has two phases. In the first phase we separate normal traffic from suspicious traffic and send only suspicious traffic to the second phase. This way the overhead of the process in the second phase is decreased considerably. In the second phase we aim to detect attacks with respect to two optimum parameters of threshold and memory. We compared our method with snort. Results showed that our method achieves better performance in both hit rate and false alarm rate. In this paper, a novel method for real-time pose-invariant face recognition is proposed from only a single image in a gallery including any facial expressions. A 3D Facial Expression Generic Elastic Model (3D FE-GEM) is proposed to reconstruct 3D model of each human face in the present database using only a single 2D frontal image. Then, for each person in the database, a Triplet Pose Sparse Matrix (TPSM) is created from all face poses by rotating the 3D reconstructed models and extracting features in rotated face. Each TPSM is subsequently rendered based on triplet angles of face poses. Before matching to TPSM, an initial estimate of triplet angles of face poses is obtained in the test face image/video using an automatic head pose estimation approach. Then, an array of the TPSM is selected based on the estimated triplet angles for each subject. Finally, the selected arrays from TPSMs are compared with target image by joint dynamic sparse representation classification. Favorable outcomes were acquired to handle pose and expression changes on the available image and video databases based on the proposed method compared to several state-of-the-arts in pose-invariant face recognition. ATM as a high-speed cell switching technology can support multiple classes of traffic with different quality of service (QoS) requirements and diverse traffic characteristics. A main QoS requirement is the cell loss ratio (CLR). We need a real-time expression for the CLR calculation in ATM networks where the statistical multiplexing is an important factor. The existing analytical methods for the CLR estimation are mostly based on fluid-flow and stationary approximate models. In this paper, we first evaluate these methods against the results obtained through simulation. The simulation is done at the cell level that provides very accurate results with buffer size as a variant. It is shown that the CLR estimation based on existing analytical models are widely overestimated. We have, then, proposed three new approaches that yield significant improvement in the accuracy of the CLR approximation. First, we have found global correction coefficients to compensate for the error of the current analytical methods. Second, we have proposed a new upper bound based on exact modeling of system behavior in the finite buffer case. This is a novel approach that combines fluid-flow and stationary approximate models and outperforms all the previous ones. The accuracy of the proposed model is verified by simulation. Third, we have found a tight piece-wise linear approximation that can be calculated in real-time. We have studied application of these bounds in non-homogeneous as well as homogeneous cases. In this paper, we propose and implement a novel and real-time method for recognizing hand gestures using depth map. The depth map contains information relating to the distance of objects from a viewpoint. Microsoft\\u2019s Kinect sensor is used as input device to capture both the color image and its corresponding depth map. We first detect bare hand in cluttered background using distinct gray-level of the hand which is located near to the sensor. Then, scale invariance feature transform (SIFT) algorithm is used to extract feature vectors. Lastly, vocabulary tree along with K-means clustering method are used to partition the hand postures to ten simple sets as: \\\"one\\\", \\\"two\\\", \\\"three\\\", \\\"four\\\", \\\"five\\\", \\\"six\\\", \\\"seven\\\", \\\"eight\\\", \\\"nine\\\" and \\\"ten\\\" numbers based on the number of extended fingers. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently. Consequently, it leads to an improvement in accuracy of the clustering. The experimental results show superiority of the proposed method over other available approaches. With this approach, we are able to recognize 'numbers' gestures with over 90% accuracy. DOI: http://dx.doi.org/10.11591/ijece.v3i6.3954 Abstract Feature selection (FS) is a most important step which can affect the performance of a pattern recognition system. This paper proposes a novel feature selection method based on ant colony optimization (ACO). ACO algorithm is inspired of ant\\u2019s social behavior in their search for the shortest paths to food sources. Most common techniques for ACO-based feature selection use the priori information of features. However, in the proposed algorithm classifier performance and the length of the selected feature vector are adopted as heuristic information for ACO. So, we can select the optimal feature subset in terms of shortest feature length and the best performance of classifier. The experimental results on face recognition system using ORL database show that the proposed approach is easily implemented and without any priori information of features, its total performance is better than that of GA-based and other ACO-based feature selection methods. This paper presents a new algorithm for jointly optimal control of session rate, link attempt rate, and link power in contention based MultiHop Wireless Networks. Formulating the problem in the framework of nonlinear optimization, we derive the required updates at end points and links to reach the optimal operating point. The proposed algorithm is a cross layer algorithm considering power control at the physical layer, attempt rate control at the Medium Access Control (MAC) layer and rate control at the transport layer of the network. The optimization variables are coordinated through two shadow prices. The first one regulates each session rate to the throughput of the links in its path, and the second one controls the attempt rates to meet maximal clique capacity constraint. Considering a model for successful transmission, the excitatory and inhibitory factors affecting each variable are derived. The proposed algorithm can be implemented in distributed fashion by message passing in the network. Simulation results at the link level verify the analytical approach and show that the algorithm converge and reach joint optimal point. Given large number of words to be recognized, a two-stage strategy for eliminating unlikely candidates before recognition can be a reasonable and powerful approach for increasing the recognition speed. A holistic lexicon reduction technique for offline handwritten Arabic word recognition is proposed in this paper. The principle of this technique involves the extraction of dots and subwords from the cursive Arabic word image to describe its shape. In the first stage of reduction, the number of subwords in the input word is estimated. Then in the second stage, the word descriptor, based on the dots information, is used while taking into account only the candidates selected in the first stage. Experimental results on IFN/ENIT database, consisting of 26,459 cursive Arabic word images, show a lexicon reduction of 92.5% with accuracy of 74%. This paper introduces an efficient method for face localization and recognition in color images. The proposed method uses the location of eyes for computation and extraction of a face's bounding ellipse. In this way, parameters of a face's ellipse (center, orientation, major and minor axis), is computed by the location of eyes in a face image. In the next step, we apply pseudo Zernike moments (PZM), Zernike moments (ZM) and principal component analysis (PCA) for feature extraction. For classification of these feature vectors a new structure of RBF neural networks with a novel distance function is introduced and a new method for determination of RBF unit parameters is proposed. Finally, we compare the efficiency of the proposed system for three types of feature vectors (PZM, ZM and PCA). Results emphasize the high accuracy and efficiency of the PZM features proportion to other features (ZM and PCA) for use in the proposed recognition system. This paper proposes a new method for extracting the Foetal Electrocardiogram (FECG) signal from two ECG signals recorded at thoracic and abdominal areas of mother. The thoracic ECG is assumed to be completely maternal ECG (MECG) while the abdominal ECG is assumed to be a combination of mother's and fetus's ECG signals and random noise. The maternal component of the abdominal ECG is a nonlinearly transformed version of MECG. The method uses Adaptive Nero-Fuzzy Inference System (ANFIS) structure to identify the nonlinear transformation. We have used Particle Swarm Optimization (PSO) as a new tool for training the ANFIS structure. By identifying the nonlinear transformation, we have extracted FECG by subtracting the aligned version of the MECG signal from the abdominal ECG (AECG) signal. We validate our new method on both real and synthetic ECG signals. The results shows improvement in extraction of foetal electrocardiogram signal with our proposed method. Pathophysiologic neural synchronization is a sign of several neurological disorders such as parkinson and epilepsy. In addition, based on established neurophysiologic findings, astrocytes (more type of glial cells) regulate dynamically the synaptic transmission and have key roles in stabilizing neural synchronization. Therefore, in the present study, a new model for digital astrocyte-inspired stimulator is proposed and constructed to break the synchronous oscillations of a minimal network. The minimal network is composed of two Hopf oscillators connected via gap-junction. The complete digital circuit of the closed loop system that is the proposed astrocyte-inspired stimulator and the coupled Hopf oscillators are implemented in hardware on the ZedBoard development kit. The results of MATLAB, ModelSim simulations and FPGA implementations confirm that the digital proposed astrocyte-inspired stimulator can effectively desynchronize the synchronous oscillations of the coupled Hopf oscillator with a demand-controlled characteristic. In this way, the designed digital stimulator not only does not suppress oscillator natural features but also it successfully maintains the desired asynchronous activity. Slant correction is an important part of the normalization task in OCR applications. Due to some special specifications of Farsi and Arabic manuscripts, conventional deslanting methods proposed for other languages do not work properly. In this paper, a fast method is first introduced to estimate the overall tilt of a handwritten word based on directional filters. After overall deslanting, a novel non-uniform slant estimation algorithm computes the remaining slant of each near-vertical stroke of the word, separately. Each candidate stroke is traced and its slant is calculated. A non-uniform slant correction algorithm is also proposed to reduce the remaining slants of each candidate stroke keeping the distortions of other strokes of the word at a minimum level. Thanks to the special characteristics of Farsi/Arabic scripts, slants are estimated in a specific strip of the written words. A comparison between our approach and three other prevalent methods is drawn. Experiments show that the proposed overall slant estimation method not only represents the least estimation error, but is also the fastest algorithm. The best results are achieved using the proposed overall and non-uniform deslanting methods. It is concluded that successful results can be achieved by considering the special specifications of these two languages. Feature selection and feature extraction are the most important steps in classification and regression systems. Feature selection is commonly used to reduce the dimensionality of datasets with tens or hundreds of thousands of features, which would be impossible to process further. Recent example includes quantitative structure\\u2013activity relationships (QSAR) dataset including 1226 features. A major problem of QSAR is the high dimensionality of the feature space; therefore, feature selection is the most important step in this study. This paper presents a novel feature selection algorithm that is based on entropy. The performance of the proposed algorithm is compared with that of a genetic algorithm method and a stepwise regression method. The root mean square error of prediction in a QSAR study using entropy, genetic algorithm and stepwise regression using multiple linear regressions model for training set and test set were 0.3433, 0.3591 and 0.5500, 0.4326 and 0.6373, 0.6672, respectively. In this paper, a novel method for expression-insensitive face recognition is proposed from only a 2D single image in a gallery including any facial expressions. A 3D Generic Elastic Model (3D GEM) is used to reconstruct a 3D model of each human face in the present database using only a single 2D frontal image with/without facial expressions. Then, the rigid parts of the face are extracted from both the texture and reconstructed depth based on 2D facial land-marks. Afterwards, the Gabor filter bank was applied to the extracted rigid-part of the face to extract the feature vectors from both texture and reconstructed depth images. Finally, by combining 2D and 3D feature vectors, the final feature vectors are generated and classified by the Support Vector Machine (SVM). Favorable outcomes were acquired to handle expression changes on the available image database based on the proposed method compared to several state-of-the-arts in expression-insensitive face recognition. This paper compares performance of the feature extraction techniques, Zernike moment (ZM), pseudo-Zernike moment (PZM) and Legendre moment (LM), in the application of face recognition and in presence of salt-pepper noise. In this study, after preprocessing and face localization of an image, its features are extracted. Also RBF neural network (RBFNN) with HLA learning algorithm has been used as a classifier. We trained the classifier three times for each group of extracted features of images. Then we added salt-pepper noises to images with three different probabilities, 0.02, 0.05 and 0.08. The trained RBFNN is tested with original and noisy versions of images. Experimental results on AUTDB show that the performance of the LM in all cases is better than the others. In this paper a new license plates recognition method using a Neural Network, trained by Chaotic Imperialistic Algorithms (CICA), is introduced. In this paper the background of the plate image is omitted, the characters are separated, and then the features of the characters are extracted. The features vector is feed into a multi layered perception neural network trained by CICA. Our dataset include 250 Farsi license plate images for train and 50 images for test in which the test images were noisy. The empirical results of the CICA-NN for license plate recognition are compared with the PSO-NN, GA-NN and MLP neural network. The results show that our method is faster and more accurate than the other methods. Ear structure as a new class of biometrics can be used in many applications such as security systems. Ear structure is physiologically unique and stable, so ear recognition can be a good choice for a biometric security system. Even though, using ear biometric is not customary but it can be used with other biometrics like face or fingerprint simultaneously to increase the reliability of a biometric security system. In this paper, regarding existence of important information in edges and high frequency points in ear structure, we apply 2D wavelet to the normalized image. By decomposing the image into three images (horizontal, vertical and diagonal) using wavelet, we find three independent features in three directions. We combine these decomposed images to reach the feature matrix. This allows considering the changes in the ear images from three basic directions simultaneously. We apply PCA on feature matrix to feature dimension reduction and classification. Our experience in using this approach for different images demonstrates the accuracy of 90.5% and 95.05% recognition rate for two sets of databases. This paper introduces a new method for the recognition of human faces in 2-dimensional digital images using a new localization of facial information and Pseudo Zernike Moment Invariants (PZMI) as features and a radial basis function (RBF) neural network as the classifier. In this paper the effect of two parameters in recognition rate improvement are studied. These include the order of the PZMI as well as facial candidate ratio (FCR) of images. The tests are carried out on the Olivetti Research Laboratory (ORL) database and a comparative study with two of the existing techniques are included to show the effectiveness of the proposed technique. Moments and functions of moments have been utilized as pattern features in various applications to achieve invariant recognition of two-dimensional image patterns. This paper introduces an experimental evaluation of the effectiveness of utilizing orthogonal moments such as Zernike moments, pseudo Zernike moments, and Legendre moments in recognition of the handwritten Farsi numerals. We used evolution strategies (ESs) for clustering of handwritten Farsi numerals, so that the clusters are formed only based on the inherent properties of the pattern features. Considering the fact that the classification is unsupervised, the error rate is about 5% for moments of orders higher than 5. The pseudo Zernike moments of order of 5 have the best performance among all the moment invariants. Mobile Ad Hoc Network is a Network composed of mobile nodes that topology is constantly changing. If in the routing in these networks, the nodes that are selected as route nodes are not stable and have a high mobility, data transmission route is broken quickly. Therefore, the method can be considered in the routing that from the nodes are between source and destination, nodes that are more stable, are selected as intermediate nodes. In this paper we propose a novel routing metric for MANETs that is called Signal Strength Based Reliability (SSBR). In this scheme, by measuring signal strength changes of neighbor nodes, we can identify nodes that have a lot of mobility and can cause link failure. Thus, we don't select them as route nodes. Advantage of this scheme is by selecting reliable nodes we can create a stable route that it has long lifetime. Simulation results show SSBR has better performance than other methods in same scenarios. The Imperialist Competitive Algorithm (ICA) that was recently introduced has shown its good performance in optimization problems. This novel optimization algorithm is inspired by socio-political process of imperialistic competition in the real world. In this paper a new Imperialist Competitive Algorithm using chaotic maps (CICA) is proposed. In the proposed algorithm, the chaotic maps are used to adapt the angle of colonies movement towards imperialist\\u2019s position to enhance the escaping capability from a local optima trap. The ICA is easily stuck into a local optimum when solving high-dimensional multi-model numerical optimization problems. To overcome this shortcoming, we use four different chaotic map incorporated into ICA to enhance the exploration capability. Some famous unconstraint benchmark functions are used to test the CICA performance. Simulation results show this variant can improve the performance significantly In this article the LEACH (low energy adaptive clustering hierarchy) in the homogeneous wireless sensor networks (WSNs) was considered and a new protocol with an approach toward promoting LEACH was introduced. In the new protocol which is called LELE (leader election with load balancing energy), the major work was how to elect the leader which was done through changing the sensor's energy, LELE protocol compares the amount of energy and distance of a node with its neighbouring sensors to elect it as leader. And, finally the sensor with the higher energy (has more suitable position as compared with the neighbouring nodes ) for chosen as the leader. After simulating the new protocol and comparing it with the previous protocol it was well observed that LELE in addition to balancing the energy and throughput, increases the network's lifetime as compared with that of LEACH. In the new protocol the time distance between the observation of the first dead sensor and the last sensor's having lost its energy is so short. This paper introduces an efficient method for human face recognition that employs a set of different kinds of feature domains with RBF neural network classifiers, and which is denoted the hybrid N-feature (HNF) human face recognition. A combination of RBF neural network classifiers with fuzzy integral has been proposed to achieve face classification with higher performance. The feature extractor projects the face images in each appropriately selected transform domain in parallel. Experimental results on the ORL database confirm that the proposed method lends itself to higher classification accuracy relative to existing techniques. In this paper, we proposed a new method for isolated handwritten Farsi/Arabic numerals recognition using quad tree-based fractal representation and iterated function system. Fractal codes represent affine transformations which when iteratively applied to the range-domain pairs in an arbitrary initial image, the result is close to the given image. Each fractal code consists of six parameters such as the corresponding domain coordinates for each range block, brightness offset and an affine transformation. Based on fixed point theorem in iterated function system, we introduced fractal transformation classifier for optical character recognition. We also used Euclidean distance between fractal codes of a query image and fractal codes of all images in the database as a measure of distance for classification. Since fractal codes have different lengths, we applied PCA algorithm to normalize their lengths. There are ten digits in Farsi/Arabic language and since two of them are not used in Iran postal codes, therefore 8 classes are needed for digits. By using fractal codes with nearest neighbor classifier and fractal transformation, the recognition rate of 92.6% is obtained on our numeral database which contains 480 samples per digit and was gathered from more than 200 people with different ages and different educational background. Recently, the authors proposed a distributed dynamic route guidance system called UTOSPF (Urban Traffic Open Shortest Path First) based on Wireless Sensor Networks (WSNs) and Open Shortest Path First (OSPF) protocol. In UTOSPF, the routes will be found only based on Estimated Street Travel Times (ESTTs) which will be needed to pass the streets and regardless of Waiting Times for Green Light (WTGLs) which drivers spend behind the red lights. In this paper, a simple method will be proposed to involve the WTGLs in calculation of optimal routes and its simulation results will be presented. Simulation results show that this method can improve the average speed of vehicles up to 66% as compared to random routes and up to 27% as compared to original UTOSPF, in the simulated scenarios. A system for recognition of segmented handwritten Persian/Arabic numerals irrespective of size and translation is developed. The image is represented by invariant features obtained from a new shadow coding scheme designed for the considered shapes. Classification is performed by a modified version of a four-layer probabilistic neural network (PNN) called the edited PNN (EPNN). Due to an editing and condensation procedure on the training samples, the EPNN has better performance and the network size is smaller. The performance of the system is evaluated on a database consisting of 2600 digits written by 10 different people. The obtained recognition accuracy is 97.8 percent. The developed system can process approximately two digits per second on a Intel 486 based PC with a 66 MHz clock. In this article we applied modified binary particle swarm optimization algorithm for solving the sensor placement in distributed sensor networks. PSO is a real value algorithm, and the discrete PSO is proposed to be adapted to discrete binary space. In the distributed sensor networks, the sensor placement is NP-complete for arbitrary sensor fields and it is one of the most important issues in the research fields, so the proposed algorithms are going to solve this problem by considering two factors: one is the complete coverage and the second one is the minimum costs. The proposed method on sensors surrounding is examined in different area. The results not only confirmed the successes of using new method in sensor replacement, but also they showed that the new method performs more efficiently compared to the Simulated Annealing Algorithm. This paper introduces a novel method for the recognition of human faces in digital images using a new feature extraction method that combines the global and local information in frontal view of facial images. Radial basis function (RBF) neural network with a hybrid learning algorithm (HLA) has been used as a classifier. The proposed feature extraction method includes human face localization derived from the shape information. An efficient distance measure as facial candidate threshold (FCT) is defined to distinguish between face and nonface images. Pseudo-Zernike moment invariant (PZMI) with an efficient method for selecting moment order has been used. A newly defined parameter named axis correction ratio (ACR) of images for disregarding irrelevant information of face images is introduced. In this paper, the effect of these parameters in disregarding irrelevant information in recognition rate improvement is studied. Also we evaluate the effect of orders of PZMI in recognition rate of the proposed technique as well as RBF neural network learning speed. Simulation results on the face database of Olivetti Research Laboratory (ORL) indicate that the proposed method for human face recognition yielded a recognition rate of 99.3%. Scheduling algorithms are used in operating systems to optimize the usage of processors. One of the most efficient algorithms for scheduling is Multi-Layer Feedback Queue (MLFQ) algorithm which uses several queues with different quanta. The most important weakness of this method is the inability to define the optimized the number of the queues and quantum of each queue. These factors affect the response time directly. Also this algorithm does not show any considerable improvement in response time of the processes in comparison with the other scheduling algorithms. In this paper, a new algorithm is presented for solving these problems and minimizing the response time. In this algorithm Recurrent Neural Network has been utilized to find both the number of queues and the optimized quantum of each queue. Also in order to prevent any probable faults in processes' response time computation, a new fault tolerant approach has been presented. The experimental results show that using the IMLFQ algorithm results in better response and waiting time in comparison with other scheduling algorithms. In this paper, we propose an approach for routing in SPEED protocol considering residual energy in routing decisions. Due to the limited energy of a sensor node, energy-efficient routing is a very important issue in sensor networks. This approach finds energy-efficient paths for delay-constrained data in real-time traffic. The SPEED protocol does not consider any energy metric in its routing. In our approach, routing is based on a weight function, which is a combination of the three factors: Delay, Energy & Speed. Here, the node with the greatest value in the weight function is to be selected as the next hop forwarding. We increase the network lifetime by considering energy metric in routing decisions. This method aims to construct a nearly stateless routing protocol, which can be used to route data based on the nodes\\u2019 residual energy. Simulation results demonstrate that the new scheme improves network lifetime about 15% longer than the traditional SPEED protocol. Abstract A holistic system for the recognition of handwritten Farsi/Arabic words using right\\u2013left discrete hidden Markov models (HMM) and Kohonen self-organizing vector quantization is presented. The histogram of chain-code directions of the image strips, scanned from right to left by a sliding window, is used as feature vectors. The neighborhood information preserved in the self-organizing feature map (SOFM), is used for smoothing the observation probability distributions of trained HMMs. Experiments carried out on test samples show promising performance results. This paper proposed a new image steganalysis scheme based on statistical moments of histogram of multi-level wavelet subbands in frequency domain. Different frequencies of histogram have different sensitivity to various data embedding. Then we decompose the test image using three-level Haar discrete wavelet transform (DWT) into 13 subbands (here the image itself is considered as the LLO subband). The DFT of each subband, is calculated. It is divided into low and high frequency bands. The first three statistical moments of each band are selected to form a 78-dimensional feature vector for Steganalysis. Support vector machines (SVM) classifier is then used to discriminate between stego- images and innocent images. Experimental results show that the proposed algorithm outperforms previously existing techniques. This paper presents an off-line signature verification system. The verification is based on signature shape descriptors such as the skeleton, upper and lower envelopes, and the high pressure region of the signatures. Multiple multilayer perceptron neural network modules cooperating in taking a verification decision via a fuzzy integral voter are used. This method is capable of verifying simple and skilled forgeries with a good performance. Solar panels are the power sources in photovoltaic applications which provide electrical power. Solar panel characteristics depend on environmental conditions (solar radiation level, temperature and etc.). In this paper, estimation of maximum power point of silicon solar panels is presented. We applied two different neural networks (back propagation and RBF) for the purpose of estimation in different environmental conditions. These neural networks estimate Maximum power point of solar panels accurately. We used Matlab environment for the purpose of simulation, training and evaluation of these neural networks. It is shown that the responses of RBF neural network are faster and more accurate than back propagation. A statistical method embedded with statistical features is proposed for Farsi/Arabic handwritten zip code recognition in this paper. The numeral is first smoothed and the skeleton is obtained. A set of feature points are then detected and the skeleton is decomposed into primitives. A primitive code includes the information of each primitive and a global code is derived from the primitive codes to describe the topological structure of the skeleton. By using the average and variance of X and Y changes in each primitive, the direction and curvature of the skeleton can be statistically described. Since the global codes have different lengths, we applied PCA algorithm to normalize their lengths. Thanks to statistically description of the skeleton, we can use the nearest neighbor classifier for recognition. According to experimental results, classification rate of 94.44% is obtained for numerals on the test sets gathered from various people with different educational background and different ages. Our database includes 480 samples per digit. We used 280 samples of each digit for training and the rest [200] for test. We like to find the cell loss ratio in ATM networks when the statistical multiplexing is an important factor. In this paper, first we have proposed the combination of three analytical expressions, which approximate the cell loss probability, based on the fluid-flow approximation model and two stationary approximation models. Second, we have provided a very accurate numerical model for the finite buffer, which lies at the input of each VP. The sources are statistically independent and each traffic source has a two-state Markov model. This simulation is done at the cell level and its results are very accurate. We have compared the results of the numerical simulation with the results of the analytical approximation models. Also we have used linear estimation to find an accurate expression for cell loss approximation in ATM networks. In this paper, we presented a novel approach for automated 3D face recognition using range data. An object recognition system generally consists of two main parts: data registration and data comparison. In first step, the nose tip was used as the reference point and 3D face shape was normalized to standard image size. The 2DPCA was applied to the resultant range data and the corresponding principal images were used as the feature vectors. Classification was carried out by calculating the similarity score between the feature vectors. The SVM classifier was used in choosing the closest match. Recognition rate of 97% rank-four was achieved. Recognition of QRS-Wave in the ECG signal is one of the important stages for ECG signal processing and most of the ECG noise removal algorithms, and automatic ECG interpreter systems need to detect these points. In most cases ECG signals are noisy and we need to detect these points using noisy signals. We have developed a QRS-wave recognition system using MART (multi-channel ART) neural network. In this method signal of two leads of ECG is used for detection, so our method has low sensitivity to noises. We tested our method for noiseless and noisy ECG signals and we compared results against those of an older one, which uses ART2 neural network. Results showed that our method has good results for noisy signals. In this paper, we present a novel face detection approach based on adaboosted relevance vector machine (RVM). The novelty of this paper comes from the construction of the kernel classifier with different kernel parameters. We use Fisher's criterion to choose a subset of Haar-like features. The proposed combination outperforms in generalization in comparison with support vector machine (SVM) on imbalanced classification problem. The combination of boosting algorithm and RVM classifier will yield accurate and sparse model which will perform well in real-time application. This method is compared, in terms of classification accuracy, to other commonly used methods, such as SVM and RVM without boosting, on CBCL face database. Results indicate that the performance of the proposed method is overall superior to previous approaches with very good sparsity. Recent advancements in wireless communications and electronics have caused the emergence of a new type of networks called Wireless Sensor Networks (WSNs). WSNs have some unique features such as autonomy, low cost, tiny size, simple and fast deployment and high scalability, and many applications have been forecasted for them. In recent years, many WSN-based applications were proposed for Intelligent Transportation Systems (ITS). In this paper, UTOSPF-Urban Traffic Open Shortest Path First-a novel distributed system based on WSNs and Open Shortest Path First (OSPF) protocol is proposed that collects the real-time traffic information from roads, and based on this information, the optimal routes for every destination will be found and drivers will be informed via Variable Message Signs (VMS) or special transceivers. Fast and low cost deployment, high scalability, autonomy and distributed processing are main features of this system that make it suitable for using in urban environments. Simulation results showed that UTOSPF improve the average speed of vehicles up to 40% as compared to random routes in the simulated scenario. This paper addresses the problem of designing a power-aware multiple access control (MAC) protocol for multihop wireless networks (MHWN) using slotted aloha code division multiple access (CDMA) at the physical layer. The problem is formulated as one of a cross layer network utility maximization that considers the interaction of transport, MAC, and physical layers in the protocol stack. Assuming physical model for successful transmission, a tractable formula for link throughput as a function of link attempt rate and power vectors is derived. Considering the effect of self-interference in analysis, an algorithm for adjusting the sessions rate and links state, i.e., links attempt rate and power, is derived. Sessions rate and links state are coordinated by congestion signals feedback to sources and messages broadcasted by links respectively. Analysis and simulation results show that adjusting the link state can mitigate the interference and improve the utilization of network resources. Also, it is shown that for a given link at the equilibrium, the link state should be adjusted on the basis of the link location in the network. This result emphasizes that in MHWN, MAC should be designed by considering both time and space contentions between links, which in turn, are provided by adjusting the links attempt rate and power. An efficient method for expression-invariant three-dimensional (3-D) face reconstruction from a frontal face image with a variety of facial expressions (FE) using the FE generic elastic model (GEM) is proposed. Three generic models are employed for FE modeling in the generic elastic model (GEM) framework, which are combined based on the similarity distance around the lips. Exclusively, FE-GEM demonstrated that it is more precisely able to estimate a 3-D model of a frontal face, attaining a more robust and better quality 3-D face reconstruction under a variety of FEs compared to the original GEM approach. It is tested on an available 3-D face database and its accuracy and robustness are demonstrated compared to the GEM approach under a variety of FEs. Also, the FE-GEM method is tested on available two-dimensional face databases and a new synthesized pose is generated from gallery images for handling pose variations in face recognition. In this paper, a hybrid method is proposed for multi-channel electroencephalograms (EEG) signal compression. This new method takes advantage of two different compression techniques: fractal and wavelet-based coding. First, an effective decorrelation is performed through the principal component analysis of different channels to efficiently compress the multi-channel EEG data. Then, the decorrelated EEG signal is decomposed using wavelet packet transform (WPT). Finally, fractal encoding is applied to the low frequency coefficients of WPT, and a modified wavelet-based coding is used for coding the remaining high frequency coefficients. This new method provides improved compression results as compared to the wavelet and fractal compression methods. Clustering is one of the popular techniques for data analysis. In this paper, we proposed a new method for the simultaneously clustering and feature selection through the use of the multi-objective particle swarm optimization (PSO). Since different features may have different important in various contexts; some features may be irrelevant and some of them may be misleading in clustering. Therefore, we weighted features and by using a threshold value which is automatically produced by the algorithm itself; then some of features with low weight is omitted. Evolutionary algorithms are the most famous technique for clustering. There are two main problems with clustering algorithms based on evolutionary algorithms. First, they are slow; second, they are dependent on the shape of the cluster and mostly work well with a specific dataset. To solve the first problem and increased the speed of the algorithm, we use two local searches to improve cluster centers and to estimate the threshold value. To handle the second problem, we evaluate the clustering by combine the two validation criterion methods of a new proposed KMPBM validation criterion and Conn validation criterion as a multi-objective fitness function. These two validation criterion because based on compactness and connectedness criterion can work independent of the shape of clusters. Experimental on the three Synthetics datasets and three real datasets shows that our proposed algorithm performs clustering independently for the shape of clusters and it can have good accuracy on dataset with any shape. This paper presents a new two-stage method for estimating and correcting the baseline of handwritten subwords in Farsi and Arabic text lines. Based on the template matching algorithm, the candidate baseline pixels are detected. The writing path and the baseline of the subwords are estimated in the first and second stages of the proposed algorithm, respectively. After the estimation in each stage, the baseline is adjusted in the correction phase. Experimental results show the effectiveness of this approach in adjusting the baseline close to the correct position. In this paper, a novel multimodal biometric recognition system using three modalities including face, ear and gait, based on Gabor+PCA feature extraction method with fusion at matching score level is proposed. The performance of our approach has been studied under three different normalization methods (min-max, median- MAD and z-score) and two different fusion methods (weighted sum and weighted product). Our new method has been successfully tested using 360 images corresponding to 120 subjects from three databases including ORL face database, USTB ear database, and CASIA gait database. Because of these biometric traits, our proposed method requires no significant user cooperation and also can work from a long distance. According to the experimental results our proposed method exhibits excellent recognition performance and outperforms unimodal systems. The best recognition performance that our proposed method achieved is %97.5. Wavelet transform has been emerged over recent years as a powerful time-frequency analysis and signal coding tool favored for the interrogation of complex non stationary signals. Its application to bio-signal processing has been at the forefront of these developments where it has been found particularly useful in the study of these, often problematic, signals: none more so than the Electrocardiogram (ECG). In this paper, the emerging roles of the wavelet transform in the ECG preprocessing and noise removing step is discussed in detail. One of the most important noise sources, baseline wandering, which can be affected ECG signal analysis is introduced and a new method based on wavelet transform is being proposed. The proposed method construct a model of baseline wander with multiresolution analysis of the signal using discrete wavelet transform and then remove the baseline wander from the ECG signal using the constructed model. Simulations were carried out to show the performance of the algorithm using the MIT-BIH noise stress test database and PTB diagnosis database. The quality of the results by the proposed technique is found to meet or exceed that of published results using other conventional methods such as kalman filtering and conventional digital filters. A hybrid system for the recognition of handwritten Farsi words using self-organizing feature map, right-left discrete hidden Markov models, and evolutionary programming is presented. The histogram of chain-code directions of the image strips, scanned from right to left by a sliding window, is used as feature vectors. The self-organizing feature map is used for constructing the codebook and also smoothing the observation probability distributions. A population based approach using evolutionary programming with a self-adaptive Cauchy mutation operator is used to find an appropriate initial model as starting point for the classical Baum-Welch algorithm. Experimental results were found to be promising. Fractal Theory has been used for computer graphics, image compression and different fields of pattern recognition. In this paper we simplified a general purpose two-dimensional fractal coder used for image compression. Since in the case of on-line signature recognition, we loose gray levels, contrast and luminosity information, we do NOT employ these parameters in the fractal coder. Instead, we focused on geometrical relationship between the range block and its best domain block. Then, some features were extracted directly by the proposed one dimensional fractal coder. We will show their usefulness in the application of Persian on-line signature recognition. Feature selection (FS) is a most important step which can affect the performance of pattern recognition system. This paper presents a novel feature selection method that is based on ant colony optimization (ACO). ACO algorithm is inspired of ant's social behavior in their search for the shortest paths to food sources. In the proposed algorithm, classifier performance and the length of selected feature vector are adopted as heuristic information for ACO. So, we can select the optimal feature subset without the priori knowledge of features. Simulation results on face recognition system and ORL database show the superiority of the proposed algorithm In the present investigation attempts were made for the first time to use the fundamental color stimulus as the input for a fixed optimized neural network match prediction system. Four sets of data having different origins (i.e. different substrate, different colorant sets and different dyeing procedures) were used to train and test the performance of the network. The results showed that the use of fundamental color stimulus greatly reduces the errors as depicted by the MSE and \\u2206 Cave data and improves the performance of the neural network prediction system. Additionally the use of fundamental color stimulus makes provisions for predicting the concentrations of one data set whilst being trained by a second data set of completely different origin. In this article, parameters affecting on formation and elimination of hydrocarbons using artificial neural network are considered and a model to predict THC (total hydrocarbon) amount in air using neural network is earned. Also using neural network model and surveying effect of each parameters on THC amount, optimization of offered model is done. The database to get mentioned model consists 1500 samples of current information in two stations of quality control of Tehran city air. Results of using artificial neural network in prediction of THC amount indicate that neural network model is suitable for predicting THC amount. Also to compare improvement of implementing THC prediction model using artificial neural network, a multivariable regression model is used to predict THC amount and its results indicate that MSE is very low when we use artificial neural network. In this paper, we propose a reversible data hiding method with low time complexity and high embedding capacity for gray-scale images. This method presents a block-based lossless data hiding schema to utilize the similarity between neighborhood pixels in the block to improve the marked-image quality. The experimental results show that our method has increased the hiding capacity with keeping acceptable marked-image quality. Given large number of words to be recognized, lexicon reduction strategy for eliminating unlikely candidates before recognition can be a reasonable and powerful approach for increasing the recognition speed. In this paper, we describe a holistic approach for large Arabic handwritten lexicon reduction which is based on inherent properties of Arabic writing. The principal of this technique involves extraction of dots, diacritics and subwords from the cursive Arabic word image to describe its shape. In the first stage of lexicon reduction, the number of subwords in the input word is estimated. Then, in the second stage, the word descriptor, based on the dots and diacritics information, is used while taking into account only the candidates selected in the first stage. Experimental results on IFN/ENIT database, consisting of 26,459 cursive Arabic word images, show a lexicon reduction of 92.5% with accuracy of 74%. In the recent years, face recognition has obtained much attention. Using combined 2D and 3D face recognition is an alternative method to deal with face recognition. A novel multimodal face recognition algorithm based on Gabor wavelet information is presented in this paper. The Principal Component Analysis (PCA) and the Linear Discriminant analysis (LDA) have been used for size reduction. The system has combined 2D and 3D systems in the decision level which presents higher performance in contrast with methods which use only 2D and 3D systems, separately. The proposed algorithm is examined with FRAV3D database that has faces with pose variation and 95% performance that is achieved in rank-one for fusion experiment. The Circle Hough Transform (CHT) has become a common method for circle detection in numerous image processing applications. Because of its drawbacks, various modifications to the basic CHT method have been suggested. This paper presents an algorithm to find circles which are totally brighter or darker than their backgrounds. The method is size-invariant, and such circular shapes can be found very fast and accurately. Though Fast Circle Detection (FCD) method loses the generality of the CHT, we show that there are many applications that can use this method after a simple preprocessing and gain a considerable improvement in performance against the CHT or its modified versions. This method has been evaluated in some famous industrial and medical fields, and the results show a significant improvement of finding circular shape objects. Fractal theory has been used for computer graphics, image compression and different fields of pattern recognition. In this paper, a fractal based method for recognition of both on-line and off-line Farsi/ Arabic handwritten digits is proposed. Our main goal is to verify whether fractal theory is able to capture discriminatory information from digits for pattern recognition task. Digit classification problem (on-line and off-line) deals with patterns which do not have complex structure. So, a general purpose fractal coder, introduced for image compression, is simplified to be utilized for this application. In order to do that, during the coding process, contrast and luminosity information of each point in the input pattern are ignored. Therefore, this approach can deal with on-line data and binary images of handwritten Farsi digits. In fact, our system represents the shape of the input pattern by searching for a set of geometrical relationship between parts of it. Some fractal-based features are directly extracted by the fractal coder. We show that the resulting features have invariant properties which can be used for object recognition. Standard databases play very important roles in pattern recognition tasks. To compare the performances of different algorithms, they must be tested on a same dataset. In Farsi, there is not a database of handwritten texts to evaluate different algorithms. In this paper, an unconstraint Farsi handwritten text database is introduced. 250 participants in different ages and education levels filled 1000 forms. Duo to the characteristics of the database, it can be used in many OCR applications. A large number of writers, big lexicon size, out-of-straight textlines and various categories of texts are some of the characteristics. A new method has been presented for real time seismic signal processing. A fuzzy model is extracted for the background noise dynamics, using the ARMA model coefficients. A fuzzy rule base has been generated, and a fuzzy inference engine has been used to detect the variations in the nature of the background noise. The conventional envelope detection algorithm for on-set estimation, has been affected by the fuzzy inference engine to make it more flexible and robust, in the presence of a large amount of background noise. The fuzzy inference engine also serves as a proper indicator for sudden variations in the nature of the background noise. If the detected variation is due to the first arrival phase of a seismic event, then a higher order ARMA model is derived and its coefficients are used as the inputs to a trained neural network, for seismic classification. The experimental results are promising and there are some remarkable advantages over the previous methods. A holistic system for the recognition of handwritten Farsi/Arabic words using right-left discrete hidden Markov models (HMM) and Kohonen self-organizing vector quantization is presented. The histogram of chain-code directions of the image strips, scanned from right to left by a sliding window, is used as feature vectors. The neighborhood information preserved in the self-organizing feature map (SOFM), was used for smoothing the observation probability distributions of trained HMMs. Experiments carried out on test samples show promising performance results. The study of handwritten words is tied to the development of recognition methods to be used in real-world applications involving handwritten words, such as handwritten texts, bank checks, and postal envelopes, among others. In this paper an approach for Farsi bank checks was proposed in which the legal amounts are used to aid the recognition results of courtesy amounts. The legal amounts which are set of some of 40 specified words are divided into their sub- words. Some if-then rules are extracted from validation set to validate the recognized digits. These rules are used to confirm, correct or reject the recognized digit. The experimental results reveal a recognition rate of 85.33% without the legal amounts, and a reliability rate 99.31% with a rejection rate of 3.67%. This paper presents a new method for extracting the Foetal Electrocardiogram (FECG) signal form the two ECG signals recorded at the thoracic and abdominal areas of the mother's skin. The thoracic ECG is assumed to be completely maternal ECG (MECG) while the abdominal ECG is assumed to be a combination of mother's and foetus's ECG signals and random noise. The maternal component of the in the abdominal ECG is a nonlinearly transformed version of the MECG. The method uses Adaptive Nero-Fuzzy Inference System (ANFIS) structure for identifying the nonlinear transformation. We have used Particle Swarm Optimization (PSO) as a new tool for training the ANFIS structure. By identifying the nonlinear transformation, we have extracted FECG by subtracting the aligned version of the MECG signal from the abdominal ECG (AECG) signal. We validate our new method on both real and synthetic ECG signals. For synthetic signals, we have used a subjective criterion in addition to objective criteria. The results of extracting the FECG are promising comparing with conventional methods. Vital problems in transportation such as mobility and safety of transportation, especially in highways and road ways, are considered as very important nowadays. Road traffic monitoring aims at acquisition and analysis of traffic signs, such as presence and numbers of vehicles, and automatic driver warning systems developed mainly for localization and safety purposes. In the past some methods have been presented for road and vehicle recognition, which either the accuracy of the results was not acceptable, or tolerable results were achieved only in specific conditions. This article presents a new approach for recognizing the vehicle and the road in satellite high-resolution images in non-urban areas. One of the results of this research was to control the traffic jam in roads and to recognize the traffic density quickly and accurately\\u2025 For recognition, they used feature extraction, image processing and machine learning like Hough transform, Gradient, and thresholding operation for road detection and feature extraction and SVM for Vehicles' detection. The average of results is about 85 percent and it shows that the used procedure has the suitable efficiency. We have introduced a new method, taking advantage of an image moment transformation combined with a fuzzy logic approach. For this purpose first we tried to model the noise embedded in signature patterns inherently and separate that from environmental effects. On the basis of the first step results, we have extracted the most optimum mapping to a unit circle using LMS criteria. Then we derived some orientation invariant moments introduced in former reports and studied their own statistical properties in our special input space, using a new defined criterion. Afterwards we defined an error matrix for signature patterns and studied its behavior and concluded that a fuzzy classifier seems to be the best choice for our application. Then we defined a fuzzy complex space and also a fuzzy complex similarity measure in this space, and constructed a training algorithm to learn the fuzzy classifier. Thus any input pattern could be compared to the learned prototypes through a pre-defined fuzzy similarity measure and attributed to one of the learned classes. The fuzzy classifier is applied to each of the above derived moments which constituted an individual feature space separately and miss-classifications were detected as a measure of the error magnitude. Finally a comparison is made between the above considered image transformations and we have pointed out some of the advantages of this method. A system is developed for recognition of handwritten Farsi/Arabic characters and numerals. The discrete wavelet transform is utilized to produce wavelet coefficients, which are used for classification. We used Haar wavelet for feature extraction in this system. The extracted features are used as training inputs to a feed forward neural network using the backpropagation learning rule. The learning and test patterns were gathered from various people with different educational backgrounds and different ages. We categorize 32 characters in Farsi language to 8 different classes in which characters of each class are very similar to each others. There are ten digits in Farsi/Arabic languages, but two of them are not used in postal codes in Iran, so we have 8 different extra classes for digits. This system yields the classification rates of 92.33% and 91.81% for these 8 classes of handwritten Farsi characters and numerals respectively. We used this system for recognizing the handwritten postal addresses which contain the names of cities and their postal codes. Our database contains 579 postal addresses in Iran. The system yields a recognition rate of 97.24% for these postal addresses. In this paper, we propose a method for online Farsi handwritten words recognition. At first, words are broken to their sub-words. Each sub-word is made of some strokes. The sign of the sub-word is found from the positions and shapes of its sub-strokes. After that, we classify sub-words according to their signs. Some online features are extracted from the main-stroke after the preprocessing stage. Preprocessing contains operations such as dehooking, smoothing, normalization and boundary size equalization. A combination of 3 cascaded RBF neural networks are learned and used in hierarchical recognition system. The first RBF net divides sub-words into classes, while the second one subdivides each class into sub-classes. The third RBF network recognizes sub-words in each sub-class. In this paper, we use a 1000-sub-word database of the most frequently used Farsi words. The performance of the system in the first and the second RBF classifiers is 99.7% and 98.9% respectively. The rate of correct performance of the third RBF net is 82.46% making the total recognition rate of the system on the database 81.3%. Reduction of search region in stereo correspondence can increase performances of the matching process, in the context of execution time and accuracy. For an edge-based stereo matching, we establish relationships between the search space and the parameters like relative displacement of edges, disparity under consideration, image resolution, CCD (Charge-Coupled Device) dimension and focal length of the stereo system. Then, we propose a novel matching strategy for the edge-based stereo. Afterward, we develop a fast edge-based stereo algorithm with combination of the obtained matching strategy and a multiresolution technique using Haar wavelet. Considering the conventional multiresolution technique using Haar wavelet, the execution times of our proposed method are decreased between 26% to 47% in the feature matching stage. Moreover, the execution time of the overall algorithms (including feature extraction and feature matching) is decreased between 15% to 20%. Theoretical investigation and experimental results show that our algorithm has a very good performance; therefore this new algorithm is very suitable for fast edge-based stereo applications like stereo robot vision. Introduces a method for the recognition of human faces in 2-dimensional digital images using a new hybrid learning algorithm (HLA) for radial basis function (RBF) neural network as classifier and pseudo Zernike moment invariant (PZMI) as face feature. Also we evaluate the effect of orders of the PZMI on recognition rate, in the proposed technique. Simulation has been carried out on the face database of Olivetti Research Laboratory (ORL) and recognition rate of 98.7% is obtained using this proposed technique. We propose a new method for isolated handwritten Farsi/Arabic characters and numerals recognition using fractal codes. Fractal codes represent affine transformations which, when iteratively applied to the range-domain pairs in an arbitrary initial image, give results close to the given image. Each fractal code consists of six parameters, such as corresponding domain coordinates for each range block, brightness offset and an affine transformation, which are used as inputs for a multilayer perceptron neural network for learning and identifying an input. This method is robust to scale and frame size changes. Farsi's 32 characters are categorized to 8 different classes in which the characters are very similar to each other. There are ten digits in the Farsi/Arabic languages, but since two of them are not used in postal codes in Iran, only 8 more classes are needed for digits. According to experimental results, classification rates of 91.37% and 87.26% were obtained for digits and characters respectively on the test sets gathered from various people with different educational background and different ages. This paper compares performances of the Zernike moment invariant (ZMI) and wavelet transform features in the application of face recognition. In this study, after preprocessing and face localization of an image, we optimize the exact location of oval shape of face in the image with genetic algorithm which improves the recognition rate. High order ZMI and discrete wavelet transform (Haar wavelet) is utilized to produce feature vectors. In the wavelet transform step, we used Mallat pyramid algorithm for finding approximation of the image in lower resolution and decomposed each image in 4 resolution level. Also SVMs classifier which is a new learning machine and has very good generalization ability has been used as a classifier with two different kernel functions. Simulation results on ORL database show that approximately the same results are obtained for both ZMI and wavelet features. But feature extraction using wavelet transform has a rate of 0.078 image/sec that is about 11 times faster than the rate of ZMI feature This paper presents a novel technique to increase the quality of medical images based on histogram equalization. In the proposed method first we have applied a noise reduction method and then we apply some suitable preprocessing on histogram of the medical images and after that histogram equalization has been applied on the new histogram. Our proposed method in despite of its simplicity has better results in compare to other usual methods based on histogram equalization. The quality of resulted images after applying our proposed methods has been tested on a database (medical images) with a confirmed criterion by viewer. Also we have considered a mathematical criterion for comparing our proposed algorithm with other available methods for contrast enhancement. Results show the better efficiency of the proposed method. Position based routing protocols are kinds of routing protocols, which use nodes location information, instead of links information to routing. In position based routing protocols, it supposed that the packet source node has position information of itself and it's neighbors and the packet destination node. Greedy is a very important position based routing protocol. In one of it's kinds, named MFR (Most Forward Within Radius), the source node or packet forwarder node, sends packet to one of it's neighbors with most forward progress towards destination node (closest neighbor to destination). Using distance deciding metric in Greedy to forward packet to a neighbor node, is not suitable for all conditions. If closest neighbor to destination node, has high speed, in compare with the source node or intermediate packet forwarder node speed or has very low remained battery power, then the packet loss probability will increase. The Proposed strategy uses combination of metrics distance-velocity similarity-power, to deciding about to which neighbor the packet should be given. Simulation results show that the proposed strategy has lower lost packets average than Greedy, so it is more reliable. In this paper, a novel feature extraction method is proposed to handle facial makeup in face recognition. To develop a face recognition method robust to facial makeup, features are extracted from face depth in which facial makeup is not effective. Then, face depth features are added to face texture features to perform feature extraction. Accordingly, a 3D face is reconstructed from only a single 2D frontal image with/without facial expressions. Then, the texture and depth of the face are extracted from the reconstructed model. Afterwards, the Dual-Tree Complex Wavelet Transform (DT-CWT) is applied to both texture and reconstructed depth of the face to extract the feature vectors from both texture and reconstructed depth images. Finally, by combining 2D and 3D feature vectors, the final feature vectors are generated and classified by the Support Vector Machine (SVM). Promising results were achieved for makeup-invariant face recognition on the available image database based on the present method compared to several state-of-the-art methods. In this paper, a distributed power-aware medium access control (MAC) algorithm for ad hoc wireless networks is presented. The algorithm is developed based on proposing a power-aware MAC game which is analyzed in the game theory framework. The aim is to adjust each active link persistence probability and power by maximizing a defined local link payoff function. The payoff function is such that its selfish maximization at the links leads to an efficient use of medium resources and has two terms. The first term is the link utility while the second one reflects the cost of using the medium resources. The existence and uniqueness of the game Nash equilibrium are investigated analytically. Also, it is shown that this equilibrium is Pareto optimal indicating its efficiency. Simulation results are provided to evaluate the algorithm and are compared to a scenario in which only powers are tuned. This results emphasize that the link persistence and power should be adjusted simultaneously according to the link location in ad hoc networks. Traditionally, finding the corresponding points has considered to be the most difficult part of stereo matching algorithms. Usually, the correspondence for a feature point in the first image is obtained by searching in a predefined region of the second image, based on the epipolar line and the maximum disparity. Reduction of the search region can increase the performance of the matching process, in the context of execution time and accuracy. We proposed a new matching strategy to reduce the search space for the edge based stereo correspondence algorithms. Considering the maximum of the disparity gradient in the real scene, we formulated the relation between the maximum search space in the second images with respect to the relative displacement of the continuous edges (as the feature points) in the successive scan lines of the first images. Then we developed some very fast stereo matching algorithms, based on the non-horizontal edges as feature points, and the normalized cross correlation criteria (NCC) with different sizes of the matching block (as the similarity measures). We applied these new algorithms on the Renault stereo image and compared the result with those of a traditional matching algorithm (20 pixels search regions and NCC with size of 15/spl times/15). The speed up of these new algorithms is between 2.8 to 13.8 and the percentage of errors is between 0.5 to 5.4. In this paper, we propose a new multi-resolution fusion algorithm for spatially registered multi-sensor images fusion. First, we use a modified watershed algorithm to produce a region map of source images. This segmentation is then used to generate region-based decision map, which is obtained based on local texture features in the dual-tree discrete wavelet transform domain. The region-based decision map is very accurate for selecting high frequency coefficients between the source images compare to the pixel-based decision map, but very important details of source images is missed in the final fused image. In this study, we propose an algorithm based on fuzzy logic, which uses both pixel and region-based decision map based on a dissimilarity measure of source images for fusion rule. Also a region-based activity measure of each source image is used for low frequency fusion rule instead of simple averaging. Our new method provides improved subjective and objectives results compared to the previously pixel and region-based fusion methods. This paper presents a new pricing function for noncooperative power control game in a single cell CDMA data network. Considering a utility function for each terminal, the purpose of power control in wireless data networks is to maximize network utility. In the proposed game, the pricing function is a linear function of the terminal's signal to interference plus noise ratio (SINR). We first prove that the new game is a supermodular game and then we show the strategy space of the new game is such that it is possible to reach better equilibrium point compared to pricing function based on terminal's power. Simulation results show that the game with the proposed pricing function can improve the utility and power consumption of the terminals at equilibrium. In this paper voice activity detection (VAD) is formulated as a two-class classification problem using sup- port vector machines (SVM). The proposed method com- bines a noise robust speech processing feature extraction process together with SVM models trained in different back- ground noises for speech/non-speech classification. A multi- class SVM is also used to classify background noises in order to select SVM model for VAD. The proposed VAD is tested with TIMIT data artificially distorted by different additive noise types and is compared with state-of-the-art VADs. Experimental results show that the proposed VAD can extract speech activity under poor SNR conditions, and it is also insensitive to variable levels of noise. In this paper, we propose a new approach for detecting and recognizing numerical strings in Farsi/Arabic handwritten or machine-printed document images. We assign a label to each of the connected components as they belong to a numerical string or not. First, in order to differentiate between digit and non-digit connected components, some simple features are extracted from all connected components in each text line. Then, these features are classified with a fuzzy rule-based classifier to extract some candidate strings. After using a digit recognizer, syntax of the numerical strings are validated by a syntactic verifier. Experimental results show an acceptable detection rate with low false positive rate. Ear is a new class of relatively stable biometric that is invariant from childhood to early old age. In most cases techniques already working in other biometric fields, such as PCA are applied to ear. Eigen-ears provide high recognition rate only in closely controlled conditions. Indeed, even a slight amount of rotation can cause a significant drop in system performance and in unattended systems rotations occur very frequently. HMAX is a feature extraction method and this method is motivated by a quantitative model of visual cortex. Also, SVMs are classifiers which have demonstrated high generalization capabilities in many different tasks, including the object recognition problem. In this paper we combine these two techniques for the robust Ear verification problem. The USTB database is exploited to test our approach. Experimental results using the combination HMAX model and support vector machine (SVM) classifier (with kernel=1), obtains higher recognition rate than those obtained with HMAX model and k-nearest neighbors classifier in ear verification. In addition to, demonstrated that this method is rotate-and scale-invariant, and also, in experiment, it was found that, using of Gaussian filter in HMAX model in compared to using of Gabor filter, increases performance of ear recognition. Imperialist Competitive Algorithm (ICA) is a novel optimization algorithm that inspired by socio-political process of imperialistic competition. ICA shown its excellent capability in diverse optimization tasks. In this paper, a new method for training an Artificial Neural Network using Chaotic Imperialist Competitive Algorithm is proposed. In Chaotic Imperialist Competitive Algorithm (CICA) the chaos theory has been used to adjust the movement angle of colonies towards the imperialists. Using Chaotic Imperialist Competitive Algorithm (CICA), the weights of the Neural Network in its training phase are updated. In this paper, a multi layer Perceptron Neural Network used for prediction of the maximum worth of the stocks change in Tehran's Bourse Market. We trained this Neural Network with CICA, ICA, PSO and GA algorithms and compared the experimental results obtained from these four methods. The consideration of results showed that the training and test error of the network trained by CICA algorithm has been reduced in comparison to the other three methods. However, the run time of the proposed algorithm in training the neural network is less than PSO and GA algorithm it is a little more than ICA algorithm. The bandwidth-delay-constrained least-cost multicast routing is a challenging problem in high-speed multimedia networks. Computing such a constrained Steiner tree is an NP-complete problem. In this paper, we propose several novel solutions to this problem based on genetic algorithms (GA). The proposed solutions consist of six different schemes for genotype representation, and also several new heuristic algorithms for mutation, crossover, and creation of random individuals. We evaluate the performance and efficiency of the proposed algorithms in comparison with other existing heuristic and GA-based algorithms using simulation results. The most efficient combination of various proposed alternative algorithms is selected as our final solution by the simulation results. This proposed GA-based algorithm has overcome the existing algorithms considering average tree cost and running time. An unconstrained Farsi handwritten word recognition system based on fuzzy vector quantization (FVQ) and hidden Markov model (HMM) for reading city names in postal addresses is presented. Preprocessing techniques including binarization, noise removal, slope correction and baseline estimation are described. Each word image is represented by its contour information. The histogram of chain code slopes of the image strips (frames), scanned from right to left by a sliding window, is used as feature vectors. Fuzzy c-means (FCM) clustering is used for generating a fuzzy codebook. A separate HMM is trained by modified Baum\\u2010Welch algorithm for each city name. A test image is recognized by finding the best match (likelihood) between the image and all of the HMM word models using forward algorithm. Experimental results show the advantages of using FVQ/HMM recognizer engine instead of conventional discrete HMMs. \\u201d 2001 Elsevier Science B.V. All rights reserved. This paper presents a multimodal biometric identification system based on new features extraction of palm and ear. We describe a new biometric approach to personal identification using robust pattern recognition Each element of this set is a complex feature obtained by combining position- and scale-tolerant edge detectors over neighboring positions and multiple orientations. Our system?s architecture is motivated by a quantitative model of visual cortex, with fusion applied at the matching-score level. The identification process can be divided into the following phases: capturing the image; pre-processing; extracting and normalizing the palm and ear; feature extraction; matching and fusion; and finally, a decision based on the k-NN and SVM classifiers. The system was tested on a database of 600 people (300 palm and 300 ear images). The experimental results showed the effectiveness of the system in terms of the recognition rate (100 percent). In this paper an algorithm is developed for recognition of printed Farsi characters with various fonts, irrespective of size, rotation and stork. The system uses pseudo-Zernike moments as input features and the classifier consists of a complex of neural networks (NN) and fuzzy neural networks (FNN). The advantage of using FNN is it's ability to classify similar patterns. The performance of the system is evaluated on a database consisting of more than 3700 character samples. The achieved accuracy is 99.85%. In this paper we present a novel LSB matching steganalysis method based on feature vectors derived from co-occurrence matrix in spatial domain, which is sensitive to data embedding process. This matrix is derived from an image that some of its most significant bit planes are removed. By this preprocessing in addition to decrease the size of feature vector also preserve effects of embedding. We investigate how LSB matching embedding effect more least significant bits and obtain better case for steganalysis. We use SVM for classification and our experimental results have demonstrated that the proposed scheme can increase detection rate of stegnalysis technique in attacking the LSB marching algorithm. Iris recognition is one of the most reliable biometric technologies. In this paper, we presented a novel method for iris recognition, using a complex mapping procedure and best-fitting line for the iris segmentation and ID Gabor filter with 2DPCA for the recognition approach. We used an intensity threshold method with Canny edge detector to extract the rough region of the pupil. For the outer boundary a median filter with prewitt compass edge detector were used to localize the rough region of the outer boundary. By selecting the bottom point of the pupil (which is not usually occluded by the eyelids and eyelashes) as a reference point, two sets of intersecting points between the horizontal lines and pupil's inner and outer boundaries were created. Each point set was map into a new complex domain using the complex inversion map function and the best-fitting line was found on the range. Exact inner and outer boundaries of the iris were found by remapping the best-fitting lines to original domain. In the recognition procedure, we used the real term of ID Gabor filter. In order to reduce the dimensionality of the extracted features, the new introduced 2DPCA method was used. We tested our proposed algorithm by implementing a ground truth method. Experimental results show that the proposed method has an encouraging performance in both segmentation and recognition approaches. This paper proposed a method for facial expression recognition.In proposed method, facial depth has been added to facial texture for feature extraction.We demonstrated that adding the facial depth to feature extraction is effective.The 3DH-LLBP is proposed for feature extraction from facial depth images.2D facial expression recognition is performed by combination of 2D and 3D features through feature fusion. In this paper, a novel feature extraction method is proposed for facial expression recognition by extracting the feature from facial depth and 3D mesh alongside texture. Accordingly, the 3D Facial Expression Generic Elastic Model (3D FE-GEM) method is used to reconstruct an expression-invariant 3D model from the human face. Then, the texture, depth and mesh are extracted from the reconstructed face model. Afterwards, the Local Binary Pattern (LBP), proposed 3D High-Low Local Binary Pattern (3DH-LLBP) and Local Normal Binary Patterns (LNBPs) are applied to texture, depth and mesh of the face, respectively, to extract the feature from 2D images. Finally, the final feature vectors are generated through feature fusion and are classified by the Support Vector Machine (SVM). Convincing results are acquired for facial expression recognition on the CK+, CK, JAFFE and Bosphorus image databases compared to several state-of-the-art methods. In this paper, we describe four important indirect methods which be used to extract the Fetal Electrocardiogram (FECG) signal from an ECG recorded on the mother\\u2019s abdomen. These methods include the following ones: Singular Value Decomposition (SVD) method, Independent Component Analysis (ICA) method, Wavelet Based methods and Adaptive Filtering method. The mentioned methods use signal processing techniques for extracting FECG from Abdominal Electrocardiogram (AECG). We have explained advantages and disadvantages of each method. The methods have also applied on both synthetic and real ECG signals. Efficiencies of the methods compared together based on three important criterions and results are stated and best method based on three criterions is selected. Focused and incidental scene text images are processed in a separate manner.Low rank matrix recovery is exploited to process the incidental scene text images.A text confidence map was designed via fuzzy inference system.The proposed algorithm handles both Latin and Farsi/Arabic scripts.Farsi/Arabic scene texts at arbitrary orientations are localized for the first time. In this paper a framework is proposed to localize both Farsi/Arabic and Latin scene texts with different sizes, fonts and orientations. First, candidate text regions are extracted via an MSER detector enhanced by weighted median filtering to adopt the low resolution texts. At the same time based on fuzzy inference system (FIS), the input image is classified into images with a focused text content and incidental scene text images which the image does not focus on the text content. For the focused scene text images the non-text candidates are filtered via an FIS. On the other hand, for the incidental scene text images apart from the FIS, an extra filtering algorithm based on low rank matrix recovery is proposed. Finally, a new approach based on the clustering, minimum area rectangle and radon transform techniques is proposed to create the single arbitrarily oriented text lines from the remaining text regions. To evaluate the proposed algorithm, we created a collection of natural images containing both Farsi/Arabic and Latin texts. Compared with the state-of-the-art methods, the proposed method achieves the best performance on our and Epshtein datasets and competitive performances on the ICDAR dataset. Routers use lookup tables to forward packets. They also classify packets to determine which flow they belong to and what quality of service (QoS) they should receive. Increasing the rate of communication links is in contrast to the practical processing power of routers and switches. We propose some neural network algorithms to solve the IP lookup problem. One of these algorithms, back propagation, gives promising results; however, it has problems in training time. Another algorithm, a 12 layer neural network, represents acceptable results on error rate and training time. Parallel processing of neural networks provides huge processing power to do IP lookup. The algorithm can be implemented in hardware on a single chip. Our method can perform an IP lookup in 4.5 nanoseconds, which implies the support for a 60 Gbps link rate. Pipelining and parallel processing can be used to increase the link rate up to 400 Gbps and also decrease the learning time. Keywords\\u2013 IP lookup, packet classification, neural network, ART1, back propagation This paper presents a new comprehensive database for isolated offline handwritten Farsi/Arabic numbers and characters for use in optical character recognition research. The database is freely available for academic use. So far no such a freely database in Farsi language is available. Grayscale images of 52,380 characters and 17,740 numerals are included. Each image was scanned from Iranian school entrance exam forms during the years 2004-2006 at 300 dpi. The only restriction imposed on the writers is to write each character within a rectangular box. The number of samples in each class of the database is non-uniform corresponding to their real life distributions. Also, for comparison purposes, each dataset has been properly divided into respective training and test sets. Abstract Hyper-synchronous neural oscillations are the character of several neurological diseases such as epilepsy. On the other hand, glial cells and particularly astrocytes can influence neural synchronization. Therefore, based on the recent researches, a new bio-inspired stimulator is proposed which basically is a dynamical model of the astrocyte biophysical model. The performance of the new stimulator is investigated on a large-scale, cortical network. Both excitatory and inhibitory synapses are also considered in the simulated spiking neural network. The simulation results show that the new stimulator has a good performance and is able to reduce recurrent abnormal excitability which in turn avoids the hyper-synchronous neural firing in the spiking neural network. In this way, the proposed stimulator has a demand controlled characteristic and is a good candidate for deep brain stimulation (DBS) technique to successfully suppress the neural hyper-synchronization. This article aims to deal with the purposeful election of the cluster head(Leader) in the Wireless Sensor Networks (WSNs). The basic protocol to choose the cluster head(CH) in such networks is LEACH (low-energy adaptive clustering hierarchy) which somehow will realize the election of CH in the network and needs no program for full coverage of the CH. This article by introducing a new protocol called CHEFC (Cluster Head Election Full Coverage) is able to tackle with the problem of CH vacancy in the different parts of the network. The function of this protocol is as follows: each sensor should usually have a CH in it vicinity, and unlike the previous protocol, there is no need for each sensor spends more energy to be connected with its own CH located in a farther distance. This protocol which acts on a distributed basis will lengthen the lifetime of the wireless sensor network as well as fully covering the CH in the network. In addition, after simulation and comparison based on LEACH protocol, it was clearly observed that CHEFC protocol has a higher conductivity which means the higher number of the CHs in each round of the network activity as compared with previous protocol One of the most research challenges in image processing is image enhancement and reducing impulse noise from digital images. There are various methods for impulse noise reduction such as median based filters or nonlinear filters, but these methods more or less cause images to blur and to remove important details from images, as in high noise ratio in that noise reduction will destroy vital information such as edges and high amount of noise causes the image information be destroyed. Some ways are proposed to impulse noise reduction using soft computing that has a good performance. This paper presents an efficient method in two passes for reduction of impulse noise. At the first pass impulse noise detection using ANFIS, and at the second pass the impulse noise estimation, that corrupted noise pixel replaced with new value based on ANN. Our method is experimented on some popular grayscale test images and is compared to other methods using subjective and objective measures. Results show that our proposed method is efficient in impulse noise reduction and works better than the other compared methods. In this paper, we propose an efficient method to reconstruct the 3D models of a human face from a single 2D face image robustness under a variety facial expressions using the Deformable Generic Elastic Model (D-GEM). We extended the Generic Elastic Model (GEM) approach and combined it with statistical information of the human face and deformed generic depth models by computing the distance around face lips. Particularly, we demonstrate that D-GEM can approximate the 3D shape of the input face image more accurately, achieving a better and higher quality of 3D face modeling and reconstruction robustness under a variety of facial expressions compared to the original GEM and Gender and Ethnicity-GEM (GE-GEM) approach. It has been tested on an available 2D face database and new synthesized facial expression and large pose changes together from gallery images. We acquire promising results for handling pose and expression changes based on the proposed method compared to the GEM and GE-GEM. In this paper a new on-demand routing protocol for mobile ad hoc networks is presented based on link/route stability estimation. AODV decreases maintenance overhead in ad hoc networks, but some path breaks can significantly cause overhead. Reverse AODV (RAODV) routing algorithm is one of the AODV versions which reduces overhead of routing. Here, we applied link/route stability in RAODV for decrease overhead of discovery and maintenance of routing, a new protocol then was created. Our protocol also increased the packet delivery ratio in mobile ad hoc networks (MANET's). When an active route fails, the source node with the awareness of stabilities of routes, can select the best route in set of available routes. We proposed a stability estimation method and applied that in an optimized version of ad hoc on demand distance vector (AODV) routing algorithm by doing some modification at RAODV algorithm. In our algorithm namely Modified Reverse Ad Hoc On-demand Vector (MRAODV), the route request packet didn't change and it is like as AODV, but rout reply packet must be changed for route stability estimation purpose. Computer simulation using ns-2 simulator was performed to compare with other methods and effectiveness of the proposed method was quantitatively validated. In this paper, we propose an efficient method to reconstructing the 3D models of a human face from a single 2D face image robustness under a variety facial expressions using the Deformable Generic Elastic Model (D-GEM). We extended the Generic Elastic Model (GEM) approach and combined it with statistical information of the human face and deformed generic depth models by computing the distance around face lips. Particularly, we demonstrate that D-GEM can approximate the 3D shape of the input face image more accurately, achieving a better and higher quality of 3D face modeling and reconstruction robustness under a variety of facial expressions compared to the original GEM and Gender and Ethnicity-GEM (GE-GEM) approach. It has been tested on an available 3D face database, demonstrating its accuracy and robustness compared to the GEM and GE-GEM approach under a variety of imaging conditions, including facial expressions, gender and ethnicity. Biometric technology is an efficient personal authentication and identification technique. As one of the main-stream branches, dorsal hand vein recognition has been recently attracted the attention of researchers. It is more preferable than the other types of biometrics becuse it\\u2019s impossible to steal or counterfeit the patterns and the pattern of the vessels of back of the hand is fixed and unique with repeatable biometric features. Also, the recent researches have been obtained no certain recognition rate yet becuse of the noises in the imaging patterns, and impossibility of Dimension reducing because of the non-complexity of the models, and proof of correctness of identification is required. Therefore, in this paper, first, the images of blood vessels on back of the hands of people is analysed, and after pre-processing of images and feature extraction (in the intersection between the vessels) we began to identify people using firefly clustering algorithms. This identification is done based on the distance patterns between crossing vessels and their matching place. The identification will be done based on the classification of each part of NCUT data set and it consisting of 2040 dorsal hand vein images. High speed in patterns recognition and less computation are the advantages of this method. The recognition rate of this method is more accurate and the error is less than one percent. At the end the correctness percentage of this method (CLU-D-F-A) for identification is compared with other various algorithms, and the superiority of the proposed method is proved. DOI: http://dx.doi.org/10.11591/ijece.v3i1.1760\",\n",
            "  \"2043899690\": \"There are two distinct logics that could provide the foundation for the development of service(s) science. One, goods-dominant logic, is based on the idea that services are special forms of intangible goods and therefore points toward a model concerned with the production of intangible units of output. The other is based on a concept of service as a process of the cocreation of reciprocal value. We propose that a service science built on the latter, servicedominant logic, is more likely to result in a science that makes significant strides in the knowledge base than is a services science based on a goods logic. We then point toward additional foundational shifts that are indicated by the adoption of this service logic. The rapid rise of powerful social customers has drastically changed the e-business landscape. Social CRM (SCRM) emerged in late 2009 as an e-business strategy for companies to enable customer relationship management (with social customers) utilizing social technology. Despite the many applications that are labeled as SCRM, there is a dearth of guidelines for SCRM design and development. Many companies are trapped using social media as just another communication channel, and have naively applied traditional Electronic CRM (ECRM) practices on social platforms based on a model of exchange that centers on goods (e.g., goods-dominant logic or G-D logic), with value created by the firm and relationship implying multiple transactions of value-laden output. G-D logic might have served companies in the pre-Web 2.0 environment in which the interaction with customers could be contained in one-to-one, closed, well-defined channels. However, in a collaborative open, social environment in which interactions cannot be contained and are often unpredictable, this firm-centric, transaction-oriented approach is at odds with how social customers behave and expect, and therefore becomes inadequate in fostering true relationships that cultivate devoted advocates and brand co-creators. In this chapter, we offer an alternative logic called service-dominant (S-D) logic for SCRM design to meet new challenges. S-D logic is based on the reciprocal application of applied competences (service), which sees relationship in terms of co-creation of value. We argue that the S-D logic perspective for SCRM is appropriate, if not essential. We offer S-D logic-informed strategies for SCRM and next-generation CRM system design. Foreword Preface Part I. Premises: 1. The service-dominant mindset 2. Roots and heritage 3. Axioms and foundational premises 4. Service as a guiding framework Part II. Perspectives: 5. It's all actor-to-actor (A2A) 6. The nature, scope, and integration of resources 7. Collaboration 8. Service ecosystems Part III. Possibilities: 9. Strategic thinking 10. Conclusions and considerations Appendix Index. EXECUTIVE SUMMARY Satisfaction has traditionally been conceptualized as a global affective response toward offering usage/consumption (Westbrook 1987). To date, its study has yet to uncover the complexities related to the antecedents leading to global satisfaction, which are customers. reactions to different foci of offering usage/consumption. However, a number of research streams in a variety of disciplines have investigated these antecedents and suggested that they are usually independent and multidimensional. These investigations have implications for managers that are more complex and often contrary to those associated with the traditional conceptualization of (dis)satisfaction. They also have implications for the underlying models and measurement techniques that may be appropriate for understanding (dis)satisfaction. These streams of research, their prospective implications, and supportive findings from related research are reviewed, and their associated issues and directions for additional research are discussed. Scholars from disciplines as diverse as human resources, engineering, and marketing have identified different antecedents to (dis)satisfaction. The primary distinction among these antecedents is that (1) some increase satisfaction when present but do not increase dissatisfaction when absent, (2) some increase dissatisfaction when absent but do not increase satisfaction when present, (3) some impact both satisfaction and dissatisfaction and negative evaluations to the extent that they are present or absent, and (4) some have no impact on satisfaction and dissatisfaction. Each of the research streams has its own distinct terminology for identifying these factors. Because of their marketing origins coupled with their direct connection to (dis)satisfaction, we use the terms satisfiers, dissatisfiers, criticals, and neutrals (Cadotte and Turgeon 1988), respectively, as generic terms to refer to these factors. A cross disciplinary review of the literature suggests that satisfiers meet the intrinsic needs of individuals, which are considered to be ends in themselves. Dissatisfiers tend to meet the extrinsic needs of individuals and their minimal requirements, which are in turn related to the functional performance of offerings and are means to ends. This suggests that customers can be highly satisfied only if the functional or utilitarian aspects of the offerings are controlled and the psychological or hedonistic aspects are offered in addition. While they do not specifically deal with (dis)satisfaction, a number of models and theories help illustrate the relative impact of satisfiers, dissatisfiers, criticals, and neutrals on (dis)satisfaction and the evolution cycle among these factors. For example, prospect theory (Kahneman and Tversky 1979) suggests that the impact of losses (negative outcomes) is greater than the impact of gains (positive outcomes). This finding lends support to the idea that dissatisfiers have priority over satisfiers. Further, Levitt's (1986) total product model suggests there is an evolving cycle among these antecedents. For example product attributes that once were satisfiers become criticals and then dissatisfiers over time (Brandt 1988; Cadotte and Turgeon 1988; Kano et al. 1984). The review of these research streams reveals several unresolved issues. The most important of these is the question of how to integrate the existence of satisfiers, dissatisfiers, criticals, and neutrals into customer satisfaction models. However, most research in customer satisfaction relies on the disconfirmation of expectations (DE) model, which has not accounted for the multidimensional nature of these antecedents leading to satisfaction. As such, further extensions to the DE model or an alternative model are needed. As an extension to the DE model, the need-gratification model (Oliver 1997) captures the curve-linear relationship between the change in product attribute level and (dis)satisfaction. \\u2026 Over the last decade, there has been an increasing focus on service across socioeconomic sectors coupled with transformational developments in information and communication technologies (ICTs). Together these developments are engendering dramatic new opportunities for service innovation, the study of which is both timely and important. Fully understanding these opportunities challenges us to question conventional approaches that construe service as a distinctive form of socioeconomic exchange (i.e., as services) and to reconsider what service means and thus how service innovation may develop. The aim of this special issue, therefore, is to bring together some of the latest scholarship from the Marketing and Information Systems disciplines to advance theoretical developments on service innovation in a digital age. Abstract There are two logics or mindsets from which to consider and motivate a transition from goods to service(s). The first, \\u201cgoods-dominant (G-D) logic\\u201d, views services in terms of a type of (e.g., intangible) good and implies that goods production and distribution practices should be modified to deal with the differences between tangible goods and services. The second logic, \\u201cservice-dominant (S-D) logic\\u201d, considers service \\u2013 a process of using ones resources for the benefit of and in conjunction with another party \\u2013 as the fundamental purpose of economic exchange and implies the need for a revised, service-driven framework for all of marketing. This transition to a service-centered logic is consistent with and partially derived from a similar transition found in the business-marketing literature \\u2014 for example, its shift to understanding exchange in terms value rather than products and networks rather than dyads. It also parallels transitions in other sub-disciplines, such as service marketing. These parallels and the implications for marketing theory and practice of a full transition to a service-logic are explored. Advancing service science requires a service-centered conceptual foundation. Toward this goal, we suggest that an emerging logic of value creation and exchange called service-dominant logic is a more robust framework for service science than the traditional goods-dominant logic. The primary tenets of service-dominant logic are: (1) the conceptualization of service as a process, rather than a unit of output; (2) a focus on dynamic resources, such as knowledge and skills, rather than static resources, such as natural resources; and (3) an understanding of value as a collaborative process between providers and customers, rather than what producers create and subsequently deliver to customers. These tenets are explored and a foundational lexicon for service science is suggested. Abstraction is a powerful thing. During the nineteenth century, the Industrial Revolution was built on many powerful abstractions, such as mass, energy, work, and power. During the twentieth century, the information revolution was built on many powerful abstractions, such as binary digit or bit, binary coding, and algorithmic complexity. Here, we propose an abstraction for the twenty-first century, in which there is an emerging revolution in thinking about business and economics based on a service-dominant logic. The worldview of service-dominant logic stands in sharp contrast to the worldview of the goods-dominant logic of the past, as it holds service\\u2014the application of competences for benefit of others\\u2014rather than goods to be the fundamental basis of economic exchange. Within this new worldview, we suggest the basic abstraction is the service system, a configuration of people, technologies, and other resources that interact with other service systems to create mutual value. Many systems can be viewed as service systems, including families, cities, and companies, among many others. In this paper, we show how the service-system abstraction can be used to understand how value is co-created, in the process laying the foundation for an integrated science of service. The progress of service-dominant logic and service science, and their expanding theoretical basis, has created ambiguities in relation to understanding the micro-foundations of value co-creation. Based on a conceptual analysis of resource integration and value proposing as the foundational practices of value co-creation, this article portrays value propositions as the co-created forms of shared resources and understanding, which constitutes service systems. This view is inherently socially constructed and perceives value propositions as institutionalized, taken-for-granted social structures that influence local instances of resource integration within and between service systems. Because of the idiosyncratic local contexts for resource integration, however, value propositions do not completely govern resource integration. Instead, they are reproduced and transformed through the local interactions they influence. This mutually constituting nature between value propositions and resource integration affects our understanding of stability and change, creating novel avenues for studying innovation on the basis of service science. Abstract The delineation of B2B from \\u2018mainstream\\u2019 marketing reflects the limitations of the traditional, goods-dominant (G-D) model of exchange and a conceptualization of value creation based on the \\u2018producer\\u2019 versus \\u2018consumer\\u2019 divide. Service-dominant (S-D) logic broadens the perspective of exchange and value creation and implies that all social and economic actors engaged in exchange (e.g., firms, customers, etc.) are service-providing, value-creating enterprises; thus, in this sense, all exchange can be considered B2B. From this perspective, the contributions of B2B marketing (and other sub-disciplines) can be seen as applicable to \\u2018mainstream\\u2019 marketing. This generic, actor-to-actor (A2A) orientation, in turn, points toward a dynamic, networked and systems orientation to value creation. This article discusses this systems-oriented framework and elaborates the steps necessary for developing it further into a general theory of the market, informed by the marketing sub-disciplines, marketing practices, and disciplines external to marketing. Abstract This article explores the role of institutions in innovation from a service-ecosystems perspective, which helps to unify diverging views on innovation and extend the research regarding innovation systems. Drawing on institutional theories, this approach broadens the scope of innovation beyond firm-centered production activities and collaboration networks, and emphasizes the social practices and processes that drive value creation and, more specifically, innovation \\u2014 the combinatorial evolution of new, useful knowledge. Based on this ecosystems view, we argue for institutionalization \\u2013 the maintenance, disruption and change of institutions \\u2013 as a central process of innovation for both technology and markets. In this view, technology is conceptualized as potentially useful knowledge, or a value proposition, which is both an outcome and a medium of value co-creation and innovation. Market innovation, then, is driven by the combinatorial evolution of value propositions and the emergence and institutionalization of new solutions. This article discusses how the core concepts of service-dominant logic \\u2014 service-for-service exchange, value co-creation, value propositions, resource integration, and highly collaborative relationships \\u2014 point to a generic actor conceptualization in which all actors engaged in exchange (e.g., firms, customers, etc.) are viewed as service providing, value-creating enterprises. In other words, all social and economic actors are essentially doing the same thing: creating value for themselves and others through reciprocal resource integration and service provision. The authors suggest that this generic actor-to-actor (A2A) orientation, in turn, points toward the dynamic and systemic nature of social and economic exchange. To account for the complexity, indeterminacy, and viability of these dynamic systems, they highlight the importance of general systems theory, complexity theory, and the viable systems approach and propose that cross-disciplinary scholarly efforts are necessary in order to develop models and frameworks that can simplify the complexity of social and economic exchange in meaningful ways and ultimately inform practice and public policy. Abstract This commentary addresses some common themes of the papers from the 3rd EMAC/ANZMAC Research Symposium track on service-dominant (S-D) logic: (1) issues and approaches to measurement, (2) the need for plural versus singular paradigms, and (3) questions concerning the boundary conditions of S-D logic. It suggests that since S-D logic is not a normative theory and is an alternative to the current (goods-) dominant logic, caution must be exercised in designing empirical tests. It also suggests that since S-D logic transcends the goods vs. services logics, it obviates the need for pluralism and is capable of spanning boundaries created by goods-dominant-logic\\u2026possible path for its development. This article anchors a special issue on a service-dominant logic perspective on relationship, stemming from a special session at the 9th International Conference on Relationship Marketing in Berlin in 2009. It also proposes and elaborates a service-dominant-logic-based, transcending conceptualization of relationship that was the basis for that special session and links it to a model of service ecosystems through which value creation can be better understood and, thus, businesses can be better informed. In this paper, we explore the role and scope of technology in value co-creation, service innovation and service systems--value co-creation configurations of people technology and value propositions (Maglio and Spohrer in J Acad Mark Sci 36:18---20, 2008). We draw on a structurational model of technology (Orlikowsky in Organ Sci 3(3):398---427, 1992) to provide a framework for considering the role of technology in service systems and how it influences and is influenced by human actions (i.e., practices) and institutions. We broaden the scope of technology in this model, beyond a material artifact, or outcome of human actions, by applying an S-D logic, service ecosystems (Vargo and Lusch in J Market 68(1):1---17, 2004, Ind Mark Manag 40(2):181---187, 2011a) approach, which focuses on the processes by which value is co-created and new ways of creating value (i.e., innovation) emerge. In this view, technology can be conceptualized as an operant resource--one that is capable of acting on other resources to create value--and, thus, becomes a critical resource for value co-creation, service innovation and systems (re)formation. We argue that the consideration of technology as an operant resource in service (eco)systems provides a more encompassing view for systematically studying the way in which technologies are integrated as resources, value is collaboratively created, and service is innovated. Purpose \\u2013 The purpose of this paper is to propose and elaborate on a service\\u2010dominant\\u2010logic\\u2010based conceptualization of relationship that transcends traditional conceptualizations.Design/methodology/approach \\u2013 The paper consists of a review of traditional conceptualizations of relationship, a review of service\\u2010dominant logic foundational premises that are useful in reframing the concept, and supporting views from the institutional economics and business ecosystems literature.Findings \\u2013 A transcending, service\\u2010dominant\\u2010logic\\u2010based conceptualization of relationship as a general term representing the network\\u2010with\\u2010and\\u2010within\\u2010network nature of value creation, with transactions as \\u201ctemporal isolates\\u201d of relationships is suggested.Originality/value \\u2013 This higher\\u2010order conceptualization of relationship provides a foundation for better understanding the role of relationship in value creation, as well as its correspondence to transactions and products. Service-dominant logic (S-D logic) is contrasted with goods-dominant (G-D) logic to provide a framework for thinking more clearly about the concept of service and its role in exchange and competition. Then, relying upon the nine foundational premises of S-D logic [Vargo, Stephen L. and Robert F. Lusch (2004). \\u201cEvolving to a New Dominant Logic for Marketing,\\u201d Journal of Marketing, 68 (January) 1\\u201317; Lusch, Robert F. and Stephen L. Vargo (2006), \\u201cService-Dominant Logic as a Foundation for Building a General Theory,\\u201d in The Service-Dominant Logic of Marketing: Dialog, Debate and Directions. Robert F. Lusch and Stephen L. Vargo (eds.), Armonk, NY: M.E. Sharpe, 406\\u2013420] nine derivative propositions are developed that inform marketers on how to compete through service. Purpose \\u2013 The purpose of this paper is to examine shopper marketing through service-dominant logic and service ecosystem lenses. In doing so, the authors reveal challenges and opportunities for supply chain management. Design/methodology/approach \\u2013 The work is conceptual, drawing on contemporary service-dominant logic thinking. Findings \\u2013 Examination of shopper marketing reveals that it is currently stuck in goods-dominant logic and micro-level ways of thinking. By taking a macro service ecosystem view, all actors, including shoppers, are seen as resource integrators seeking resource density. The macro view highlights a significant amount of goods and information flow and variance now being added throughout shopper marketing systems. Research limitations/implications \\u2013 A guiding framework with appropriate terms defined offers new research directions and new ways practitioners can approach challenges in the industry. Research programs are suggested in the areas of facilitating resource density, examining t... Abstraction is a powerful thing. During the 19th century, the industrial revolution was built on many powerful abstractions, such as mass, energy, work, and power. During the 20th century, the information revolution was built on many powerful abstractions, such as binary digit or bit, binary coding, and algorithmic complexity. Here, we propose an abstraction that will be important to the service revolution of the 21st century: the service system, which is a configuration of people, technologies, and other resources that interact with other service systems to create mutual value. Many systems can be viewed as service systems, including families, cities, and companies, among many others. In this paper, we show how the service-system abstraction can be used to understand how value is created, in the process unifying concepts from many disciplines and creating the foundation for an integrated science of service. Purpose \\u2013 The purpose of this paper is to extend conceptually the context of service beyond service encounters and servicescapes by applying a service-ecosystem approach to context and experiential view on value. Design/methodology/approach \\u2013 We develop a conceptual framework of an extended service context that is based on an S-D logic, service-ecosystems view. Findings \\u2013 The service ecosystem approach proposed here contributes to the advancement of \\u201cservices\\u201d marketing research by extending the context of service in two ways: its emphasis on service as the basis of all exchange allows the consideration of all instances of value-in-use, in-context, to be considered as a service experience; its conceptualization of context broadens the time/place dimensions that conventionally restrain research in service encounters and servicescapes beyond physical, social, symbolic and relational dimensions to consider the multiplicity of institutions across a wider socio-historic space. Research limitations/implications... Service science is an emerging discipline concerned with the evolution, interaction, and reciprocal cocreation of value among service systems (Maglio and Spohrer [Maglio, P. P., J. Spohrer. 2008. Fundamentals of Service Science. Journal of the Academy of Marketing Science 36(1) 18\\u201320.]; Spohrer et al. [Spohrer, J., S. Vargo, N. Caswell, P. Maglio. 2008. The Service System is the Basic Abstraction of Service Science. 41st Annual HICSS Conference Proceedings.]). Service-dominant (S-D) logic (Vargo and Lusch [Vargo, S., R. F. Lusch. 2004a. Evolving to a New Dominant Logic for Marketing. Journal of Marketing 68(1) 1\\u201317.] [Vargo, S., R. F. Lusch. 2008. Service-Dominant Logic: Continuing the Evolution. Journal of the Academy of Marketing Science 36(1) 1\\u201310.]) is an alternative to the traditional, goods-dominant (G-D) paradigm for understanding economic exchange and value creation. This service-centered view is based on the idea that service \\u2013 the application of competences for the benefit of another \\u2013 is the ba... Since the introductory article for what has become known as the \\u201cservice-dominant (S-D) logic of marketing,\\u201d \\u201cEvolving to a New Dominant Logic for Marketing,\\u201d was published in the Journal of Marketing (Vargo, S. L., & Lusch, R. F. (2004a)), there has been considerable discussion and elaboration of its specifics. This article highlights and clarifies the salient issues associated with S-D logic and updates the original foundational premises (FPs) and adds an FP. Directions for future work are also discussed. The purpose of this paper is to explore the role of context in service provision and, more broadly, in market co-creation. We oscillate foci from an individual actor at the micro level to a market at the macro level to make the scaleable influence of context more salient. This reveals the meso level, which is nestled between the micro and macro levels. We discuss how these market levels influence one another. We conceptualize markets as simultaneous, continuous exchanges that are bounded by each of these levels of context. This article introduces the special section of \\u2018\\u2018Extending Service-Dominant Logic\\u2019\\u2019 in this journal, which is part of jointly published special sections in the Australasian Marketing Journal, European Journal of Marketing, Journal of Macromarketing, and Marketing Theory on the same topic, all based on articles developed from manuscripts submitted to the Forum on Markets and Marketing in Sydney, Australia. It links the three articles in this issue and also links the associated macro-, meso-, and microperspectives. Abstract To strengthen the theoretical foundations of international marketing (IM), the authors propose a framework for conceptualizing the complexity of the context that frames international and global exchange systems. In particular, they apply a service ecosystems approach, which is grounded in service-dominant logic and its foundational premise that service is the basis of all exchange. The proposed framework provides insight into the nature of context, a distinguishing feature of IM. The authors argue that the embeddedness of social networks and the multiplicity of institutions within a service ecosystem influence the complexity of context. They articulate the way the (co)creation of value influences and is influenced by the enactment of practices and the integration of resources through various levels (micro, meso, and macro) of interaction and institutions. They introduce the concept of \\u201cvalue in cultural context\\u201d to emphasize the influence of the symbolic and social components of context. The arti... Research Paper Purpose Drawing from S-D logic this paper builds on the proposition of customers as resource integrators, identifying six styles of customer co-creation, and linking customer cocreation styles to outcomes. Design/methodology/approach A theoretical framework is proposed linking customer co-creation styles to outcomes. The research is based on twenty in-depth interviews and four focus groups of patients across a wide range of cancers, gender and length of treatment with oncology patients at two different clinics. Findings Six styles of co-creation are identified. They are \\u201cTeam Manager\\u201d, \\u201cIsolate Controller\\u201d, \\u201cPartner\\u201d, \\u201cSpiritualist\\u201d, \\u201cAdaptive Realist\\u201d and \\u201cPassive Compliant\\u201d. Individuals who exhibit an \\u201cAdaptive Realist\\u201d style tend to demonstrate high quality of life, on psychological, existential and support dimensions. In contrast, the lowest quality of life was evidenced by those exhibiting \\u201cPassive Compliant\\u201d and \\u201cIsolate Controller\\u201d styles. Practicality Our findings provide insights into which customer co-creation styles offer greatest patient outcomes. Research limitations/implications The present research provides a starting point for further research on modelling the relationship between customer co-creation styles and outcomes. Originality/value This is the first study to operationalize co-creation, relating co-creation to coproduction, and demonstrating a direct relationship between co-creation activities, cocreation styles and outcomes. Furthermore, the research develops theory, building on S-D logic. Traditional Customer Relationship Management (CRM) systems are based on a model of exchange that centres on goods (goods-dominant logic), with value created by the firm and relationship implying multiple transactions of value-laden output. We explore an alternative logic of exchange (service-dominant logic) based on the reciprocal application of applied competences (service), which sees relationship in terms of the co-creation of value. We then review the partial transition of recent CRM models toward a service-dominant logic perspective and offer an initial service-dominant logic-informed next generation CRM system design. This article explores a service-dominant (S-D) logic, service-ecosystems approach to studying value cocreation and the (re)formation of service systems. We outline the central premises of S-D logic and elaborate the concept of a service ecosystem to propose a framework that focuses on resource integration as a central means for connecting people and technology within and among service systems. This ecosystems view emphasizes the social factors that influence, and are influenced by, service-for-service exchange. We draw on systems theory and a structurational model of technology to underscore the importance of networks of actors, as well as institutions---e.g., rules, social norms---as critical components of service systems. We argue that this service-ecosystems framework provides a robust and dynamic approach for studying resource integration, value cocreation, and the (re)formation of service systems, and provides important insights for systematically innovating service. Service Science is an interdisciplinary effort to understand how service systems interact and co-create value. Service-dominant (S-D) logic is an alternative perspective to the traditional, goods-dominant (G-D) logic paradigm, which has been recognized as a potential theoretical foundation on which a science of service can be developed. While there are efforts to support and develop an S-D-logic-grounded service science, the paradigmatic power of G-D logic remains strong. This is evidenced by several recurring misconceptions about S-D logic and its application in service science. This chapter aims to guide the advancement of an S-D-logic-grounded service science by clarifying several misconstruals associated with S-D logic and moving forward with the formalization of key concepts associated with S-D logic and service science. Abstract Purpose This chapter explores the nature of the cultural context that frames value creation and provides insight to the way in which value is collaboratively created, or co-created, in markets.   Methodology/approach We develop a conceptual framework and research propositions for studying the co-creation of value-in-cultural-context through the intersection of consumer culture theory (CCT) and service-dominant (S-D) logic and the integration of a practice-theoretic approach for value co-creation.   Research implications The integration of CCT, S-D logic, and practice theory provides a conceptual framework for studying the co-creation of value among multiple stakeholders and the (re)formation of markets.   Practical implications Drawing on this framework, marketers can contribute to the co-creation of new markets by influencing changes in cultural contexts \\u2013 practices, norms, meanings, and resources \\u2013 that frame value co-creation and exchange.   Originality/value of chapter This chapter explores the integration of CCT and S-D logic by focusing on value co-creation and applying a practice approach to further weave together these distinct research areas. In addition, the proposed framework elaborates the conceptualization of value-in-context to consider the cultural context that influences and is influenced by the co-creation of value. This article explores how seemingly distinct actors contribute to value creation and evaluation in a fundamentally similar way. It shows that the division of actors into dichotomies such as \\u2018producers\\u2019 and \\u2018consumers,\\u2019 \\u2018paying\\u2019 and \\u2018non-paying\\u2019 customers, and \\u2018adopters\\u2019 and \\u2018non-adopters,\\u2019 is based on narrow, unidirectional, transactional, and dyadic views on value creation and delivery. The article highlights the limitations of these views and draws on a service ecosystems perspective and its broader notion of co-created and contextual value to overcome these limitations. More specifically, the article, by connecting two frameworks (markets-as-practice and institutional work), extends a generic actor-to-actor conceptualization of value creation, in showing that all economic and social actors participate in value creation in a fundamentally similar way. That is, they enact value co-creation practices and simultaneously shape these practices by creating, maintaining and disrupting the institutions that gui... Purpose \\u2013 The purpose of this essay is to explore further the concept of value cocreation from a service-ecosystems view, by considering the importance of networks and the configuration of relationships and resources in markets.  Methodology/approach \\u2013 We use a conceptual approach to extend a service-dominant (S-D) logic, ecosystems view of value cocreation by drawing on the literature regarding networks in marketing and related research.  Findings \\u2013 A service-ecosystems approach to cocreating value-in-context is proposed, which points toward networks as mediating factors in value cocreation because they influence the ability to access, adapt, and integrate resources by establishing exchange relationships and shaping the social contexts through which value is experienced.  Research implications \\u2013 This research suggests that value cocreation is a complex and multidimensional process that is best studied in the context of dynamic networks or ecosystems of service exchange.  Practical implications \\u2013 This research suggests that networks mediate value cocreation, and thus, firms should consider the configurations of relationships and resources to develop more compelling value propositions.  Social implications \\u2013 This research draws on the idea that exchange relationships are embedded within society and suggests that processes of value cocreation not only draw on but also contribute to the social contexts that frame market exchange.  Originality/value of essay \\u2013 This research extends the value cocreation and S-D logic literature by exploring the role of networks in service ecosystems. In this framework, networks are mediators of value cocreation because they enable access to resources and help to (re)shape social contexts through which value is derived. This paper extends research on innovation as institutional change within service science and service-dominant S-D logic by conceptualizing the emergence of novel solutions in service ecosystems. We pay particular attention to how actors individuals and organizations are able to create new solutions that change the very institutional arrangements that guide and constrain them. We propose that institutional complexity-the multiplicity of institutional arrangements confronting actors with conflicting prescriptions for action-drives the emergence of novelty. Institutional complexity reduces the influence of prevailing institutions by activating conscious problem solving and making available multiple institutional \\\"toolkits.\\\" These dynamic toolkits consist of the cultural norms and meanings, as well as material practices, associated with specific institutional arrangements, with which actors can jointly reconstruct and change value cocreation practices and advance change in the institutional arrangements of service ecosystems. This paper contributes to service science and S-D logic by providing a more comprehensive understanding of innovation driven by institutional complexity, in which the stability of institutional arrangements is reconciled with the actor-driven creation of novel solutions constitutive of institutional change.\",\n",
            "  \"2072204366\": \"Junction structures, as the natural anatomical markers, are useful to study the organ or tumor motion. However, detection and tracking of the junctions in four-dimensional (4D) images are challenging. The paper presents a novel framework to automate this task. Detection of their centers and sizes is first achieved by an analysis of local shape profiles on one segmented reference image. Junctions are then separately tracked by simultaneously using neighboring intensity features from all images. Defined by a closed B-spline space curve, the individual trajectory is assumed to be cyclic and obtained by maximizing the metric of combined correlation coefficients. Local extrema are suppressed by improving the initial conditions using random walks from pairwise optimizations. Our approach has been applied to analyze the vessel junctions in five real 4D respiration-gated computed tomography (CT) image datasets with promising results. More than 500 junctions in the lung are detected with an average accuracy of greater than 85% and the mean error between the automated and the manual tracking is sub-voxel. The success of radiotherapy critically depends on the imaging modality used for treatment planning and the level of integration of the available imaging information. The use of functional/metabolic imaging provides us much more than a tool to delineate better the boundary of a tumor target. Together with anatomical CT or MRI images, functional imaging affords valuable 3D structural plus 1D metabolic data for both tumor and sensitive structures, valuable for guiding us to design spatially non-uniform dose distributions to deliver high doses to where the tumor burdens are high and differentially spare the sensitive structures according to the functional importance distributions. The integration and utilization of the functional data in radiation therapy treatment planning become increasingly important to improve clinical cancer management. While it is straightforward to modify the radiation portals to accommodate any changes in treatment volume, new methods of dose optimization and medical decision-making must be developed to take full advantage of the metabolic information and IMRT. How to achieve biologically conformal doses, instead of the geometrically conformal dose distribution, presents a new challenge to radiation oncology discipline. Hopefully, with the efforts from multiple institutions, the new approach of imaging, planning and decision-making will be resolved. Ultimately, whether using deliberately inhomogeneous dose distributions obtained under the guidance of functional imaging such as MRSI can improve patient survival and reduce the side effects associated with radiation treatment should be established through extensive clinical trials. A method is provided for interactive treatment planning of IMRT or other radiation modalities that employs non-uniform tuning or optimization. One aspect provides a voxel-dependent penalty scheme by varying the importance factor associated with a voxel, the prescription at the voxel, or the form of the penalty function at the voxel in a non-uniform manner. Another aspect provides the dose shape at a specified sub-volume tuned by varying the local importance factor(s) or the local prescription or the form/value of penalty function. Yet another aspect provides the use of a non-uniform penalty scheme (non-uniform importance factors, non-uniform prescription in one or more structures, or non-uniform form of the objective function). Still another aspect provides the method of pre-estimating the values of the voxel-specific importance factors using prior dosimetric knowledge of a given system. A general imaging scheme is proposed for applications of CBCT. The approach provides a superior CBCT image quality by effective scatter correction and noise reduction. Specifically, in its implementation of CBCT imaging for radiation therapy, the proposed approach achieves an accurate patient setup using a partially blocked CBCT with a significantly reduced radiation dose. The image quality improvement due to the proposed scatter correction and noise reduction also makes CBCT-based dose calculation a viable solution to adaptive treatment planning. A method of acquiring scatter data and image projection data in computed tomography is provided that includes attenuating a radiation source using a pattern of blockers arranged to provide blocked and unblocked regions of the radiation source, and acquiring image data and scatter data of a target using an imaging device. A scatter map in the projection image can be estimated by interpolation and/or extrapolation of the projection image using an appropriately programmed computer, subtracting the estimated scatter map from the projection image to obtain scatter-corrected projections, reconstructing a CBCT volume using a total variation regularization algorithm, and applying an iterative regularization process to suppress the noise level on the reconstructed CBCT volume. Reconstructing a CBCT volume can include using a total variation regularization algorithm and applying an iterative regularization process to suppress the noise level on the reconstructed CBCT volume, where scatter-induced artifacts are corrected in the projection image. We apply the recently proposed superiorization methodology (SM) to the inverse planning problem in radiation therapy. The inverse planning problem is represented here as a constrained minimization problem of the to- tal variation (TV) of the intensity vector over a large system of linear two-sided inequalities. The SM can be viewed conceptually as lying between feasibility- seeking for the constraints and full-edged constrained minimization of the objective function subject to these constraints. It is based on the discovery that many feasibility-seeking algorithms (of the projection methods variety) are perturbation-resilient, and can be proactively steered toward a feasible so- lution of the constraints with a reduced, thus superiorized, but not necessarily minimal, objective function value. The presence of metals in patients causes streaking artifac ts in X-ray CT and has been recognized as a problem that limits various applications of CT imaging. Accurate localization of metals in CT images is a critical step for metal arti facts reduction in CT imaging and many practical applications of CT images. The purpose of this work is to develop a method of auto-determination of the shape and location of metallic object(s) in the image space. The proposed method is based on the fact that when a metal object is present in a patient, a CT image can be divided into two prominent components: high density metal and low density normal tissues. This prior knowledge is incorporated into an objective function as the regularization t erm whose role is to encourage the solution to take a form of two intensity levels. A computer simulation study and four experimental studies are performed to evaluate the proposed approach. Both simulation and experimental studies show that the presented algorithm works well even in the presence of complicated shaped metal objects. For a hexagonally shaped metal embedded in a water phantom, for example, it is found that the accuracy of metal reconstruction is within sub-millimeter. A method for selecting an orientation of a treatment beam for intensity modulated radiation therapy based on analysis performed on a model of a patient geometry prior to the actual treatment includes a planning target volume (PTV) and the structures at risk. The method calls for assigning a tolerance parameter to the structures at risk, subdividing the treatment beam into beamlets, selecting a treatment parameter and selecting a set of orientations for the treatment beam and deriving a score for each of the orientations by weighting each of the beamlets so as to maximize the treatment parameter in the PTV while not exceeding the tolerance parameter in the structures at risk at each orientation. The derived scores are used to construct a scoring function to aid in the selection of one or more treatment beam orientations to be used during the actual intensity modulated radiation therapy. Monte Carlo (MC) methods are the gold standard for modeling photon and electron transport in a heterogeneous medium; however, their computational cost prohibits their routine use in the clinic. Cloud computing, wherein computing resources are allocated on-demand from a third party, is a new approach for high performance computing and is implemented to perform ultra-fast MC calculation in radiation therapy. We deployed the EGS5 MC package in a commercial cloud environment. Launched from a single local computer with Internet access, a Python script allocates a remote virtual cluster. A handshaking protocol designates master and worker nodes. The EGS5 binaries and the simulation data are initially loaded onto the master node. The simulation is then distributed among independent worker nodes via the message passing interface, and the results aggregated on the local computer for display and data analysis. The described approach is evaluated for pencil beams and broad beams of high-energy electrons and photons. The output of cloud-based MC simulation is identical to that produced by single-threaded implementation. For 1 million electrons, a simulation that takes 2.58 h on a local computer can be executed in 3.3 min on the cloud with 100 nodes, a 47\\u00d7 speed-up. Simulation time scales inversely with the number of parallel nodes. The parallelization overhead is also negligible for large simulations. Cloud computing represents one of the most important recent advances in supercomputing technology and provides a promising platform for substantially improved MC simulation. In addition to the significant speed up, cloud computing builds a layer of abstraction for high performance parallel computing, which may change the way dose calculations are performed and radiation treatment plans are completed. The purpose of this work is to develop an effective technique to automatically propagate contours from planning CT to cone beam CT (CBCT) to facilitate CBCT-guided prostate adaptive radiation therapy. Different from other disease sites, such as the lungs, the contour mapping here is complicated by two factors: (i) the physical one-to-one correspondence may not exist due to the insertion or removal of some image contents within the region of interest (ROI); and (ii) reduced contrast to noise ratio of the CBCT images due to increased scatter. To overcome these issues, we investigate a strategy of excluding the regions with variable contents by a careful design of a narrow shell signifying the contour of an ROI. For rectum, for example, a narrow shell with the delineated contours as its interior surface was constructed to avoid the adverse influence of the day-to-day content change inside the rectum on the contour mapping. The corresponding contours in the CBCT were found by warping the narrow shell through the use of BSpline deformable model. Both digital phantom experiments and clinical case testing were carried out to validate the proposed ROI mapping method. It was found that the approach was able to reliably warp the constructed narrow band with an accuracy better than 1.3 mm. For all five clinical cases enrolled in this study, the method yielded satisfactory results even when there were significant rectal content changes between the planning CT and CBCT scans. The overlapped area of the auto-mapped contours over 90% to the manually drawn contours is readily achievable. The proposed approach permits us to take advantage of the regional calculation algorithm yet avoiding the nuisance of rectum/bladder filling and provide a useful tool for adaptive radiotherapy of prostate in the future. In the presence of intrafraction organ motion, target localization uncertainty can greatly hamper the advantage of highly conformal dose techniques such as intensity modulated radiation therapy (IMRT). To minimize the adverse dosimetric effect caused by tumor motion, a real-time knowledge of the tumor position is required throughout the beam delivery process. The recent integration of onboard kV diagnostic imaging together with MV electronic portal imaging devices on linear accelerators can allow for real-time three-dimensional (3D) tumor position monitoring during a treatment delivery. The aim of this study is to demonstrate a near real-time 3D internal fiducial tracking system based on the combined use of kV and MV imaging. A commercially available radiotherapy system equipped with both kV and MV imaging systems was used in this work. A hardware video frame grabber was used to capture both kV and MV video streams simultaneously through independent video channels at 30 frames per second. The fiducial locations were extracted from the kV and MV images using a software tool. The geometric tracking capabilities of the system were evaluated using a pelvic phantom with embedded fiducials placed on a moveable stage. The maximum tracking speed of the kV\\u2215MV system is approximately 9 Hz, which is primarily limited by the frame rate of the MV imager. The geometric accuracy of the system is found to be on the order of less than 1 mm in all three spatial dimensions. The technique requires minimal hardware modification and is potentially useful for image-guided radiation therapy systems. An intensity-modulated radiation therapy (IMRT) field is composed of a series of segmented beams. It is practically important to reduce the number of segments while maintaining the conformality of the final dose distribution. In this article, the authors quantify the complexity of an IMRT fluence map by introducing the concept of sparsity of fluence maps and formulate the inverse planning problem into a framework of compressing sensing. In this approach, the treatment planning is modeled as a multiobjective optimization problem, with one objective on the dose performance and the other on the sparsity of the resultant fluence maps. A Pareto frontier is calculated, and the achieved dose distributions associated with the Pareto efficient points are evaluated using clinical acceptance criteria. The clinically acceptable dose distribution with the smallest number of segments is chosen as the final solution. The method is demonstrated in the application of fixed-gantry IMRT on a prostate patient. The result shows that the total number of segments is greatly reduced while a satisfactory dose distribution is still achieved. With the focus on the sparsity of the optimal solution, the proposed method is distinct from the existing beamlet- or segment-based optimization algorithms. Purpose: Monoscopic x-ray imaging with on-board kV devices is an attractive approach for real-time image guidance in modern radiation therapy such as VMAT or IMRT, but it falls short in providing reliable information along the direction of imaging x-ray. By effectively taking consideration of projection data at prior times and/or angles through a Bayesian formalism, the authors develop an algorithm for real-time and full 3D tumor localization with a single x-ray imager during treatment delivery. Methods: First, a prior probability density function is constructed using the 2D tumor locations on the projection images acquired during patient setup. Whenever an x-ray image is acquired during the treatment delivery, the corresponding 2D tumor location on the imager is used to update the likelihood function. The unresolved third dimension is obtained by maximizing the posterior probability distribution. The algorithm can also be used in a retrospective fashion when all the projection images during the treatment delivery are used for 3D localization purposes. The algorithm does not involve complex optimization of any model parameter and therefore can be used in a ''plug-and-play'' fashion. The authors validated the algorithm using (1) simulated 3D linear and elliptic motion and (2) 3D tumor motion trajectories of amore\\u00a0\\u00bb lung and a pancreas patient reproduced by a physical phantom. Continuous kV images were acquired over a full gantry rotation with the Varian TrueBeam on-board imaging system. Three scenarios were considered: fluoroscopic setup, cone beam CT setup, and retrospective analysis. Results: For the simulation study, the RMS 3D localization error is 1.2 and 2.4 mm for the linear and elliptic motions, respectively. For the phantom experiments, the 3D localization error is < 1 mm on average and < 1.5 mm at 95th percentile in the lung and pancreas cases for all three scenarios. The difference in 3D localization error for different scenarios is small and is not statistically significant. Conclusions: The proposed algorithm eliminates the need for any population based model parameters in monoscopic image guided radiotherapy and allows accurate and real-time 3D tumor localization on current standard LINACs with a single x-ray imager.\\u00ab\\u00a0less Purpose: Conventional volumetric modulated arc therapy (VMAT) discretizes the angular space into equally spaced control points during planning and then optimizes the apertures and weights of the control points. The aperture at an angle in between two control points is obtained through interpolation. This approach tacitly ignores the differential need for intensity modulation of different angles. As such, multiple arcs are often required, which may oversample some angle(s) and undersample others. The purpose of this work is to develop a segmentally boosted VMAT scheme to eliminate the need for multiple arcs in VMAT treatment with improved dose distribution and/or delivery efficiency. Methods: The essence of the new treatment scheme is how to identify the need of individual angles for intensity modulation and to provide the necessary beam intensity modulation for those beam angles that need it. We introduce a \\u201cdemand metric\\u201d at each control point to decide which station or control points need intensity modulation. To boost the modulation at selected stations, additional segments are added in the vicinity of the selected stations. The added segments are then optimized together with the original set of station or control points as a whole. The authors apply the segmentally boosted planning technique to four previously treated clinical cases: two head and neck (HN) cases, one prostate case, and one liver case. The proposed planning technique is compared with conventional one-arc and two-arc VMAT. Results: The proposed segmentally boosted VMAT technique achieves better critical structure sparing than one-arc VMAT with similar or better target coverage in all four clinical cases. The segmentally boosted VMAT also outperforms two-arc VMAT for the two complicated HN cases, yet with \\u223c30% reduction in the machine monitor units (MUs) relative to two-arc VMAT, which leads to less leakage/scatter dose to the patient and can potentially translate into faster dose delivery. For the less challenging prostate and liver cases, similar critical structure sparing as the two-arc VMAT plans was obtained using the segmentally boosted VMAT. The benefit for the two simpler cases is the reduction of MUs and improvement of treatment delivery efficiency. Conclusions: Segmentally boosted VMAT achieves better dose conformality and/or reduced MUs through effective consideration of the need of individual beam angles for intensity modulation. Elimination of the need for multiple arcs in rotational arc therapy while improving the dose distribution should lead to improved workflow and treatment efficacy, thus may have significant implication to radiation oncology practice. This work provides a comprehensive Monte Carlo study of X-ray fluorescence computed tomography (XFCT) and K-edge imaging system, including the system design, the influence of various imaging components, the sensitivity and resolution under various conditions. We modified the widely used EGSnrc/DOSXYZnrc code to simulate XFCT images of two acrylic phantoms loaded with various concentrations of gold nanoparticles and Cisplatin for a number of XFCT geometries. In particular, reconstructed signal as a function of the width of the detector ring, its angular coverage and energy resolution were studied. We found that XFCT imaging sensitivity of the modeled systems consisting of a conventional X-ray tube and a full 2-cm-wide energy-resolving detector ring was 0.061% and 0.042% for gold nanoparticles and Cisplatin, respectively, for a dose of ~10cGy. Contrast-to-noise ratio (CNR) of XFCT images of the simulated acrylic phantoms was higher than that of transmission K-edge images for contrast concentrations below 0.4%. Abstract Purpose: To develop a novel dose optimization algorithm for improving the sparing of critical structures during gamma knife radiosurgery by shaping the plug pattern of each individual shot. Method and Materials: We first use a geometric information (medial axis) aided guided evolutionary simulated annealing (GESA) optimization algorithm to determine the number of shots and isocenter location, size, and weight of each shot. Then we create a plug quality score system that checks the dose contribution to the volume of interest by each plug in the treatment plan. A positive score implies that the corresponding source could be open to improve tumor coverage, whereas a negative score means the source could be blocked for the purpose of sparing normal and critical structures. The plug pattern is then optimized via the GESA algorithm that is integrated with this score system. Weight and position of each shot are also tuned in this procedure. Results: An acoustic tumor case is used to evaluate our algorithm. Compared to the treatment plan generated without plug patterns, adding an optimized plug pattern into the treatment planning process boosts tumor coverage index from 95.1% to 97.2%, reduces RTOG conformity index from 1.279 to 1.167, lowers Paddick\\u2019s index from 1.34 to 1.20, and trims the critical structure receiving more than 30% maximum dose from 16 mm 3 to 6 mm 3 . Conclusions: Automated GESA-based plug pattern optimization of gamma knife radiosurgery frees the treatment planning team from the manual forward planning procedure and provides an optimal treatment plan. Volumetric cone-beam CT (CBCT) images are acquired repeatedly during a course of radiation therapy and a natural question to ask is whether CBCT images obtained earlier in the process can be utilized as prior knowledge to reduce patient imaging dose in subsequent scans. The purpose of this work is to develop an adaptive prior image constrained compressed sensing (APICCS) method to solve this problem. Reconstructed images using full projections are taken on the first day of radiation therapy treatment and are used as prior images. The subsequent scans are acquired using a protocol of sparse projections. In the proposed APICCS algorithm, the prior images are utilized as an initial guess and are incorporated into the objective function in the compressed sensing (CS)-based iterative reconstruction process. Furthermore, the prior information is employed to detect any possible mismatched regions between the prior and current images for improved reconstruction. For this purpose, the prior images and the reconstructed images are classified into three anatomical regions: air, soft tissue and bone. Mismatched regions are identified by local differences of the corresponding groups in the two classified sets of images. A distance transformation is then introduced to convert the information into an adaptive voxel-dependent relaxation map. In constructing the relaxation map, the matched regions (unchanged anatomy) between the prior and current images are assigned with smaller weight values, which are translated into less influence on the CS iterative reconstruction process. On the other hand, the mismatched regions (changed anatomy) are associated with larger values and the regions are updated more by the new projection data, thus avoiding any possible adverse effects of prior images. The APICCS approach was systematically assessed by using patient data acquired under standard and low-dose protocols for qualitative and quantitative comparisons. The APICCS method provides an effective way for us to enhance the image quality at the matched regions between the prior and current images compared to the existing PICCS algorithm. Compared to the current CBCT imaging protocols, the APICCS algorithm allows an imaging dose reduction of 10?40 times due to the greatly reduced number of projections and lower x-ray tube current level coming from the low-dose protocol. PURPOSE: To improve the quality of low-dose X-ray fluoroscopic images using statistics-based restoration algorithm so that the patient fluoroscopy can be performed with reduced radiation dose. METHOD AND MATERIALS: Noise in the low-dose fluoroscopy was suppressed by temporal and spatial filtering. The temporal correlation among neighboring frames was considered by the Karhunen-Loeve (KL) transform (i.e., principal component analysis). After the KL transform, the selected neighboring frames of fluoroscopy were decomposed to uncorrelated and ordered principal components. For each KL component, a penalized weighted least-squares (PWLS) objective function was constructed to restore the ideal image. The penalty was chosen as anisotropic quadratic, and the penalty parameter in each KL component was inversely proportional to its corresponding eigenvalue. Smaller KL eigenvalue is associated with the KL component of lower signal-to-noise ratio (SNR), and a larger penalty parameter should be used for such KL component. The low-dose fluoroscopic images were acquired using a Varian Acuity simulator. A quality assurance phantom and an anthropomorphic chest phantom were used to evaluate the presented algorithm. RESULTS: In the images restored by the proposed KL domain PWLS algorithm, noise is greatly suppressed, whereas fine structures are well preserved. Average improvement rate of SNR is 75% among selected regions of interest. Comparison studies with traditional techniques, such as the mean and median filters, show that the proposed algorithm is advantageous in terms of structure preservation. CONCLUSIONS: The proposed noise reduction algorithm can significantly improve the quality of low-dose X-ray fluoroscopic image and allows for dose reduction in X-ray fluoroscopy. A commonly known deficiency of currently available inverse planning systems is the difficulty in fine-tuning the final dose distribution. In practice, it is not uncommon that just a few unsatisfactory regions in the planning target volume or an organ at risk prevent an intensity modulated radiation therapy(IMRT) plan from being clinically acceptable. The purpose of this work is to introduce a mechanism for controlling the regional doses after a conventional IMRT plan is obtained and to demonstrate its clinical utility. Two types of importance factors are introduced in the objective function to model the tradeoffs of different clinical objectives. The first is the conventional structure-dependent importance factor, which quantifies the interstructure tradeoff. The second type is the voxel-dependent importance factor which \\u201cmodulates\\u201d the importance of different voxels within a structure. The planning proceeds in two major steps. First a conventional inverse planning is performed, where the structure-dependent importance factors are determined in a trial-and-error fashion. The next level of planning involves fine-tuning the regional doses to meet specific clinical requirements. To achieve this, the voxels where doses need to be modified are identified either graphically on the isodose layouts, or on the corresponding dose-volume histogram (DVH) curves. The importance value of these voxels is then adjusted to increase/decrease the penalty at the corresponding regions. The technique is applied to two clinical cases. It was found that both tumor hot spots and critical structure maximal doses can be easily controlled by varying the regional penalty. One to three trials were sufficient for the conventionally optimized dose distributions to be adjusted to meet clinical expectation. Thus introducing the voxel-dependent penalty scheme provides an effective means for IMRTdose distributions painting and sculpting. Clinical IMRT treatment plans are currently made using dose-based optimization algorithms, which do not consider the nonlinear dose\\u2013volume effects for tumours and normal structures. The choice of structure specific importance factors represents an additional degree of freedom of the system and makes rigorous optimization intractable. The purpose of this work is to circumvent the two problems by developing a biologically more sensible yet clinically practical inverse planning framework. To implement this, the dose\\u2013volume status of a structure was characterized by using the effective volume in the voxel domain. A new objective function was constructed with the incorporation of the volumetric information of the system so that the figure of merit of a given IMRT plan depends not only on the dose deviation from the desired distribution but also the dose\\u2013volume status of the involved organs. The conventional importance factor of an organ was written into a product of two components: (i) a generic importance that parametrizes the relative importance of the organs in the ideal situation when the goals for all the organs are met; (ii) a dose-dependent factor that quantifies our level of clinical/dosimetric satisfaction for a given plan. The generic importance can be determined a priori, and in most circumstances, does not need adjustment, whereas the second one, which is responsible for the intractable behaviour of the trade-off seen in conventional inverse planning, was determined automatically. An inverse planning module based on the proposed formalism was implemented and applied to a prostate case and a head\\u2013neck case. A comparison with the conventional inverse planning technique indicated that, for the same target dose coverage, the critical structure sparing was substantially improved for both cases. The incorporation of clinical knowledge allows us to obtain better IMRT plans and makes it possible to auto-select the importance factors, greatly facilitating the inverse planning process. The new formalism proposed also reveals the relationship between different inverse planning schemes and gives important insight into the problem of therapeutic plan optimization. In particular, we show that the EUD-based optimization is a special case of the general inverse planning formalism described in this paper. Abstract Purpose: The purpose of this work was to apply the method of constrained least-squares to inverse treatment planning and to explore its potential for providing a fast interactive planning environment for intensity-modulated radiation therapy (IMRT). Methods and Materials: The description of the dose inside a patient is a linear matrix transformation of beamlet weights. The constrained least-squares method adds additional matrix operators and produces beamlet weights by a direct linear transformation. These matrix operators contain a priori knowledge about the radiation distribution. The constrained least-squares technique was applied to obtain IMRT plans for prostate and paraspinal cancer patients and compared with the corresponding plans optimized using the CORVUS inverse planning system. Results: It was demonstrated that a constrained least-squares technique is suitable for IMRT plan optimization with significantly increased computing speed. For the two cases we have tested, the constrained least-squares method was an order of magnitude faster than conventional iterative techniques because of the avoidance of the iterative calculations. We also found that the constrained least-squares method is capable of generating clinically acceptable treatment plans with less trial-and-error adjustments of system variables, and with improved target volume coverage as well as sensitive structure sparing in comparison with that obtained using CORVUS. Conclusions: The constrained least-squares method has the advantage that it does not require iterative calculation and thus significantly speeds up the therapeutic plan optimization process. Besides shedding important insight into the inverse planning problem, the technique has strong potential to provide a fast and interactive environment for IMRT treatment planning. Purpose: A new treatment scheme coined as dense angularly sampled and sparse intensity modulated radiation therapy (DASSIM-RT) has recently been proposed to bridge the gap between IMRT and VMAT. By increasing the angular sampling of radiation beams while eliminating dispensable segments of the incident fields, DASSIM-RT is capable of providing improved conformity in dose distributions while maintaining high delivery efficiency. The fact that DASSIM-RT utilizes a large number of incident beams represents a major computational challenge for the clinical applications of this powerful treatment scheme. The purpose of this work is to provide a practical solution to the DASSIM-RT inverse planning problem. Methods: The inverse planning problem is formulated as a fluence-map optimization problem with total-variation (TV) minimization. A newly released L1-solver, template for first-order conic solver (TFOCS), was adopted in this work. TFOCS achieves faster convergence with less memory usage as compared with conventional quadratic programming (QP) for the TV form through the effective use of conic forms, dual-variable updates, and optimal first-order approaches. As such, it is tailored to specifically address the computational challenges of large-scale optimization in DASSIM-RT inverse planning. Two clinical cases (a prostate and a head and neck case) are used to evaluate the effectiveness and efficiency of the proposed planning technique. DASSIM-RT plans with 15 and 30 beams are compared with conventional IMRT plans with 7 beams in terms of plan quality and delivery efficiency, which are quantified by conformation number (CN), the total number of segments and modulation index, respectively. For optimization efficiency, the QP-based approach was compared with the proposed algorithm for the DASSIM-RT plans with 15 beams for both cases. Results: Plan quality improves with an increasing number of incident beams, while the total number of segments is maintained to be about the same in both cases. For the prostate patient, the conformation number to the target was 0.7509, 0.7565, and 0.7611 with 80 segments for IMRT with 7 beams, and DASSIM-RT with 15 and 30 beams, respectively. For the head and neck (HN) patient with a complicated target shape, conformation numbers of the three treatment plans were 0.7554, 0.7758, and 0.7819 with 75 segments for all beam configurations. With respect to the dose sparing to the critical structures, the organs such as the femoral heads in the prostate case and the brainstem and spinal cord in the HN case were better protected with DASSIM-RT. For both cases, the delivery efficiency has been greatly improved as the beam angular sampling increases with the similar or better conformal dose distribution. Compared with conventional quadratic programming approaches, first-order TFOCS-based optimization achieves far faster convergence and smaller memory requirements in DASSIM-RT. Conclusions: The new optimization algorithm TFOCS provides a practical and timely solution to the DASSIM-RT or other inverse planning problem requiring large memory space. The new treatment scheme is shown to outperform conventional IMRT in terms of dose conformity to both the targetand the critical structures, while maintaining high delivery efficiency. Four-dimensional computed tomography (4DCT) offers an extra dimension of 'time' on the three-dimensional patient model with which we can incorporate target motion in radiation treatment (RT) planning and delivery in various ways such as in the concept of internal target volume, in gated treatment or in target tracking. However, for all these methodologies, different phases are essentially considered as non-interconnected independent phases for the purpose of optimization, in other words, the 'time' dimension has yet to be incorporated explicitly in the optimization algorithm and fully exploited. In this note, we have formulated a new 4D inverse planning technique that treats all the phases in the 4DCT as one single entity in the optimization. The optimization is formulated as a quadratic problem for disciplined convex programming that enables the problem to be analyzed and solved efficiently. In the proof-of-principle examples illustrated, we show that the temporal information of the spatial relation of the target and organs at risk could be 'exchanged' amongst different phases so that an appropriate weighting of dose deposition could be allocated to each phase, thus enabling a treatment with a tight target margin and a full duty cycle otherwise not achievable by either of the aforementioned methodologies. Yet there are practical issues to be solved in the 4D RT planning and delivery. The 4D concept in the optimization we have formulated here does provide insight on how the 'time' dimension can be exploited in the 4D optimization process. Purpose: Understanding the kinetics of tumor growth/shrinkage represents a critical step in quantitative assessment of therapeutics and realization of adaptive radiation therapy. This article presents a novel framework for image-based modeling of tumor change and demonstrates its performance with synthetic images and clinical cases. Methods: Due to significant tumor tissue content changes, similarity-based models are not suitable for describing the process of tumor volume changes. Under the hypothesis that tissue features in a tumor volume or at the boundary region are partially preserved, the kinetic change was modeled in two steps: (1) Autodetection of homologous tissue features shared by two input images using the scale invariance feature transformation (SIFT) method; and (2) establishment of a voxel-to-voxel correspondence between the images for the remaining spatial points by interpolation. The correctness of the tissue feature correspondence was assured by a bidirectional association procedure, where SIFT features were mapped from template to target images and reversely. A series of digital phantom experiments and five head and neck clinical cases were used to assess the performance of the proposed technique. Results: The proposed technique can faithfully identify the known changes introduced when constructing the digital phantoms. The subsequent feature-guided thin plate spline calculation reproducedmore\\u00a0\\u00bb the ''ground truth'' with accuracy better than 1.5 mm. For the clinical cases, the new algorithm worked reliably for a volume change as large as 30%. Conclusions: An image-based tumor kinetic algorithm was developed to model the tumor response to radiation therapy. The technique provides a practical framework for future application in adaptive radiation therapy.\\u00ab\\u00a0less Intensity modulated arc therapy (IMAT) delivers conformal dose distributions through continuous gantry rotation with constant or variable speed while modulating the field aperture shape and weight. The enlarged angular space and machine delivery constraints make inverse planning of IMAT more intractable as compared to its counterpart of fixed gantry IMRT. Currently, IMAT inverse planning is being done using two extreme methods: the first one computes in beamlet domain with a subsequent arc leaf sequencing, and the second proceeds in machine parameter domain with entire emphasis placed on a pre-determined delivery method without exploring potentially better alternative delivery schemes. Towards truly optimizing the IMAT treatment on a patient specific basis, in this work we propose a total-variation based inverse planning framework for IMAT, which takes advantage of the useful features of the above two existing approaches while avoiding their shortcomings. A quadratic optimization algorithm has been implem... Respiratory motion artifacts in radionuclide imaging can substantially increase the apparent volume of malignant lesions, and result in reduced activity and signal-to-noise ratios (SNRs) within the tumor region. We present a corrective algorithm, coined retrospective stacking (RS), that combines retrospective amplitude-based binning of data acquired in small time intervals, with rigid or deformable image registration methods. Retrospective stacking is first applied to numerically simulated radionuclide images of a lesion moving with regular and irregular linear motion, as well as hysteresis characteristic of tumors near the lung. The dependence of RS on spatial and temporal resolution is explored, by comparing cross-section visualizations, activity profiles, and SNRs of retrospectively stacked images with those from a simulated motionless lesion. The simulation results are subsequently validated with a phantom positron emission tomography experiment representing a hot lesion oscillating within a warm background. It is seen that by sufficiently reducing the data acquisition timestep, RS can restore the lesion image to nearly its original shape, intensity and SNR, even under noisy conditions. Simultaneous imaging of multiple probes or biomarkers represents a critical step toward high specificity molecular imaging. In this work, we propose to utilize the element-specific nature of the X-ray fluorescence (XRF) signal for imaging multiple elements simultaneously (multiplexing) using XRF computed tomography (XFCT). A 5-mm-diameter pencil beam produced by a polychromatic X-ray source (150 kV, 20 mA) was used to stimulate emission of XRF photons from 2% (weight/volume) gold (Au), gadolinium (Gd), and barium (Ba) embedded within a water phantom. The phantom was translated and rotated relative to the stationary pencil beam in a first-generation CT geometry. The X-ray energy spectrum was collected for 18 s at each position using a cadmium telluride detector. The spectra were then used to isolate the K shell XRF peak and to generate sinograms for the three elements of interest. The distribution and concentration of the three elements were reconstructed with the iterative maximum likelihood expectation maximization algorithm. The linearity between the XFCT intensity and the concentrations of elements of interest was investigated. We found that measured XRF spectra showed sharp peaks characteristic of Au, Gd, and Ba. The narrow full-width at half-maximum (FWHM) of the peaks strongly supports the potential of XFCT for multiplexed imaging of Au, Gd, and Ba (FWHMAu,K\\u03b11 = 0.619 keV, FWHMAu,K\\u03b12=1.371 keV , FWHMGd,K\\u03b1=1.297 keV, FWHMGd,K\\u03b2=0.974 keV , FWHMBa,K\\u03b1=0.852 keV, and FWHMBa,K\\u03b2=0.594 keV ). The distribution of Au, Gd, and Ba in the water phantom was clearly identifiable in the reconstructed XRF images. Our results showed linear relationships between the XRF intensity of each tested element and their concentrations (R2Au=0.944 , RGd2=0.986, and RBa2=0.999), suggesting that XFCT is capable of quantitative imaging. Finally, a transmission CT image was obtained to show the potential of the approach for providing attenuation correction and morphological information. In conclusion, XFCT is a promising modality for multiplexed imaging of high atomic number probes. Purpose To report a tissue feature-based image registration strategy with explicit inclusion of the differential motions of thoracic structures. Methods and Materials The proposed technique started with auto-identification of a number of corresponding points with distinct tissue features. The tissue feature points were found by using the scale-invariant feature transform method. The control point pairs were then sorted into different \\u201ccolors\\u201d according to the organs in which they resided and used to model the involved organs individually. A thin-plate spline method was used to register a structure characterized by the control points with a given \\u201ccolor.\\u201d The proposed technique was applied to study a digital phantom case and 3 lung and 3 liver cancer patients. Results For the phantom case, a comparison with the conventional thin-plate spline method showed that the registration accuracy was markedly improved when the differential motions of the lung and chest wall were taken into account. On average, the registration error and standard deviation of the 15 points against the known ground truth were reduced from 3.0 to 0.5 mm and from 1.5 to 0.2 mm, respectively, when the new method was used. A similar level of improvement was achieved for the clinical cases. Conclusion The results of our study have shown that the segmented deformable approach provides a natural and logical solution to model the discontinuous organ motions and greatly improves the accuracy and robustness of deformable registration. In current inverse planning algorithms it is common to treat all voxels within a target or sensitive structure equally and use structure specific prescriptions and weighting factors as system parameters. In reality, the voxels within a structure are not identical in complying with their dosimetric goals and there exists strong intrastructural competition. Inverse planning objective function should not only balance the competing objectives of different structures but also that of the individual voxels in various structures. In this work we propose to model the intrastructural tradeoff through the modulation of voxel-dependent importance factors and deal with the challenging problem of how to obtain a sensible set of importance factors with a manageable amount of computing. Instead of letting the values of voxel-dependent importance to vary freely during the search process, an adaptive algorithm, in which the importance factors were tied to the local radiation doses through a heuristically constructed relation, was developed. It is shown that the approach is quite general and the EUD-based optimization is a special case of the proposed framework. The new planning tool was applied to study a hypothetical phantom case and a prostate case. Comparison of the results with that obtained using conventional inverse planning technique with structure specific importance factors indicated that the dose distributions from the conventional inverse planning are at best suboptimal and can be significantly improved with the help of the proposed nonuniform penalty scheme. Kilovotage cone-beam computed tomography (kV-CBCT) has shown potentials to improve the accuracy of a patient setup in radiotherapy. However, daily and repeated use of CBCT will deliver high extra radiation doses to patients. One way to reduce the patient dose is to lower mAs when acquiring projection data. This, however, degrades the quality of low mAs CBCT images dramatically due to excessive noises. In this work, we aim to improve the CBCT image quality from low mAs scans. Based on the measured noise properties of the sinogram, a penalized weighted least-squares (PWLS) objective function was constructed, and the ideal sinogram was then estimated by minimizing the PWLS objection function. To preserve edge information in the projection data, an anisotropic penalty term was designed using the intensity difference between neighboring pixels. The effectiveness of the presented algorithm was demonstrated by two experimental phantom studies. Noise in the reconstructed CBCT image acquired with a low mAs protocol was greatly suppressed after the proposed sinogram domain image processing, without noticeable sacrifice of the spatial resolution. Purpose: Radiation therapy with high dose rate and flattening filter-free (FFF) beams has the potential advantage of greatly reduced treatment time and out-of-field dose. Current inverse planning algorithms are, however, not customized for beams with nonuniform incident profiles and the resultant IMRT plans are often inefficient in delivery. The authors propose a total-variation regularization (TVR)-based formalism by taking the inherent shapes of incident beam profiles into account. Methods: A novel TVR-based inverse planning formalism is established for IMRT with nonuniform beam profiles. The authors introduce a TVR term into the objective function, which encourages piecewise constant fluence in the nonuniform FFF fluence domain. The proposed algorithm is applied to lung and prostate and head and neck cases and its performance is evaluated by comparing the resulting plans to those obtained using a conventional beamlet-based optimization (BBO). Results: For the prostate case, the authors' algorithm produces acceptable dose distributions with only 21 segments, while the conventional BBO requires 114 segments. For the lung case and the head and neck case, the proposed method generates similar coverage of target volume and sparing of the organs-at-risk as compared to BBO, but with a markedly reduced segment number. Conclusions: TVR-based optimization in nonflat beammore\\u00a0\\u00bb domain provides an effective way to maximally leverage the technical capacity of radiation therapy with FFF fields. The technique can generate effective IMRT plans with improved dose delivery efficiency without significant deterioration of the dose distribution.\\u00ab\\u00a0less The success of an IMRT treatment relies on the positioning accuracy of the MLC (multileaf collimator) leaves for both step-and-shoot and dynamic deliveries. In practice, however, there exists no effective and quantitative means for routine MLC QA and this has become one of the bottleneck problems in IMRT implementation. In this work we present an electronic portal image device (EPID) based method for fast and accurate measurement of MLC leaf positions at arbitrary locations within the 40 cm \\u00d7 40 cm radiation field. The new technique utilizes the fact that the integral signal in a small region of interest (ROI) is a sensitive and reliable indicator of the leaf displacement. In this approach, the integral signal at a ROI was expressed as a weighted sum of the contributions from the displacements of the leaf above the point and the adjacent leaves. The weighting factors or linear coefficients of the system equations were determined by fitting the integral signal data for a group of pre-designed MLC leaf sequences to the known leaf displacements that were intentionally introduced during the creation of the leaf sequences. Once the calibration is done, the system can be used for routine MLC leaf positioning QA to detect possible leaf errors. A series of tests was carried out to examine the functionality and accuracy of the technique. Our results show that the proposed technique is potentially superior to the conventional edge-detecting approach in two aspects: (i) it deals with the problem in a systematic approach and allows us to take into account the influence of the adjacent MLC leaves effectively; and (ii) it may improve the signal-to-noise ratio and is thus capable of quantitatively measuring extremely small leaf positional displacements. Our results indicate that the technique can detect a leaf positional error as small as 0.1 mm at an arbitrary point within the field in the absence of EPID set-up error and 0.3 mm when the uncertainty is considered. Given its simplicity, efficiency and accuracy, we believe that the technique is ideally suitable for routine MLC leaf positioning QA. Purpose: The streak artifacts caused by metal implants have long been recognized as a problem that limits various applications of CT imaging. In this work, the authors propose an iterative metal artifact reduction algorithm based on constrained optimization.  Methods: After the shape and location of metal objects in the image domain is determined automatically by the binary metal identification algorithm and the segmentation of \\u201cmetal shadows\\u201d in projection domain is done, constrained optimization is used for image reconstruction. It minimizes a predefined function that reflects a priori knowledge of the image, subject to the constraint that the estimated projection data are within a specified tolerance of the available metal-shadow-excluded projection data, with image non-negativity enforced. The minimization problem is solved through the alternation of projection-onto-convex-sets and the steepest gradient descent of the objective function. The constrained optimization algorithm is evaluated with a penalized smoothness objective.  Results: The study shows that the proposed method is capable of significantly reducing metal artifacts, suppressing noise, and improving soft-tissue visibility. It outperforms the FBP-type methods and ART and EM methods and yields artifacts-free images.  Conclusions: Constrained optimization is an effective way to deal with CT reconstruction with embedded metal objects. Although the method is presented in the context of metal artifacts, it is applicable to general \\u201cmissing data\\u201d image reconstruction problems. Many image registration algorithms rely on the use of homologous control points on the two input image sets to be registered. In reality, the interactive identification of the control points on both images is tedious, difficult, and often a source of error. We propose a two-step algorithm to automatically identify homologous regions that are used as a priori information during the image registration procedure. First, a number of small control volumes having distinct anatomical features are identified on the model image in a somewhat arbitrary fashion. Instead of attempting to find their correspondences in the reference image through user interaction, in the proposed method, each of the control regions is mapped to the corresponding part of the reference image by using an automated image registration algorithm. A normalized cross-correlation (NCC) function or mutual information was used as the auto-mapping metric and a limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm (L-BFGS) was employed to optimize the function to find the optimal mapping. For rigid registration, the transformation parameters of the system are obtained by averaging that derived from the individual control volumes. In our deformable calculation, the mapped control volumes are treated as the nodes or control points with known positions on the twomore\\u00a0\\u00bb images. If the number of control volumes is not enough to cover the whole image to be registered, additional nodes are placed on the model image and then located on the reference image in a manner similar to the conventional BSpline deformable calculation. For deformable registration, the established correspondence by the auto-mapped control volumes provides valuable guidance for the registration calculation and greatly reduces the dimensionality of the problem. The performance of the two-step registrations was applied to three rigid registration cases (two PET-CT registrations and a brain MRI-CT registration) and one deformable registration of inhale and exhale phases of a lung 4D CT. Algorithm convergence was confirmed by starting the registration calculations from a large number of initial transformation parameters. An accuracy of {approx}2 mm was achieved for both deformable and rigid registration. The proposed image registration method greatly reduces the complexity involved in the determination of homologous control points and allows us to minimize the subjectivity and uncertainty associated with the current manual interactive approach. Patient studies have indicated that the two-step registration technique is fast, reliable, and provides a valuable tool to facilitate both rigid and nonrigid image registrations.\\u00ab\\u00a0less Positron emission tonography (PET) is useful in diagnosis and radiation treatment planning for a variety of cancers. For patients with cancers in thoracic or upper abdominal region, the respiratory motion produces large distortions in the tumor shape and size, affecting the accuracy in both diagnosis and treatment. Four-dimensional (4D) (gated) PET aims to reduce the motion artifacts and to provide accurate measurement of the tumor volume and the tracer concentration. A major issue in 4D PET is the lack of statistics. Since the collected photons are divided into several frames in the 4D PET scan, the quality of each reconstructed frame degrades as the number of frames increases. The increased noise in each frame heavily degrades the quantitative accuracy of the PETimaging. In this work, we propose a method to enhance the performance of 4D PET by developing a new technique of 4D PETreconstruction with incorporation of an organ motion model derived from 4D-CT images. The method is based on the well-known maximum-likelihood expectation-maximization (ML-EM) algorithm. During the processes of forward- and backward-projection in the ML-EM iterations, all projection data acquired at different phases are combined together to update the emission map with the aid of deformable model, the statistics is therefore greatly improved. The proposed algorithm was first evaluated with computer simulations using a mathematical dynamic phantom. Experiment with a moving physical phantom was then carried out to demonstrate the accuracy of the proposed method and the increase of signal-to-noise ratio over three-dimensional PET. Finally, the 4D PETreconstruction was applied to a patient case. On-board cone-beam computed tomography (CBCT) has recently become available to provide volumetric information of a patient in the treatment position, and holds promises for improved target localization and irradiation dose verification. The design of currently available on-board CBCT, however, is far from optimal. Its quality is adversely influenced by many factors, such as scatter, beam hardening, and intra-scanning organ motion. In this work we quantitatively study the influence of organ motion on CBCT imaging and investigate a strategy to acquire high quality phase-resolved [four-dimensional (4D)] CBCT images based on phase binning of the CBCT projection data. An efficient and robust method for binning CBCT data according to the patient's respiratory phase derived in the projection space was developed. The phase-binned projections were reconstructed using the conventional Feldkamp algorithm to yield 4D CBCT images. Both phantom and patient studies were carried out to validate the technique and to optimize the 4D CBCT data acquisition protocol. Several factors that are important to the clinical implementation of the technique, such as the image quality, scanning time, number of projections, and radiation dose, were analyzed for various scanning schemes. The general references drawn from this study are: (i) reliable phase binning of CBCT projectionsmore\\u00a0\\u00bb is accomplishable with the aid of external or internal marker and simple analysis of its trace in the projection space, and (ii) artifact-free 4D CBCT images can be obtained without increasing the patient radiation dose as compared to the current 3D CBCT scan.\\u00ab\\u00a0less Purpose: X-ray scatter results in a significant degradation of image quality in computed tomography (CT), representing a major limitation in cone-beam CT (CBCT) and large field-of-view diagnostic scanners. In this work, a novel scatter estimation and correction technique is proposed that utilizes peripheral detection of scatter during the patient scan to simultaneously acquire image and patient-specific scatter information in a single scan, and in conjunction with a proposed compressed sensing scatter recovery technique to reconstruct and correct for the patient-specific scatter in the projection space.  Methods: The method consists of the detection of patient scatter at the edges of the field of view (FOV) followed by measurement based compressed sensing recovery of the scatter through-out the projection space. In the prototype implementation, the kV x-ray source of the Varian TrueBeam OBI system was blocked at the edges of the projection FOV, and the image detector in the corresponding blocked region was used for scatter detection. The design enables image data acquisition of the projection data on the unblocked central region of and scatter data at the blocked boundary regions. For the initial scatter estimation on the central FOV, a prior consisting of a hybrid scatter model that combines the scatter interpolation method and scatter convolution model is estimated using the acquired scatter distribution on boundary region. With the hybrid scatter estimation model, compressed sensing optimization is performed to generate the scatter map by penalizing the L1 norm of the discrete cosine transform of scatter signal. The estimated scatter is subtracted from the projection data by soft-tuning, and the scatter-corrected CBCT volume is obtained by the conventional Feldkamp-Davis-Kress algorithm. Experimental studies using image quality and anthropomorphic phantoms on a Varian TrueBeam system were carried out to evaluate the performance of the proposed scheme.  Results: The scatter shading artifacts were markedly suppressed in the reconstructed images using the proposed method. On the Catphan\\u00a9504 phantom, the proposed method reduced the error of CT number to 13 Hounsfield units, 10% of that without scatter correction, and increased the image contrast by a factor of 2 in high-contrast regions. On the anthropomorphic phantom, the spatial nonuniformity decreased from 10.8% to 6.8% after correction.  Conclusions: A novel scatter correction method, enabling unobstructed acquisition of the high frequency image data and concurrent detection of the patient-specific low frequency scatter data at the edges of the FOV, is proposed and validated in this work. Relative to blocker based techniques, rather than obstructing the central portion of the FOV which degrades and limits the image reconstruction, compressed sensing is used to solve for the scatter from detection of scatter at the periphery of the FOV, enabling for the highest quality reconstruction in the central region and robust patient-specific scatter correction. Recent advances in radiation delivery techniques, such as intensity-modulated radiation therapy, provide unprecedented ability to exquisitely control three-dimensional dose distribution. Development of on-board imaging and other image-guidance methods significantly improved our ability to better target a radiation beam to the tumor volume. However, in reality, accurate definition of the location and boundary of the tumor target is still problematic. Biologic and physiologic imaging promises to solve the problem in a fundamental way and has a more and more important role in patient staging, treatment planning, and therapeutic assessment in radiation therapy clinics. The last decade witnessed a dramatic increase in the use of positron emission tomography and computed tomography in radiotherapy practice. To ensure safe and effective use of nuclide imaging, a rigorous quality assurance (QA) protocol of the imaging tools and integration of the imaging data must be in place. The application of nuclide imaging in radiation oncology occurs at different levels of sophistication. Quantitative use of the imaging data in treatment planning through image registration and standardized uptake value calculation is often involved. Thus, QA should not be limited to the performance of the scanner, but should also include the process of implementing image data in treatment planning, such as data transfer, image registration, and quantitation of data for delineation of tumors and sensitive structures. This presentation discusses various aspects of nuclide imaging as applied to radiotherapy and describes the QA procedures necessary for the success of biologic image-guided radiation therapy. Abstract Purpose: Selection of beam configuration in currently available intensity-modulated radiotherapy (IMRT) treatment planning systems is still based on trial-and-error search. Computer beam orientation optimization has the potential to improve the situation, but its practical implementation is hindered by the excessive computing time associated with the calculation. The purpose of this work is to provide an effective means to speed up the beam orientation optimization by incorporating a priori geometric and dosimetric knowledge of the system and to demonstrate the utility of the new algorithm for beam placement in IMRT. Methods and Materials: Beam orientation optimization was performed in two steps. First, the quality of each possible beam orientation was evaluated using beam's-eye-view dosimetrics (BEVD) developed in our previous study. A simulated annealing algorithm was then employed to search for the optimal set of beam orientations, taking into account the BEVD scores of different incident beam directions. During the calculation, sampling of gantry angles was weighted according to the BEVD score computed before the optimization. A beam direction with a higher BEVD score had a higher probability of being included in the trial configuration, and vice versa. The inclusion of the BEVD weighting in the stochastic beam angle sampling process made it possible to avoid spending valuable computing time unnecessarily at \\\"bad\\\" beam angles. An iterative inverse treatment planning algorithm was used for beam intensity profile optimization during the optimization process. The BEVD-guided beam orientation optimization was applied to an IMRT treatment of paraspinal tumor. The advantage of the new optimization algorithm was demonstrated by comparing the calculation with the conventional scheme without the BEVD weighting in the beam sampling. Results: The BEVD tool provided useful guidance for the selection of the potentially good directions for the beams to incident and was used to guide the search for the optimal beam configuration. The BEVD-guided sampling improved both optimization speed and convergence of the calculation. A comparison of several five-field IMRT treatment plans obtained with and without BEVD guidance indicated that the computational efficiency was increased by a factor of \\u223c10. Conclusion: Incorporation of BEVD information allows for development of a more robust tool for beam orientation optimization in IMRT planning. It enables us to more effectively use the angular degree of freedom in IMRT without paying the excessive computing overhead and brings us one step closer to the goal of automated selection of beam orientations in a clinical environment. X-ray fluorescence computed tomography (XFCT) imaging has been focused on the detection of K-shell X-rays. The potential utility of L-shell x-ray XFCT is, however, not well studied. Here we report the first Monte Carlo (MC) simulation of preclinical L-shell XFCT imaging of Cisplatin. We built MC models for both L- and K-shell XFCT with different excitation energies (15 and 30 keV for L-shell and 80 keV for K-shell XFCT). Two small-animal sized imaging phantoms of 2-cm and 4-cm diameter containing a series of objects of 0.6 to 2.7 mm in diameter at 0.7 to 16 mm depths with 10 to 250 \\u03bcg/mL concentrations of Pt are used in the study. Transmitted and scattered x-rays were collected with photon-integrating transmission detector and photon-counting detector arc, respectively. Collected data were rearranged into XFCT and transmission CT sinograms for image reconstruction. XFCT images were reconstructed with filtered back-projection (FBP) and with iterative maximum-likelihood expectation maximization (ML-EM) without and with attenuation correction. While K-shell XFCT was capable of providing accurate measurement of Cisplatin concentration, its sensitivity was 4.4 and 3.0 times lower than that of L-shell XFCT with 15 keV excitation beam for the 2-cm and 4-cm diameter phantom, respectively. With inclusion of excitation and fluorescence beam attenuation correction, we found that L-shell XFCT was capable of providing fairly accurate information of Cisplatin concentration distribution. With a dose of 29 and 58 mGy, clinically relevant Cisplatin Pt concentrations of 10 \\u03bcg/mg could be imaged with L-shell XFCT inside a 2-cm and 4-cm diameter object, respectively. A leaf-setting algorithm is developed for generating arbitrary beam intensity profiles in discrete levels using dynamic multileaf collimators (DMLCs). The algorithm starts with the algebraic expression for the area under the beam profile. It is shown that the coefficients in this expression can be transformed into the specifications for the leaf-setting sequence. It is proven that the algorithm optimizes beam delivery time and total monitor units for the DMLC leaf setting for intensity modulated radiotherapy (IMRT). The algorithm is demonstrated to be applicable to both the 'step-and-shoot' and 'dynamic' type of beam delivery. The graphical interpretation and numerical implementation scheme of the algorithm is illustrated using a simplified example. MRI significantly improves the accuracy and reliability of target delineation in radiation therapy for certain tumors due to its superior soft tissue contrast compared to CT. A treatment planning process with MRI as the sole imaging modality will eliminate systematic CT/MRI co-registration errors, reduce cost and radiation exposure, and simplify clinical workflow. However, MRI lacks the key electron density information necessary for accurate dose calculation and generating reference images for patient setup. The purpose of this work is to develop a unifying method to derive electron density from standard T1-weighted MRI. We propose to combine both intensity and geometry information into a unifying probabilistic Bayesian framework for electron density mapping. For each voxel, we compute two conditional probability density functions (PDFs) of electron density given its: (1) T1-weighted MRI intensity, and (2) geometry in a reference anatomy, obtained by deformable image registration between the MRI of the atlas and test patient. The two conditional PDFs containing intensity and geometry information are combined into a unifying posterior PDF, whose mean value corresponds to the optimal electron density value under the mean-square error criterion. We evaluated the algorithm's accuracy of electron density mapping and its ability to detect bone in the head for eight patients, using an additional patient as the atlas or template. Mean absolute HU error between the estimated and true CT, as well as receiver operating characteristics for bone detection (HU > 200) were calculated. The performance was compared with a global intensity approach based on T1 and no density correction (set whole head to water). The proposed technique significantly reduced the errors in electron density estimation, with a mean absolute HU error of 126, compared with 139 for deformable registration (p = 2 \\u00d7 10\\u22124), 283 for the intensity approach (p = 2 \\u00d7 10\\u22126) and 282 without density correction (p = 5 \\u00d7 10\\u22126). For 90% sensitivity in bone detection, the proposed method achieved a specificity of 86%, compared with 80, 11 and 10% using deformable registration, intensity and without density correction, respectively. Notably, the Bayesian approach was more robust against anatomical differences between patients, with a specificity of 62% in the worst case (patient), compared to 30% specificity in registration-based approach. In conclusion, the proposed unifying Bayesian method provides accurate electron density estimation and bone detection from MRI of the head with highly heterogeneous anatomy. Three types of iterative algorithms, algebraic inverse treatment planning (AITP), simultaneous iterative inverse treatment planning (SIITP), and iterative least-square inverse treatment planning (ILSITP), differentiated according to their updating sequences, were generalized to three dimension with true beam geometry and dose model. A rapid ray-tracing approach was developed to optimize the primary beam components. Instead of recalculating the dose matrix at each iteration, the dose distribution was generated by scaling up or down the dose matrix elements of the previous iteration. This significantly increased the calculation speed. The iterative algorithms started with an initial intensity profile for each beam, specified by a two-dimensional pixel beam map of M elements. The calculation volume was divided into N voxels, and the calculation was done by repeatedly comparing the calculated and desired doses and adjusting the values of the beam map elements to minimize an objective function. In AITP, the iteration is performed voxel by voxel. For each voxel, the dose discrepancy was evaluated and the contributing pencil beams were updated. In ILSITP and SIITP, the iteration proceeded pencil beam by pencil beam instead of voxel by voxel. In all cases, the iteration procedure was repeated until the best possible dose distribution was achieved. The algorithms were applied to two examples and the results showed that the iterative techniques were able to produce superior isodose distributions. On-board imager (OBI) based cone-beam computed tomography (CBCT) has become available in radiotherapy clinics to accurately identify the target in the treatment position. However, due to the relatively slow gantry rotation (typically about 60 s for a full 360\\u00b0 scan) in acquiring the CBCT projection data, the patient's respiratory motion causes serious problems such as blurring, doubling, streaking and distortion in the reconstructed images, which heavily degrade the image quality and the target localization. In this work, we present a motion compensation method for slow-rotating CBCT scans by incorporating into image reconstruction a patient-specific motion model, which is derived from previously obtained four-dimensional (4D) treatment planning CT images of the same patient via deformable registration. The registration of the 4D CT phases results in transformations representing a temporal sequence of three-dimensional (3D) deformation fields, or in other words, a 4D model of organ motion. The algorithm was developed heuristically in two-dimensional (2D) parallel-beam geometry and extended to 3D cone-beam geometry. By simulations with digital phantoms capable of translational motion and other complex motion, we demonstrated that the algorithm can reduce the motion artefacts locally, and restore the tumour size and shape, which may thereby improve the accuracy of target localization and patient positioning when CBCT is used as the treatment guidance. Scatter correction is crucial to the quality of reconstructed images in x-raycone-beam computed tomography(CBCT). Most of existing scatter correction methods assume smooth scatter distributions. The high-frequency scatternoise remains in the projection images even after a perfect scatter correction. In this paper, using a clinical CBCT system and a measurement-based scatter correction, the authors show that a scatter correction alone does not provide satisfactory image quality and the loss of the contrast-to-noise ratio(CNR) of the scatter corrected image may overwrite the benefit of scatter removal. To circumvent the problem and truly gain from scatter correction, an effective scatternoise suppression method must be in place. They analyze the noise properties in the projections after scatter correction and propose to use a penalized weighted least-squares (PWLS) algorithm to reduce the noise in the reconstructed images. Experimental results on an evaluation phantom (Catphan\\u00a9600) show that the proposed algorithm further reduces the reconstruction error in a scatter corrected image from 10.6% to 1.7% and increases the CNR by a factor of 3.6. Significant image quality improvement is also shown in the results on an anthropomorphic phantom, in which the global noise level is reduced and the local streaking artifacts around bones are suppressed. Purpose: To obtain on-treatment volumetric patient anatomy during respiratory gated volumetric modulated arc therapy (VMAT). Methods: On-board imaging device integrated with Linacs offers a viable tool for obtaining patient anatomy during radiation treatment delivery. In this study, the authors acquired beam-level kV images during gated VMAT treatments using a Varian TrueBeam\\u2122STx Linac. These kV projection images are triggered by a respiratory gating signal and can be acquired immediately before treatment MV beam on at every breathing cycle during delivery. Because the kV images are acquired with an on-board imaging device during a rotational arc therapy, they provide the patient anatomical information from many different angles or projection views (typically 20\\u201340). To reconstruct the volumetric image representing patient anatomy during the VMAT treatment, the authors used a compressed sensing method with a fast first-order optimization algorithm. The conventional FDK reconstruction was also used for comparison purposes. The method was tested on a dynamic anthropomorphic physical phantom as well as a lung patient. Results: The reconstructed volumetric images for a dynamic anthropomorphic physical phantom and a lung patient showed clearly visible soft-tissue target as well as other anatomical structures, with the proposed compressed sensing-based image reconstruction method. Compared with FDK, the compressed sensing method leads to a \\u223ctwo and threefold increase in contrast-to-noise ratio around the target area in the phantom and patient case, respectively. Conclusions: The proposed technique provides on-treatment volumetric patient anatomy, with only a fraction (<10%) of the imaging dose used in conventional CBCT procedures. This anatomical information may be valuable for geometric verification and treatment guidance, and useful for verification of treatment dose delivery, accumulation, and adaptation in the future. The purpose of this work is to develop a novel feature-based registration strategy to automatically map the rectal contours from planning computed tomography (CT) (pCT) to cone beam CT (CBCT). The rectal contours were manually outlined on the pCT. A narrow band with the outlined contour as its interior surface was then constructed, so that we can exclude the volume inside the rectum in the registration process. The corresponding contour in the CBCT was found by using a feature-based registration algorithm, which consists of two steps: (1) automatically searching for control points in the pCT and CBCT based on the features of the surrounding tissue and matching the homologous control points using the scale invariance feature transformation; and (2) using the control points for a thin plate spline transformation to warp the narrow band and mapping the corresponding contours from pCT to CBCT. The proposed contour propagation technique is applied to digital phantoms and clinical cases and, in all cases, the contour mapping results are found to be clinically acceptable. For clinical cases, the method yielded satisfactory results even when there were significant rectal content changes between the pCT and CBCT scans. As a consequence, the accordance between the rectal volumes after deformable registration and the manually segmented rectum was found to be more than 90%. The proposed technique provides a powerful tool for adaptive radiotherapy of prostate, rectal, and gynecological cancers in the future. Purpose: Metal artifact reduction has long been an important topic in x-ray CT image reconstruction. In this work, the authors propose an iterative method that sequentially minimizes a reweighted total variation (TV) of the image and produces substantially artifact-reduced reconstructions.  Methods: A sequentially reweighted TV minimization algorithm is proposed to fully exploit the sparseness of image gradients (IG). The authors first formulate a constrained optimization model that minimizes a weighted TV of the image, subject to the constraint that the estimated projection data are within a specified tolerance of the available projection measurements, with image non-negativity enforced. The authors then solve a sequence of weighted TV minimization problems where weights used for the next iteration are computed from the current solution. Using the complete projection data, the algorithm first reconstructs an image from which a binary metal image can be extracted. Forward projection of the binary image identifies metal traces in the projection space. The metal-free background image is then reconstructed from the metal-trace-excluded projection data by employing a different set of weights. Each minimization problem is solved using a gradient method that alternates projection-onto-convex-sets and steepest descent. A series of simulation and experimental studies are performed to evaluate the proposed approach.  Results: Our study shows that the sequentially reweighted scheme, by altering a single parameter in the weighting function, flexibly controls the sparsity of the IG and reconstructs artifacts-free images in a two-stage process. It successfully produces images with significantly reduced streak artifacts, suppressed noise and well-preserved contrast and edge properties.  Conclusions: The sequentially reweighed TV minimization provides a systematic approach for suppressing CT metal artifacts. The technique can also be generalized to other \\u201cmissing data\\u201d problems in CT image reconstruction. The purpose of this work is to develop a novel strategy to automatically map organ contours from one phase of respiration to all other phases on a four-dimensional computed tomography (4D CT). A region of interest (ROI) was manually delineated by a physician on one phase specific image set of a 4D CT. A number of cubic control volumes of the size of \\u223c 1 cm were automatically placed along the contours. The control volumes were then collectively mapped to the next phase using a rigid transformation. To accommodate organ deformation, a model-based adaptation of the control volume positions was followed after the rigid mapping procedure. This further adjustment of control volume positions was performed by minimizing an energy function which balances the tendency for the control volumes to move to their correspondences with the desire to maintain similar image features and shape integrity of the contour. The mapped ROI surface was then constructed based on the central positions of the control volumes using a triangulated surface construction technique. The proposed technique was assessed using a digital phantom and 4D CTimages of three lung patients. Our digital phantom study data indicated that a spatial accuracy better than 2.5 mm is achievable using the proposed technique. The patient study showed a similar level of accuracy. In addition, the computational speed of our algorithm was significantly improved as compared with a conventional deformable registration-based contour mapping technique. The robustness and accuracy of this approach make it a valuable tool for the efficient use of the available spatial-tempo information for 4D simulation and treatment. Purpose : X-rayscatter incurred to detectors degrades the quality of cone-beam computed tomography(CBCT) and represents a problem in volumetric image guided and adaptive radiation therapy. Several methods using a beam blocker for the estimation and subtraction of scatter have been proposed. However, due to missing information resulting from the obstruction of the blocker, such methods require dual scanning or dynamically moving blocker to obtain a complete volumetric image. Here, we propose a half beam blocker-based approach, in conjunction with a total variation (TV) regularized Feldkamp\\u2013Davis\\u2013Kress (FDK) algorithm, to correct scatter-induced artifacts by simultaneously acquiring image and scatter information from a single-rotation CBCT scan. Methods : A half beam blocker, comprising lead strips, is used to simultaneously acquire image data on one side of the projection data and scatter data on the other half side. One-dimensional cubic B-Spline interpolation/extrapolation is applied to derive patient specific scatter information by using the scatter distributions on strips. The estimated scatter is subtracted from the projection image acquired at the opposite view. With scatter-corrected projections where this subtraction is completed, the FDK algorithm based on a cosine weighting function is performed to reconstructCBCT volume. To suppress the noise in the reconstructedCBCTimages produced by geometric errors between two opposed projections and interpolatedscatter information, total variation regularization is applied by a minimization using a steepest gradient descent optimization method. The experimental studies using Catphan504 and anthropomorphic phantoms were carried out to evaluate the performance of the proposed scheme. Results : The scatter-induced shading artifacts were markedly suppressed in CBCT using the proposed scheme. Compared with CBCT without a blocker, the nonuniformity value was reduced from 39.3% to 3.1%. The root mean square error relative to values inside the regions of interest selected from a benchmark scatter free image was reduced from 50 to 11.3. The TV regularization also led to a better contrast-to-noise ratio. Conclusions : An asymmetric half beam blocker-based FDK acquisition and reconstruction technique has been established. The proposed scheme enables simultaneous detection of patient specific scatter and complete volumetric CBCTreconstruction without additional requirements such as prior images, dual scans, or moving strips. An efficient technique to optimize beam weights and wedge angles in radiotherapytreatment planning has been developed. Based on the fact that a wedged field can be regarded as a superposition of an open field and a nominal wedged field, this approach reduces the problem of finding J beam weights and the corresponding wedge angles to optimizing a linear system with 2J unknowns (weights of J open beams and J nominal wedged beams), where J is the total number of incident beam directions. Two iterative algorithms similar to the iterative-least-square technique in image reconstruction are used to optimize the system. Application of the algorithms to two specific examples shows that this technique can reduce treatment planning time and effort, and promises to create a better solution for an arbitrarily complex treatment plan. The endorectal coil is being increasingly used in magnetic resonance imaging (MRI) and MR spectroscopic imaging (MRSI) to obtain anatomic and metabolic images of the prostate with high signal-to-noise ratio (SNR). In practice, however, the use of endorectal probe inevitably distorts the prostate and other soft tissue organs, making the analysis and the use of the acquired image data in treatment planning difficult. The purpose of this work is to develop a deformable image registration algorithm to map the MRI/MRSI information obtained using an endorectal probe onto CT images and to verify the accuracy of the registration by phantom and patient studies. A mapping procedure involved using a thin plate spline (TPS) transformation was implemented to establish voxel-to-voxel correspondence between a reference image and a floating image with deformation. An elastic phantom with a number of implanted fiducial markers was designed for the validation of the quality of the registration. Radiographic images of the phantom were obtained before and after a series of intentionally introduced distortions. After mapping the distorted phantom to the original one, the displacements of the implanted markers were measured with respect to their ideal positions and the mean error was calculated. In patient studies, CT imagesmore\\u00a0\\u00bb of three prostate patients were acquired, followed by 3 Tesla (3 T) MR images with a rigid endorectal coil. Registration quality was estimated by the centroid position displacement and image coincidence index (CI). Phantom and patient studies show that TPS-based registration has achieved significantly higher accuracy than the previously reported method based on a rigid-body transformation and scaling. The technique should be useful to map the MR spectroscopic dataset acquired with ER probe onto the treatment planning CT dataset to guide radiotherapy planning.\\u00ab\\u00a0less Abstract Background and purpose : To describe a wedge filter optimization technique which automatically chooses the beam weights and wedge filters and to demonstrate the implementation of the algorithm in clinical three-dimensional (3D) radiotherapy treatment planning. Material and methods : Given the incident directions and beam energies of J beams, the dose distribution is a function of the beam weights, wedge angles, and wedge orientations. Instead of decomposing an incident field into a superposition of an open and two nominal wedged fields and then optimizing their weights, the algorithm optimizes the objective function with respect to the beam weights, wedge angles and wedge orientations directly. A salient feature of the algorithm is that no planner intervention was required in the selection of wedge filters during the optimization process. A dose-based objective function which incorporated the relative importance of structures was adopted in this work. The objective function was minimized by the method of simulated annealing. The technique was demonstrated by using a phantom study and two clinical cases. Results : For the phantom case, the classical wedge pair result was obtained, providing a useful test of the algorithm. Dose distributions and dose\\u2013volume histograms for the target and surrounding organs were presented for the two clinical cases. It was also shown that dose homogeneity to the target could be compromised by increasing the relative importance factors to the surrounding organs. Conclusions : A 3D wedge filter optimization algorithm has been developed. The technique has the potential to fully automate the 3D radiotherapy treatment planning process. In addition, treatment planning time and efforts were significantly reduced. Iterative algorithms used in imaging science are generalized to study inverse treatment planning in radiotherapy. These algorithms consist of iteratively changing trial incident fluence functions which eventually yield a dose distribution consistent with the prescribed distribution. Application of the algorithms is presented for test cases of a circular phantom with cylindrically symmetric dose distributions. A comparison of the iterative beam profiles with previous analytic solutions shows excellent agreement, indicating that iterative techniques are promising to create sophisticated treatment plans. The relationship between the algorithms and several existing inverse treatment planning methods is discussed. X-ray luminescence and X-ray fluorescence computed tomography (CT) are two emerging technologies in X-ray imaging that provide functional and molecular imaging capability. Both emission-type tomographic imaging modalities use external X-rays to stimulate secondary emissions, either light or secondary X-rays, which are then acquired for tomographic reconstruction. These modalities surpass the limits of sensitivity in current X-ray imaging and have the potential of enabling X-ray imaging to extract molecular imaging information. These new modalities also promise to break through the spatial resolution limits of other in vivo molecular imaging modalities. This paper reviews the development of X-ray luminescence and X-ray fluorescence CT and their relative merits. The discussion includes current problems and future research directions and the role of these modalities in future molecular imaging applications. Purpose: Endorectal (ER) coil-based magnetic resonance imaging (MRI) and magnetic resonance spectroscopic imaging (MRSI) is often used to obtain anatomic and metabolic images of the prostate and to accurately identify and assess the intraprostatic lesions. Recent advancements in high-field (3 Tesla or above) MR techniques affords significantly enhanced signal-to-noise ratio and makes it possible to obtain high-quality MRI data. In reality, the use of rigid or inflatable endorectal probes deforms the shape of the prostate gland, and the images so obtained are not directly usable in radiation therapy planning. The purpose of this work is to apply a narrow band deformable registration model to faithfully map the acquired information from the ER-based MRI/MRSI onto treatment planning computed tomography (CT) images. Methods and Materials: A narrow band registration, which is a hybrid method combining the advantages of pixel-based and distance-based registration techniques, was used to directly register ER-based MRI/MRSI with CT. The normalized correlation between the two input images for registration was used as the metric, and the calculation was restricted to those points contained in the narrow bands around the user-delineated structures. The narrow band method is inherently efficient because of the use of a priori information of the meaningful contour data. The registration was performed in two steps. First, the two input images were grossly aligned using a rigid registration. The detailed mapping was then modeled by free form deformations based on B-spline. The limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm ( L-BFGS ), which is known for its superior performance in dealing with high-dimensionality problems, was implemented to optimize the metric function. The convergence behavior of the algorithm was studied by self-registering an MR image with 100 randomly initiated relative positions. To evaluate the performance of the algorithm, an MR image was intentionally distorted, and an attempt was then made to register the distorted image with the original one. The ability of the algorithm to recover the original image was assessed using a checkerboard graph. The mapping of ER-based MRI onto treatment planning CT images was carried out for two clinical cases, and the performance of the registration was evaluated. Results: A narrow band deformable image registration algorithm has been implemented for direct registration of ER-based prostate MRI/MRSI and CT studies. The convergence of the algorithm was confirmed by starting the registration experiment from more than 100 different initial conditions. It was shown that the technique can restore an MR image from intentionally introduced deformations with an accuracy of approximately 2 mm. Application of the technique to two clinical prostate MRI/CT registrations indicated that it is capable of producing clinically sensible mapping. The whole registration procedure for a complete three-dimensional study (containing 256 \\u00d7 256 \\u00d7 64 voxels) took less than 15 min on a standard personal computer, and the convergence was usually achieved in fewer than 100 iterations. Conclusions: A deformable image registration procedure suitable for mapping ER-based MRI data onto planning CT images was presented. Both hypothetical tests and patient studies have indicated that the registration is reliable and provides a valuable tool to integrate the ER-based MRI/MRSI information to guide prostate radiation therapy treatment. Purpose To develop a regional narrow-band algorithm to auto-propagate the contour surface of a region of interest (ROI) from one phase to other phases of four-dimensional computed tomography (4D-CT). Methods and Materials The ROI contours were manually delineated on a selected phase of 4D-CT. A narrow band encompassing the ROI boundary was created on the image and used as a compact representation of the ROI surface. A BSpline deformable registration was performed to map the band to other phases. A Mattes mutual information was used as the metric function, and the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm was used to optimize the function. After registration the deformation field was extracted and used to transform the manual contours to other phases. Bidirectional contour mapping was introduced to evaluate the proposed technique. The new algorithm was tested on synthetic images and applied to 4D-CT images of 4 thoracic patients and a head-and-neck Cone-beam CT case. Results Application of the algorithm to synthetic images and Cone-beam CT images indicates that an accuracy of 1.0 mm is achievable and that 4D-CT images show a spatial accuracy better than 1.5 mm for ROI mappings between adjacent phases, and 3 mm in opposite-phase mapping. Compared with whole image\\u2013based calculations, the computation was an order of magnitude more efficient, in addition to the much-reduced computer memory consumption. Conclusions A narrow-band model is an efficient way for contour mapping and should find widespread application in future 4D treatment planning. The purpose of this paper is to investigate the use of a power function as a cost function in inverse planning optimization. The cost function for each structure is implemented as an exponential power function of the deviation between the resultant dose and prescribed or constrained dose. The total cost function for all structures is a summation of the cost function of every structure. When the exponents of all terms in the cost function are set to 2, the cost function becomes a classical quadratic cost function. An independent optimization module was developed and interfaced with a research treatment planning system from the University of North Carolina for dose calculation and display of results. Three clinical cases were tested for this study with various exponents set for tumor targets and sensitive structures. Treatment plans with these exponent settings were compared, using dose volume histograms. The results of our study demonstrated that using an exponent higher than 2 in the cost function for the target achieved better dose homogeneity than using an exponent of 2. An exponent higher than 2 for serial sensitive structures can effectively reduce the maximum dose. Varying the exponent from 2 to 4 resulted in the most effective changes in dose volume histograms while the change from 4 to 8 is less drastic, indicating a situation of saturation. In conclusion, using a power function with exponent greater than 2 as a cost function can effectively achieve homogeneous dose inside the target and/or minimize maximum dose to the critical structures. Intensity modulated radiation therapy (IMRT) inverse planning using total-variation (TV) regularization has been proposed to reduce the complexity of fluence maps and facilitate dose delivery. Conventionally, the optimization problem with L-1 norm is solved with quadratic programming (QP), which is time consuming and memory expensive due to the second-order Newton update. This study proposes to use a new algorithm, template for first-order conic solver (TFOCS), for fast and memory-efficient optimization in IMRT inverse planning. The TFOCS utilizes dual-variable updates and first-order approaches for TV minimization without the need to compute and store the enlarged Hessian matrix required for Newton update in the QP technique. To evaluate the effectiveness and efficiency of the proposed method, two clinical cases were used for IMRT inverse planning: a head and neck case and a prostate case. For comparison, the conventional QP-based method for the TV form was adopted to solve the fluence map optimization problem in the above two cases. The convergence criteria and algorithm parameters were selected to achieve similar dose conformity for a fair comparison between the two methods. Compared with conventional QP-based approach, the proposed TFOCS-based method shows a remarkable improvement in computational efficiency for fluence map optimization, while maintaining the conformal dose distribution. Compared with QP-based algorithms, the computational speed using TFOCS for fluence optimization is increased by a factor of 4 to 6, and at the same time the memory requirement is reduced by a factor of 3 to 4. Therefore, TFOCS provides an effective, fast and memory-efficient method for IMRT inverse planning. The unique features of the approach should be particularly important in inverse planning involving a large number of beams, such as in VMAT and dense angularly sampled and sparse intensity modulated radiation therapy (DASSIM-RT). The purpose of this study was to increase the sensitivity of XFCT imaging by optimizing the data acquisition geometry for reduced scatter X-rays. The placement of detectors and detector energy window were chosen to minimize scatter X-rays. We performed both theoretical calculations and Monte Carlo simulations of this optimized detector configuration on a mouse-sized phantom containing various gold concentrations. The sensitivity limits were determined for three different X-ray spectra: a monoenergetic source, a Gaussian source, and a conventional X-ray tube source. Scatter X-rays were minimized using a backscatter detector orientation (scatter direction > 110\\u00b0 to the primary X-ray beam). The optimized configuration simultaneously reduced the number of detectors and improved the image signal-to-noise ratio. The sensitivity of the optimized configuration was 10 \\u03bcg/mL (10 pM) at 2 mGy dose with the mono-energetic source, which is an order of magnitude improvement over the unoptimized configuration (102 pM without the optimization). Similar improvements were seen with the Gaussian spectrum source and conventional X-ray tube source. The optimization improvements were predicted in the theoretical model and also demonstrated in simulations. The sensitivity of XFCT imaging can be enhanced by an order of magnitude with the data acquisition optimization, greatly enhancing the potential of this modality for future use in clinical molecular imaging. As a solution to iterative CT image reconstruction, first-order methods are prominent for the large-scale capability and the fast convergence rate . In practice, the CT system matrix with a large condition number may lead to slow convergence speed despite the theoretically promising upper bound. The aim of this study is to develop a Fourier-based scaling technique to enhance the convergence speed of first-order methods applied to CT image reconstruction. Instead of working in the projection domain, we transform the projection data and construct a data fidelity model in Fourier space. Inspired by the filtered backprojection formalism, the data are appropriately weighted in Fourier space. We formulate an optimization problem based on weighted least-squares in the Fourier space and total-variation (TV) regularization in image space for parallel-beam, fan-beam and cone-beam CT geometry. To achieve the maximum computational speed, the optimization problem is solved using a fast iterative shrinkage-thresholding algorithm with backtracking line search and GPU implementation of projection/backprojection. The performance of the proposed algorithm is demonstrated through a series of digital simulation and experimental phantom studies. The results are compared with the existing TV regularized techniques based on statistics-based weighted least-squares as well as basic algebraic reconstruction technique. The proposed Fourier-based compressed sensing (CS) method significantly improves both the image quality and the convergence rate compared to the existing CS techniques. Intensity modulated radiation therapy (IMRT) inverse planning is usually performed by pre-selecting parameters such as beam modality, beam configuration and importance factors and then optimizing the fluence profiles or beamlet weights. In reality, the IMRT dose optimization problem may be ill-conditioned and there may not be a physical solution to account for the chosen parameters and constraints. Planner intervention is often required to conduct a multiple trial-and-error process where several parameters are sequentially varied until an acceptable compromise is achieved. The resulting solution reflects a balance between the conflicting requirements of the target and the sensitive structures. A major problem of the conventional inverse planning formalism is that there exists no effective mechanism for a planner to fine-tune the dose distribution on a local level or to differentially modify the dose\\u2013volume histograms (DVHs) of the involved structures. In this paper we introduce a new inverse planning scheme with voxel-dependent importance factors and demonstrate that it provides us with an effective link between the system parameters and the dosimetric behaviour at a local level. The planning proceeds in two steps. After a conventional trial-and-error inverse planning procedure is completed, we identify the dose interval at which the fractional volume on the DVH curve needs to be changed. The voxels that receive dose in the selected range are then located and their voxel-dependent importance factors are adjusted accordingly. The fine-tuning of the DVHs is iterative in nature and, using widely available computer graphic software tools, the process can be made graphically interactive. The new IMRT planning scheme is applied to two test cases and the results indicate that our control over the differential shapes of the DVHs of the involved structures is greatly enhanced. Thus the technique may have significant practical implications in facilitating the IMRT treatment planning process. Object displacement in a CT scan is generally reflected in CT projection data or sinogram. In this work, the direct relationship between object motion and the change of CT projection data (sinogram) is investigated and this knowledge is applied to create a novel algorithm for sinogram registration. Calculated and experimental results demonstrate that the registration technique works well for registering rigid 2D or 3D motion in parallel and fan beam samplings. Problem and solution for 3D sinogram-based registration of metallic fiducials are also addressed. Since the motion is registered before image reconstruction, the presented algorithm is particularly useful when registering images with metal or truncation artifacts. In addition, this algorithm is valuable for dealing with situations where only limited projection data are available, making it appealing for various applications in image guided radiation therapy. It is well known that the spatial biology distribution (e.g., clonogen density, radiosensitivity, tumor proliferation rate, functional importance) in most tumors and sensitive structures is heterogeneous. Recent progress in biological imaging is making the mapping of this distribution increasingly possible. The purpose of this work is to establish a theoretical framework to quantitatively incorporate the spatial biology data into intensity modulated radiation therapy(IMRT) inverse planning. In order to implement this, we first derive a general formula for determining the desired dose to each tumor voxel for a known biology distribution of the tumor based on a linear-quadratic model. The desired target dose distribution is then used as the prescription for inverse planning. An objective function with the voxel-dependent prescription is constructed with incorporation of the nonuniform dose prescription. The functional unit density distribution in a sensitive structure is also considered phenomenologically when constructing the objective function. Two cases with different hypothetical biology distributions are used to illustrate the new inverse planning formalism. For comparison, treatments with a few uniform dose prescriptions and a simultaneous integrated boost are also planned. The biological indices, tumor control probability (TCP) and normal tissue complication probability (NTCP), are calculated for both types of plans and the superiority of the proposed technique over the conventional dose escalation scheme is demonstrated. Our calculations revealed that it is technically feasible to produce deliberately nonuniform dose distributions with consideration of biological information. Compared with the conventional dose escalation schemes, the new technique is capable of generating biologically conformal IMRT plans that significantly improve the TCP while reducing or keeping the NTCPs at their current levels. Biologically conformal radiation therapy (BCRT) incorporates patient-specific biological information and provides an outstanding opportunity for us to truly individualize radiation treatment. The proposed formalism lays a technical foundation for BCRT and allows us to maximally exploit the technical capacity of IMRT to more intelligently escalate the radiationdose. Four-dimensional (4D) cone-beam CT (CBCT) is commonly obtained by respiratory phase binning of the projections, followed by independent reconstructions of the rebinned data in each phase bin. Due to the significantly reduced number of projections per reconstruction, the quality of the 4DCBCT images is often degraded by view-aliasing artifacts easily seen in the axial view. Acquisitions using multiple gantry rotations or slow gantry rotation can increase the number of projections and substantially improve the 4D images. However, the extra cost of the scan time may set fundamental limits to their applications in clinics. Improving the trade-off between image quality and scan time is the key to making 4D onboard imaging practical and more useful. In this article, we present a novel technique toward high-quality 4DCBCT imaging without prolonging the acquisition time, referred to as the 'enhanced 4DCBCT'. The method correlates the data in different phase bins and integrates the internal motion into the 4DCBCT image formulation. Several strategies of the motion derivation are discussed, and the resultant images are assessed with numerical simulations as well as a clinical case. Abstract Purpose: Leaf motion synchronization of dynamic multileaf collimators (DMLC) for intensity-modulated radiotherapy (IMRT) is important in improving dose distribution and reducing \\\"tongue-and-groove\\\" effects for a prescribed intensity profile. Leaf synchronization could also be used in transforming a one-dimensional leaf-setting algorithm into a two-dimensional leaf-setting algorithm. In this work, we aim to develop a generalized leaf synchronization method for delivering IMRT with the minimized beam delivery time and the optimized subfield variations for a leaf-setting sequence. Methods and Materials: With the leaf synchronization procedure, all active MLC leaf pairs start and finish off a leaf sequence simultaneously. In this work, the MLC leaf pairs were synchronized under the condition that the resulting leaf sequence produces the desired intensity profile with the minimum beam delivery time. The parameter of the leaf synchronization function was determined through the least-square minimization of the area variations of all subfields within a leaf sequence. The leaf synchronization and optimization procedure were applied and analyzed for clinical relevant intensity profiles for treating the head-and-neck cancer patients using IMRT. Results: The total monitor units and the optimized beam delivery time of generating a two-dimensional intensity profile was proven through this work to be the global minimum of all leaf-setting sequences including the unsynchronized leaf-setting sequences. The optimized parameter for subfield variations of the synchronized leaf trajectories was found to be dependent on individual intensity profiles. For all our studied cases, the unsynchronized leaf trajectories always have significantly larger subfield variations than the synchronized leaf trajectories. Conclusion: It is important and also feasible to synchronize and optimize dynamic MLC leaf motions while still keeping the total beam delivery time minimum for delivering arbitrary two-dimensional intensity-modulated fields. On-board cone-beam computed tomography (CBCT) is a new imaging technique for radiation therapy guidance, which provides volumetric information of a patient at treatment position. CBCT improves the setup accuracy and may be used for dose reconstruction. However, there is great concern that the repeated use of CBCT during a treatment course delivers too much of an extra dose to the patient. To reduce the CBCT dose, one needs to lower the total mAs of the x-ray tube current, which usually leads to reduced image quality. Our goal of this work is to develop an effective method that enables one to achieve a clinically acceptable CBCT image with as low as possible mAs without compromising quality. An iterative image reconstruction algorithm based on a penalized weighted least-squares (PWLS) principle was developed for this purpose. To preserve edges in the reconstructed images, we designed an anisotropic penalty term of a quadratic form. The algorithm was evaluated with a CT quality assurance phantom and an anthropomorphic head phantom. Compared with conventional isotropic penalty, the PWLS image reconstruction algorithm with anisotropic penalty shows better resolution preservation. The purpose of this study is to develop a 4D inverse planning strategy capable of controlling the appearance of the implanted fiducial(s) in segmented IMRT fields for cine MV or combined MV\\u2215kV image-guided IMRT. This work is focused on enhancing the visibility of the implanted fiducial(s) in 4D IMRT inverse planning, whose goal is to derive a set of time-resolved (or phase-tagged) MLC segments to cater for the motion of the patient anatomy extracted from the emerging 4D images. The task is to optimize the shapes and weights of all the segments for each incident beam, with the fiducial(s) being forced\\u2215encouraged to be inside the segmented fields. The system is modeled by a quadratic objective function with inclusion of a hard\\u2215soft constraint characterizing the authors\\u2019 level of preference for the fiducial(s) to be included in the segmented fields. A simulated annealing algorithm is employed to optimize the system. The proposed technique is demonstrated using two clinical cases. A segment-based inverse planning framework for 4D radiation therapy, capable of providing tempospatially optimized IMRT plans, has been established. Furthermore, using the described 4D optimization approach, it is demonstrated that the MLC blockage of the implanted fiducial(s) during the segmented delivery is avoided without severely compromising the final dose distribution. The visibility of implanted fiducials in 4D IMRT can be improved without significantly deteriorating final dose distribution. This is a foundation for the authors to use cine MV or combined MV\\u2215KV to effectively guide the 4D IMRT delivery. Beam direction optimization is an important problem in radiation therapy. In intensity modulated radiation therapy (IMRT), the difficulty for computer optimization of the beam directions arises from the fact that they are coupled with the intensity profiles of the incident beams. In order to obtain the optimal incident beam directions using iterative or stochastic methods, the beam profiles ought to be optimized after every change of beam configuration. In this paper we report an effective algorithm to optimize gantry angles for IMRT. In our calculation the gantry angles and the beam profiles (beamlet weights) were treated as two separate groups of variables. The gantry angles were sampled according to a simulated annealing algorithm. For each sampled beam configuration, beam profile calculation was done using a fast filtered backprojection (FBP) method. Simulated annealing was also used for beam profile optimization to examine the performance of the FBP for beam orientation optimization. Relative importance factors were incorporated into the objective function to control the relative importance of the target and the sensitive structures. Minimization of the objective function resulted in the best possible beam orientations and beam profiles judged by the given objective function. The algorithm was applied to several model problems and the results showed that the approach has potential for IMRT applications. Purpose: Developing an imaging method to directly monitor the spatial distribution of platinum-based (Pt) drugs at the tumor region is of critical importance for early assessment of treatment efficacy and personalized treatment. In this study, the authors investigated the feasibility of imaging platinum (Pt)-based drug distribution using x-ray fluorescence (XRF, a.k.a. characteristic x ray) CT (XFCT). Methods: A 5-mm-diameter pencil beam produced by a polychromatic x-ray source equipped with a tungsten anode was used to stimulate emission of XRF photons from Pt drug embedded within a water phantom. The phantom was translated and rotated relative to the stationary pencil beam in a first-generation CT geometry. The x-ray energy spectrum was collected for 18 s at each position using a cadmium telluride detector. The spectra were then used for the K-shell XRF peak isolation and sinogram generation for Pt. The distribution and concentration of Pt were reconstructed with an iterative maximum likelihood expectation maximization algorithm. The capability of XFCT to multiplexed imaging of Pt, gadolinium (Gd), and iodine (I) within a water phantom was also investigated. Results: Measured XRF spectrum showed a sharp peak characteristic of Pt with a narrow full-width at half-maximum (FWHM) (FWHMK\\u03b11 = 1.138 keV, FWHMK\\u03b12 = 1.052 keV). The distribution of Pt drug in the water phantom was clearly identifiable on the reconstructed XRF images. Our results showed a linear relationship between the XRF intensity of Pt and its concentrations (R 2 = 0.995), suggesting that XFCT is capable of quantitative imaging. A transmission CT image was also obtained to show the potential of the approach for providing attenuation correction and morphological information. Finally, the distribution of Pt, Gd, and I in the water phantom was clearly identifiable in the reconstructed images from XFCT multiplexed imaging. Conclusions: XFCT is a promising modality for monitoring the spatial distribution of Pt drugs. The technique may be useful in tailoring tumor treatment regimen in the future. Purpose: This article considers the problem of reconstructingcone-beam computed tomography(CBCT)images from a set of undersampled and potentially noisy projection measurements. Methods: The authors cast the reconstruction as a compressed sensing problem based on l 1 norm minimization constrained by statistically weighted least-squares of CBCT projection data. For accurate modeling, the noise characteristics of the CBCT projection data are used to determine the relative importance of each projection measurement. To solve the compressed sensing problem, the authors employ a method minimizing total-variation norm, satisfying a prespecified level of measurement consistency using a first-order method developed by Nesterov. Results: The method converges fast to the optimal solution without excessive memory requirement, thanks to the method of iterative forward and back-projections. The performance of the proposed algorithm is demonstrated through a series of digital and experimental phantom studies. It is found a that high quality CBCTimage can be reconstructed from undersampled and potentially noisy projection data by using the proposed method. Both sparse sampling and decreasing x-ray tube current (i.e., noisy projection data) lead to the reduction of radiationdose in CBCTimaging. Conclusions: It is demonstrated that compressed sensing outperforms the traditional algorithm when dealing with sparse, and potentially noisy, CBCT projection views. Currently, there are two types of treatment planning algorithms for intensity modulated radiation therapy (IMRT). The beamlet-based algorithm generates beamlet intensity maps with high complexity, resulting in large numbers of segments in the delivery after a leaf-sequencing algorithm is applied. The segment-based direct aperture optimization (DAO) algorithm includes the physical constraints of the deliverable apertures in the calculation, and achieves a conformal dose distribution using a small number of segments. However, the number of segments is pre-fixed in most of the DAO approaches, and the typical random search scheme in the optimization is computationally intensive. A regularization-based algorithm is proposed to overcome the drawbacks of the DAO method. Instead of smoothing the beamlet intensity maps as in many existing methods, we include a total-variation term in the optimization objective function to reduce the number of signal levels of the beam intensity maps. An aperture rectification algorithm is then applied to generate a significantly reduced number of deliverable apertures. As compared to the DAO algorithm, our method has an efficient form of quadratic optimization, with an additional advantage of optimizing field-specific numbers of segments based on the modulation complexity. The proposed approach is evaluated using two clinical cases. Under the condition that the clinical acceptance criteria of the treatment plan are satisfied, for the prostate patient, the total number of segments for five fields is reduced from 61 using the Eclipse planning system to 35 using the proposed algorithm; for the head and neck patient, the total number of segments for seven fields is reduced from 107 to 28. The head and neck result is also compared to that using an equal number of four segments for each field. The comparison shows that using field-specific numbers of segments achieves a much improved dose distribution. Methods of linear algebra are applied to the choice of beam weights for intensity modulated radiation therapy (IMRT). It is shown that the physical interpretation of the beam weights, target homogeneity and ratios of deposited energy can be given in terms of matrix equations and quadratic forms. The methodology of fitting using linear algebra as applied to IMRT is examined. Results are compared with IMRT plans that had been prepared using a commercially available IMRT treatment planning system and previously delivered to cancer patients. To exploit the potential dosimetric advantages of intensity modulated radiation therapy (IMRT) and volumetric modulated arc therapy (VMAT), an in-depth approach is required to provide efficient computing methods. This needs to incorporate clinically related organ specific constraints, Monte Carlo (MC) dose calculations, and large-scale plan optimization. This paper describes our first steps toward a web-based real-time radiation treatment planning system in a cloud computing environment (CCE). The Amazon Elastic Compute Cloud (EC2) with a master node (named m2.xlarge containing 17.1 GB of memory, two virtual cores with 3.25 EC2 Compute Units each, 420 GB of instance storage, 64-bit platform) is used as the backbone of cloud computing for dose calculation and plan optimization. The master node is able to scale the workers on an 'on-demand' basis. MC dose calculation is employed to generate accurate beamlet dose kernels by parallel tasks. The intensity modulation optimization uses total-variation regularization (TVR) and generates piecewise constant fluence maps for each initial beam direction in a distributed manner over the CCE. The optimized fluence maps are segmented into deliverable apertures. The shape of each aperture is iteratively rectified to be a sequence of arcs using the manufacture's constraints. The output plan file from the EC2 is sent to the simple storage service. Three de-identified clinical cancer treatment plans have been studied for evaluating the performance of the new planning platform with 6 MV flattening filter free beams (40 \\u00d7 40 cm2) from the Varian TrueBeamTM STx linear accelerator. A CCE leads to speed-ups of up to 14-fold for both dose kernel calculations and plan optimizations in the head and neck, lung, and prostate cancer cases considered in this study. The proposed system relies on a CCE that is able to provide an infrastructure for parallel and distributed computing. The resultant plans from the cloud computing are identical to PC-based IMRT and VMAT plans, confirming the reliability of the cloud computing platform. This cloud computing infrastructure has been established for a radiation treatment planning. It substantially improves the speed of inverse planning and makes future on-treatment adaptive re-planning possible. Radiation therapy is an image-guided process whose success critically depends on the imaging modality used for treatment planning and the level of integration of the available imaging information. In this work, we establish a dose optimization framework for incorporating metabolic information from functional imaging modalities into the intensity-modulated radiation therapy (IMRT) inverse planning process and to demonstrate the technical feasibility of planning deliberately non-uniform dose distributions in accordance with functional imaging data. For this purpose, a metabolic map from functional images is discretized into a number of abnormality levels (ALs) and then fused with CT images. To escalate dose to the metabolically abnormal regions, we assume, for a given spatial point, a linear relation between the AL and the prescribed dose. But the formalism developed here is independent of the assumption and any other relation between AL and prescription is applicable. For a given AL and prescription relation, it is only necessary to prescribe the dose to the lowest AL in the target and the desired doses to other regions with higher AL values are scaled accordingly. To accomplish differential sparing of a sensitive structure when its functional importance (FI) distribution is known, we individualize the tolerance doses of the voxels within the structure according to their FI levels. An iterative inverse planning algorithm in voxel domain is used to optimize the system with inhomogeneous dose prescription. To model intra-structural trade-off, a mechanism is introduced through the use of voxel-dependent weighting factors, in addition to the conventional structure specific weighting factors which model the inter-structural trade-off. The system is used to plan a phantom case with a few hypothetical functional distributions and a brain tumour treatment with incorporation of magnetic resonance spectroscopic imaging data. The results indicated that it is technically feasible to produce deliberately non-uniform dose distributions according to the functional imaging requirements. Integration of functional imaging information into radiation therapy dose optimization allows for consideration of patient-specific biologic information and provides a significant opportunity to truly individualize radiation treatment. This should enhance our capability to safely and intelligently escalate dose and lays the technical foundation for future clinical studies of the efficacy of functional imaging-guided IMRT. Intra-fraction tumor tracking methods can improve radiation delivery during radiotherapy sessions. Image acquisition for tumor tracking and subsequent adjustment of the treatment beam with gating or beam tracking introduces time latency and necessitates predicting the future position of the tumor. This study evaluates the use of multi-dimensional linear adaptive filters and support vector regression to predict the motion of lung tumors tracked at 30 Hz. We expand on the prior work of other groups who have looked at adaptive filters by using a general framework of a multiple-input single-output (MISO) adaptive system that uses multiple correlated signals to predict the motion of a tumor. We compare the performance of these two novel methods to conventional methods like linear regression and single-input, single-output adaptive filters. At 400 ms latency the average root-mean-square-errors (RMSEs) for the 14 treatment sessions studied using no prediction, linear regression, single-output adaptive filter, MISO and support vector regression are 2.58, 1.60, 1.58, 1.71 and 1.26 mm, respectively. At 1 s, the RMSEs are 4.40, 2.61, 3.34, 2.66 and 1.93 mm, respectively. We find that support vector regression most accurately predicts the future tumor position of the methods studied and can provide a RMSE of less than 2 mm at 1 s latency. Also, a multi-dimensional adaptive filter framework provides improved performance over single-dimension adaptive filters. Work is underway to combine these two frameworks to improve performance. Purpose: Beam orientation optimization in intensity-modulated radiation therapy (IMRT) is computationally intensive, and various single beam ranking techniques have been proposed to reduce the search space. Up to this point, none of the existing ranking techniques considers the clinically important dose\\u2013volume effects of the involved structures, which may lead to clinically irrelevant angular ranking. The purpose of this work is to develop a clinically sensible angular ranking model with incorporation of dose\\u2013volume effects and to show its utility for IMRT beam placement. Methods and Materials: The general consideration in constructing this angular ranking function is that a beamlet/beam is preferable if it can deliver a higher dose to the target without exceeding the tolerance of the sensitive structures located on the path of the beamlet/beam. In the previously proposed dose-based approach, the beamlets are treated independently and, to compute the maximally deliverable dose to the target volume, the intensity of each beamlet is pushed to its maximum intensity without considering the values of other beamlets. When volumetric structures are involved, the complication arises from the fact that there are numerous dose distributions corresponding to the same dose\\u2013volume tolerance. In this situation, the beamlets are not independent and an optimization algorithm is required to find the intensity profile that delivers the maximum target dose while satisfying the volumetric constraints. In this study, the behavior of a volumetric organ was modeled by using the equivalent uniform dose (EUD). A constrained sequential quadratic programming algorithm (CFSQP) was used to find the beam profile that delivers the maximum dose to the target volume without violating the EUD constraint or constraints. To assess the utility of the proposed technique, we planned a head-and-neck and abdominal case with and without the guidance of the angular ranking information. The qualities of the two types of IMRT plans were compared quantitatively. Results: An effective angular ranking model with consideration of volumetric effect has been developed. It is shown that the previously reported dose-based angular ranking represents a special case of the general formalism proposed here. Application of the technique to a abdominal and a head-and-neck IMRT case indicated that the proposed technique is capable of producing clinically sensible angular ranking. In both cases, we found that the IMRT plans obtained under the guidance of EUD-based angular ranking were improved in comparison with that obtained using the conventional uniformly spaced beams. Conclusions: The EUD-based function is a general approach for angular ranking and allows us to identify the potentially good and bad angles for clinically complicated cases. The ranking can be used either as a guidance to facilitate the manual beam placement or as prior information to speed up the computer search for the optimal beam configuration. Thus the proposed technique should have positive clinical impact in facilitating the IMRT planning process. Purpose: Four-dimensional CT (4DCT) and cone beam CT (CBCT) are widely used in radiation therapy for accurate tumor target definition and localization. However, high-resolution and dynamic image reconstruction is computationally demanding because of the large amount of data processed. Efficient use of these imaging techniques in the clinic requires high-performance computing. The purpose of this work is to develop a novel ultrafast, scalable and reliable image reconstruction technique for 4D CBCT/CT using a parallel computing framework called MapReduce. We show the utility of MapReduce for solving large-scale medical physics problems in a cloud computing environment.  Methods: In this work, we accelerated the Feldcamp\\u2013Davis\\u2013Kress (FDK) algorithm by porting it to Hadoop, an open-source MapReduce implementation. Gated phases from a 4DCT scans were reconstructed independently. Following the MapReduce formalism, Map functions were used to filter and backproject subsets of projections, and Reduce function to aggregate those partial backprojection into the whole volume. MapReduce automatically parallelized the reconstruction process on a large cluster of computer nodes. As a validation, reconstruction of a digital phantom and an acquired CatPhan 600 phantom was performed on a commercial cloud computing environment using the proposed 4D CBCT/CT reconstruction algorithm.  Results: Speedup of reconstruction time is found to be roughly linear with the number of nodes employed. For instance, greater than 10 times speedup was achieved using 200 nodes for all cases, compared to the same code executed on a single machine. Without modifying the code, faster reconstruction is readily achievable by allocating more nodes in the cloud computing environment. Root mean square error between the images obtained using MapReduce and a single-threaded reference implementation was on the order of 10\\u22127. Our study also proved that cloud computing with MapReduce is fault tolerant: the reconstruction completed successfully with identical results even when half of the nodes were manually terminated in the middle of the process.  Conclusions: An ultrafast, reliable and scalable 4D CBCT/CT reconstruction method was developed using the MapReduce framework. Unlike other parallel computing approaches, the parallelization and speedup required little modification of the original reconstruction code. MapReduce provides an efficient and fault tolerant means of solving large-scale computing problems in a cloud computing environment. Intensity-modulated radiation therapy (IMRT) requires the determination of the appropriate multileaf collimator settings to deliver an intensity map. The purpose of this work was to attempt to reduce the number of segments required for IMRT delivery and the number of monitor units required to deliver an intensity map. An intensity map may be written as a matrix. Leaf sequencing was formulated as a problem of decomposing the matrix into a series of sub-matrices. Sets of random intensity matrices were created and the segmentations produced by applying different algorithms were compared. The number of segments, important if verification and record (VR) overhead is significant, and beam on times were examined. It is shown that reducing the value of the matrix entries by the maximum amount at each stage results in the smallest number of steps. Reducing the 2-norm (sum of the squares) of the matrix entries by the maximum amount at each step results in the smallest beam on time. Three new algorithms are introduced, two of which produce results that are superior to those generated by the algorithms of other researchers. The resulting methods can be expanded upon to include tongue and groove effects and leaf inter-digitization. With square random matrices of the order 15, the reduction in beam time and segmentation is up to 30\\u201340%. Compared to previous algorithms, those presented here have demonstrated a reduction in the beam on time required to deliver an intensity map by 30\\u201340%. Similarly, the number of segments needed to deliver an intensity map is also reduced. An important issue in intensity modulated radiation therapy (IMRT) is the verification of the monitor unit (MU) calculation of the planning system using an independent procedure. Because of the intensity modulation and the dynamic nature of the delivery process, the problem becomes much more involved than that in conventional radiation therapy. In this work, a closed formula for MU calculation is derived. The approach is independent of the specific form of leaf sequence algorithms. It is straightforward to implement the procedure using a simple computer program. The approach is illustrated by a simplified example and is demonstrated by a few CORVUS (NOMOS Corporation, Sewickley, PA) treatment plans. The results indicate that it is robust and suitable for IMRT MU verification. Abstract Purpose Spatially resolved metabolite maps, as measured by magnetic resonance spectroscopic imaging (MRSI) methods, are being increasingly used to acquire metabolic information to guide therapy, with metabolite ratio maps perhaps providing the most diagnostic information. We present a quality assurance procedure for MRSI-derived metabolic data acquired ultimately for guiding conformal radiotherapy. Methods and materials An MRSI phantom filled with brain-mimicking solutions was custom-built with an insert holding eight vials containing calibration solutions of precisely varying metabolite concentrations that emulated increasing grade/density of brain tumor. Phantom metabolite ratios calculated from fully relaxed 1D, 2D, and 3D MRS data for each vial were compared with calibrated metabolite ratios acquired at 9.4 T. Additionally, 3D ratio maps were \\\"discretized\\\" to eight pseudoabnormality levels on a slice-by-slice basis and the accuracy of this procedure was verified. Results Regression analysis revealed expected linear relationships between experimental and calibration metabolite ratios with intercepts close to zero for the three acquisition modes. 1D MRS data agreed most with theoretical considerations (regression coefficient, b=0.969; intercept 0.008). The 2D (b = 1.049; intercept \\u22120.199) and 3D (correlation coefficient r 2 = 0.9978\\u20130.7336 for five slices) MRSI indicated reduced MRS data quality in regions of degraded B 0 and B 1 homogeneity. Pseudoabnormality levels were found to be consistent with expectations within regions of adequate B 0 homogeneity. Conclusions This simple phantom-based approach to generate baseline calibration curves for all MRS acquisition modes may be useful to identify temporal deviations from acceptable data quality in a routine clinical environment or for testing new MRS and MRSI acquisition software. Purpose: Real-time tracking of implanted fiducials in cine megavoltage (MV) imaging during volumetric modulated arc therapy (VMAT) delivery is complicated due to the inherent low contrast of MV images and potential blockage of dynamic leaves configurations. The purpose of this work is to develop a clinically practical autodetection algorithm for motion management during VMAT.  Methods: The expected field-specific segments and the planned fiducial position from the Eclipse (Varian Medical Systems, Palo Alto, CA) treatment planning system were projected onto the MV images. The fiducials were enhanced by applying a Laplacian of Gaussian filter in the spatial domain for each image, with a blob-shaped object as the impulse response. The search of implanted fiducials was then performed on a region of interest centered on the projection of the fiducial when it was within an open field including the case when it was close to the field edge or partially occluded by the leaves. A universal template formula was proposed for template matching and normalized cross correlation was employed for its simplicity and computational efficiency. The search region for every image was adaptively updated through a prediction model that employed the 3D position of the fiducial estimated from the localized positions in previous images. This prediction model allowed the actual fiducial position to be tracked dynamically and was used to initialize the search region. The artifacts caused by electronic interference during the acquisition were effectively removed. A score map was computed by combining both morphological information and image intensity. The pixel location with the highest score was selected as the detected fiducial position. The sets of cine MV images taken during treatment were analyzed with in-house developed software written in MATLAB (The Mathworks, Inc., Natick, MA). Five prostate patients were analyzed to assess the algorithm performance by measuring their positioning accuracy during treatment.  Results: The algorithm was able to accurately localize the fiducial position on MV images with success rates of more than 90% per case. The percentage of images in which each fiducial was localized in the studied cases varied between 23% and 65%, with at least one fiducial having been localized between 40% and 95% of the images. This depended mainly on the modulation of the plan and fiducial blockage. The prostate movement in the presented cases varied between 0.8 and 3.5 mm (mean values). The maximum displacement detected among all patients was of 5.7 mm.  Conclusions: An algorithm for automatic detection of fiducial markers in cine MV images has been developed and tested with five clinical cases. Despite the challenges posed by complex beam aperture shapes, fiducial localization close to the field edge, partial occlusion of fiducials, fast leaf and gantry movement, and inherently low MV image quality, good localization results were achieved in patient images. This work provides a technique for enabling real-time accurate fiducial detection and tumor tracking during VMAT treatments without the use of extra imaging dose. Real-time knowledge of tumor position during radiation therapy is essential to overcome the adverse effect of intra-fractional organ motion. The goal of this work is to develop a tumor tracking strategy by effectively utilizing the inherent image features of stereoscopic x-ray images acquired during dose delivery. In stereoscopic x-ray image guided radiation delivery, two orthogonal x-ray images are acquired either simultaneously or sequentially. The essence of markerless tumor tracking is the reliable identification of inherent points with distinct tissue features on each projection image and their association between two images. The identification of the feature points on a planar x-ray image is realized by searching for points with high intensity gradient. The feature points are associated by using the scale invariance features transform descriptor. The performance of the proposed technique is evaluated by using images of a motion phantom and four archived clinical cases acquired using either a CyberKnife equipped with a stereoscopic x-ray imaging system, or a LINAC equipped with an onboard kV imager and an electronic portal imaging device. In the phantom study, the results obtained using the proposed method agree with the measurements to within 2 mm in all three directions. In the clinical study, the mean error is 0.48 \\u00b1 0.46 mm for four patient data with 144 sequential images. In this work, a tissue feature-based tracking method for stereoscopic x-ray image guided radiation therapy is developed. The technique avoids the invasive procedure of fiducial implantation and may greatly facilitate the clinical workflow. The graphics processing unit (GPU) has emerged as a competitive platform for computing massively parallel problems. Many computing applications in medical physics can be formulated as data-parallel tasks that exploit the capabilities of the GPU for reducing processing times. The authors review the basic principles of GPU computing as well as the main performance optimization techniques, and survey existing applications in three areas of medical physics, namely image reconstruction,dose calculation and treatment plan optimization, and image processing. Inverse treatment planning starts with a treatment objective and obtains the solution by optimizing an objective function. The clinical objectives are usually multifaceted and potentially incompatible with one another. A set of importance factors is often incorporated in the objective function to parametrize trade-off strategies and to prioritize the dose conformality in different anatomical structures. Whereas the general formalism remains the same, different sets of importance factors characterize plans of obviously different flavour and thus critically determine the final plan. Up to now, the determination of these parameters has been a `guessing' game based on empirical knowledge because the final dose distribution depends on the parameters in a complex and implicit way. The influence of these parameters is not known until the plan optimization is completed. In order to compromise properly the conflicting requirements of the target and sensitive structures, the parameters are usually adjusted through a trial-and-error process. In this paper, a method to estimate these parameters computationally is proposed and an iterative computer algorithm is described to determine these parameters numerically. The treatment plan selection is done in two steps. First, a set of importance factors are chosen and the corresponding beam parameters (e.g. beam profiles) are optimized under the guidance of a quadratic objective function using an iterative algorithm reported earlier. The `optimal' plan is then evaluated by an additional scoring function. The importance factors in the objective function are accordingly adjusted to improve the ranking of the plan. For every change in the importance factors, the beam parameters need to be re-optimized. This process continues in an iterative fashion until the scoring function is saturated. The algorithm was applied to two clinical cases and the results demonstrated that it has the potential to improve significantly the existing method of inverse planning. It was noticed that near the final solution the plan became insensitive to small variations of the importance factors. An essential step towards optimizing and automating radiation therapytreatment planning is to develop an effective algorithm to find the optimal beam weights and wedge filters for a given set of beam directions and modalities. This problem is solved by introducing a variable transformation based on the universal and omni wedge principles. Instead of directly optimizing an objective function with respect to wedge angles and orientations, each field is first decomposed into a superposition of an open field and two orthogonal wedged fields. This transforms the problem of finding J beam weights, wedge angles, and orientations to that of optimizing a system with 3J beam weights (J open beams and 2J nominal wedged beams), where J is the total number of incident beam directions. An iterative algorithm based on a method originally developed for image reconstruction is used to find the 3J beam weights. The technique is applied to a few clinical cases. Treatment plans are improved compared to those obtained through the conventional manual trial and error planning process. In addition, planning time and effort are greatly reduced. Abstract Purpose: To introduce the concept of pseudo beam\\u2019s-eye\\u2013view (pBEV), to establish a framework for computer-assisted beam orientation selection in intensity-modulated radiation therapy (IMRT), and to evaluate the utility of the proposed technique. Methods and Materials: To facilitate the selection of beam orientations for IMRT treatment planning, a scoring of beam direction was introduced. The score function was based on the maximum target dose deliverable by the beam without exceeding the tolerance doses of the critical structures. For the score function calculation, the beam portal at given gantry and couch angles was divided into a grid of beamlets. Each beamlet crossing the target was assigned the maximum intensity that could be used without exceeding the dose tolerances of the organs at risk (OARs) and normal tissue. Thereafter, a score was assigned to the beam according to the target dose delivered. The beams for the treatment were selected among those with the highest scores. In a sense, this technique is similar to the beam\\u2019s-eye\\u2013view approach used in conventional radiation therapy, except that the evaluation by a human is replaced by a score function, and beam modulation is taken into account. Results: The pBEV technique was tested on two clinical cases: a paraspinal treatment and a nasopharyngeal cancer with both coplanar and noncoplanar beam configurations. The plans generated under the guidance of pBEV for the paraspinal treatment offered superior target dose uniformity and reduced OAR doses. For the nasopharyngeal cancer case, it was also found that the pBEV-selected coplanar and noncoplanar beams significantly improved the target coverage without compromising the sparing of the OARs. Conclusions: The pBEV technique developed in this work provides a comprehensive tool for beam orientation selection in IMRT. It is especially valuable for complicated cases, where the target is surrounded by several sensitive structures and where it is difficult to select a set of good beam orientations. The pBEV technique has considerable potential for simplifying the IMRT treatment planning process and for maximizing the technical capacity of IMRT. Intra-fraction internal anatomy motion is one of the major causes of the uncertainty in prostate radiation therapy. Real-time tracking of the intra-fraction prostate motion during radiation therapy is necessary to truly benefit from the highly conformal dose distribution. With the widespread use of kV on-board imaging devices, it is desirable to estimate the 3D tumor position from a single x-ray imager during treatment delivery. In this work, we will present an improved real-time 3D Bayesian tracking algorithm with an online update scheme. By incorporating all previously acquired images during dose delivery and updating the motion probability density function in an online fashion, the algorithm is able to track large and abrupt changes that are typical of prostate motion. The flexible Bayesian formulation allows one to easily incorporate this information and obtain the 3D motion with minimal computational cost. The new tracking algorithm has been tested on actual prostate trajectories recorded with implanted electromagnetic transponders for a total of 10 patients. The new algorithm outperformed a previous one with a fixed prior built from the original setup images (Li et al. 2011). It was found that with the new algorithm, the mean 3D tracking error is about 0.15 mm and the 95th percentile error is about 0.45 mm on average for all the patients. The proposed 3D tracking algorithm is useful for real-time image guidance in prostate radiation therapy. Purpose: To develop a binary image reconstruction method for the autolocalization of metallic object(s) in CT with sparse projections. Methods: The authors divide the system into two types of contents: Metal(s) and nonmetal(s). The boundaries of metallic objects are obtained by using a penalized weighted least-squares algorithm with the adequate intensity gradient-controlled. A novel mechanism of \\u201camplifying\\u201d the difference between metal(s) and nonmetallic substances is introduced by preprocessing the sinogram data, which is shown to be necessary in dealing with a case with sparse projection data. A series of experimental studies are performed to evaluate the proposed approach. Results: A novel binary CTimage reconstruction formalism is established for the autodetermination of the shape and location of metallic objects in the presence of limited number of projections. Experimental studies reveal that the presented algorithm works well even when the embedded metal object(s) has different shape(s). It is also shown that when the projection data are sparse, a differential manipulation of projection data can greatly facilitate the binary reconstruction process and allow the authors to obtain accurate binary CTimages that would otherwise be unattainable. Conclusions: Binary CTreconstruction provides a viable method for determining the geometric distribution information of the implanted metal objects in CTimaging. Purpose: This study presents an improved technique to further simplify the fluence-map in intensity modulated radiation therapy (IMRT) inverse planning, thereby reducing plan complexity and improving delivery efficiency, while maintaining the plan quality.  Methods: First-order total-variation (TV) minimization (min.) based on L1-norm has been proposed to reduce the complexity of fluence-map in IMRT by generating sparse fluence-map variations. However, with stronger dose sparing to the critical structures, the inevitable increase in the fluence-map complexity can lead to inefficient dose delivery. Theoretically, L0-min. is the ideal solution for the sparse signal recovery problem, yet practically intractable due to its nonconvexity of the objective function. As an alternative, the authors use the iteratively reweighted L1-min. technique to incorporate the benefits of the L0-norm into the tractability of L1-min. The weight multiplied to each element is inversely related to the magnitude of the corresponding element, which is iteratively updated by the reweighting process. The proposed penalizing process combined with TV min. further improves sparsity in the fluence-map variations, hence ultimately enhancing the delivery efficiency. To validate the proposed method, this work compares three treatment plans obtained from quadratic min. (generally used in clinic IMRT), conventional TV min., and our proposed reweighted TV min. techniques, implemented by a large-scale L1-solver (template for first-order conic solver), for five patient clinical data. Criteria such as conformation number (CN), modulation index (MI), and estimated treatment time are employed to assess the relationship between the plan quality and delivery efficiency.  Results: The proposed method yields simpler fluence-maps than the quadratic and conventional TV based techniques. To attain a given CN and dose sparing to the critical organs for 5 clinical cases, the proposed method reduces the number of segments by 10\\u201315 and 30\\u201335, relative to TV min. and quadratic min. based plans, while MIs decreases by about 20%\\u201330% and 40%\\u201360% over the plans by two existing techniques, respectively. With such conditions, the total treatment time of the plans obtained from our proposed method can be reduced by 12\\u201330 s and 30\\u201380 s mainly due to greatly shorter multileaf collimator (MLC) traveling time in IMRT step-and-shoot delivery.  Conclusions: The reweighted L1-minimization technique provides a promising solution to simplify the fluence-map variations in IMRT inverse planning. It improves the delivery efficiency by reducing the entire segments and treatment time, while maintaining the plan quality in terms of target conformity and critical structure sparing. Adaptive radiation therapy (ART) is the incorporation of daily images in the radiotherapy treatment process so that the treatment plan can be evaluated and modified to maximize the amount of radiation dose to the tumor while minimizing the amount of radiation delivered to healthy tissue. Registration of planning images with daily images is thus an important component of ART. In this article, the authors report their research on multiscale registration of planning computed tomography (CT) images with daily cone beam CT (CBCT) images. The multiscale algorithm is based on the hierarchical multiscale image decomposition of E. Tadmor, S. Nezzar, and L. Vese [Multiscale Model. Simul. 2(4), pp. 554\\u2013579 (2004)]. Registration is achieved by decomposing the images to be registered into a series of scales using the (BV, L2) decomposition and initially registering the coarsest scales of the image using a landmark-based registration algorithm. The resulting transformation is then used as a starting point to deformably register the next coarse scales with one another. This procedure is iterated at each stage using the transformation computed by the previous scale registration as the starting point for the current registration. The authors present the results of studies of rectum, head-neck, and prostate CT-CBCT registration, and validate their registration method quantitatively using synthetic results in which the exact transformations our known, and qualitatively using clinical deformations in which the exact results are not known. Intrafraction organ motion can limit the advantage of highly conformal dose techniques such as intensity modulated radiation therapy (IMRT) due to target position uncertainty. To ensure high accuracy in beam targeting, real-time knowledge of the target location is highly desired throughout the beam delivery process. This knowledge can be gained through imaging of internally implanted radio-opaque markers with fluoroscopic or electronic portal imaging devices (EPID). In the case of MV based images, marker detection can be problematic due to the significantly lower contrast between different materials in comparison to their kV-based counterparts. This work presents a fully automated algorithm capable of detecting implanted metallic markers in both kV and MV images with high consistency. Using prior CT information, the algorithm predefines the volumetric search space without manual region-of-interest (ROI) selection by the user. Depending on the template selected, both spherical and cylindrical markers can be detected. Multiple markers can be simultaneously tracked without indexing confusion. Phantom studies show detection success rates of 100% for both kV and MV image data. In addition, application of the algorithm to real patient image data results in successful detection of all implanted markers for MV images. Near real-time operational speeds of \\u223c10 frames\\u2215sec for the detection of five markers in a 1024\\u00d7768 image are accomplished using an ordinary PC workstation. Cone-beam CT (CBCT) is being increasingly used in modern radiation therapy. However, as compared to conventional CT, the degraded image quality of CBCT hampers its applications in radiation therapy. Due to the large volume of x-ray illumination, scatter is considered as one of the fundamental limitations of CBCT image quality. Many scatter correction algorithms have been proposed in the literature, while drawbacks still exist. In this work, we propose a correction algorithm which is particularly useful in radiation therapy. Since the same patient is scanned repetitively during one radiation treatment course, we measure the scatter distribution in one scan, and use the measured scatter distribution to estimate and correct scatter in the following scans. A partially blocked CBCT is used in the scatter measurement scan. The x-ray beam blocker has a strip pattern, such that the whole-field scatter distribution can be estimated from the detected signals in the shadow region and the patient rigid transformation can be determined from the reconstructed image using the illuminated detector projection data. From the derived patient transformation, the measured scatter is then modified accordingly and used for scatter correction in the following regular CBCT scans. The proposed method has been evaluated using Monte Carlo simulations and physical experiments on an anthropomorphic chest phantom. The results show a significant suppression of scatter artifacts using the proposed method. Using the reconstruction in a narrow collimator geometry as a reference, the comparison also shows that the proposed method reduces reconstruction error from 13.2% to 3.8%. The proposed method is attractive in applications where a high CBCT image quality is critical, for example, dose calculation in adaptive radiation therapy. Multiscale image registration techniques are presented for the reg- istration of medical images using deformable registration models. The tech- niques are particularly effective for registration problems in which one or both of the images to be registered contains significant levels of noise. A brief overview of existing deformable registration techniques is presented, and exper- iments using B-spline free-form deformation registration models demonstrate that ordinary deformable registration techniques fail to produce accurate re- sults in the presence of significant levels of noise. The hierarchical multiscale image decomposition described in E. Tadmor, S. Nezzar, and L. Vese's, ''A multiscale image representation using hierarchical (BV,L2) decompositions'' ( Multiscale Modeling and Simulations , 2 (2004): 4, pp. 554-579) is reviewed, and multiscale image registration algorithms are developed based on the mul- tiscale decomposition. Accurate registration of noisy images is achieved by obtaining a hierarchical multiscale decomposition of the images and iteratively registering the resulting components. This approach enables a successful reg- istration of images that contain noise levels well beyond the level at which ordinary deformable registration fails. Numerous image registration experi- ments demonstrate the accuracy and efficiency of the multiscale registration techniques. Purpose: On-board 4D cone beam CT (4DCBCT) offers respiratory phase-resolved volumetric imaging, and improves the accuracy of target localization in image guided radiation therapy. However, the clinical utility of this technique has been greatly impeded by its degraded image quality, prolonged imaging time, and increased imaging dose. The purpose of this letter is to develop a novel iterative 4DCBCT reconstruction method for improved image quality, increased imaging speed, and reduced imaging dose.  Methods: The essence of this work is to introduce the spatiotemporal tensor framelet (STF), a high-dimensional tensor generalization of the 1D framelet for 4DCBCT, to effectively take into account of highly correlated and redundant features of the patient anatomy during respiration, in a multilevel fashion with multibasis sparsifying transform. The STF-based algorithm is implemented on a GPU platform for improved computational efficiency. To evaluate the method, 4DCBCT full-fan scans were acquired within 30 s, with a gantry rotation of 200\\u00b0; STF is also compared with a state-of-art reconstruction method via spatiotemporal total variation regularization.  Results: Both the simulation and experimental results demonstrate that STF-based reconstruction achieved superior image quality. The reconstruction of 20 respiratory phases took less than 10 min on an NVIDIA Tesla C2070 GPU card. The STF codes are available at https://sites.google.com/site/spatiotemporaltensorframelet.  Conclusions: By effectively utilizing the spatiotemporal coherence of the patient anatomy among different respiratory phases in a multilevel fashion with multibasis sparsifying transform, the proposed STF method potentially enables fast and low-dose 4DCBCT with improved image quality. In this study, the authors introduce and demonstrate quality control procedures for evaluating the geometric and dosimetric fidelity of dynamic treatment delivery techniques involving treatment couch motion synchronous with gantry and multileaf collimator (MLC). Tests were designed to evaluate positional accuracy, velocity constancy and accuracy for dynamic couch motion under a realistic weight load. A test evaluating the geometric accuracy of the system in delivering treatments over complex dynamic trajectories was also devised. Custom XML scripts that control the Varian TrueBeam\\u2122 STx (Serial #3) axes in Developer Mode were written to implement the delivery sequences for the tests. Delivered dose patterns were captured with radiographic film or the electronic portal imaging device. The couch translational accuracy in dynamic treatment mode was 0.01 cm. Rotational accuracy was within 0.3\\u00b0, with 0.04 cm displacement of the rotational axis. Dose intensity profiles capturing the velocity constancy and accuracy for translations and rotation exhibited standard deviation and maximum deviations below 3%. For complex delivery involving MLC and couch motions, the overall translational accuracy for reproducing programmed patterns was within 0.06 cm. The authors conclude that in Developer Mode, TrueBeam\\u2122 is capable of delivering dynamic treatment delivery techniques involving couch motion with good geometric and dosimetric fidelity. The purpose of this work is to relate the gating window and displacement of a moving tumor target and develop a systematic method to individualize the gating window for respiration-gated radiation therapy (RT). As the relationship between patient anatomy and respiration phase is contained in 4D images, we aim to quantify this information and utilize the data to guide gated treatment planning. After 4D image acquisition, the target and organs at risk were delineated manually on the selected gating phase. The contours were propagated automatically onto every phase-specific image set using a control volume-based contour mapping technique. The mean and maximum distances between the contours in the gating phase and each of other phases were evaluated in three dimensions. The gating window was determined in such a way that the residual movement of the target within the window is smaller or equal to the patient's setup error. The proposed method was applied to plan the gated treatments of 12 lung cancer patients. As a result of this work, a method to calculate patient-specific gating windows has been developed. The general reference drawn from this study is that, with the aide of 4D images and automated 4D contour propagation, it is feasible to individualize the gating widow selection. As compared with the current practice, the proposed technique has a potential to eliminate the guesswork involved in choosing a gating window and avoid dosimetric error in planning gated RT. In conclusion, individualization of gating windows reduces the subjectivity in respiration-gated RT and improves the treatment of moving targets. Purpose: 3D optical surface imaging has been applied to patient positioning in radiation therapy (RT). The optical patient positioning system is advantageous over conventional method using cone-beam computed tomography (CBCT) in that it is radiation free, frameless, and is capable of real-time monitoring. While the conventional radiographic method uses volumetric registration, the optical system uses surface matching for patient alignment. The relative accuracy of these two methods has not yet been sufficiently investigated. This study aims to investigate the theoretical accuracy of the surface registration based on a simulation study using patient data. Methods: This study compares the relative accuracy of surface and volumetric registration in head-and-neck RT. The authors examined 26 patient data sets, each consisting of planning CT data acquired before treatment and patient setup CBCT data acquired at the time of treatment. As input data of surface registration, patient\\u2019s skin surfaces were created by contouring patient skin from planning CT and treatment CBCT. Surface registration was performed using the iterative closest points algorithm by point\\u2013plane closest, which minimizes the normal distance between source points and target surfaces. Six degrees of freedom (three translations and three rotations) were used in both surface and volumetric registrations and the results were compared. The accuracy of each method was estimated by digital phantom tests. Results: Based on the results of 26 patients, the authors found that the average and maximum root-mean-square translation deviation between the surface and volumetric registrations were 2.7 and 5.2 mm, respectively. The residual error of the surface registration was calculated to have an average of 0.9 mm and a maximum of 1.7 mm. Conclusions: Surface registration may lead to results different from those of the conventional volumetric registration. Only limited accuracy can be achieved for patient positioning with an approach based solely on surface information. Respiratory tumor motion is a major challenge in radiation therapy for thoracic and abdominal cancers. Effective motion management requires an accurate knowledge of the real-time tumor motion. External respiration monitoring devices (optical, etc) provide a noninvasive, non-ionizing, low-cost and practical approach to obtain the respiratory signal. Due to the highly complex and nonlinear relations between tumor and surrogate motion, its ultimate success hinges on the ability to accurately infer the tumor motion from respiratory surrogates. Given their widespread use in the clinic, such a method is critically needed. We propose to use a powerful memory-based learning method to find the complex relations between tumor motion and respiratory surrogates. The method first stores the training data in memory and then finds relevant data to answer a particular query. Nearby data points are assigned high relevance (or weights) and conversely distant data are assigned low relevance. By fitting relatively simple models to local patches instead of fitting one single global model, it is able to capture highly nonlinear and complex relations between the internal tumor motion and external surrogates accurately. Due to the local nature of weighting functions, the method is inherently robust to outliers in the training data. Moreover, both training and adapting to new data are performed almost instantaneously with memory-based learning, making it suitable for dynamically following variable internal/external relations. We evaluated the method using respiratory motion data from 11 patients. The data set consists of simultaneous measurement of 3D tumor motion and 1D abdominal surface (used as the surrogate signal in this study). There are a total of 171 respiratory traces, with an average peak-to-peak amplitude of \\u223c15\\u00a0mm and average duration of \\u223c115\\u00a0s per trace. Given only 5\\u00a0s (roughly one breath) pretreatment training data, the method achieved an average 3D error of 1.5\\u00a0mm and 95th percentile error of 3.4\\u00a0mm on unseen test data. The average 3D error was further reduced to 1.4\\u00a0mm when the model was tuned to its optimal setting for each respiratory trace. In one trace where a few outliers are present in the training data, the proposed method achieved an error reduction of as much as \\u223c50% compared with the best linear model (1.0\\u00a0mm versus 2.1\\u00a0mm). The memory-based learning technique is able to accurately capture the highly complex and nonlinear relations between tumor and surrogate motion in an efficient manner (a few milliseconds per estimate). Furthermore, the algorithm is particularly suitable to handle situations where the training data are contaminated by large errors or outliers. These desirable properties make it an ideal candidate for accurate and robust tumor gating/tracking using respiratory surrogates. In a treatment planning system for intensity modulated radiation therapy(IMRT), the time sequence of multileaf collimator(MLC) settings are derived from an optimal fluence map as a post-optimization process using a software module called a \\u201cleaf sequencer.\\u201d The dosimetric accuracy of the dynamic delivery depends on the functionality of the module and it is important to verify independently the correctness of the leaf sequences for each field of a patient treatment. This verification is unique to the IMRT treatment and has been done using radiographic film, electronic portal imaging device(EPID) or electronic imaging system (BIS). The measurement tests both the leaf sequencer and the dynamic multileaf collimator(MLC)delivery system, providing a reliable assurance of clinical IMRT treatment. However, this process is labor intensive and time consuming. In this paper, we propose to separate quality assurance (QA) of the leaf sequencer from the dynamic MLCdelivery system. We describe a simple computer algorithm for the verification of the leaf sequences. The software reads in the leaf sequences and simulates the motion of the MLC leaves. The generated fluence map is then compared quantitatively with the reference map from the treatment planning system. A set of pre-defined QA indices is introduced to measure the \\u201ccloseness\\u201d between the computed and the reference maps. The approach has been used to validate the CORVUS (NOMOS Co., Sewickley, PA) treatment plans. The results indicate that the proposed approach is robust and suitable to support the complex IMRT QA process. Three-dimensional x-ray cone-beam CT (CBCT) is being increasingly used in radiation therapy. Since the whole treatment course typically lasts several weeks, the repetitive x-ray imaging results in large radiation dose delivered on the patient. In the current radiation therapy treatment, CBCT is mainly used for patient set-up, and a rigid transformation of the CBCT data from the planning CT data is also assumed. For an accurate rigid registration, it is not necessary to acquire a full 3D image. In this paper, we propose a patient set-up protocol based on partially blocked CBCT. A sheet of lead strips is inserted between the x-ray source and the scanned patient. From the incomplete projection data, only several axial slices are reconstructed and used in the image registration for patient set-up. Since the radiation is partially blocked, the dose delivered onto the patient is significantly reduced, with an additional benefit of reduced scatter signals. The proposed approach is validated using experiments on ... Conventional VMAT optimizes aperture shapes and weights at uniformly sampled stations, which is a generalization of the concept of a control point. Recently, rotational station parameter optimized radiation therapy (SPORT) has been proposed to improve the plan quality by inserting beams to the regions that demand additional intensity modulations, thus formulating non-uniform beam sampling. This work presents a new rotational SPORT planning strategy based on reweighted total-variation (TV) minimization (min.), using beam?s-eye-view dosimetrics (BEVD) guided beam selection. The convex programming based reweighted TV min. assures the simplified fluence-map, which facilitates single-aperture selection at each station for single-arc delivery. For the rotational arc treatment planning and non-uniform beam angle setting, the mathematical model needs to be modified by additional penalty term describing the fluence-map similarity and by determination of appropriate angular weighting factors. The proposed algorithm with additional penalty term is capable of achieving more efficient and deliverable plans adaptive to the conventional VMAT and SPORT planning schemes by reducing the dose delivery time about 5 to 10?s in three clinical cases (one prostate and two head-and-neck (HN) cases with a single and multiple targets). The BEVD guided beam selection provides effective and yet easy calculating methodology to select angles for denser, non-uniform angular sampling in SPORT planning. Our BEVD guided SPORT treatment schemes improve the dose sparing to femoral heads in the prostate and brainstem, parotid glands and oral cavity in the two HN cases, where the mean dose reduction of those organs ranges from 0.5 to 2.5?Gy. Also, it increases the conformation number assessing the dose conformity to the target from 0.84, 0.75 and 0.74 to 0.86, 0.79 and 0.80 in the prostate and two HN cases, while preserving the delivery efficiency, relative to conventional single-arc VMAT plans. Purpose Newly emerged four-dimensional (4D) imaging techniques such as 4D-computed tomography (CT), 4D-cone beam CT, 4D-magnetic resonance imaging, and 4D-positron emission tomography are effective tools to reveal the spatiotemporal details of patients' anatomy. To use the 4D data acquired under different conditions or using different modalities, an algorithm for registering 4D images must be in place. We developed an automated 4D-4D registration method to take advantage of 4D information. Methods and Materials We used 4D-4D matching to find the appropriate three-dimensional anatomy in the fixed image for each phase of the moving image and spatially register them. A search algorithm was implemented to simultaneously find the best phase and spatial match of two 4D inputs. An interpolation scheme capable of deriving an image set based on temporally adjacent three-dimensional data sets was developed to deal with the situation in which the discrete temporal points of the two inputs do not coincide or correspond. Results In a phantom study, our technique was able to reproduce the known \\u201cground truth\\u201d with high spatial fidelity. The technique regenerated all deliberately introduced \\u201cmissing\\u201d three-dimensional images at different phases of the input using temporal interpolation. In the registration of gated-magnetic resonance imaging and 4D-CT, the algorithm was able to select the appropriate CT phase. The technique was also able to register 4D-CT with 4D-cone beam CT and two 4D-CT scans acquired at different times. A spatial accuracy of Conclusion Automated 4D-4D registration can find the best possible spatiotemporal match between two 4D data sets and is useful for image-guided radiotherapy applications. In intensity-modulated radiation therapy (IMRT), the incident beam orientations are often determined by a trial and error search. The conventional beam's-eye view (BEV) tool becomes less helpful in IMRT because it is frequently required that beams go through organs at risk (OARs) in order to achieve a compromise between the dosimetric objectives of the planning target volume (PTV) and the OARs. In this paper, we report a beam's-eye view dosimetrics (BEVD) technique to assist in the selection of beam orientations in IMRT. In our method, each beam portal is divided into a grid of beamlets. A score function is introduced to measure the `goodness' of each beamlet at a given gantry angle. The score is determined by the maximum PTV dose deliverable by the beamlet without exceeding the tolerance doses of the OARs and normal tissue located in the path of the beamlet. The overall score of the gantry angle is given by a sum of the scores of all beamlets. For a given patient, the score function is evaluated for each possible beam orientation. The directions with the highest scores are then selected as the candidates for beam placement. This procedure is similar to the BEV approach used in conventional radiation therapy, except that the evaluation by a human is replaced by a score function to take into account the intensity modulation. This technique allows one to select beam orientations without the excessive computing overhead of computer optimization of beam orientation. It also provides useful insight into the problem of selection of beam orientation and is especially valuable for complicated cases where the PTV is surrounded by several sensitive structures and where it is difficult to select a set of `good' beam orientations. Several two-dimensional (2D) model cases were used to test the proposed technique. The plans obtained using the BEVD-selected beam orientations were compared with the plans obtained using equiangular spaced beams. For all the model cases investigated, the use of BEVD-selected beam orientations improved the dose distributions significantly. These examples indicate that the technique has considerable potential for simplifying the IMRT treatment planning process and allows for better utilization of the technical capacity of IMRT. The advantage of highly conformal dose techniques such as 3DCRT and IMRT is limited by intrafraction organ motion. A new approach to gain near real-time 3D positions of internally implanted fiducial markers is to analyze simultaneous onboard kV beam and treatment MV beam images (from fluoroscopic or electronic portal image devices). Before we can use this real-time image guidance for clinical 3DCRT and IMRT treatments, four outstanding issues need to be addressed. (1) How will fiducial motion blur the image and hinder tracking fiducials? kV and MV images are acquired while the tumor is moving at various speeds. We find that a fiducial can be successfully detected at a maximum linear speed of 1.6 cm/s. (2) How does MV beam scattering affect kV imaging? We investigate this by varying MV field size and kV source to imager distance, and find that common treatment MV beams do not hinder fiducial detection in simultaneous kV images. (3) How can one detect fiducials on images from 3DCRT and IMRT treatment beams when the MV fields are modified by a multileaf collimator (MLC)? The presented analysis is capable of segmenting a MV field from the blocking MLC and detecting visible fiducials. This enables themore\\u00a0\\u00bb calculation of nearly real-time 3D positions of markers during a real treatment. (4) Is the analysis fast enough to track fiducials in nearly real time? Multiple methods are adopted to predict marker positions and reduce search regions. The average detection time per frame for three markers in a 1024x768 image was reduced to 0.1 s or less. Solving these four issues paves the way to tracking moving fiducial markers throughout a 3DCRT or IMRT treatment. Altogether, these four studies demonstrate that our algorithm can track fiducials in real time, on degraded kV images (MV scatter), in rapidly moving tumors (fiducial blurring), and even provide useful information in the case when some fiducials are blocked from view by the MLC. This technique can provide a gating signal or be used for intra-fractional tumor tracking on a Linac equipped with a kV imaging system. Any motion exceeding a preset threshold can warn the therapist to suspend a treatment session and reposition the patient.\\u00ab\\u00a0less Purpose: To develop a method for deriving the phase-binned four-dimensional computed tomography (4D CT) image sets through interpolation of the images acquired at some known phases. Methods and Materials: Four-dimensional computed tomography data sets for 3 patients were acquired. For each patient, the correlation between inhale and exhale phases was studied and quantified using a BSpline deformable model. Images at an arbitrary phase were deduced by an interpolation of the deformation coefficients. The accuracy of the proposed scheme was assessed by comparing marker trajectories and by checkerboard/difference display of the interpolated and acquired images. Results: The images at intermediate phases could be derived by an interpolation of the deformation field. An analysis of marker movements indicated that 3 mm accuracy is achievable by the interpolation. The subtraction of image analysis indicated a similar level of success. The proposed technique was useful also for automatically mapping the organ contours in a known phase to other phases, and for designing patient-specific margins in the presence of respiratory motion. Finally, the technique led to a 90% reduction in the acquired data, because in the BSpline model, a lattice of only a few thousand values is sufficient to describe a CT data set of 25 million pixels. Conclusions: Organ deformation can be well modeled by using a BSpline model. The proposed technique may offer useful means for radiation dose reduction, binning artifacts removal, and disk storage improvement in 4D imaging. The looming potential of deformable alignment tools to play an integral role in adaptive radiotherapy suggests a need for objective assessment of these complex algorithms. Previous studies in this area are based on the ability of alignment to reproduce analytically generated deformations applied to sample image data, or use of contours or bifurcations as ground truth for evaluation of alignment accuracy. In this study, a deformable phantom was embedded with 48 small plastic markers, placed in regions varying from high contrast to roughly uniform regional intensity, and small to large regional discontinuities in movement. CT volumes of this phantom were acquired at different deformation states. After manual localization of marker coordinates, images were edited to remove the markers. The resulting image volumes were sent to five collaborating institutions, each of which has developed previously published deformable alignment tools routinely in use. Alignments were done, and applied to the list of reference coordinates at the inhale state. The transformed coordinates were compared to the actual marker locations at exhale. A total of eight alignment techniques were tested from the six institutions. All algorithms performed generally well, as compared to previous publications. Average errors in predicted location ranged from 1.5 to 3.9 mm, depending on technique. No algorithm was uniformly accurate across all regions of the phantom, with maximum errors ranging from 5.1 to 15.4 mm. Larger errors were seen in regions near significant shape changes, as well as areas with uniform contrast but large local motion discontinuity. Although reasonable accuracy was achieved overall, the variation of error in different regions suggests caution in globally accepting the results from deformable alignment. The cone-beam computed tomography (CBCT) imaging modality is an integral component of image-guided adaptive radiation therapy (IGART), which uses patient-specific dynamic/temporal information for potential treatment plan modification. In this study, an offline process for the integral component IGART framework has been implemented that consists of deformable image registration (DIR) and its validation, dose reconstruction, dose accumulation and dose verification. This study compares the differences between planned and estimated delivered doses under an IGART framework of five patients undergoing prostate cancer radiation therapy. The dose calculation accuracy on CBCT was verified by measurements made in a Rando pelvic phantom. The accuracy of DIR on patient image sets was evaluated in three ways: landmark matching with fiducial markers, visual image evaluation and unbalanced energy (UE); UE has been previously demonstrated to be a feasible method for the validation of DIR accuracy at a voxel level. The dose calculated on each CBCT image set was reconstructed and accumulated over all fractions to reflect the \\u2018actual dose\\u2019 delivered to the patient. The deformably accumulated (delivered) plans were then compared to the original (static) plans to evaluate tumor and normal tissue dose discrepancies. The results support the utility of adaptive planning, which can be used to fully elucidate the dosimetric impact based on the simulated delivered dose to achieve the desired tumor control and normal tissue sparing, which may be of particular importance in the context of hypofractionated radiotherapy regimens. Cone-beam CT (CBCT) is being increasingly used in modern radiation therapy for patient setup and adaptive replanning. However, due to the large volume of x-ray illumination, scatter becomes a rather serious problem and is considered as one of the fundamental limitations of CBCT image quality. Many scatter correction algorithms have been proposed in literature, while a standard practical solution still remains elusive. In radiation therapy, the same patient is scanned repetitively during a course of treatment, a natural question to ask is whether one can obtain the scatter distribution on the first day of treatment and then use the data for scatter correction in the subsequent scans on different days. To realize this scatter removal scheme, two technical pieces must be in place: (i) A strategy to obtain the scatter distribution in on-board CBCT imaging and (ii) a method to spatially match a prior scatter distribution with the on-treatment CBCT projection data for scatter subtraction. In this work, simple solutions to the two problems are provided. A partially blocked CBCT is used to extract the scatter distribution. The x-ray beam blocker has a strip pattern, such that partial volume can still be accurately reconstructed and the whole-field scatter distribution can be estimated from the detected signals in the shadow regions using interpolation\\u2215extrapolation. In the subsequent scans, the patient transformation is determined using a rigid registration of the conventional CBCT and the prior partial CBCT. From the derived patient transformation, the measured scatter is then modified to adapt the new on-treatment patient geometry for scatter correction. The proposed method is evaluated using physical experiments on a clinical CBCT system. On the Catphan\\u00a9600 phantom, the errors in Hounsfield unit (HU) in the selected regions of interest are reduced from about 350 to below 50 HU; on an anthropomorphic phantom, the error is reduced from 15.7% to 5.4%. The proposed method is attractive in applications where a high CBCT image quality is critical, for example, dose calculation in adaptive radiation therapy. X-ray luminescence tomography (XLT) has recently been proposed as a new imaging modality for biological imaging applications. This modality utilizes phosphor nanoparticles which luminesce near-infrared light when excited by x-ray photons. The advantages of this modality are that it uniquely combines the high sensitivity of radioluminescent nanoparticles and the high spatial localization of collimated x-ray beams. Currently, XLT has been demonstrated using x-ray spatial encoding to resolve the imaging volume. However, there are applications where the x-ray excitation may be limited by geometry, where increased temporal resolution is desired, or where a lower dose is mandatory. This paper extends the utility of XLT to meet these requirements by incorporating a photon propagation model into the reconstruction algorithm in an x-ray limited-angle (LA) geometry. This enables such applications as image-guided surgery, where the ability to resolve lesions at depths of several centimeters can be the key to successful resection. The hybrid x-ray/diffuse optical model is first formulated and then demonstrated in a breast-sized phantom, simulating a breast lumpectomy geometry. Both numerical and experimental phantoms are tested, with lesion-simulating objects of various sizes and depths. Results show localization accuracy with median error of 2.2 mm, or 4% of object depth, for small 2\\u201314 mm diameter lesions positioned from 1 to 4.5 cm in depth. This compares favorably with fluorescence optical imaging, which is not able to resolve such small objects at this depth. The recovered lesion size has lower size bias in the x-ray excitation direction than the optical direction, which is expected due to the increased optical scatter. However, the technique is shown to be quite invariant in recovered size with respect to depth, as the standard deviation is less than 2.5 mm. Sensitivity is a function of dose; radiological doses are found to provide sufficient recovery for \\u00b5g ml\\u22121 concentrations, while therapy dosages provide recovery for ng ml\\u22121 concentrations. Experimental phantom results agree closely with the numerical results, with positional errors recovered within 8.6% of the effective depth for a 5 mm object, and within 5.2% of the depth for a 10 mm object. Object-size median error is within 2.3% and 2% for the 5 and 10 mm objects, respectively. For shallow-to-medium depth applications where optical and radio-emission imaging modalities are not ideal, such as in intra-operative procedures, LAXLT may be a useful tool to detect molecular signatures of disease. A multiscale image registration technique is presented for the registration of medical images that contain significant levels of noise. An overview of the medical image registration problem is presented, and various registration techniques are discussed. Experiments using mean squares, normalized correlation, and mutual information optimal linear registration are presented that determine the noise levels at which registration using these techniques fails. Further experiments in which classical denoising algorithms are applied prior to registration are presented, and it is shown that registration fails in this case for significantly high levels of noise, as well. The hierarchical multiscale image decomposition of E. Tadmor, S. Nezzar, and L. Vese [20] is presented, and accurate registration of noisy images is achieved by obtaining a hierarchical multiscale decomposition of the images and registering the resulting components. This approach enables successful registration of images that contain noise levels well beyond the level at which ordinary optimal linear registration fails. Image registration experiments demonstrate the accuracy and efficiency of the multiscale registration technique, and for all noise levels, the multiscale technique is as accurate as or more accurate than ordinary registration techniques. Purpose: To report the characteristics of prostate motion as tracked by the stereoscopic X-ray images of the implanted fiducials during hypofractionated radiotherapy with CyberKnife. Methods and Materials: Twenty-one patients with prostate cancer who were treated with CyberKnife between January 2005 and September 2007 were selected for this retrospective study. The CyberKnife uses a stereoscopic X-ray system to obtain the position of the prostate target through the monitoring of implanted gold fiducial markers. If there is a significant deviation, the treatment is paused while the patient is repositioned by moving the couch. The deviations calculated from X-ray images acquired within the time interval between two consecutive couch motions constitute a data set. Results: Included in the analysis were 427 data sets and 4,439 time stamps of X-ray images. The mean duration for each data set was 697 sec. At 30 sec, a motion >2 mm exists in about 5% of data sets. The percentage is increased to 8%, 11%, and 14% at 60 sec, 90 sec, and 120 sec, respectively. A similar trend exists for other values of prostate motion. Conclusions: With proper monitoring and intervention during treatment, the prostate shifts observed among patients can be kept within the tracking rangemore\\u00a0\\u00bb of the CyberKnife. On average, a sampling rate of {approx}40 sec between consecutive X-rays is acceptable to ensure submillimeter tracking. However, there is significant movement variation among patients, and a higher sampling rate may be necessary in some patients.\\u00ab\\u00a0less The medical linear accelerator (linac) integrated with a kilovoltage (kV) flat-panel imager has been emerging as an important piece of equipment for image-guided radiation therapy. Due to the sagging of the linac head and the flexing of the robotic arms that mount the x-ray tube and flat-panel detector, geometric nonidealities generally exist in the imaging geometry no matter whether it is for the two-dimensional projection image or three-dimensional cone-beam computed tomography. Normally, the geometric parameters are established during the commissioning and incorporated in correction software in respective image formation or reconstruction. A prudent use of an on-board imaging system necessitates a routine surveillance of the geometric accuracy of the system like the position of the x-ray source, imager position and orientation, isocenter, rotation trajectory, and source-to-imager distance. Here we describe a purposely built phantom and a data analysis software for monitoring these important parameters of the system in an efficient and automated way. The developed tool works equally well for the megavoltage (MV) electronic portal imaging device and hence allows us to measure the coincidence of the isocenters of the MV and kV beams of the linac. This QA tool can detect an angular uncertainty of 0.1 deg. of the more\\u00a0\\u00bb x-ray source. For spatial uncertainties, such as the source position, the imager position, or the kV/MV isocenter misalignment, the demonstrated accuracy of this tool was better than 1.6 mm. The developed tool provides us with a simple, robust, and objective way to probe and monitor the geometric status of an imaging system in a fully automatic process and facilitate routine QA workflow in a clinic. \\u00ab\\u00a0less Intensity modulated radiation therapy (IMRT) inverse planning is conventionally done in two steps. Firstly, the intensity maps of the treatment beams are optimized using a dose optimization algorithm. Each of them is then decomposed into a number of segments using a leaf-sequencing algorithm for delivery. An alternative approach is to pre-assign a fixed number of field apertures and optimize directly the shapes and weights of the apertures. While the latter approach has the advantage of eliminating the leaf-sequencing step, the optimization of aperture shapes is less straightforward than that of beamlet-based optimization because of the complex dependence of the dose on the field shapes, and their weights. In this work we report a genetic algorithm for segment-based optimization. Different from a gradient iterative approach or simulated annealing, the algorithm finds the optimum solution from a population of candidate plans. In this technique, each solution is encoded using three chromosomes: one for the position of the left-bank leaves of each segment, the second for the position of the right-bank and the third for the weights of the segments defined by the first two chromosomes. The convergence towards the optimum is realized by crossover and mutation operators that ensure proper exchange of information between the three chromosomes of all the solutions in the population. The algorithm is applied to a phantom and a prostate case and the results are compared with those obtained using beamlet-based optimization. The main conclusion drawn from this study is that the genetic optimization of segment shapes and weights can produce highly conformal dose distribution. In addition, our study also confirms previous findings that fewer segments are generally needed to generate plans that are comparable with the plans obtained using beamlet-based optimization. Thus the technique may have useful applications in facilitating IMRT treatment planning. Purpose: To implement and evaluate clinic-ready adaptive imaging protocols for online patient repositioning (motion tracking) during prostate IMRT using treatment beam imaging supplemented by minimal, as-needed use of on-board kV. Methods: The authors examine the two-step decision-making strategy: (1) Use cine-MV imaging and online-updated characterization of prostate motion to detect target motion that is potentially beyond a predefined threshold and (2) use paired MV-kV 3D localization to determine overthreshold displacement and, if needed, reposition the patient. Two levels of clinical implementation were evaluated: (1) Field-by-field based motion correction for present-day linacs and (2) instantaneous repositioning for new-generation linacs with capabilities of simultaneous MV-kV imaging and remote automatic couch control during treatment delivery. Experiments were performed on a Varian Trilogy linac in clinical mode using a 4D motion phantom programed with prostate motion trajectories taken from patient data. Dosimetric impact was examined using a 2D ion chamber array. Simulations were done for 536 trajectories from 17 patients. Results: Despite the loss of marker detection efficiency caused by the MLC leaves sometimes obscuring the field at the marker\\u2019s projected position on the MV imager, the field-by-field correction halved (from 23% to 10%) the mean percentage of time that target displacement exceeded a 3 mm threshold, as compared to no intervention. This was achieved at minimal cost in additional imaging (average of one MV-kV pair per two to three treatment fractions) and with a very small number of repositionings (once every four to five fractions). Also with low kV usage ( \\u223c 2 / fraction ) , the instantaneous repositioning approach reduced overthreshold time by more than 75% (23% to 5%) even with severe MLC blockage as often encountered in current IMRT and could reduce the overthreshold time tenfold (to 2 % ) if the MLC blockage problem were relieved. The information acquired for repositioning using combined MV-kV images was found to have submillimeter accuracy. Conclusions: This work demonstrated with a current clinical setup that substantial reduction of adverse targeting effects of intrafraction prostate motion can be realized. The proposed adaptive imaging strategy incurs minimal imagingdose to the patient as compared to other stereoscopic imaging techniques. We propose a hybrid multiobjective (MO) evolutionary optimization algorithm (MOEA) for intensity-modulated radiotherapy inverse planning and apply it to optimize the number of incident beams, their orientations and intensity profiles. The algorithm produces a set of efficient solutions, which represent different clinical trade-offs and contains information such as variety of dose distributions and dose?volume histograms. No importance factors are required and solutions can be obtained in regions not accessible by conventional weighted sum approaches. The application of the algorithm using a test case, a prostate and a head and neck tumour case is shown. The results are compared with MO inverse planning using a gradient-based optimization algorithm. It is arguable that the imaging and delivery hardware necessary for delivering real-time adaptive image-guided radiotherapy is available on high-end linear accelerators. Robust and computationally efficient software is the limiting factor in achieving highly accurate and precise radiotherapy to the constantly changing anatomy of a cancer patient. The limitations are not caused by the availability of algorithms but rather issues of reliability, integration, and calculation time. However, each of the software components is an active area of research and development at academic and commercial centers. This article describes the software solutions in 4 broad areas: deformable image registration, adaptive replanning, real-time image guidance, and dose calculation and accumulation. Given the pace of technological advancement, the integration of these software solutions to develop real-time adaptive image-guided radiotherapy and the associated challenges they bring will be implemented to varying degrees by all major manufacturers over the coming years. Non-coplanar beams are important for treatment of both cranial and noncranial tumors. Treatment verification of such beams with couch rotation/kicks, however, is challenging, particularly for the application of cone beam CT (CBCT). In this situation, only limited and unconventional imaging angles are feasible to avoid collision between the gantry, couch, patient, and on-board imaging system. The purpose of this work is to develop a CBCT verification strategy for patients undergoing non-coplanar radiation therapy. We propose an image reconstruction scheme that integrates a prior image constrained compressed sensing (PICCS) technique with image registration. Planning CT or CBCT acquired at the neutral position is rotated and translated according to the nominal couch rotation/translation to serve as the initial prior image. Here, the nominal couch movement is chosen to have a rotational error of 5\\u00b0 and translational error of 8 mm from the ground truth in one or more axes or directions. The proposed reconstruction scheme alternates between two major steps. First, an image is reconstructed using the PICCS technique implemented with total-variation minimization and simultaneous algebraic reconstruction. Second, the rotational/translational setup errors are corrected and the prior image is updated by applying rigid image registration between the reconstructed image and the previous prior image. The PICCS algorithm and rigid image registration are alternated iteratively until the registration results fall below a predetermined threshold. The proposed reconstruction algorithm is evaluated with an anthropomorphic digital phantom and physical head phantom. The proposed algorithm provides useful volumetric images for patient setup using projections with an angular range as small as 60\\u00b0. It reduced the translational setup errors from 8 mm to generally <1 mm and the rotational setup errors from 5\\u00b0 to <1\\u00b0. Compared with the PICCS algorithm alone, the integration of rigid registration significantly improved the reconstructed image quality, with a reduction of mostly 2\\u20133 folds (up to 100) in root mean square image error. The proposed algorithm provides a remedy for solving the problem of non-coplanar CBCT reconstruction from limited angle of projections by combining the PICCS technique and rigid image registration in an iterative framework. In this proof of concept study, non-coplanar beams with couch rotations of 45\\u00b0 can be effectively verified with the CBCT technique. This paper presents a fast and accurate marker-based automatic registration technique for aligning uncalibrated projections taken from a transmission electron microscope (TEM) with different tilt angles and orientations. Most of the existing TEM image alignment methods estimate the similarity between images using the projection model with least-squares metric and guess alignment parameters by computationally expensive nonlinear optimization schemes. Approaches based on the least-squares metric which is sensitive to outliers may cause misalignment since automatic tracking methods, though reliable, can produce a few incorrect trajectories due to a large number of marker points. To decrease the influence of outliers, we propose a robust similarity measure using the projection model with a Gaussian weighting function. This function is very effective in suppressing outliers that are far from correct trajectories and thus provides a more robust metric. In addition, we suggest a fast search strategy based on the non-gradient Powell's multidimensional optimization scheme to speed up optimization as only meaningful parameters are considered during iterative projection model estimation. Experimental results show that our method brings more accurate alignment with less computational cost compared to conventional automatic alignment methods. On-board CBCT images are used to generate patient geometric models to assist patient setup. The image data can also, potentially, be used for dose reconstruction in combination with the fluence maps from treatment plan. Here we evaluate the achievable accuracy in using a kV CBCT for dose calculation. Relative electron density as a function of HU was obtained for both planning CT (pCT) and CBCT using a Catphan-600 calibration phantom. The CBCT calibration stability was monitored weekly for 8 consecutive weeks. A clinical treatment planning system was employed for pCT- and CBCT-based dose calculations and subsequent comparisons. Phantom and patient studies were carried out. In the former study, both Catphan-600 and pelvic phantoms were employed to evaluate the dosimetric performance of the full-fan and half-fan scanning modes. To evaluate the dosimetric influence of motion artefacts commonly seen in CBCT images, the Catphan-600 phantom was scanned with and without cyclic motion using the pCT and CBCT scanners. The doses computed based on the four sets of CT images (pCT and CBCT with/without motion) were compared quantitatively. The patient studies included a lung case and three prostate cases. The lung case was employed to further assess the adverse effect of intra-scan organ motion. Unlike the phantom study, the pCT of a patient is generally acquired at the time of simulation and the anatomy may be different from that of CBCT acquired at the time of treatment delivery because of organ deformation. To tackle the problem, we introduced a set of modified CBCT images (mCBCT) for each patient, which possesses the geometric information of the CBCT but the electronic density distribution mapped from the pCT with the help of a BSpline deformable image registration software. In the patient study, the dose computed with the mCBCT was used as a surrogate of the 'ground truth'. We found that the CBCT electron density calibration curve differs moderately from that of pCT. No significant fluctuation was observed in the calibration over the period of 8 weeks. For the static phantom, the doses computed based on pCT and CBCT agreed to within 1%. A notable difference in CBCT- and pCT-based dose distributions was found for the motion phantom due to the motion artefacts which appeared in the CBCT images (the maximum discrepancy was found to be ~3.0% in the high dose region). The motion artefacts-induced dosimetric inaccuracy was also observed in the lung patient study. For the prostate cases, the mCBCT- and CBCT-based dose calculations yielded very close results (<2%). Coupled with the phantom data, it is concluded that the CBCT can be employed directly for dose calculation for a disease site such as the prostate, where there is little motion artefact. In the prostate case study, we also noted a large discrepancy between the original treatment plan and the CBCT (or mCBCT)-based calculation, suggesting the importance of inter-fractional organ movement and the need for adaptive therapy to compensate for the anatomical changes in the future. Respiratory motion poses a major challenge in lung radiotherapy. Based on 4D CT images, a variety of intensity-based deformable registration techniques have been proposed to study the pulmonary motion. However, the accuracy achievable with these approaches can be sub-optimal because the deformation is defined globally in space. Therefore, the accuracy of the alignment of local structures may be compromised. In this work, we propose a novel method to detect a large collection of natural junction structures in the lung and use them as the reliable markers to track the lung motion. Specifically, detection of the junction centers and sizes is achieved by analysis of local shape profiles on one segmented image. To track the temporal trajectory of a junction, the image intensities within a small region of interest surrounding the center are selected as its signature. Under the assumption of the cyclic motion, we describe the trajectory by a closed B-spline curve and search for the control points by maximizing a metric of combined correlation coefficients. Local extrema are suppressed by improving the initial conditions using random walks from pair-wise optimizations. Several descriptors are introduced to analyze the motion trajectories. Our method was applied to 13 real 4D CT images. More than 700 junctions in each case are detected with an average positive predictive value of greater than 90%. The average tracking error between automated and manual tracking is sub-voxel and smaller than the published results using the same set of data. Purpose: The feasibility of medical imaging using a medical linear accelerator to generate acoustic waves is investigated. This modality, x-ray acoustic computed tomography (XACT), has the potential to enable deeper tissue penetration in tissue than photoacoustic tomography via laser excitation.  Methods: Short pulsed (\\u03bcs-range) 10 MV x-ray beams with dose-rate of approximately 30 Gy/min were generated from a medical linear accelerator. The acoustic signals were collected with an ultrasound transducer (500 KHz central frequency) positioned around an object. The transducer, driven by a computer-controlled step motor to scan around the object, detected the resulting acoustic signals in the imaging plane at each scanning position. A pulse preamplifier, with a bandwidth of 20 KHz\\u20132 MHz at \\u22123 dB, and switchable gains of 40 and 60 dB, received the signals from the transducer and delivered the amplified signals to a secondary amplifier. The secondary amplifier had bandwidth of 20 KHz\\u201330 MHz at \\u22123 dB, and a gain range of 10\\u201360 dB. Signals were recorded and averaged 128 times by an oscilloscope. A sampling rate of 100 MHz was used to record 2500 data points at each view angle. One set of data incorporated 200 positions as the receiver moved 360\\u00b0. The x-ray generated acoustic image was then reconstructed with the filtered back projection algorithm.  Results: The x-ray generated acoustic signals were detected from a lead rod embedded in a chicken breast tissue. The authors found that the acoustic signal was proportional to the x-ray dose deposition, with a correlation of 0.998. The two-dimensional XACT images of the lead rod embedded in chicken breast tissue were found to be in good agreement with the shape of the object.  Conclusions: The first x-ray acoustic computed tomography image is presented. The new modality may be useful for a number of applications, such as providing the location of a fiducial, or monitoring x-ray dose distribution during radiation therapy. Although much work is needed to improve the image quality of XACT and to explore its performance in other irradiation energies, the benefits of this modality, as highlighted in this work, encourage further study. Purpose To develop methods to monitor prostate intrafraction motion during fixed-gantry intensity-modulated radiotherapy using MV treatment beam imaging together with minimal kV imaging for a failure detection strategy that ensures prompt detection when target displacement exceeds a preset threshold. Methods and Materials Real-time two-dimensional (2D) marker position in the MV image plane was obtained by analyzing cine-MV images. The marker's in-line movement, and thus its time-varying three-dimensional (3D) position, was estimated by combining the 2D projection data with a previously established correlative relationship between the directional components of prostate motion. A confirmation request for more accurate localization using MV-kV triangulation was triggered when the estimated prostate displacement based on the cine-MV data was greater than 3 mm. An interventional action alert followed on positive MV-kV confirmation. To demonstrate the feasibility and accuracy of the proposed method, simulation studies of conventional-fraction intensity-modulated radiotherapy sessions were done using 536 Calypso-measured prostate trajectories from 17 radiotherapy patients. Results A technique for intrafraction prostate motion management has been developed. The technique, using \\\"freely available\\\" cine-MV images and minimum on-board kV imaging (on average 2.5 images/fraction), successfully limited 3D prostate movement to within a range of 3 mm relative to the MV beam for 99.4% of the total treatment time. On average, only approximately one intervention/fraction was needed to achieve this level of accuracy. Conclusion Instead of seeking to accurately and continuously localize the prostate target as existing motion tracking systems do, the present technique effectively uses cine-MV data to provide a clinically valuable way to minimize kV usage, while maintaining high targeting accuracy. The task of treatment planning for prostate implants is to find an optimal seed configuration, comprising the target coverage and dosimetric consideration of critical structures such as the rectum and urethra. An efficient method to accomplish this is to use an inverse planning technique that derives the optimized solution from a prescribed treatment goal. The goal can be specified in the voxel domain as the desired doses to the voxels of the target and critical structures, or in the dose volume representation as the desired dose volume histograms (DVHs) of the target and critical structures. The DVH based optimization has been successfully used in plan optimization for intensity-modulated radiation therapy(IMRT) but little attention has been paid to its application in prostate implants. Clinically, it has long been known that some normal structure tolerances are more accurately assessed by volumetric information. Dose-volume histograms are also widely used for plan evaluation. When working in the DVH domain for optimization one has more control over the final DVHs. We have constructed an objective function sensitive to the DVHs of the target and critical structures. The objective function is minimized using an iterative algorithm, starting from a randomly selected initial seed configuration. At each iteration step, a trial position is given to a randomly selected source and the trial position is accepted if the objective function is decreased. To avoid being trapped in a less optimal local minimum, the optimization process is repeated. The final plan is selected from a pool of optimized plans obtained from a series of randomized initial seed configurations. Purpose: To evaluate the geometric accuracy of beam targeting in external surrogate-based gated volumetric modulated arc therapy (VMAT) using kilovoltage (kV) x-ray images acquired during dose delivery. Methods: Gated VMAT treatments were delivered using a Varian TrueBeam STx Linac for both physical phantoms and patients. Multiple gold fiducial markers were implanted near the target. The reference position was created for each implanted marker, representing its correct position at the gating threshold. The gating signal was generated from the RPM system. During the treatment, kV images were acquired immediately before MV beam-on at every breathing cycle, using the on-board imaging system. All implanted markers were detected and their 3D positions were estimated using in-house developed software. The positioning error of a marker is defined as the distance of the marker from its reference position for each frame of the images. The overall error of the system is defined as the average over all markers. For the phantom study, both sinusoidal motion (1D and 3D) and real human respiratory motion was simulated for the target and surrogate. In the baseline case, the two motions were synchronized for the first treatment fraction. To assess the effects of surrogate-target correlation on the geometric accuracy,more\\u00a0\\u00bb a phase shift of 5% and 10% between the two motions was introduced. For the patient study, intrafraction kV images of five stereotactic body radiotherapy (SBRT) patients were acquired for one or two fractions. Results: For the phantom study, a high geometric accuracy was achieved in the baseline case (average error: 0.8 mm in the superior-inferior or SI direction). However, the treatment delivery is prone to geometric errors if changes in the target-surrogate relation occur during the treatment: the average error was increased to 2.3 and 4.7 mm for the phase shift of 5% and 10%, respectively. Results obtained with real human respiratory curves show a similar trend. For a target with 3D motion, the technique is able to detect geometric errors in the left-right (LR) and anterior-posterior (AP) directions. For the patient study, the average intrafraction positioning errors are 0.8, 0.9, and 1.4 mm and 95th percentile errors are 1.7, 2.1, and 2.7 mm in the LR, AP, and SI directions, respectively. Conclusions: The correlation between external surrogate and internal target motion is crucial to ensure the geometric accuracy of surrogate-based gating. Real-time guidance based on kV x-ray images overcomes the potential issues in surrogate-based gating and can achieve accurate beam targeting in gated VMAT.\\u00ab\\u00a0less Purpose: Station parameter optimized radiation therapy (SPORT) was recently proposed to fully utilize the technical capability of emerging digital linear accelerators, in which the station parameters of a delivery system, such as aperture shape and weight, couch position/angle, gantry/collimator angle, can be optimized simultaneously. SPORT promises to deliver remarkable radiation dose distributions in an efficient manner, yet there exists no optimization algorithm for its implementation. The purpose of this work is to develop an algorithm to simultaneously optimize the beam sampling and aperture shapes. Methods: The authors build a mathematical model with the fundamental station point parameters as the decision variables. To solve the resulting large-scale optimization problem, the authors devise an effective algorithm by integrating three advanced optimization techniques: column generation, subgradient method, and pattern search. Column generation adds the most beneficial stations sequentially until the plan quality improvement saturates and provides a good starting point for the subsequent optimization. It also adds the new stations during the algorithm if beneficial. For each update resulted from column generation, the subgradient method improves the selected stations locally by reshaping the apertures and updating the beam angles toward a descent subgradient direction. The algorithm continues to improve the selected stations locallymore\\u00a0\\u00bb and globally by a pattern search algorithm to explore the part of search space not reachable by the subgradient method. By combining these three techniques together, all plausible combinations of station parameters are searched efficiently to yield the optimal solution. Results: A SPORT optimization framework with seamlessly integration of three complementary algorithms, column generation, subgradient method, and pattern search, was established. The proposed technique was applied to two previously treated clinical cases: a head and neck and a prostate case. It significantly improved the target conformality and at the same time critical structure sparing compared with conventional intensity modulated radiation therapy (IMRT). In the head and neck case, for example, the average PTV coverage D99% for two PTVs, cord and brainstem max doses, and right parotid gland mean dose were improved, respectively, by about 7%, 37%, 12%, and 16%. Conclusions: The proposed method automatically determines the number of the stations required to generate a satisfactory plan and optimizes simultaneously the involved station parameters, leading to improved quality of the resultant treatment plans as compared with the conventional IMRT plans.\\u00ab\\u00a0less Purpose: To develop a 4D volumetric modulated arc therapy (VMAT) inverse planning framework.  Methods: 4D VMAT inverse planning aims to derive an aperture and weight modulated arc therapy treatment plan that optimizes the accumulated dose distribution from all gantry angles and breathing phases. Under an assumption that the gantry rotation and patient breathing are synchronized (i.e., there is a functional relationship between the phase of the patient breathing cycle and the beam angle), the authors compute the contribution from different respiration phases through the registration of the phased CT images. The accumulative dose distribution is optimized by iteratively adjusting the aperture shape and weight of each beam through the minimization of the planning objective function. For comparison, traditional 3D VMAT plans are also performed for the two cases and the performance of the proposed technique is demonstrated.  Results: A framework for 4D VMAT inverse planning has been proposed. With the consideration of the extra dimension of time in VMAT, a tighter target margin can be achieved with a full duty cycle, which is otherwise not achievable simultaneously by either 3D VMAT optimization or gated VMAT.  Conclusions: The 4D VMAT planning formulism proposed here provides useful insight on how the \\u201ctime\\u201d dimension can be exploited in rotational arc therapy to maximally compensate for the intrafraction organ motion. Respiratory motion artifacts in positron emission tomography (PET) imaging can alter lesion intensity profiles, and result in substantially reduced activity and contrast-to-noise ratios (CNRs). We propose a corrective algorithm, coined 'retrospective stacking' (RS), to restore image quality without requiring additional scan time. Retrospective stacking uses b-spline deformable image registration to combine amplitude-binned PET data along the entire respiratory cycle into a single respiratory end point. We applied the method to a phantom model consisting of a small, hot vial oscillating within a warm background, as well as to {sup 18}FDG-PET images of a pancreatic and a liver patient. Comparisons were made using cross-section visualizations, activity profiles, and CNRs within the region of interest. Retrospective stacking was found to properly restore the lesion location and intensity profile in all cases. In addition, RS provided CNR improvements up to three-fold over gated images, and up to five-fold over ungated data. These phantom and patient studies demonstrate that RS can correct for lesion motion and deformation, while substantially improving tumor visibility and background noise. The last few years have seen a number of technical and clinical advances which give rise to a need for innovations in dose optimization and delivery strategies. Technically, a new generation of digital linac has become available which offers features such as programmable motion between station parameters and high dose-rate Flattening Filter Free (FFF) beams. Current inverse planning methods are designed for traditional machines and cannot accommodate these features of new generation linacs without compromising either dose conformality and/or delivery efficiency. Furthermore, SBRT is becoming increasingly important, which elevates the need for more efficient delivery, improved dose distribution. Here we will give an overview of our recent work in SPORT designed to harness the digital linacs and highlight the essential components of SPORT. We will summarize the pros and cons of traditional beamlet-based optimization (BBO) and direct aperture optimization (DAO) and introduce a new type of algorithm, compressed sensing (CS)-based inverse planning, that is capable of automatically removing the redundant segments during optimization and providing a plan with high deliverability in the presence of a large number of station control points (potentially non-coplanar, non-isocentric, and even multi-isocenters). We show that CS-approach takes the interplay between planning and delivery into account and allows us to balance the dose optimality and delivery efficiency in a controlled way and, providing a viable framework to address various unmet demands of the new generation linacs. A few specific implementation strategies of SPORT in the forms of fixed-gantry and rotational arc delivery are also presented. A treatment planning system to compute intensity modulated radiotherapy(IMRT) treatments using inverse planning was investigated. The system was designed to optimize the intensity patterns required to treat a specified target volume with specified normal structure constraints. A beam model that uses the convolution of pencil beams was used to compute the dose distributions. A multileaf collimator leaf-setting sequence intended to produce the intensity pattern was computed along with the monitor units required to deliver each of a number of fixed-gantry modulated fields. Computer calculations are commonly verified using an independent manual procedure. It is difficult to calculate treatment delivery monitor units for this variant of IMRT using manual methods. Since manual calculations are not feasible, it is important both to understand and to verify the calculation of treatment monitor units by the planning system algorithm. A formal analysis was made of the dose calculation model and the monitor unit calculation embedded in the algorithm. Experimental verification of the dosedelivered by plans computed with the methodology demonstrated an agreement of better than 4% between the dose model and measurements. Monte Carlo simulation is considered the most reliable method for modeling photon migration in heterogeneous media. However, its widespread use is hindered by the high computational cost. The purpose of this work is to report on our implementation of a simple MapReduce method for performing fault-tolerant Monte Carlo computations in a massively-parallel cloud computing environment. We ported the MC321 Monte Carlo package to Hadoop, an open-source MapReduce framework. In this implementation, Map tasks compute photon histories in parallel while a Reduce task scores photon absorption. The distributed implementation was evaluated on a commercial compute cloud. The simulation time was found to be linearly dependent on the number of photons and inversely proportional to the number of nodes. For a cluster size of 240 nodes, the simulation of 100 billion photon histories took 22 min, a 1258 \\u00d7 speed-up compared to the single-threaded Monte Carlo program. The overall computational throughput was 85,178 photon histories per node per second, with a latency of 100 s. The distributed simulation produced the same output as the original implementation and was resilient to hardware failure: the correctness of the simulation was unaffected by the shutdown of 50% of the nodes. C 2011 Society of Photo-Optical Instrumentation Engineers (SPIE). (DOI: 10.1117/1.3656964) Purpose: To utilize image-guided radiotherapy (IGRT) in near real time by obtaining and evaluating the online positions of implanted fiducials from continuous electronic portal imaging device (EPID) imaging of prostate intensity-modulated radiotherapy (IMRT) delivery. Methodsand Materials: Uponinitial setupusingtwoorthogonal images,the three-dimensional(3D)positionsof all implanted fiducial markers are obtained, and their expected two-dimensional (2D) locations in the beam\\u2019seye-view (BEV) projection are calculated for each treatment field. During IMRT beam delivery, EPID images of the megavoltage treatment beam are acquired in cine mode and subsequently analyzed to locate 2D locations of fiducials in the BEV. Simultaneously, 3D positions are estimated according to the current EPID image, informationfromthesetupportalimages,andimagesacquiredatothergantryangles(thecompletedtreatmentfields). The measured 2D and 3D positions of each fiducial are compared with their expected 2D and 3D setup positions, respectively. Any displacements larger than a predefined tolerance may cause the treatment system to suspend the beam delivery and direct the therapists to reposition the patient. Results: Phantom studies indicate that the accuracyof 2D BEVand 3D tracking are better than 1 mm and 1.4 mm, respectively. A total of 7330 images fromprostate treatmentswereacquiredand analyzed, showing amaximum 2D displacement of 6.7 mm and a maximum 3D displacement of 6.9 mm over 34 fractions. Conclusions: This EPID-based, real-time IGRT method can be implemented on any external beam machine with portal imaging capabilities without purchasing any additional equipment, and there is no extra dose delivered to the patient. ! 2009 Elsevier Inc. IGRT, Real-time tracking, Fiducial marker, Prostate, EPID. An image registration technique is presented for the registration of medical images using a hybrid combination of coarse-scale landmark and B-splines deformable registration techniques. The technique is particularly ef- fective for registration problems in which the images to be registered contain large localized deformations. A brief overview of landmark and deformable reg- istration techniques is presented. The hierarchical multiscale image decompo- sition of E. Tadmor, S. Nezzar, and L. Vese, A multiscale image representation using hierarchical (BV;L 2 ) decompositions, Multiscale Modeling and Simula- tions, vol. 2, no. 4, pp. 554{579, 2004, is reviewed, and an image registration algorithm is developed based on combining the multiscale decomposition with landmark and deformable techniques. Successful registration of medical im- ages is achieved by flrst obtaining a hierarchical multiscale decomposition of the images and then using landmark-based registration to register the resulting coarse scales. Corresponding bony structure landmarks are easily identifled in the coarse scales, which contain only the large shapes and main features of the image. This registration is then flne tuned by using the resulting transforma- tion as the starting point to deformably register the original images with each other using an iterated multiscale B-splines deformable registration technique. The accuracy and e-ciency of the hybrid technique is demonstrated with sev- eral image registration case studies in two and three dimensions. Additionally, the hybrid technique is shown to be very robust with respect to the location of landmarks and presence of noise. While ART has been studied for years, the specific quantitative implementation details have not. In order for this new scheme of radiation therapy (RT) to reach its potential, an effective ART treatment planning strategy capable of taking into account the dose delivery history and the patient's on-treatment geometric model must be in place. This paper performs a theoretical study of dynamic closed-loop control algorithms for ART and compares their utility with data from phantom and clinical cases. We developed two classes of algorithms: those Adapting to Changing Geometry and those Adapting to Geometry and Delivered Dose. The former class takes into account organ deformations found just before treatment. The latter class optimizes the dose distribution accumulated over the entire course of treatment by adapting at each fraction, not only to the information just before treatment about organ deformations but also to the dose delivery history. We showcase two algorithms in the class of those Adapting to Geometry and Delivered Dose. A comparison of the approaches indicates that certain closed-loop ART algorithms may significantly improve the current practice. We anticipate that improvements in imaging, dose verification and reporting will further increase the importance of adaptive algorithms. The purpose of this work was to use Monte Carlo simulations to verify the accuracy of the dose distributions from a commercial treatment planning optimization system (Corvus, Nomos Corp., Sewickley, PA) for intensity-modulated radiotherapy (IMRT). A Monte Carlo treatment planning system has been implemented clinically to improve and verify the accuracy of radiotherapy dose calculations. Further modifications to the system were made to compute the dose in a patient for multiple fixed-gantry IMRT fields. The dose distributions in the experimental phantoms and in the patients were calculated and used to verify the optimized treatment plans generated by the Corvus system. The Monte Carlo calculated IMRT dose distributions agreed with the measurements to within 2% of the maximum dose for all the beam energies and field sizes for both the homogeneous and heterogeneous phantoms. The dose distributions predicted by the Corvus system, which employs a finite-size pencil beam (FSPB) algorithm, agreed with the Monte Carlo simulations and measurements to within 4% in a cylindrical water phantom with various hypothetical target shapes. Discrepancies of more than 5% (relative to the prescribed target dose) in the target region and over 20% in the critical structures were found in some IMRT patient calculations. The FSPB algorithm as implemented in the Corvus system is adequate for homogeneous phantoms (such as prostate) but may result in significant under- or over-estimation of the dose in some cases involving heterogeneities such as the air-tissue, lung-tissue and tissue-bone interfaces. (Some figures in this article are in colour only in the electronic version; see www.iop.org) Dose verification by ion chamber measurements Is a time-consuming quality assurance (QA) process for intensity modulation radiotherapy (IMRT). A Clarkson summation algorithm was investigated as an alternative to calculate the dose to the isocenter of IMRT treatments. Scatter contributions to the isocenter dose were calculated using a method similar to that of the Clarkson calculation for the central axis dose of an irregular field. The independent dose calculation was performed using the leaf sequences generated by a commercial treatment planning system (Corvus, NOMOS Corporation, Sewickley, PA) and the patient geometry obtained from CT-simulations. The isocenter dose was decomposed into contributions from the surrounding finite-size beamlets. Each beamlet contribution was calculated by differentiating the phantom scatter data for open fields. The result for a spherically shaped target agreed with the Corvus calculation to within 3%. Special corrections were needed for those cases in which the isocenter was at a low dose point under the shadow of the MLC. This Clarkson-type calculation was found to overestimate the dose when the beams pass through a large air volume, since no tissue heterogeneity was considered in the algorithm. In such cases the dose calculation is performed using a homogeneous phantom plan with the same fields and compared with the Corvus calculation for the same phantom. In inverse planning, the likelihood for the points in a target or sensitive structure to meet their dosimetric goals is generally heterogeneous and represents the a priori knowledge of the system once the patient and beam configuration are chosen. Because of this intrinsic heterogeneity, in some extreme cases, a region in a target may never meet the prescribed dose without seriously deteriorating the doses in other areas. Conversely, the prescription in a region may be easily met without violating the tolerance of any sensitive structure. In this work, we introduce the concept of dosimetric capability to quantify the a priori information and develop a strategy to integrate the data into the inverse planning process. An iterative algorithm is implemented to numerically compute the capability distribution on a case specific basis. A method of incorporating the capability data into inverse planning is developed by heuristically modulating the importance of the individual voxels according to the a priori capability distribution. The formalism is applied to a few specific examples to illustrate the technical details of the new inverse planning technique. Our study indicates that the dosimetric capability is a useful concept to better understand the complex inverse planning problem and an effective use of the information allows us to construct a clinically more meaningful objective function to improve IMRT dose optimization techniques. In IMRT the quality of a treatment plan depends greatly upon how well mutually-exclusive dosimetric goals for different structures within a patient are balanced and prioritized. Ideally, each voxel has to be assigned with an individual penalty factor and a prescription dose which take into account inter-voxel correlations. However, in most plans, an optimal solution is calculated by assigning a uniform importance and prescription to all voxels within a given structure volume. Previous studies have demonstrated that a significant improvement in IMRT planning can be achieved with voxel specific importance factors. On the other hand, prescription dose choice remains fixed and patient independent. We show that by removing the requirement for the prescription dose to be fixed for each voxel a significant improvement can be achieved when compared to IMRT plans with uniform fixed prescription. We explore the following approach: a spatially-dependent penalty scheme based on how well a given voxel satisfies its prescription. In this scenario both under and overdosed voxels are penalized with a penalty weight proportional to the amount of the dose discrepancy. The scheme is equivalent to the iterative adjustment of individual voxel prescriptions based on the departure from the desirable voxel doses. The method is capable of producing improved treatment plans without expert\\u2019s intervention. A clinical head and neck case was used to test this method. By adjusting voxel prescriptions to compensate for the inequalities between the actual and desired doses calculated, substantial improvements are obtained for the treatment plan as large dose reductions were achieved in almost all of the critical structures present. For instance, we demonstrate fivefold reduction in the maximum dose to the brainstem. Other organs at risk experience dose reduction ranging from 100 to 300 percents. Volumetric modulated arc therapy (VMAT) has recently emerged as a new clinical modality for conformal radiation therapy. The aim of this work is to establish a methodology and procedure for retrospectively reconstructing the actual dose delivered in VMAT based on the pre-treatment cone-beam computed tomography (CBCT) and dynamic log files. CBCT was performed before the dose delivery and the system's log files were retrieved after the delivery. Actual delivery at a control point including MLC leaf positions, gantry angles and cumulative monitor units (MUs) was recorded in the log files and the information was extracted using in-house developed software. The extracted information was then embedded into the original treatment DICOM-radiation therapy (RT) file to replace the original control point parameters. This reconstituted DICOM-RT file was imported into the Eclipse treatment planning system (TPS) and dose was computed on the corresponding CBCT. A series of phantom experiments was performed to show the feasibility of dose reconstruction, validate the procedure and demonstrate the efficacy of this methodology. The resultant dose distributions and dose\\u2013volume histograms (DVHs) were compared with those of the original treatment plan. The studies indicated that CBCT-based VMAT dose reconstruction is readily achievable and provides a valuable tool for monitoring the dose actually delivered to the tumor target as well as the sensitive structures. In the absence of setup errors, the reconstructed dose shows no significant difference from the original pCT-based plan. It is also elucidated that the proposed method is capable of revealing the dosimetric changes in the presence of setup errors. The method reported here affords an objective means for dosimetric evaluation of VMAT delivery and is useful for adaptive VMAT in future. To minimize the adverse dosimetric effect caused by tumor motion, it is desirable to have real-time knowledge of the tumor position throughout the beam delivery process. A promising technique to realize the real-time image guided scheme in external beam radiation therapy is through the combined use of MV and onboard kV beam imaging. The success of this MV?kV triangulation approach for fixed-gantry radiation therapy has been demonstrated. With the increasing acceptance of modern arc radiotherapy in the clinics, a timely and clinically important question is whether the image guidance strategy can be extended to arc therapy to provide the urgently needed real-time tumor motion information. While conceptually feasible, there are a number of theoretical and practical issues specific to the arc delivery that need to be resolved before clinical implementation. The purpose of this work is to establish a robust procedure of system calibration for combined MV and kV imaging for internal marker tracking during arc delivery and to demonstrate the feasibility and accuracy of the technique. A commercially available LINAC equipped with an onboard kV imager and electronic portal imaging device (EPID) was used for the study. A custom built phantom with multiple ball bearings was used to calibrate the stereoscopic MV?kV imaging system to provide the transformation parameters from imaging pixels to 3D world coordinates. The accuracy of the fiducial tracking system was examined using a 4D motion phantom capable of moving in accordance with a pre-programmed trajectory. Overall, spatial accuracy of MV?kV fiducial tracking during the arc delivery process for normal adult breathing amplitude and period was found to be better than 1 mm. For fast motion, the results depended on the imaging frame rates. The RMS error ranged from ~0.5 mm for the normal adult breathing pattern to ~1.5 mm for more extreme cases with a low imaging frame rate of 3.4 Hz. In general, highly accurate real-time tracking of implanted markers using hybrid MV?kV imaging is achievable and the technique should be useful to improve the beam targeting accuracy of arc therapy. Increased noise is a general concern for dual-energy material decomposition. Here, we develop an image-domain material decomposition algorithm for dual-energy CT (DECT) by incorporating an edge-preserving filter into the Local HighlY constrained backPRojection reconstruction (HYPR-LR) framework. With effective use of the non-local mean, the proposed algorithm, which is referred to as HYPR-NLM, reduces the noise in dual-energy decomposition while preserving the accuracy of quantitative measurement and spatial resolution of the material-specific dual-energy images. We demonstrate the noise reduction and resolution preservation of the algorithm with an iodine concentrate numerical phantom by comparing the HYPR-NLM algorithm to the direct matrix inversion, HYPR-LR and iterative image-domain material decomposition (Iter-DECT). We also show the superior performance of the HYPR-NLM over the existing methods by using two sets of cardiac perfusing imaging data. The DECT material decomposition comparison study shows that all four algorithms yield acceptable quantitative measurements of iodine concentrate. Direct matrix inversion yields the highest noise level, followed by HYPR-LR and Iter-DECT. HYPR-NLM in an iterative formulation significantly reduces image noise and the image noise is comparable to or even lower than that generated using Iter-DECT. For the HYPR-NLM method, there are marginal edge effects in the difference image, suggesting the high-frequency details are well preserved. In addition, when the search window size increases from to , there are no significant changes or marginal edge effects in the HYPR-NLM difference images. The reference drawn from the comparison study includes: (1) HYPR-NLM significantly reduces the DECT material decomposition noise while preserving quantitative measurements and high-frequency edge information, and (2) HYPR-NLM is robust with respect to parameter selection. Purpose: Due to the increased axial coverage of multislice computed tomography (CT) and the introduction of flat detectors, the size of x-ray illumination fields has grown dramatically, causing an increase in scatter radiation. For CT imaging, scatter is a significant issue that introduces shading artifact, streaks, as well as reduced contrast and Hounsfield Units (HU) accuracy. The purpose of this work is to provide a fast and accurate scatter artifacts correction algorithm for cone beam CT (CBCT) imaging.  Methods: The method starts with an estimation of coarse scatter profiles for a set of CBCT data in either image domain or projection domain. A denoising algorithm designed specifically for Poisson signals is then applied to derive the final scatter distribution. Qualitative and quantitative evaluations using thorax and abdomen phantoms with Monte Carlo (MC) simulations, experimental Catphan phantom data, and in vivo human data acquired for a clinical image guided radiation therapy were performed. Scatter correction in both projection domain and image domain was conducted and the influences of segmentation method, mismatched attenuation coefficients, and spectrum model as well as parameter selection were also investigated.  Results: Results show that the proposed algorithm can significantly reduce scatter artifacts and recover the correct HU in either projection domain or image domain. For the MC thorax phantom study, four-components segmentation yields the best results, while the results of three-components segmentation are still acceptable. The parameters (iteration number K and weight \\u03b2) affect the accuracy of the scatter correction and the results get improved as K and \\u03b2 increase. It was found that variations in attenuation coefficient accuracies only slightly impact the performance of the proposed processing. For the Catphan phantom data, the mean value over all pixels in the residual image is reduced from \\u221221.8 to \\u22120.2 HU and 0.7 HU for projection domain and image domain, respectively. The contrast of the in vivo human images is greatly improved after correction.  Conclusions: The software-based technique has a number of advantages, such as high computational efficiency and accuracy, and the capability of performing scatter correction without modifying the clinical workflow (i.e., no extra scan/measurement data are needed) or modifying the imaging hardware. When implemented practically, this should improve the accuracy of CBCT image quantitation and significantly impact CBCT-based interventional procedures and adaptive radiation therapy. Nowadays magnetic resonance imaging (MRI) has been profoundly used in radiotherapy (RT) planning to aid the contouring of targets and critical organs in brain and intracranial cases, which is attributable to its excellent soft tissue contrast and multi-planar imaging capability. However, the lack of electron density information in MRI, together with the image distortion issues, precludes its use as the sole image set for RT planning and dose calculation. The purpose of this preliminary study is to probe the feasibility and evaluate an MRI-based radiation dose calculation process by providing MR images the necessary electron density (ED) information from a patient's readily available diagnostic/staging computed tomography (CT) images using an image registration model. To evaluate the dosimetric accuracy of the proposed approach, three brain and three intracranial cases were selected retrospectively for this study. For each patient, the MR images were registered to the CT images, and the ED information was ... Purpose:To establish an inverse planning framework with adjustable voxel penalty for more conformal IMRT dose distribution as well as improved interactive controllability over the regional dose distribution of the resultant plan.Materials and Method:In the proposed coarse-to-fine planning scheme, a conventional inverse planning with organ specific parameters is first performed. The voxel penalty scheme is then \\u201cswitched on\\u201d by allowing the prescription dose to change on an individual voxel scale according to the deviation of the actual voxel dose from the ideally desired dose. The rationale here is intuitive: when the dose at a voxel does not meet its ideal dose, it simply implies that this voxel is not competitive enough when compared with the ones that have met their planning goal. In this case, increasing the penalty of the voxel by varying the prescription can boost its competitiveness and thus improve its dose. After the prescription adjustment, the plan is re-optimized. The dose adjustment/re-optimi... A medical imaging-based system and method uses both kV and MV images captured during a treatment period for organ motion tracking. 3D geometric locations of internal features are computationally tracked as a function of time from internal features, such as natural biological features or implanted fiducials, which are computationally extracted from the captured kV and MV images. A partial information method allows 3D tracking to be maintained in the event that imaging information is temporarily not available. Real-time 3D tracking of anatomical positions during radiation therapy uses acquired image data from an MV treatment beam as it is rotated around the patient during arc radiotherapy treatment. The acquired image data and associated angular positions are computationally combined during the arc radiotherapy treatment to estimate in real time 3D positions of anatomical features of the patient, e.g., combining present image data and prior image data at earlier times. Supplementary image data from a kV imaging system may be acquired on an as-needed basis if MV position estimates indicate movement exceeding a predetermined threshold, and the supplementary kV image data combined with the acquired MV image data to improve an accuracy of the estimated 3D positions. : To develop automatic and efficient liver contouring software for planning 3D-CT and four-dimensional computed tomography (4D-CT) for application in clinical radiation therapy treatment planning systems.The algorithm comprises three steps for overcoming the challenge of similar intensities between the liver region and its surrounding tissues. First, the total variation model with the L1 norm (TV-L1), which has the characteristic of multi-scale decomposition and an edge-preserving property, is used for removing the surrounding muscles and tissues. Second, an improved level set model that contains both global and local energy functions is utilized to extract liver contour information sequentially. In the global energy function, the local correlation coefficient (LCC) is constructed based on the gray level co-occurrence matrix both of the initial liver region and the background region. The LCC can calculate the correlation of a pixel with the foreground and background regions, respectively. The LCC is combined with intensity distribution models to classify pixels during the evolutionary process of the level set based method. The obtained liver contour is used as the candidate liver region for the following step. In the third step, voxel-based texture characterization is employed for refining the liver region and obtaining the final liver contours.The proposed method was validated based on the planning CT images of a group of 25 patients undergoing radiation therapy treatment planning. These included ten lung cancer patients with normal appearing livers and ten patients with hepatocellular carcinoma or liver metastases. The method was also tested on abdominal 4D-CT images of a group of five patients with hepatocellular carcinoma or liver metastases. The false positive volume percentage, the false negative volume percentage, and the dice similarity coefficient between liver contours obtained by a developed algorithm and a current standard delineated by the expert group are on an average 2.15-2.57%, 2.96-3.23%, and 91.01-97.21% for the CT images with normal appearing livers, 2.28-3.62%, 3.15-4.33%, and 86.14-93.53% for the CT images with hepatocellular carcinoma or liver metastases, and 2.37-3.96%, 3.25-4.57%, and 82.23-89.44% for the 4D-CT images also with hepatocellular carcinoma or liver metastases, respectively.The proposed three-step method can achieve efficient automatic liver contouring for planning CT and 4D-CT images with follow-up treatment planning and should find widespread applications in future treatment planning systems. Purpose: To develop a fast optimization method for station parameter optimized radiation therapy (SPORT) and show that SPORT is capable of matching VMAT in both plan quality and delivery efficiency by using three clinical cases of different disease sites. Methods: The angular space from 0\\u00b0 to 360\\u00b0 was divided into 180 station points (SPs). A candidate aperture was assigned to each of the SPs based on the calculation results using a column generation algorithm. The weights of the apertures were then obtained by optimizing the objective function using a state-of-the-art GPU based proximal operator graph solver. To avoid being trapped in a local minimum in beamlet-based aperture selection using the gradient descent algorithm, a stochastic gradient descent was employed here. Apertures with zero or low weight were thrown out. To find out whether there was room to further improve the plan by adding more apertures or SPs, the authors repeated the above procedure with consideration of the existing dose distribution from the last iteration. At the end of the second iteration, the weights of all the apertures were reoptimized, including those of the first iteration. The above procedure was repeated until the plan could not be improved any further. The optimization technique was assessed by using three clinical cases (prostate, head and neck, and brain) with the results compared to that obtained using conventional VMAT in terms of dosimetric properties, treatment time, and total MU. Results: Marked dosimetric quality improvement was demonstrated in the SPORT plans for all three studied cases. For the prostate case, the volume of the 50% prescription dose was decreased by 22% for the rectum and 6% for the bladder. For the head and neck case, SPORT improved the mean dose for the left and right parotids by 15% each. The maximum dose was lowered from 72.7 to 71.7 Gy for the mandible, and from 30.7 to 27.3 Gy for the spinal cord. The mean dose for the pharynx and larynx was reduced by 8% and 6%, respectively. For the brain case, the doses to the eyes, chiasm, and inner ears were all improved. SPORT shortened the treatment time by \\u223c1 min for the prostate case, \\u223c0.5 min for brain case, and \\u223c0.2 min for the head and neck case. Conclusions: The dosimetric quality and delivery efficiency presented here indicate that SPORT is an intriguing alternative treatment modality. With the widespread adoption of digital linac, SPORT should lead to improved patient care in the future. Purpose Accurate segmentation of pelvic organs in CT images is of great importance in external beam radiotherapy for prostate cancer. The aim of this studying is to develop a novel method for automatic, multiorgan segmentation of the male pelvis.  Methods The authors\\u2019 segmentation method consists of several stages. First, a pretreatment includes parameterization, principal component analysis (PCA), and an established process of region-specific hierarchical appearance cluster (RSHAC) model which was executed on the training dataset. After the preprocessing, online automatic segmentation of new CT images is achieved by combining the RSHAC model with the PCA-based point distribution model. Fifty pelvic CT from eight prostate cancer patients were used as the training dataset. From another 20 prostate cancer patients, 210 CT images were used for independent validation of the segmentation method.  Results In the training dataset, 15 PCA modes were needed to represent 95% of shape variations of pelvic organs. When tested on the validation dataset, the authors\\u2019 segmentation method had an average Dice similarity coefficient and mean absolute distance of 0.751 and 0.371 cm, 0.783 and 0.303 cm, 0.573 and 0.604 cm for prostate, bladder, and rectum, respectively. The automated segmentation process took on average 5 min on a personal computer equipped with Core 2 Duo CPU of 2.8 GHz and 8 GB RAM.  Conclusions The authors have developed an efficient and reliable method for automatic segmentation of multiple organs in the male pelvis. This method should be useful for treatment planning and adaptive replanning for prostate cancer radiotherapy. With this method, the physicist can improve the work efficiency and stability.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(json.dumps(user_profiles, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F88P_9ZjiTQ4"
      },
      "source": [
        "# Loading Queries for Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c43MBEvFJnrr"
      },
      "source": [
        "User_queries = Queries to test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "id": "3vTYf0i1ULuV"
      },
      "outputs": [],
      "source": [
        "# Load the first 20 queries from JSONL file\n",
        "def load_queries(file_path):\n",
        "    queries = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            queries.append(json.loads(line))\n",
        "\n",
        "    return queries\n",
        "\n",
        "test_queries = load_queries('queries-20.jsonl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH5qg_hLdLUp",
        "outputId": "32b5db48-0c58-4bf8-a0c0-dd7c76fe7f04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'id': '2013326020', 'text': 'construct gazetteer volunteer big geo data base hadoop', 'timestamp': 1483225200, 'rel_doc_ids': ['148230327', '1713128408', '2000566545', '2015145891', '2030639709', '2087889651', '2108191078', '2114272918', '2115583184', '2129651192', '2140947881', '2158600124', '2160580257'], 'user_id': '1767269677', 'user_doc_ids': ['17181688', '37668753', '78351435', '93719868', '180503673', '185153854', '191211481', '196324764', '645682261', '795233632', '838079216', '873900664', '1481463619', '1483127950', '1544547318', '1547763358', '1553439079', '1553933943', '1559941189', '1563699022', '1563944509', '1570393125', '1589261835', '1590390465', '1599231733', '1663034667', '1692010344', '1759766770', '1759794405', '1764053936', '1767133166', '1769360309', '1808554854', '1809235633', '1841408218', '1846997392', '1945144656', '1947485713', '1950879617', '1965552061', '1971375508', '1979227258', '1988700461', '1990585273', '2000566545', '2001323649', '2005308683', '2015145891', '2020694859', '2022607562', '2053082582', '2054610764', '2058145017', '2077098878', '2081721999', '2086579594', '2095683278', '2102802363', '2109813820', '2112178600', '2116137000', '2117361273', '2118633099', '2123719297', '2124846565', '2127581732', '2129667801', '2132537081', '2146960954', '2153827416', '2157035111', '2160150134', '2161108194', '2162131147', '2170528992', '2171385712', '2183745830', '2208062848', '2221767630', '2230084733', '2265208668', '2294292686', '2402975919', '2407444278', '2432623660', '2622578904', '2741641967', '2949167002'], 'bm25_doc_ids': ['2949167002', '2203839147', '2158130508', '1910470015', '2567977967', '2028639006', '2226371358', '2295137617', '2249696392', '1988727770', '2952644771', '1972373301', '1984002948', '2058497043', '316408160', '2035287955', '2096626568', '2131151640', '2045682179', '971625919', '2126299219', '2145007852', '2121881613', '2055533652', '2548658843', '2141389081', '1977046764', '2114383418', '2131614043', '2058012923', '1892254776', '1991045564', '2302137010', '2274345890', '2171815584', '2540857807', '1591693625', '2950633767', '2313213765', '989955087', '1871423927', '1994722517', '2547632274', '2118236405', '1429921896', '33602388', '1903120171', '2061387982', '2220625384', '2110376549', '1667385237', '1997823022', '2291253645', '2084948739', '2097279640', '640741134', '1625108039', '2480083761', '2096676230', '2066299660', '2534179372', '2155144018', '158715319', '2203472219', '1498140008', '2065180054', '2179242496', '1847932736', '2030623053', '2053525776', '58722623', '2527514331', '1888086140', '2142825844', '1601636420', '1984029466', '2184516195', '2112731002', '1597017619', '2295862636', '2108510805', '2044863935', '1540059991', '1974648336', '2143107323', '2154733731', '1938861468', '2594703974', '2198351562', '1975291854', '2000666938', '1012393692', '1185452100', '1971591154', '2343186721', '2314821926', '2095178814', '2069281202', '1997365084', '2070275167', '1548206182', '1702021470', '2020105227', '2118457725', '2276030348', '2416173365', '2342757691', '2514566103', '43387135', '2052220722', '1561122532', '1993692389', '156852397', '2182825998', '1974396493', '1122377724', '2081203589', '2951171731', '416578099', '2135472951', '2565206224', '2075339457', '2040168785', '2515027919', '2001546272', '1978162812', '2100937240', '2312747595', '1907782118', '1977165632', '2072818022', '2484253528', '2095416015', '2164840730', '2126396161', '2044971612', '1167712381', '2269220688', '2202064371', '1968742231', '1553834236', '1968888566', '2202110644', '2164561142', '2467041498', '157554676', '2102723596', '2029959218', '109998294', '1773944860', '1975836659', '2171969566', '2128357850', '2165337198', '2272536022', '2511413133', '1831229313', '2144810099', '2102840489', '2125525633', '2252684483', '2200022554', '2212383961', '2040311741', '1965086648', '2023414891', '2417221886', '2528275513', '2545883617', '2543234608', '2170997892', '2107427360', '2126623642', '2084667297', '2951918923', '2099601885', '2443014316', '1480153478', '217772211', '758106613', '2010307076', '1481455709', '2541721212', '1993077300', '2533567963', '2022048910', '2342403903', '2019227001', '2029761735', '2081322929', '1997159848', '2093523329', '1998082755', '2141944896', '2098195118', '1931838234', '1436314764', '2506618759', '2403434865', '2157202651', '2344619542', '2327798879', '2138009319', '1506215767', '2071704971', '2126071066', '2328876236', '2230666400', '2155406510', '2026548343', '2963511300', '2294312748', '2094720464', '1985921809', '1971069518', '1538296207', '2138355424', '1550505192', '2189423141', '1507375662', '2010415693', '1597455204', '2061556416', '2143787382', '1969920293', '2501856297', '1999952822', '2291297649', '2066045135', '2994408143', '1985353575', '1495984639', '2070235786', '379212275', '1998791291', '2519770961', '2160269722', '2524620548', '2072506627', '2539508357', '2120303084', '2097863616', '2074306108', '2068826488', '2077098878', '2362555197', '2005866038', '1839370635', '2086453770', '2285252321', '1583924666', '1891340464', '2147992929', '2035881173', '2514510339', '2218038495', '2768149277', '280853923', '2545366524', '2028941259', '1935979693', '1969104149', '2168347486', '2159128662', '2036215731', '2589552882', '2544752953', '2014651376', '1984979859', '2288337563', '2250093878', '2131609285', '1937348540', '2527858748', '1130270400', '1987887896', '2035399628', '2149173129', '2401487318', '1713128408', '2021757338', '152042996', '1515595738', '1980917420', '2049417163', '2018796344', '1544148736', '1022530377', '1993369399', '2129651192', '1482335839', '2289597933', '2064366207', '2477193750', '1985554463', '2170958907', '2607116089', '1492102912', '2028637178', '2115197551', '2461884057', '1971309948', '56240009', '1547250093', '2252082668', '2016884187', '2048334074', '2281814488', '2031484596', '2015145878', '2021039270', '2088840018', '2024156991', '2149140091', '2400131631', '2162818071', '2137726616', '2559753769', '1491334888', '2088655835', '1967557983', '2409687816', '2399354038', '2025001960', '2018422605', '1910356505', '1914308858', '2522680599', '227278351', '1123101191', '2023459473', '2060650311', '2506573497', '2180526715', '2238499080', '2157954477', '2949205410', '1715440516', '2282372241', '2093148268', '2100986601', '2052263562', '2031099557', '2114272918', '2008649158', '1989269499', '2025988686', '2188562330', '1061391235', '2963629118', '2546596242', '2185925226', '2037133199', '2498534971', '2075997679', '1968034773', '1974504893', '2163189336', '1671471504', '1981605924', '2013840535', '2007047977', '1983528569', '2150358078', '1664704541', '2054540987', '2029803338', '2329835557', '2103442908', '2033042438', '2035835764', '2009918839', '2158790265', '2210210641', '2076793324', '2221554372', '2030790903', '167521103', '336454867', '2476433236', '2006240110', '2086966678', '2019104976', '2015145891', '2082945266', '2030572143', '1996486509', '2001242275', '2187012331', '2000566545', '1999995476', '2002844745', '2096143454', '2031054627', '2473013648', '2313661242', '1827660870', '2503227831', '1950879617', '2251132165', '1575292336', '2008406478', '2006801410', '602067175', '2065391912', '2123866731', '2114613708', '2092086632', '1640082718', '2118506665', '2266199501', '1602763530', '2060445678', '2243065871', '1522493411', '2269136550', '1594184920', '1904861952', '2123912023', '2030639709', '853285316', '2472594302', '2273897361', '5925520', '2296194724', '2160580257', '2049589877', '2004255221', '1479719777', '2023369457', '2304352267', '1975341771', '1537886048', '2059628440', '1982982698', '2145611520', '2521149585', '1974534485', '1516837731', '2203482404', '2086250231', '2295979280', '2553745069', '2025905839', '2008835308', '1475518655', '2341841044', '2140947881', '2093777905', '1972991037', '2117581909', '2034408713', '2244992359', '2025871890', '2078824402', '2162407175', '2123166308', '1501818423', '1982861695', '2118890823', '1533097366', '2082014773', '2036230858', '2577073759', '2172220707', '2485298192', '2137806719', '2341721601', '1896497052', '1532155946', '90233317', '2181417168', '2071839720', '2151141789', '2250422843', '1963669090', '1980267251', '1965148291', '2207323202', '2063176224', '2544727875', '193586665', '2079133244', '2257156731', '2024147439', '2046173508', '2026037750', '2115515621', '2018596395', '2166782290', '2187466363', '2002667482', '1502131641', '1908617982', '2199558228', '2293983046', '2272670728', '1988596202', '85995028', '2039424696', '124369656', '1580978282', '1861833605', '316696310', '1760653846', '1999880732', '2170780721', '2411453841', '1984471750', '2175206234', '2265297126', '2044661415', '2554092342', '2170632351', '1767133166', '2046661515', '22394099', '2463827188', '1989721641', '1485408073', '2031301021', '2040213390', '2030792561', '2118096631', '2284241005', '2187025990', '2077328184', '2125675006', '2289204937', '294835284', '1908401874', '2622396004', '2127338043', '2274984116', '2023943703', '1696908457', '2086148498', '1986283364', '2018887192', '1979768461', '1963832798', '2034428161', '2021035032', '2287987052', '293491225', '2087889651', '2004163170', '2159825340', '1973747955', '2290793703', '1978573552', '2016501084', '1995566945', '2053312172', '2287394288', '2040208384', '2147470852', '2120154746', '2404910536', '2289441824', '1913845021', '2131314981', '2025749837', '2154655801', '2054924729', '2560162498', '1996001153', '1885124406', '2090420594', '1991216377', '1202670576', '2100999807', '1968587440', '1591888204', '204957433', '148230327', '2142680014', '1542772284', '2155006296', '2101114896', '2344384003', '1532726669', '2298658038', '2108473620', '1980489403', '2171294728', '2159947102', '2109701112', '2339974927', '2075783691', '2136364310', '2091499656', '2077671437', '2123559652', '2209272682', '2208735439', '194110540', '117713785', '1863355020', '2038358279', '1522584278', '2064979035', '2056451646', '2117280821', '2025669595', '2279919853', '1595687452', '2158319633', '2093104233', '145084738', '2081302971', '2130518440', '2272206587', '2088934114', '2023901674', '185077138', '2053746181', '2106289562', '1965602589', '1992866985', '1578015699', '2075520250', '2102294813', '1963347892', '2275101004', '2014959658', '2105867153', '112089898', '1992683137', '1598357442', '2295536215', '2068697097', '2035282278', '2041331470', '2601540203', '2061173099', '1547009121', '2496080211', '2225049111', '2031456460', '2054599409', '2038361169', '182089284', '1185563673', '2261000915', '1547229954', '1985039942', '2401350435', '1578038984', '2174904793', '2962978410', '2154582566', '2041780659', '2013826439', '2040907046', '1964485161', '2048113526', '996820', '2358823061', '2020541351', '2136674036', '2333673279', '2556424125', '2060399391', '2013994377', '2283276873', '2014167420', '2063444371', '1976312879', '2081109058', '2014236758', '2115583184', '2007052323', '1019397535', '1512221035', '224934307', '2544186342', '1964083923', '2122127722', '1561860492', '2155822303', '1973827879', '2011020535', '1978451448', '2007212949', '1964903492', '2155072926', '2094022523', '1487531280', '1963512661', '2981767581', '2516510633', '2095682865', '2211004931', '209364199', '2003425021', '596850320', '2108660896', '2078608412', '2126550024', '2014722699', '1994752278', '1494422182', '2548126955', '2059314911', '2003138918', '2029988023', '1485941188', '2000276611', '1956693573', '2515934971', '2104341963', '2406140636', '2075308804', '1990974055', '2087133653', '2049489396', '2417847268', '1503912319', '2154516810', '2103088010', '2962691927', '68005632', '2171220798', '2417249256', '1731459909', '2176148920', '2051583779', '1998592529', '2046041960', '1831226595', '2442939118', '1998271633', '1565980122', '2145965489', '1982363918', '2531717957', '2141353133', '2122676969', '2160363604', '1987087936', '2022267942', '2466974089', '2020181757', '1974244034', '1502383304', '2027981954', '2021660846', '1962869802', '2074410942', '2158951102', '2136209984', '2066056374', '2186499617', '2158600124', '2040395858', '2164723041', '2003384702', '2252406119', '2078489014', '90672828', '2495476114', '2001041091', '2204968697', '2002533683', '1509940525', '183136791', '2013832912', '2399983209', '2008748565', '2040513965', '2035543557', '1989941815', '2102440263', '1968801596', '2261270398', '2007227498', '1486259113', '2117798581', '2135485714', '2100416747', '2018063315', '370404953', '2140278642', '1986160824', '2196852515', '2021724486', '2004306201', '1986780933', '2102564626', '2036051474', '2070974593', '2159849495', '2042221522', '1986569402', '2021601988', '1503488576', '2294199940', '2287696922', '2182901768', '2144888533', '1969873409', '2055174571', '2082463698', '2152686410', '1967647680', '2498262529', '2039548408', '2128971941', '2228044535', '2129120163', '2036538717', '2559657912', '2128626552', '2171690278', '148247732', '1985419898', '2042441590', '1606827124', '1968379551', '2088142766', '2101731563', '2091351040', '1992342029', '1923374144', '2087177601', '52405907', '2081165495', '2040696309', '2154366508', '2028226582', '1885828288', '2166030567', '2293423600', '1876018582', '1983816770', '2003587203', '1822105118', '2069225767', '2020820745', '2178828050', '2032812664', '77020168', '2098017870', '1988117216', '2025436588', '108536804', '2126743076', '2064910086', '2626975733', '220519675', '2159069802', '2003495016', '2067051372', '2108191078', '604693501', '2085061610', '1488080478', '2042527082', '2049725826', '2281250860', '2242964011', '2013657444', '2064218007', '2463554264', '2130233511', '2016918647', '1906264264', '2066140055', '935072310', '2169759309', '2538998463', '2536355209', '1923107755', '2000780492', '2205275345', '2039276185', '1987133329', '2013267675', '2005036666', '253424701', '2155528666', '2140724713', '1965108376', '64025680', '2155909634', '159674962', '1497505881', '1972906470', '1602964769', '1969413826', '2099109560', '2052211129', '2046697115', '1489091169', '2139445852', '2115397387', '2089990924', '2153482597', '2265110944', '1606325279', '2040572466', '2533399023', '2409856616', '2123288493', '2158640797', '2172034491', '1967174607', '2144324433', '2059157192', '105623487', '2094902951', '1965821519', '2049400253', '36313341', '2138237337', '2103070976', '2106754032', '1993667637', '2063444302', '2121382795', '2286780582', '1968781019', '2276924220', '2169055761', '957115272', '2021446094', '2536067421', '2015939813', '1535697676', '2221768484', '2236753337', '2171420352', '2123784091', '2026575055', '2061453330', '2035129552', '2027092014', '154489020', '2042350736', '2091905004', '2020828423', '1985439936', '2010302109', '1974165158', '2227117022', '2119632304', '1839795708', '2087909221', '2290526030', '2527791865', '2255570075', '1533057952', '2014056498', '283064482', '2440199141', '2156512349', '2121086382', '2031728889', '2111153823', '1685296808', '1984498938', '2143604065', '2097862643', '2170704906', '2125791414', '34797047', '2118258530', '2288922580', '2003829707', '1968230071', '2056476521', '2547552176', '2113716704', '1977965885', '1980376525', '2017855273', '2082545367', '314693758', '1996438407', '1990368720', '1571624092', '1904349858', '135798469', '2138357736', '2039226002', '2558113394', '2040966272', '2093763734', '2588304959', '2398265312', '2210190034', '1854015338', '2042315955', '2065820792', '2507677059', '2417588233', '2080092502', '1889103568', '2094678200'], 'bm25_doc_scores': [62.57566, 32.207886, 29.895893, 29.16859, 27.724989, 27.713224, 27.585886, 27.50969, 26.832514, 26.58701, 26.411388, 26.27866, 26.246897, 26.103935, 26.102161, 26.019112, 25.898037, 25.890423, 25.70342, 25.569592, 25.272139, 25.078144, 24.92335, 24.847727, 24.696545, 24.681171, 24.554487, 24.529495, 24.513401, 24.488836, 24.442968, 24.381126, 24.313469, 24.29458, 24.282082, 24.280838, 24.191267, 24.191267, 24.1645, 24.079796, 24.060661, 24.033909, 24.026361, 23.991096, 23.828064, 23.811783, 23.799896, 23.778065, 23.77532, 23.731903, 23.650663, 23.641518, 23.614162, 23.606478, 23.584734, 23.583755, 23.50643, 23.49607, 23.488766, 23.437332, 23.435284, 23.432749, 23.42602, 23.367102, 23.344017, 23.220293, 23.217825, 23.214886, 23.214853, 23.214853, 23.21393, 22.98229, 22.964415, 22.905706, 22.87133, 22.85712, 22.816055, 22.786594, 22.685, 22.61573, 22.601461, 22.581654, 22.55661, 22.54788, 22.545223, 22.521032, 22.520613, 22.515646, 22.499262, 22.470348, 22.450893, 22.416866, 22.374554, 22.368055, 22.364868, 22.315466, 22.314924, 22.312088, 22.30495, 22.284658, 22.279903, 22.265566, 22.202103, 22.17132, 22.1697, 22.166815, 22.160938, 22.15866, 22.156185, 22.025772, 22.002615, 21.982704, 21.975239, 21.974857, 21.935135, 21.918314, 21.910917, 21.877834, 21.875969, 21.853817, 21.829342, 21.82223, 21.809017, 21.807293, 21.798363, 21.787828, 21.786957, 21.775364, 21.737411, 21.732061, 21.69993, 21.698042, 21.677715, 21.662315, 21.659876, 21.637072, 21.626522, 21.62147, 21.61079, 21.552408, 21.541132, 21.538115, 21.492136, 21.489147, 21.444931, 21.436161, 21.427273, 21.427177, 21.418127, 21.402233, 21.398031, 21.398031, 21.395393, 21.377542, 21.347927, 21.347214, 21.34214, 21.33195, 21.284962, 21.26759, 21.267326, 21.256516, 21.254532, 21.250854, 21.243149, 21.239336, 21.238129, 21.212639, 21.162647, 21.149887, 21.149025, 21.148008, 21.14476, 21.142801, 21.125189, 21.125189, 21.103922, 21.085255, 21.039253, 21.018475, 20.97641, 20.957561, 20.929522, 20.914635, 20.902918, 20.88908, 20.871407, 20.860748, 20.84648, 20.834402, 20.81654, 20.814491, 20.807623, 20.793982, 20.789711, 20.782185, 20.779566, 20.777431, 20.753284, 20.748816, 20.74701, 20.745178, 20.71707, 20.715284, 20.710758, 20.702988, 20.663454, 20.648016, 20.629131, 20.622759, 20.619518, 20.606768, 20.588356, 20.530964, 20.522913, 20.521902, 20.512005, 20.51094, 20.494493, 20.489595, 20.48317, 20.459015, 20.426075, 20.413578, 20.40343, 20.370546, 20.358526, 20.322256, 20.313135, 20.305359, 20.297606, 20.29414, 20.278261, 20.246614, 20.220383, 20.194038, 20.191105, 20.177063, 20.161922, 20.1469, 20.128641, 20.121143, 20.113146, 20.111109, 20.087063, 20.062622, 20.052631, 20.050138, 20.029423, 20.026745, 20.025892, 20.016224, 20.000166, 19.98302, 19.980284, 19.971188, 19.945896, 19.915482, 19.91002, 19.90843, 19.903557, 19.901787, 19.898447, 19.889523, 19.886688, 19.88372, 19.88127, 19.880928, 19.880713, 19.863094, 19.844486, 19.813519, 19.808624, 19.797876, 19.796446, 19.794735, 19.783989, 19.78101, 19.77989, 19.749075, 19.743736, 19.742981, 19.732931, 19.731445, 19.725075, 19.724586, 19.723999, 19.722645, 19.70656, 19.681341, 19.666939, 19.658949, 19.657192, 19.645721, 19.627739, 19.60649, 19.579332, 19.551292, 19.543228, 19.521976, 19.514296, 19.508364, 19.502642, 19.501526, 19.495354, 19.487665, 19.47941, 19.470228, 19.469591, 19.461771, 19.460049, 19.459803, 19.440325, 19.43728, 19.436743, 19.434048, 19.417194, 19.40601, 19.39131, 19.39131, 19.386686, 19.38371, 19.369043, 19.358976, 19.354753, 19.35475, 19.353844, 19.352802, 19.350986, 19.345665, 19.345192, 19.344078, 19.325201, 19.311417, 19.294928, 19.279852, 19.275177, 19.275177, 19.269884, 19.269068, 19.258333, 19.256771, 19.255081, 19.249435, 19.235117, 19.218834, 19.21585, 19.20752, 19.206245, 19.191181, 19.184767, 19.176674, 19.173977, 19.170258, 19.166632, 19.160112, 19.159315, 19.156717, 19.149694, 19.113611, 19.107157, 19.10107, 19.095913, 19.092266, 19.088614, 19.078255, 19.075085, 19.073095, 19.065783, 19.060415, 19.038628, 19.036118, 19.029541, 19.012373, 18.992147, 18.983322, 18.981611, 18.977634, 18.973745, 18.96507, 18.963457, 18.96074, 18.957867, 18.952488, 18.921936, 18.919153, 18.906939, 18.90166, 18.897142, 18.883947, 18.881088, 18.881088, 18.858557, 18.85742, 18.848305, 18.838888, 18.81783, 18.81505, 18.810694, 18.784336, 18.772205, 18.770521, 18.770521, 18.76905, 18.745378, 18.744368, 18.744253, 18.737442, 18.719128, 18.71901, 18.718351, 18.710384, 18.702951, 18.696806, 18.694893, 18.668186, 18.650936, 18.646301, 18.642109, 18.620054, 18.616741, 18.61451, 18.609776, 18.590807, 18.582855, 18.568913, 18.565306, 18.565159, 18.552795, 18.547646, 18.547646, 18.539108, 18.529732, 18.525126, 18.516472, 18.496609, 18.488592, 18.483452, 18.478214, 18.477976, 18.456694, 18.421558, 18.41082, 18.405136, 18.391003, 18.36726, 18.344486, 18.340437, 18.337776, 18.317139, 18.31587, 18.305126, 18.298603, 18.295256, 18.285578, 18.28138, 18.27858, 18.26297, 18.261084, 18.25264, 18.25128, 18.247643, 18.222588, 18.221046, 18.207186, 18.200756, 18.17145, 18.148323, 18.143291, 18.138407, 18.12526, 18.122404, 18.11814, 18.114931, 18.098314, 18.090685, 18.07377, 18.073006, 18.054068, 18.046938, 18.029922, 18.026022, 18.019537, 18.004948, 18.000893, 17.982948, 17.97861, 17.966635, 17.94688, 17.94061, 17.925764, 17.919743, 17.908108, 17.897186, 17.88577, 17.873962, 17.860043, 17.853954, 17.845055, 17.835089, 17.819206, 17.815924, 17.812317, 17.803291, 17.798203, 17.79423, 17.767763, 17.766872, 17.766493, 17.764221, 17.761478, 17.758188, 17.745749, 17.734734, 17.720985, 17.708815, 17.70751, 17.702038, 17.701685, 17.697674, 17.696651, 17.692543, 17.678251, 17.67208, 17.654198, 17.654198, 17.648615, 17.645674, 17.643612, 17.643044, 17.639359, 17.634409, 17.63185, 17.614058, 17.612062, 17.611246, 17.589527, 17.589365, 17.585995, 17.585835, 17.5824, 17.574448, 17.566908, 17.556282, 17.553986, 17.549849, 17.535734, 17.524445, 17.524221, 17.522034, 17.517727, 17.48564, 17.483131, 17.47044, 17.456083, 17.446629, 17.433517, 17.430622, 17.422989, 17.413555, 17.407185, 17.406155, 17.397167, 17.394587, 17.380535, 17.370584, 17.363297, 17.363007, 17.334784, 17.333025, 17.321629, 17.312283, 17.310083, 17.302095, 17.301403, 17.277386, 17.268896, 17.264072, 17.253242, 17.226482, 17.226482, 17.222673, 17.220633, 17.219444, 17.213348, 17.20989, 17.206842, 17.204817, 17.201183, 17.200111, 17.196735, 17.195358, 17.184023, 17.182444, 17.18151, 17.17581, 17.165434, 17.158587, 17.147959, 17.147923, 17.147923, 17.146057, 17.1443, 17.143528, 17.14242, 17.131088, 17.124949, 17.122322, 17.107048, 17.068644, 17.058306, 17.03868, 17.034382, 17.022705, 17.022083, 17.011507, 17.010769, 16.992243, 16.98425, 16.978859, 16.966536, 16.959547, 16.959354, 16.95918, 16.951132, 16.939169, 16.932285, 16.91884, 16.917532, 16.911188, 16.890985, 16.88798, 16.873175, 16.87205, 16.866581, 16.865385, 16.864607, 16.859547, 16.854744, 16.847958, 16.843693, 16.835365, 16.823946, 16.812185, 16.812185, 16.807117, 16.785099, 16.785099, 16.769915, 16.768833, 16.765211, 16.76313, 16.76027, 16.754276, 16.752636, 16.750437, 16.742634, 16.739353, 16.721704, 16.720366, 16.708344, 16.708344, 16.69207, 16.680305, 16.669464, 16.669464, 16.666845, 16.666845, 16.663496, 16.65704, 16.655375, 16.648764, 16.642384, 16.622189, 16.611046, 16.610369, 16.600838, 16.597486, 16.590618, 16.577782, 16.576283, 16.567987, 16.567827, 16.565243, 16.561104, 16.551058, 16.537102, 16.537102, 16.535799, 16.529877, 16.529633, 16.526285, 16.52332, 16.514795, 16.511156, 16.504393, 16.499178, 16.483095, 16.473524, 16.472748, 16.472515, 16.469616, 16.455675, 16.455395, 16.447586, 16.440521, 16.437553, 16.431934, 16.40477, 16.390312, 16.385849, 16.370687, 16.355673, 16.33818, 16.335966, 16.330553, 16.32866, 16.320671, 16.307247, 16.301502, 16.301464, 16.301464, 16.299639, 16.276192, 16.25913, 16.256132, 16.247293, 16.245277, 16.243206, 16.242857, 16.236366, 16.234941, 16.234343, 16.227953, 16.222036, 16.220854, 16.219734, 16.207317, 16.193447, 16.18977, 16.185904, 16.177746, 16.175264, 16.17175, 16.167934, 16.163555, 16.161667, 16.155357, 16.152908, 16.14669, 16.14669, 16.132618, 16.120964, 16.102942, 16.102623, 16.096832, 16.09268, 16.08965, 16.087894, 16.07977, 16.054508, 16.054274, 16.051205, 16.04565, 16.045605, 16.037842, 16.034214, 16.032331, 16.027718, 16.026672, 16.015156, 15.999383, 15.99797, 15.997045, 15.990706, 15.989962, 15.987054, 15.983526, 15.97029, 15.967253, 15.966595, 15.966595, 15.9582205, 15.954241, 15.954155, 15.949052, 15.942912, 15.935083, 15.926834, 15.921752, 15.916687, 15.903166, 15.899303, 15.891895, 15.889086, 15.886271, 15.884174, 15.877942, 15.876319, 15.867981, 15.853361, 15.85128, 15.844095, 15.840021, 15.835838, 15.831482, 15.825573, 15.823488, 15.8146305, 15.8146305, 15.810646, 15.800208, 15.799615, 15.795496, 15.7936125, 15.792395, 15.780737, 15.779408, 15.775876, 15.7731695, 15.771256, 15.762968, 15.758829, 15.750435, 15.74936, 15.74595, 15.739122, 15.73162, 15.71914, 15.717411, 15.71164, 15.702027, 15.700016, 15.676043, 15.674047, 15.674019, 15.673724, 15.670004, 15.66239, 15.661219, 15.660549, 15.659895, 15.659797, 15.65819, 15.65819, 15.657968, 15.6579, 15.657128, 15.650896, 15.646191, 15.645848, 15.645512, 15.64362, 15.643512, 15.641464, 15.637873, 15.6305485, 15.627755, 15.626024, 15.625988, 15.620529, 15.61897, 15.61897, 15.617772, 15.609142, 15.60817, 15.604282, 15.604218, 15.585968, 15.585752, 15.577232, 15.572045, 15.566439, 15.559704, 15.549608, 15.545925, 15.544872, 15.540782, 15.538142, 15.537216, 15.533659, 15.527748, 15.527372, 15.519351, 15.516838, 15.510338, 15.506339, 15.502504, 15.499283, 15.495106, 15.491866, 15.489839, 15.487052, 15.485342, 15.48443, 15.480764, 15.471149, 15.470238, 15.469038, 15.468166, 15.468166, 15.460224, 15.458875, 15.455892, 15.449851, 15.449677, 15.443775, 15.438758, 15.434357, 15.431871, 15.430965, 15.430898, 15.429302, 15.422825, 15.4218, 15.411764, 15.4115715, 15.409037, 15.405526, 15.39543, 15.3941555, 15.389447, 15.385681, 15.369303, 15.3669405, 15.365896, 15.360092, 15.359361, 15.34756, 15.345872, 15.345667, 15.334681, 15.332981, 15.330774, 15.328838, 15.325962, 15.318732, 15.308414, 15.306771, 15.2979145, 15.296956, 15.291796, 15.286102, 15.286011, 15.28351, 15.274919, 15.270482, 15.267286, 15.264994, 15.260167, 15.25624, 15.248673, 15.240816, 15.2383175, 15.235552, 15.230339, 15.227354, 15.2183, 15.216312, 15.215171, 15.208181, 15.20705, 15.20705, 15.2032795, 15.1965475, 15.186842, 15.182949, 15.178811, 15.177206, 15.176506, 15.176506, 15.173247, 15.171724, 15.171383, 15.171332, 15.163519, 15.158102, 15.154693, 15.154267, 15.153479, 15.146253, 15.145374, 15.144365, 15.143674, 15.141935, 15.1410265, 15.13902, 15.137003, 15.132008, 15.129002, 15.124261, 15.124261, 15.12124, 15.113339, 15.108758, 15.097061, 15.087836, 15.085434, 15.078173, 15.076176]}, {'id': '2246775628', 'text': 'data driven hint generation vast solution space self improve python program tutor', 'timestamp': 1488322800, 'rel_doc_ids': ['1514405275', '15150874', '1531272147', '1702105379', '173675007', '1765030997', '183173153', '2000502680', '2039113359', '207841007', '2105543722', '2137390126', '2185836578', '2395351797', '278209763', '832120473'], 'user_id': '311794911', 'user_doc_ids': ['10235929', '12070778', '12309149', '14377909', '32828536', '39098946', '54318667', '62528813', '75572175', '76668639', '91852393', '98871971', '104688952', '146879175', '147751402', '150616554', '159916904', '161511136', '167224324', '171590144', '173675007', '182809541', '183173153', '194578403', '196989192', '199486239', '204073018', '223541821', '296476643', '832120473', '1481149491', '1484977212', '1485097704', '1490935989', '1492339772', '1496268091', '1501158894', '1504261278', '1506139303', '1512657631', '1513053919', '1517725046', '1518446432', '1518613644', '1522943378', '1525672456', '1528917175', '1533754817', '1539133502', '1546796311', '1549049459', '1552971165', '1554216613', '1562092080', '1562878411', '1571563099', '1572595918', '1583569723', '1589525026', '1589990597', '1594986522', '1596401170', '1597169097', '1600506960', '1602804068', '1603249376', '1606596577', '1660939118', '1715113288', '1751507894', '1755571560', '1772281837', '1776867500', '1782683625', '1795694001', '1806683521', '1838300095', '1858868033', '1860021985', '1889198722', '1909483596', '1931182173', '1950434568', '1952537043', '1959691478', '1971746982', '1975429316', '1979855721', '1982250324', '1993868776', '1996489894', '2002208419', '2003770790', '2024253803', '2025424957', '2032196293', '2035080477', '2038558034', '2044243636', '2053092511', '2053972272', '2058522826', '2061928290', '2066816090', '2069897595', '2076273156', '2077707062', '2082236895', '2091168828', '2095895387', '2096363625', '2096817138', '2097279224', '2098567624', '2099192287', '2102404693', '2102643910', '2106779500', '2106947381', '2108080737', '2109901510', '2110898172', '2112038415', '2112248565', '2112564774', '2116730437', '2117629044', '2118127318', '2118726258', '2118768878', '2120932048', '2124689820', '2125366397', '2127523403', '2128332525', '2128365310', '2132021886', '2132425059', '2133467832', '2136070270', '2140485074', '2141176783', '2143010889', '2143143356', '2143662510', '2148480826', '2148811928', '2149109746', '2150514097', '2154585842', '2155941504', '2157300384', '2157535018', '2158703238', '2160133757', '2161929554', '2162353222', '2163640453', '2163937430', '2164638355', '2165652651', '2168194805', '2170706207', '2171825763', '2172297119', '2181808740', '2186113267', '2186420685', '2206575885', '2231051615', '2237600160', '2241275117', '2242960040', '2261910606', '2276356909', '2337738304', '2396626363', '2396746452', '2398162348', '2398722307', '2400558464', '2403849347', '2406413179', '2465743429', '2467881409', '2478876718', '2517159558', '2573723437', '2579624057', '2622005879', '2767545728'], 'bm25_doc_ids': ['2185836578', '832120473', '164716849', '173675007', '1702105379', '2113183161', '15150874', '2160308286', '1514405275', '2116311359', '1635484354', '1759655780', '1522107738', '1984964495', '2553668854', '2119985352', '38375411', '2295669350', '172250868', '2110832017', '1593091100', '1572609470', '2000502680', '2082023928', '1538287601', '56667296', '1528614120', '2029326467', '1596001864', '104688952', '2049644282', '2089211892', '1500427426', '2055636259', '345126416', '2119434486', '2117311111', '2068742478', '2169398191', '2016122903', '2141662058', '2056816375', '1518613644', '294096832', '1936568217', '123294412', '1564960024', '2117479559', '2154196599', '2108080737', '2086341847', '1993335798', '2117274534', '2141277505', '2116290472', '1815449268', '2071814635', '2404969596', '23179720', '2520360063', '2395351797', '2051379322', '66598541', '1522700601', '2182489632', '1783528648', '2107161372', '2035638261', '2154137473', '2342061046', '2091168828', '2102219255', '2610752197', '75435561', '1981593684', '1529974864', '1535211378', '131533407', '2122045419', '2247695808', '2155187039', '2004429519', '107519103', '935398018', '1958422483', '1965091287', '2008675462', '800369072', '15737819', '2139286713', '2079839311', '1549510362', '2066536396', '2060874502', '278209763', '1967775214', '1978689389', '1748547084', '2060344060', '1765030997', '58352647', '2100629176', '2093325188', '73284199', '194578403', '2035061933', '2051878669', '187993814', '2115253763', '2055183121', '2098555131', '1578897660', '2093721629', '2292516440', '2112038415', '2141180045', '2041907065', '1975885252', '2097191954', '2159898039', '1861448938', '2039670527', '2165652651', '2120199184', '2950779858', '2034438127', '2048923076', '2108125751', '2149826094', '2001032894', '2169987756', '2036375103', '1947638464', '2077542803', '1512657631', '2021184587', '44878163', '1529619255', '2406259733', '2161995137', '1774458449', '1876718644', '2141941960', '2045683699', '1566496738', '2013497272', '1838300095', '2148943480', '1970156027', '2963498916', '2158376506', '2128365310', '2153562609', '1640344466', '2208244253', '2088948864', '2170642695', '2004940445', '2159306807', '80460290', '1482491819', '2075944923', '1929610448', '2137030454', '2963424716', '2045217412', '2142836622', '2147976636', '1979340476', '2146963356', '2050375580', '1513053919', '1537038124', '1498834923', '1988780915', '1570348318', '2565964892', '2020110561', '1975429316', '1946176756', '23069016', '2245439784', '2062534405', '2121845498', '2276052610', '1601155378', '2060871333', '1983416467', '1975588116', '2157535018', '329984692', '1984832305', '59438009', '2116766394', '2036149106', '2326086442', '2239404455', '1538826635', '86864185', '1519941220', '1530151067', '2171323601', '2026940875', '1589123116', '2306437611', '2128571476', '2001122975', '2231663298', '1547015280', '1974009276', '2031634592', '2419015228', '2079703584', '1869737774', '2167494792', '1486259229', '2133386804', '2154428648', '277952563', '1978952404', '1974422482', '1884023773', '2170867027', '2116465811', '95401959', '2130198333', '2025214001', '2087461077', '1838128894', '2073826656', '1583569723', '1588447559', '268314418', '2548896891', '110644667', '2032017624', '2170706207', '1550197872', '270200766', '2143363730', '207841007', '2094385299', '2024393693', '2027237027', '2149395136', '1995343013', '1553288610', '2042742211', '2117177405', '159200191', '1576698700', '2147736027', '2127759428', '1522539682', '2086843293', '2007856524', '2951374049', '2576461972', '2199564607', '1693856887', '2165270933', '1595844084', '2094353386', '105865702', '2159927097', '2109814389', '2022224635', '2414240756', '1976346389', '2157020258', '1565890149', '1055856051', '2034343791', '104585957', '1920916604', '2767235257', '2093799248', '2250531524', '1967169840', '2079753334', '1515641485', '2002337194', '1555308388', '2042037521', '1531272147', '1484812129', '1986220734', '2086177694', '2082221645', '2951007578', '183173153', '2141176783', '2150743842', '197404575', '1946564775', '1509005419', '1979260682', '1790064750', '2237594715', '1986445122', '2007929217', '2024317539', '13581958', '155259151', '2015197254', '2179217695', '632519302', '43677906', '2007163257', '1175049046', '80280083', '2072070923', '2107162140', '2059405503', '2100266218', '2138659362', '2041888528', '2155416760', '2125320063', '1605619582', '2170233861', '2344633793', '2186498586', '2086186328', '2128107973', '2110941717', '2063832687', '1668562274', '2198123160', '147316254', '2130949383', '2125142135', '2016803695', '1971506901', '113865656', '1550802367', '1980064786', '1884763894', '2045248418', '2406533925', '2122139024', '1550851644', '2162485568', '2132022337', '1999834999', '1845252150', '1966605557', '191699736', '2112242119', '2106357912', '2102257030', '2163503255', '2149870462', '1971746982', '1597883411', '1597001949', '2028417436', '2409365700', '2021769100', '1977026748', '2020618096', '2340255158', '1983555569', '2305008618', '2051740650', '1506139303', '578770800', '2138163628', '1931788259', '229296819', '2131067980', '1565952360', '2168022840', '2241275117', '1600506960', '2026670402', '2024867567', '1988054515', '1596981493', '2144017306', '1505447236', '1965894746', '2047514318', '2134285176', '2046941175', '2141549094', '1573148365', '2009526138', '2010334716', '2040179147', '2013537260', '2137390126', '2128239275', '1988412861', '2767242699', '57898551', '2142525910', '2108115817', '2085975293', '2028974596', '2490619405', '2796427', '71566816', '2004866456', '2477259301', '2288172020', '1496048607', '2254575935', '1924044378', '261620263', '2042183142', '2949612930', '2154585842', '2064694982', '2134593176', '1556836847', '2170378453', '2089592719', '2500595894', '2011995159', '2010386944', '2210785661', '2063605822', '2744858363', '170748973', '1585932355', '2032414767', '2093007025', '2101283307', '2108284981', '2047681133', '2138439213', '2046617807', '1916694713', '2239291282', '1997133651', '2113660760', '2164033124', '1804480834', '2288138979', '2323362402', '2032001951', '2084562107', '2319805499', '1601752555', '1977604962', '2058749831', '1521924303', '2044598283', '331726321', '1574847434', '2169526768', '1490935989', '2141108110', '2231051615', '1530785623', '1984428716', '2065878523', '1604557810', '2002526995', '2155380397', '171227519', '1557466878', '112343938', '2408533986', '1970045682', '2324320295', '2050545074', '2038133856', '2031567737', '2004271588', '2125733893', '1858577629', '2111049992', '2023427265', '134713021', '2148897203', '1987792246', '2154603958', '2074019473', '1547665845', '2256221047', '2096945864', '2843391880', '2114352872', '2160329688', '2026435045', '2161090654', '1999247824', '2402102294', '2155217849', '2077450631', '2108093575', '950442279', '2065849214', '2115128623', '1984505565', '1980863555', '1846534677', '2841782423', '1504625696', '2128332525', '2037025782', '2183743556', '101642820', '1552084969', '2056376603', '1987305651', '2170207061', '2585630816', '2115003153', '2184375123', '1719901447', '1994087932', '1487208098', '2076252126', '3023456609', '2737574438', '1519497240', '1164389233', '1486264297', '83256457', '2147006095', '2306865310', '1993193917', '279699730', '1583213447', '1991481748', '577380298', '2129935721', '1571563099', '2058219274', '2074467960', '2290122798', '2053291921', '155196027', '2140613019', '2277091242', '1489129250', '1549207428', '3002568805', '2023037746', '2161486622', '2057451870', '2112393517', '2273205329', '152440458', '2007458552', '2151889555', '2114095200', '2105543722', '14377909', '192344029', '2080381769', '1540372287', '2017813341', '49623240', '1576466941', '2084728755', '2220434901', '2059919683', '2027696773', '2008666873', '2102241854', '1986163437', '1043450216', '2528691278', '2018504944', '2098567624', '2061045626', '2294363794', '179748681', '2048465561', '1577227890', '2039113359', '2259657469', '1852973223', '2156899406', '2056458682', '2053537368', '2158820915', '1489396847', '1532990179', '2074930180', '1999645150', '2000871891', '1966401487', '2040869627', '2070763873', '2060787023', '1991529781', '2064378249', '1556080347', '1027599383', '2077707062', '129963772', '2405441773', '2726186190', '2021214862', '2121088444', '2109757792', '2149172860', '2165708913', '2044845150', '87671623', '2074810228', '1955296562', '2153481645', '1592924430', '2250653987', '2228956966', '2032406331', '2025940313', '1582747576', '1515704524', '2026701697', '2119007879', '2270715256', '1539253378', '2279384388', '2077127911', '2129906961', '1496302630', '2160635375', '2065331013', '2130064409', '2166238335', '2103469574', '2757476937', '314725774', '1974086876', '2102473657', '2071057536', '1978894865', '155156446', '2175806766', '2212383961', '2040755506', '1979264725', '2142089867', '1980602928', '2224158306', '1564876955', '2055633108', '2057602088', '1567669023', '2165652195', '2128805590', '1571584848', '2209011815', '2069648634', '2101199693', '1950434568', '2132020886', '2114326383', '2110123163', '1964350601', '1557571162', '19665345', '2122160798', '1622531328', '1943675947', '2061250199', '2020042260', '1754842124', '2038924060', '2025909904', '3139219', '2140105020', '2127765966', '2011642956', '92973108', '1990134054', '2132387073', '1961976386', '2255712816', '2050606033', '1923935900', '2131850886', '1551666312', '202803080', '1507062346', '2160140570', '184734114', '2397924295', '2151466713', '2169362575', '2138102910', '2131021999', '2106055299', '2090378964', '1586618323', '1497153559', '2050876821', '1504261278', '2131646163', '2116187160', '2248455663', '2423002006', '2128631219', '2041649398', '2034559937', '2110362134', '2146039870', '1536103402', '2042904650', '2094147479', '2022363576', '2067982358', '2057483198', '2037246595', '1601193885', '1970605659', '1964694219', '1513885979', '1948833364', '2030141382', '2067395414', '2118097211', '1572009591', '2151389589', '2117737933', '1571815258', '1970716201', '2029455112', '2089510042', '75829264', '2099940214', '2290870390', '2004770851', '1565016070', '1983189597', '2108504591', '1545676661', '1880413143', '2041931148', '2006224274', '1697784338', '2012143820', '2047490332', '2044458395', '2100813464', '2157591512', '1563691708', '1605965346', '2050731970', '2171971095', '2114349268', '2016070699', '2001334177', '2598120107', '2119582595', '1533754817', '1985729415', '1986380222', '2118736259', '2026777696', '2563025099', '2405628410', '2098313157', '2535029792', '2564856016', '2036830099', '2230077625', '1985115957', '2098049546', '2115787769', '2406102601', '2098490824', '1534665536', '2252432360', '2119402819', '45477529', '1496268091', '2122811495', '2057562416', '2014848434', '1699610660', '2405880654', '2223951097', '1524586167', '98374735', '2520410942', '2125366397', '2099820523', '2744629928', '77694872', '1983303503', '2261676784', '1717366851', '2089126405', '2316501305', '1906206320', '1540453332', '1566871903', '2230602046', '110902477', '1788054970', '2113393723', '2010013160', '2278077385', '2162238313', '61355227', '2228989207', '2029218511', '335330900', '1967454648', '2554433712', '2273440734', '2163937626', '1980763452', '2139866278', '2161438006', '2146099733', '2398156266', '107670845', '1876912027', '2090984067', '2208034660', '1999651978', '90847294', '2155235647', '1991725885', '2076132422', '3023456030', '39606614', '1980125490', '1521626219', '2051545216', '1991123686', '2000517716', '2075143267', '2059205366', '2139410478', '1978249367', '2084907258', '2100014056', '1910407928', '2017942567', '1497898069', '2118902880', '2111443557', '34311366', '2141767269', '2161194057', '2218622245', '2112969475', '1790345853', '2077806522', '2162119800', '1578857261', '952777547', '2184165251', '2137726543', '2201563796', '1582115134', '125529172', '2025424957', '2151885229', '2170764013', '1562980762', '2544020351', '2083487084', '2171781807', '2078948876', '2318815583', '2296934178', '2088903128', '2044520055', '1483926990', '1596513284', '1578308552', '2032054060', '2179263669', '2084852878', '1437335841', '2114426947', '2039410502', '1545672963', '2060002925', '2076977756', '2001940060', '2023814507', '2080158413', '1510640444', '2237600160', '2001518702', '2041386509', '1968864800', '2011514820', '1587527504', '2027865491', '1576925034', '2166892612', '2096950951', '2014327882', '2116777580', '2057063125', '1586573420', '1555401473', '2231662803', '2462008265', '2131399596', '2019918933', '1987720735', '2134292017', '2152768466', '1858152760', '2163814487', '2050506395', '2131040287', '1971102487', '1514557774', '1990049219', '2100539437', '1481417764', '2023802697', '2001146563', '2113924036', '2116986122', '2259839829', '1998650170', '2169621733', '2171156763', '2163854795', '1973851443', '1988460397', '2275294428', '2078371413', '2087764001', '1483755489', '1495169727', '2119240138', '2547966690', '1486052588', '2014277198', '2059898315', '1785298306', '1514596996', '2120932048', '2018298139', '2146505185', '2141445455', '2158056686', '2951678571', '64066664', '2214227126', '74633290', '2516918235', '2093599307', '120569995', '2267556340', '2164070717', '1967023914', '2056447856', '1593269116', '2299150109', '2131970361', '1572814947', '2123159101', '2149546358', '2103016602', '2051346631', '2106061258', '1000961037', '2045139340', '1788849765', '2152236140', '2168274396', '2017347525', '1972411778', '2020458978', '2021781365', '2066837052', '1991999989', '2077572333', '1513250869', '2028053566', '2113592754', '2206071134', '2154121809', '1030808755', '2152247885', '2039406101', '2144932582', '2105507325', '1532854824', '2062153375', '2166890117', '2100775187', '2167994513', '2082519236', '2027218407', '1970150013', '29751708', '2126545654', '1869923409', '2048001653', '2271410387', '1977273827'], 'bm25_doc_scores': [42.560707, 41.068256, 41.043083, 34.801933, 33.886234, 33.277622, 33.05236, 32.05845, 31.586468, 31.401764, 31.27719, 31.27719, 31.26141, 30.646387, 30.634556, 29.807812, 29.515429, 28.923199, 27.897745, 27.77209, 27.536575, 27.42199, 27.26582, 27.207586, 26.652267, 25.975666, 25.864439, 25.851725, 25.47026, 25.20229, 25.162924, 25.081738, 25.06743, 24.966286, 24.96321, 24.954678, 24.898525, 24.876665, 24.850964, 24.803478, 24.80186, 24.653372, 24.639606, 24.595839, 24.565762, 24.465315, 24.361109, 24.286306, 24.070162, 24.017939, 24.006819, 23.947964, 23.884825, 23.82005, 23.747118, 23.732832, 23.708933, 23.660776, 23.568062, 23.517262, 23.411228, 23.284082, 23.196198, 23.145248, 23.003098, 22.949512, 22.858889, 22.83645, 22.773443, 22.729527, 22.71838, 22.709444, 22.638968, 22.635208, 22.592844, 22.52183, 22.481592, 22.467476, 22.362572, 22.362396, 22.342953, 22.276833, 22.276299, 22.261185, 22.261147, 22.18443, 22.183249, 22.179094, 22.10778, 22.097555, 22.089918, 22.010836, 22.006968, 21.909903, 21.878176, 21.850489, 21.848911, 21.8426, 21.831293, 21.81738, 21.811152, 21.784956, 21.76926, 21.745802, 21.690434, 21.68978, 21.686655, 21.664768, 21.660898, 21.599268, 21.57208, 21.549807, 21.514126, 21.486427, 21.472853, 21.435266, 21.371983, 21.363028, 21.358501, 21.332378, 21.331514, 21.260115, 21.20655, 21.190367, 21.117702, 21.112373, 21.10032, 21.054232, 21.041033, 21.032917, 21.01866, 20.996931, 20.958727, 20.9464, 20.93548, 20.908817, 20.89699, 20.894897, 20.890825, 20.88462, 20.883427, 20.875044, 20.839474, 20.838213, 20.835358, 20.824883, 20.819752, 20.807854, 20.806051, 20.800882, 20.795103, 20.781176, 20.772575, 20.7556, 20.702337, 20.675766, 20.663067, 20.636642, 20.629887, 20.601269, 20.601263, 20.598152, 20.583101, 20.58176, 20.57991, 20.57415, 20.560072, 20.5476, 20.541424, 20.539495, 20.535477, 20.530516, 20.525928, 20.52167, 20.499783, 20.443205, 20.436348, 20.398146, 20.385654, 20.367691, 20.342865, 20.341927, 20.341774, 20.321543, 20.31778, 20.30469, 20.29017, 20.249117, 20.231497, 20.230085, 20.20979, 20.18431, 20.17534, 20.170744, 20.159328, 20.14976, 20.130835, 20.12545, 20.10207, 20.090755, 20.084127, 20.077438, 20.05096, 20.038328, 20.032019, 20.02436, 20.014877, 20.007927, 19.983173, 19.969004, 19.958035, 19.949577, 19.947168, 19.94597, 19.938028, 19.9301, 19.90234, 19.894802, 19.885294, 19.884922, 19.870115, 19.850721, 19.839724, 19.833588, 19.816938, 19.803526, 19.786938, 19.75563, 19.755142, 19.755142, 19.747644, 19.737022, 19.731445, 19.73116, 19.711369, 19.709057, 19.707108, 19.694801, 19.689001, 19.686777, 19.67993, 19.678877, 19.667803, 19.664375, 19.643461, 19.614382, 19.604088, 19.596884, 19.595331, 19.584015, 19.567156, 19.564747, 19.561327, 19.534628, 19.524364, 19.51352, 19.507124, 19.503635, 19.493866, 19.4899, 19.48509, 19.46345, 19.462627, 19.460129, 19.445698, 19.439003, 19.437346, 19.431986, 19.41196, 19.372332, 19.370798, 19.351511, 19.343117, 19.319914, 19.313047, 19.311012, 19.310482, 19.307665, 19.283424, 19.280409, 19.27907, 19.263002, 19.255095, 19.249144, 19.246494, 19.239985, 19.238787, 19.234896, 19.23437, 19.232224, 19.228584, 19.216162, 19.215548, 19.210592, 19.191395, 19.18952, 19.184265, 19.149826, 19.14331, 19.137405, 19.11813, 19.116028, 19.110512, 19.104887, 19.102585, 19.093155, 19.081587, 19.078773, 19.05715, 19.03251, 19.02163, 19.003502, 18.990652, 18.98479, 18.979832, 18.974907, 18.965864, 18.962582, 18.928503, 18.920097, 18.90976, 18.907497, 18.901394, 18.900822, 18.895792, 18.878649, 18.865692, 18.86437, 18.86279, 18.862772, 18.859962, 18.84887, 18.840178, 18.837471, 18.833288, 18.822947, 18.806702, 18.806479, 18.803556, 18.799213, 18.769392, 18.769007, 18.761225, 18.740837, 18.739557, 18.73364, 18.73074, 18.72599, 18.725063, 18.72068, 18.718994, 18.717564, 18.714464, 18.703125, 18.701115, 18.693783, 18.692184, 18.687748, 18.682348, 18.681805, 18.666353, 18.65242, 18.652283, 18.63788, 18.632961, 18.62643, 18.625326, 18.622633, 18.605404, 18.591599, 18.583357, 18.580967, 18.559587, 18.549133, 18.547457, 18.53463, 18.532055, 18.527018, 18.520874, 18.519028, 18.517628, 18.492605, 18.484627, 18.466642, 18.46527, 18.458544, 18.45685, 18.45383, 18.453123, 18.452847, 18.431826, 18.423094, 18.422987, 18.415232, 18.410782, 18.406858, 18.40108, 18.394493, 18.390535, 18.359634, 18.359032, 18.35879, 18.356731, 18.355234, 18.321222, 18.320526, 18.305843, 18.303295, 18.294556, 18.288305, 18.274155, 18.268438, 18.262066, 18.259989, 18.259697, 18.244606, 18.241608, 18.239702, 18.237753, 18.227278, 18.221653, 18.22127, 18.215107, 18.210373, 18.20403, 18.1993, 18.195496, 18.190416, 18.186085, 18.176735, 18.17537, 18.172253, 18.169777, 18.168621, 18.16828, 18.162022, 18.156937, 18.154964, 18.152254, 18.147688, 18.13647, 18.130852, 18.129938, 18.119226, 18.116077, 18.115837, 18.112177, 18.109814, 18.109102, 18.10469, 18.10251, 18.102133, 18.099258, 18.098722, 18.088493, 18.086788, 18.07901, 18.075382, 18.067816, 18.065266, 18.060574, 18.059246, 18.058027, 18.048443, 18.047495, 18.044039, 18.040997, 18.032871, 18.032574, 18.029747, 18.02785, 18.016174, 18.009058, 18.004246, 18.000168, 17.990643, 17.985283, 17.985052, 17.976374, 17.971676, 17.959984, 17.959984, 17.94924, 17.934551, 17.925224, 17.921387, 17.92079, 17.919565, 17.917465, 17.913084, 17.905256, 17.904041, 17.900452, 17.898445, 17.897778, 17.893934, 17.888094, 17.883522, 17.883091, 17.877468, 17.87623, 17.874733, 17.87283, 17.8727, 17.863037, 17.862953, 17.86214, 17.854307, 17.85403, 17.852041, 17.8508, 17.836672, 17.833761, 17.830002, 17.829609, 17.829473, 17.828028, 17.82377, 17.820505, 17.817892, 17.813519, 17.81269, 17.808407, 17.806036, 17.802109, 17.797623, 17.787235, 17.781923, 17.77046, 17.757313, 17.755854, 17.755058, 17.74572, 17.740372, 17.733173, 17.728977, 17.72231, 17.720911, 17.72028, 17.719238, 17.71881, 17.715075, 17.714663, 17.714598, 17.705421, 17.704588, 17.703892, 17.703302, 17.703053, 17.691528, 17.686676, 17.681704, 17.67789, 17.676123, 17.67609, 17.675735, 17.674374, 17.67343, 17.67213, 17.67181, 17.66217, 17.661205, 17.659843, 17.657772, 17.656225, 17.654316, 17.651033, 17.641548, 17.641214, 17.64059, 17.634342, 17.625435, 17.62173, 17.62119, 17.620079, 17.617998, 17.609604, 17.605848, 17.604477, 17.601328, 17.598774, 17.597553, 17.589243, 17.587196, 17.586876, 17.585333, 17.580112, 17.579826, 17.579077, 17.577759, 17.575869, 17.574898, 17.573162, 17.567738, 17.563694, 17.554535, 17.55024, 17.546734, 17.541897, 17.540987, 17.537895, 17.536911, 17.535835, 17.533092, 17.526714, 17.524477, 17.523453, 17.5228, 17.522797, 17.520126, 17.51713, 17.512718, 17.512394, 17.508389, 17.505577, 17.504562, 17.50346, 17.49983, 17.49429, 17.486776, 17.485216, 17.483719, 17.465843, 17.464912, 17.462372, 17.462194, 17.46197, 17.460342, 17.46026, 17.457602, 17.4563, 17.452326, 17.45007, 17.449633, 17.443537, 17.442614, 17.441444, 17.439306, 17.438847, 17.437035, 17.436249, 17.433167, 17.415354, 17.411312, 17.410694, 17.406303, 17.405981, 17.404835, 17.395994, 17.394993, 17.393305, 17.389204, 17.386143, 17.383888, 17.37747, 17.3744, 17.353502, 17.351357, 17.348879, 17.34549, 17.338606, 17.338493, 17.338432, 17.338322, 17.335657, 17.33188, 17.331814, 17.32051, 17.319374, 17.318684, 17.318335, 17.318007, 17.307827, 17.301413, 17.29907, 17.297607, 17.297407, 17.296509, 17.296192, 17.28838, 17.287514, 17.28028, 17.27698, 17.262964, 17.260277, 17.260138, 17.258762, 17.255367, 17.255295, 17.254364, 17.249928, 17.248182, 17.237902, 17.236328, 17.229889, 17.22778, 17.226826, 17.224476, 17.219736, 17.208456, 17.208069, 17.208014, 17.200644, 17.200157, 17.198591, 17.198452, 17.197512, 17.193329, 17.19113, 17.189095, 17.188707, 17.178537, 17.17682, 17.174778, 17.172182, 17.168282, 17.162287, 17.161701, 17.159325, 17.158634, 17.155962, 17.153608, 17.14757, 17.14653, 17.141241, 17.127918, 17.12418, 17.118687, 17.116755, 17.115559, 17.11396, 17.112654, 17.112623, 17.109001, 17.101488, 17.099392, 17.098358, 17.087914, 17.085268, 17.08393, 17.080627, 17.079865, 17.078876, 17.07623, 17.0708, 17.070797, 17.066399, 17.065434, 17.060213, 17.054281, 17.051018, 17.051012, 17.050314, 17.046522, 17.043337, 17.042957, 17.040545, 17.039598, 17.038559, 17.03628, 17.035192, 17.033962, 17.025846, 17.024357, 17.020344, 17.020058, 17.012568, 17.01222, 17.01138, 17.009317, 17.00655, 17.000175, 16.999586, 16.995312, 16.992865, 16.990902, 16.982988, 16.982883, 16.98207, 16.97833, 16.977709, 16.977695, 16.970272, 16.96925, 16.96669, 16.962315, 16.96154, 16.958693, 16.955542, 16.951773, 16.951345, 16.949894, 16.945894, 16.942202, 16.938595, 16.936958, 16.934008, 16.923586, 16.91364, 16.913057, 16.912054, 16.905407, 16.897297, 16.896742, 16.886906, 16.885834, 16.881546, 16.88033, 16.87255, 16.871077, 16.867455, 16.865051, 16.864588, 16.863169, 16.860598, 16.859793, 16.859354, 16.854673, 16.851597, 16.851494, 16.850271, 16.84932, 16.84928, 16.846504, 16.843761, 16.837347, 16.836473, 16.836454, 16.8318, 16.8318, 16.828487, 16.827675, 16.823744, 16.821362, 16.819378, 16.817879, 16.816252, 16.8158, 16.811245, 16.803946, 16.799192, 16.79815, 16.797924, 16.796757, 16.795246, 16.79349, 16.793045, 16.790937, 16.7888, 16.782919, 16.777973, 16.776749, 16.775837, 16.7747, 16.771069, 16.769, 16.767931, 16.766903, 16.765228, 16.763523, 16.76044, 16.757805, 16.75662, 16.752756, 16.745705, 16.742474, 16.739378, 16.733126, 16.730886, 16.729372, 16.728865, 16.725174, 16.721823, 16.712421, 16.711088, 16.7087, 16.701315, 16.701315, 16.70073, 16.69708, 16.692345, 16.690445, 16.686222, 16.682127, 16.681206, 16.681194, 16.672445, 16.668846, 16.665707, 16.665522, 16.662804, 16.66212, 16.658846, 16.658657, 16.657074, 16.654858, 16.651419, 16.650887, 16.650715, 16.648346, 16.642338, 16.635267, 16.632792, 16.632643, 16.630424, 16.630268, 16.630182, 16.620636, 16.615993, 16.603851, 16.59897, 16.59878, 16.598675, 16.597502, 16.596476, 16.59501, 16.588982, 16.587275, 16.578829, 16.573551, 16.568289, 16.556879, 16.552525, 16.549822, 16.548794, 16.548443, 16.54831, 16.546022, 16.545485, 16.544281, 16.541372, 16.540745, 16.539509, 16.534101, 16.531872, 16.531673, 16.529644, 16.528347, 16.52737, 16.524118, 16.52321, 16.52021, 16.5196, 16.509632, 16.509632, 16.508917, 16.501152, 16.497707, 16.494545, 16.493656, 16.490099, 16.490025, 16.484653, 16.484509, 16.48397, 16.481344, 16.481255, 16.479908, 16.479616, 16.478346, 16.478056, 16.476294, 16.475712, 16.47566, 16.472229, 16.465605, 16.461548, 16.458813, 16.455336, 16.454844, 16.44648, 16.445116, 16.44388, 16.435991, 16.433857, 16.432531, 16.432234, 16.43223, 16.431627, 16.424843, 16.424528, 16.424292, 16.423235, 16.423187, 16.421509, 16.418701, 16.418564, 16.414295, 16.408054, 16.407742, 16.407358, 16.404285, 16.404037, 16.403648, 16.396729, 16.396145, 16.394283, 16.394018, 16.387697, 16.387695]}, {'id': '2342504937', 'text': 's1 s2 heart sound recognition use deep neural network', 'timestamp': 1485903600, 'rel_doc_ids': ['2078182564', '2120578902', '2141125852', '2147768505', '2151459678', '2157362332', '2160815625', '2162871885', '2169863454', '2964138484'], 'user_id': '2003185464', 'user_doc_ids': ['29127967', '188247790', '824914942', '999108544', '1789716261', '1944486567', '1972971397', '1975678038', '1979005715', '1990227353', '2000208395', '2019849101', '2021355302', '2032763765', '2040393993', '2061102751', '2064114555', '2066674745', '2071979989', '2103537425', '2105859030', '2131708681', '2133628825', '2136389896', '2140498480', '2157178676', '2163314930', '2165606141', '2171892193', '2290463584', '2290548146', '2509191042', '2517871893', '2577762507', '2951298482'], 'bm25_doc_ids': ['2151459678', '1896725397', '1966425029', '2153977700', '2152075801', '2019092077', '2107804602', '2169863454', '2121317749', '2160423852', '1619542984', '2153892802', '2103138788', '1016908228', '2132818006', '2077849306', '2120578902', '1983259637', '2040534868', '2109226112', '2157362332', '2101578784', '1967124262', '1974311359', '2137837503', '2075871672', '1972136701', '2073186891', '2081741488', '1971124640', '1968963366', '1988170188', '2182655205', '2020459286', '2270863009', '2060776545', '2026825387', '1983429352', '2538421162', '2013285109', '1526352122', '1555351265', '2039031643', '987751263', '2105644147', '2186477848', '2015619763', '2120239567', '1720394799', '2118060266', '1564203686', '2135718029', '1501556024', '2011934007', '2018728702', '2100598791', '1043132331', '1846490855', '1643655047', '2139490732', '2096704829', '2100866664', '2135310258', '2103456541', '2141169077', '1529889044', '2081444196', '2538507784', '2146837818', '2098886866', '1494597521', '2083108658', '2118743484', '2163518096', '2592703949', '2560261599', '1972961597', '2048772455', '2012432505', '2093347662', '2133526867', '2143957077', '2070104915', '2018922145', '2565098363', '2148048556', '1967807695', '1914774316', '2602488123', '2962179642', '2048310347', '2117863813', '64453499', '1518580288', '2163735612', '2100760766', '2114032779', '2090582869', '2098792478', '2117317412', '2001887294', '2117703162', '1846877753', '2103272816', '2001008140', '2060423228', '2097047194', '2020416425', '1987667154', '2137305396', '2107871699', '1498986326', '2021568760', '230166663', '2015987608', '2116598909', '2098654070', '2100776646', '2181981869', '2245419040', '2116479736', '2262687164', '2119523314', '1871550040', '2127684971', '2147544525', '2055428579', '2062335831', '2063477132', '1546715552', '966387185', '2204257188', '2079879762', '2564409914', '2003504795', '1550898109', '2128960666', '2078978121', '2140243869', '1607094935', '1766364400', '2410198665', '38028160', '2237132896', '2088212562', '1992280021', '2085561779', '2518911984', '1986742940', '2153141680', '2058068298', '1992771452', '1968794568', '2053131724', '2283771534', '2115165535', '2068231251', '2032213187', '1978491045', '2108507835', '33086287', '1970578576', '2096805973', '2090254930', '2101408147', '1967761919', '1592632480', '1721790504', '1988116054', '2593628220', '1482635660', '2097527558', '2160469075', '2533472045', '2148044755', '2122755543', '2070500106', '2521151855', '1570166875', '2102270414', '1505349540', '2166735184', '1965632003', '2132107743', '2089717010', '1992764310', '1992501719', '2006429146', '2007827294', '1975341580', '2127221526', '2036215557', '1598078336', '2119676644', '2028071194', '2230778958', '2197655200', '1593601264', '2134398768', '1542582354', '2135904688', '2086245174', '2311171087', '153650570', '2225675473', '2255597455', '2245669691', '1530441164', '1595720442', '2161003000', '2277596441', '2324080794', '2020603948', '1888399526', '2025334110', '1985669584', '2121645722', '1978466541', '2056395346', '2593355309', '2107702957', '2043035898', '2101054703', '2080468603', '2004724890', '2226607756', '2062550971', '2085585093', '1983266285', '1970711085', '1577837712', '2012861880', '2849987209', '2010538413', '2408078766', '2248874986', '2229951263', '1996171519', '2011972938', '2100976095', '2395650275', '2094799385', '1991223650', '2162627165', '2406495768', '2018283134', '2754807606', '2411607660', '1967172681', '2287153031', '1490172744', '1996415715', '1677920914', '2075092154', '2247429987', '2063556310', '2281412845', '1996780636', '2403376452', '2106587369', '2151458352', '2399005452', '2112037633', '2044915686', '1947174919', '935316898', '2100362892', '2873307463', '1950413524', '2088055539', '2051228590', '2082567546', '1965608936', '2299749098', '2026216837', '996792822', '1552604842', '2402111493', '2153813871', '2241710749', '2277450413', '2293772498', '2265499064', '1590305098', '2016084084', '2136368408', '2046565007', '2153873029', '2163394193', '2246431452', '2221758221', '1846473900', '1984285914', '2055854155', '2031689671', '2234489854', '2964154335', '2081926061', '1487613358', '2150145411', '1966129946', '2197654019', '2772915133', '2184127593', '1970027940', '2010859375', '2075159077', '2064908288', '1544180249', '2153400455', '2135846088', '2243781292', '2274754480', '1650531274', '1484241027', '2274025633', '2030112528', '45718640', '2260782843', '2291039417', '2255185286', '2300952630', '1552070321', '1521578822', '34507699', '1830922390', '2100204068', '15107141', '2102893026', '2122729336', '2148097638', '1540800367', '2131444712', '1983023096', '2285457248', '2154942898', '2284092874', '2265896795', '2011507645', '2169196165', '1485076802', '2295969124', '196091184', '1938732208', '2215344237', '2091432990', '2065467095', '1579256332', '1482620422', '2258680072', '2110289957', '2126031340', '2100451234', '1588851226', '2309400744', '2549858052', '1586423662', '2143612262', '2950689855', '2395429867', '2071685617', '2100841556', '2400084465', '2853937755', '2858315462', '2010748414', '2076342906', '2226187701', '2245048890', '1832003755', '2062227835', '2417801248', '1954433395', '2951409066', '1984627835', '2487315868', '2275591776', '2024645158', '2130319253', '1908126967', '2595730045', '2210678035', '2143755314', '2060877580', '1486779040', '2036242736', '2149966495', '1487949759', '2272651104', '2020297110', '2022618065', '1972351970', '2057020885', '2043369152', '2168708402', '2153264442', '2163780361', '2197434028', '952146448', '2035382871', '2167595278', '2077760499', '2295001676', '2026068747', '2097085008', '2058465059', '2552548417', '2408085038', '2003656881', '2187910885', '1998935054', '1979044330', '1526570607', '2047075732', '2529731305', '2056804968', '2022075901', '2578895956', '2231761678', '2214662884', '2258968594', '1986998019', '2000746241', '2952881492', '2131463865', '2833728555', '2113287647', '2136376748', '2295038166', '2008290849', '2060276240', '2398456099', '2494054254', '2510318047', '1555836174', '2012393389', '2270191568', '2338371038', '1936088391', '2092005146', '102376086', '2240579041', '2020593025', '2140609507', '2398268972', '2000743795', '2281407413', '2606977969', '2021191757', '2162451429', '2247879981', '2268046156', '1576171112', '1980884013', '2208899051', '2039873158', '2268105918', '2509687339', '2263468211', '2219993525', '2516800609', '2461887064', '2098385165', '2005294410', '1545769415', '2074874711', '1503143549', '2299536705', '2051546407', '2080157384', '2336441671', '2548765505', '1560688452', '2565320747', '2157623822', '2295647538', '1985830520', '2169642034', '1754018813', '2019774212', '2185531897', '1664573881', '2066289498', '1592840069', '2244488203', '2128695056', '1970167186', '1975508873', '2218277436', '2093411480', '2232597631', '2241794575', '2394879651', '1137351880', '2418115569', '2255250843', '2398187210', '2147768505', '2203782997', '2124966090', '2073247209', '2162600406', '2240323527', '1915974016', '2310300023', '2091465825', '2215107754', '2252970336', '2343954084', '2155419928', '2759796763', '2182736798', '2167354835', '1989015429', '2111159217', '2270470215', '2535110112', '9383254', '2497862917', '2119615570', '2010362084', '131053137', '2060798312', '70846133', '2090189013', '2587541464', '2147426746', '2066974899', '2243340073', '2130017568', '1600296966', '1580467924', '2590007073', '2217300399', '2059842894', '2110756895', '2130725446', '1831449718', '2004807322', '1825379080', '2090599418', '2199909742', '2287576392', '2078182564', '1556090950', '817784946', '2012747830', '1680651345', '580138839', '2030093664', '2120661434', '1553737344', '106060234', '1528681952', '2336917403', '1900346870', '2070865170', '2749604329', '2953311167', '2098950531', '2084220915', '2127898131', '2485394496', '2409373186', '984220726', '1604530095', '1537895721', '1989674192', '2396112312', '1909308924', '118756463', '993927765', '2012036483', '2030326280', '2243759483', '2024806294', '2286690381', '2963124866', '2119845115', '1546296671', '2395342389', '2285513271', '1153836417', '1605005685', '2160815625', '2047578678', '2409301420', '2470368200', '2012023583', '2087302085', '2205841239', '2214171448', '2003031643', '2162871885', '2040820346', '1169995941', '2743516192', '2024972352', '761446652', '2158436904', '56633652', '2407005679', '2023864795', '2396153645', '1922396537', '1539648621', '2229293301', '2985440675', '1549002625', '2280370717', '2092478502', '1923186697', '2950787732', '2101194023', '2200826378', '2105877594', '2056500480', '2185784447', '1561005717', '1530536511', '1508249674', '2261357609', '177705500', '2201600774', '2005708641', '2267422659', '2527649504', '2409749019', '2120459411', '2841647356', '2109184439', '2412287113', '2216985323', '2949150497', '2068730032', '1586203320', '2065176654', '1580277690', '2267053162', '2172147506', '2486513901', '1531609856', '1546804653', '2029010243', '1879833848', '2515753980', '1974038726', '1986876383', '1899604821', '303827550', '2406754140', '2144998108', '2138766934', '2075239360', '22042202', '1602061814', '2760018985', '1978426462', '2293983223', '2031257186', '2513602745', '2341393370', '2200374509', '2297160594', '2052750032', '1541298038', '2403149086', '1947690801', '2145044818', '2952563226', '2269938945', '2301815424', '1986269045', '1852941930', '1985368711', '2171577818', '2012557818', '2434634753', '2409534643', '2040429039', '2257838294', '2022161886', '2134797427', '1950317107', '2518582440', '1500281967', '201491010', '2118412326', '2074133775', '2399391331', '1510782664', '1595452285', '941172790', '2020676607', '2593792475', '2053067333', '1485542918', '2189388227', '2094349762', '2404638268', '2248239756', '1526883134', '2587586954', '2080065205', '2225533059', '2214610255', '900447646', '2093376526', '1991132496', '987338811', '2274166687', '2321388452', '1633327381', '1749949492', '161575261', '2290689761', '2417576752', '2046288537', '2167417928', '1747901909', '2109830051', '2147627672', '2093398582', '2394853805', '2281534628', '2267929801', '2144494088', '2463931728', '2004227461', '1487563966', '2028236303', '2583832915', '2417429787', '2411551825', '2339196570', '2188183693', '2430776200', '2264786899', '2403640927', '2080256878', '2435792570', '2397503568', '1616193705', '2219856611', '2147259799', '2131413431', '2073354302', '2245610257', '1507650770', '124924985', '2081835714', '2039057510', '2327301358', '2244142460', '2014867186', '1509000280', '2108069432', '2157143200', '2233294124', '2402315470', '2129134312', '1983264381', '2248949360', '1503485962', '2293167034', '2004174728', '2295505399', '2027812972', '1489125746', '1987327272', '2226440574', '2539182092', '1978202793', '1541372298', '2005405483', '2951128674', '2141125852', '2399948372', '2039361150', '2025990819', '2039285212', '2085221112', '2237637348', '2416008246', '2101079765', '1510368107', '2233814606', '1525638028', '2034940213', '277238768', '2046559208', '2556583623', '2093349546', '2041168146', '2279655419', '200283047', '2067395682', '2217895792', '2848479356', '2163969215', '2182297852', '2117840857', '2469094744', '2050305092', '2293634267', '2063728276', '1521415669', '2099767163', '2554166257', '2112021726', '1987937363', '2964325005', '2078169166', '2403040875', '2526416132', '2950051476', '2198724430', '2095168618', '2624590853', '2301202996', '128188896', '2082689158', '2035572295', '2567271240', '2013637713', '148612711', '1965318644', '2128557618', '2274166077', '2243947367', '2171486626', '2061436640', '2010291496', '2015208261', '185613617', '2272300165', '2395106899', '2550815077', '143782467', '2231225567', '1966838590', '2468059673', '1563686443', '1968274390', '1645800954', '2610735667', '2523894284', '2166951028', '1851344621', '2576530755', '2067476698', '2313320147', '2166112236', '2018074404', '2131409844', '2403112731', '2140040079', '2203517258', '189277179', '2167426485', '2012257340', '2290318471', '1497788995', '2061089700', '2531634031', '1562713453', '2051538545', '1499864241', '2088335308', '2534720278', '1479651931', '2522391316', '2000008364', '2165742236', '1535164275', '2524108254', '2243850034', '2406252158', '2952293012', '2407975742', '2076063813', '2296516251', '2398970810', '2963399116', '2474280151', '2192598490', '1993232824', '2076462394', '117865760', '2508220198', '2213952365', '2964138484', '2605307020', '2074772891', '2213270912', '2125524209', '1708979102', '2552324112', '2301739099', '2118018801', '2147841462', '1969851134', '2045571243', '2476501661', '2131860993', '2266103469', '2125635584', '2964239709', '2123237149', '2102116870', '2846720090', '1565338937', '2243738093', '2108501278', '2066629230', '89420957', '1619827102', '298999614', '2013305858', '2163005195', '1268588198', '2520176975', '1985897360', '1990105342', '2554238388', '1794977688', '2197258718', '2949113568', '2036635146', '2005000621', '1482455593', '2404848301', '2115730999', '2547018007', '2026926658', '1014965607', '1966335173', '2125496931', '2255669500', '2083128979', '2294427751', '1963855343', '2161965844', '2278322034', '2239560617', '1967566545', '2282648323', '2587717635', '1580985452', '2411429307', '2417019464', '2004074725', '1964143424', '1911362181', '2077541282', '1423376249', '28988658', '1723811852', '2059645646', '2405100751', '2230480907', '1594355793', '2063045111', '1587574217', '2180711486', '2347129260', '2551429935', '2147270499', '2000925955', '1911091339', '2830470621', '2208802265', '2952587893', '2307035320', '1942035323', '2403231578', '1490279645', '2413964082', '2096653978', '1946606198', '2035424729', '2000200144', '2013305145', '2290132443', '2148159723', '2181741066', '2143539599', '2040738616', '2292414329', '2293437127', '73090681', '2066858243', '1571745681', '2586195663', '1596237903', '2247857030', '2235224676', '2618172703', '2075925017', '2535731875'], 'bm25_doc_scores': [45.252743, 43.50666, 41.642082, 41.277443, 40.764053, 40.75379, 40.34606, 39.989723, 39.74613, 39.625416, 39.416553, 39.355026, 39.177536, 39.11129, 39.07305, 38.99705, 38.963596, 38.849857, 38.54348, 38.47977, 38.452217, 38.209522, 37.931084, 37.662853, 37.293903, 37.168003, 37.044888, 36.881626, 36.599823, 36.49556, 36.354008, 36.26932, 35.975304, 35.85839, 35.47033, 35.444607, 35.3646, 35.00784, 34.933174, 34.80197, 34.57438, 34.57438, 34.476833, 34.435165, 34.21249, 34.135914, 33.857487, 33.567253, 33.471615, 33.346275, 33.254215, 33.175377, 32.94733, 32.94204, 32.887188, 32.65609, 32.459324, 32.37051, 32.267494, 32.26026, 32.096127, 31.98634, 31.96385, 31.869154, 31.742489, 31.710236, 31.631865, 31.483978, 31.445766, 31.261951, 31.094877, 31.06785, 31.052582, 31.016241, 30.923584, 30.864897, 30.79527, 30.526941, 30.486898, 30.456215, 29.928482, 29.848692, 29.84407, 29.75741, 29.60704, 29.592878, 29.580244, 29.443388, 29.321476, 29.295958, 29.10533, 29.055176, 29.044971, 28.986897, 28.963022, 28.960852, 28.901522, 28.871368, 28.857803, 28.801237, 28.78748, 28.743109, 28.729801, 28.690622, 28.641842, 28.632095, 28.548035, 28.53345, 28.361984, 28.353987, 28.075335, 28.034042, 27.919569, 27.838203, 27.819513, 27.812725, 27.66828, 27.605608, 27.586967, 27.576855, 27.566961, 27.542507, 27.46468, 27.461992, 27.276176, 27.249199, 27.227945, 27.221336, 27.168999, 27.157206, 27.153608, 27.067081, 26.966883, 26.963942, 26.879025, 26.860588, 26.71321, 26.62975, 26.574888, 26.453156, 26.416473, 26.406794, 26.333942, 26.328732, 26.32845, 26.310017, 26.30568, 26.292692, 26.289104, 26.27255, 26.252954, 26.221283, 26.211187, 26.19926, 26.178537, 26.134375, 26.117292, 26.115273, 26.09387, 26.071333, 26.042032, 25.992628, 25.990047, 25.986647, 25.973604, 25.923721, 25.914253, 25.89369, 25.85403, 25.787085, 25.768587, 25.739325, 25.738098, 25.73744, 25.714714, 25.688744, 25.684814, 25.677284, 25.67432, 25.669563, 25.64139, 25.557125, 25.552937, 25.550274, 25.483463, 25.469908, 25.466394, 25.434814, 25.425604, 25.40391, 25.394331, 25.391987, 25.370106, 25.35947, 25.284782, 25.257534, 25.246542, 25.184423, 25.182133, 25.172447, 25.167728, 25.15278, 25.133703, 25.128271, 25.113165, 25.1071, 25.04498, 25.01114, 25.00208, 25.000431, 24.997244, 24.961533, 24.957012, 24.957008, 24.95237, 24.896015, 24.894331, 24.886759, 24.869434, 24.864437, 24.791475, 24.774635, 24.74744, 24.74528, 24.739239, 24.738773, 24.736366, 24.707434, 24.67951, 24.65213, 24.639599, 24.638706, 24.628948, 24.613867, 24.602371, 24.599913, 24.572506, 24.572, 24.529457, 24.512363, 24.505629, 24.4757, 24.474741, 24.442663, 24.391212, 24.389332, 24.383358, 24.36091, 24.359463, 24.322912, 24.314669, 24.303644, 24.302496, 24.298784, 24.285912, 24.28321, 24.27977, 24.27637, 24.273422, 24.263601, 24.263435, 24.246794, 24.245193, 24.22298, 24.202852, 24.171305, 24.170235, 24.16576, 24.058304, 24.058304, 24.057241, 24.051609, 24.049223, 24.043783, 24.02459, 24.012432, 23.992695, 23.974419, 23.964577, 23.947405, 23.940474, 23.93659, 23.924116, 23.88181, 23.862831, 23.858595, 23.854652, 23.84958, 23.847002, 23.821205, 23.807941, 23.805664, 23.80197, 23.800652, 23.794504, 23.78024, 23.77262, 23.770407, 23.715881, 23.715881, 23.706366, 23.691566, 23.691566, 23.69086, 23.68937, 23.687645, 23.687645, 23.682379, 23.666807, 23.602085, 23.583681, 23.563765, 23.560774, 23.541355, 23.534365, 23.518505, 23.513847, 23.513847, 23.501305, 23.499388, 23.499388, 23.499388, 23.495983, 23.484259, 23.48293, 23.4797, 23.478619, 23.46254, 23.390182, 23.357971, 23.35286, 23.342745, 23.292337, 23.289242, 23.286983, 23.279942, 23.264448, 23.255257, 23.234243, 23.231781, 23.22904, 23.212698, 23.194756, 23.186497, 23.182644, 23.175512, 23.172218, 23.162727, 23.156725, 23.137836, 23.121086, 23.111372, 23.101917, 23.09571, 23.067764, 23.06647, 23.044935, 23.03391, 23.03391, 23.025845, 23.025845, 23.025003, 23.017813, 23.00774, 22.989002, 22.972988, 22.89217, 22.89217, 22.89057, 22.876442, 22.87452, 22.857656, 22.8559, 22.8559, 22.834276, 22.81545, 22.812706, 22.80307, 22.800013, 22.797754, 22.784435, 22.780258, 22.778328, 22.777462, 22.76627, 22.753916, 22.75122, 22.75122, 22.75122, 22.749023, 22.719341, 22.718433, 22.7145, 22.707708, 22.706013, 22.704018, 22.702944, 22.69932, 22.634203, 22.634203, 22.63011, 22.62849, 22.618252, 22.609783, 22.600853, 22.583652, 22.579767, 22.57877, 22.542917, 22.540384, 22.52527, 22.520847, 22.520847, 22.520847, 22.516373, 22.507387, 22.50646, 22.495813, 22.49412, 22.49412, 22.49412, 22.49412, 22.475817, 22.475376, 22.472979, 22.469452, 22.4657, 22.463812, 22.461243, 22.45707, 22.449112, 22.446167, 22.43436, 22.431662, 22.408743, 22.407295, 22.403955, 22.403708, 22.392786, 22.374722, 22.374722, 22.374722, 22.367977, 22.367006, 22.365004, 22.353357, 22.348257, 22.348034, 22.347, 22.339504, 22.323574, 22.321804, 22.302517, 22.302517, 22.302517, 22.302517, 22.299547, 22.299547, 22.299547, 22.296236, 22.266932, 22.26621, 22.261414, 22.260695, 22.237923, 22.237362, 22.230143, 22.229683, 22.222046, 22.22135, 22.210844, 22.210037, 22.189116, 22.188251, 22.185472, 22.161007, 22.159561, 22.131163, 22.126272, 22.12608, 22.124794, 22.123186, 22.12126, 22.12126, 22.11266, 22.111168, 22.111168, 22.100166, 22.091772, 22.091084, 22.07691, 22.062542, 22.05786, 22.055965, 22.04788, 22.046778, 22.036203, 22.034824, 22.015886, 22.014305, 22.007385, 22.000626, 21.993847, 21.992565, 21.991854, 21.986399, 21.986399, 21.986399, 21.95688, 21.956547, 21.952833, 21.952423, 21.938414, 21.926006, 21.923403, 21.915926, 21.912893, 21.912596, 21.91242, 21.911419, 21.904951, 21.900188, 21.879698, 21.878222, 21.873688, 21.86351, 21.859583, 21.859303, 21.858744, 21.855846, 21.8551, 21.854427, 21.822613, 21.822613, 21.81229, 21.807182, 21.784801, 21.768963, 21.758148, 21.741035, 21.741035, 21.741035, 21.741035, 21.740913, 21.738518, 21.737787, 21.735998, 21.729118, 21.72335, 21.7206, 21.72015, 21.716702, 21.716015, 21.711184, 21.708822, 21.70752, 21.692766, 21.688147, 21.679634, 21.679634, 21.665161, 21.658413, 21.657137, 21.655048, 21.65392, 21.647722, 21.634274, 21.634039, 21.631542, 21.629581, 21.615936, 21.610113, 21.60247, 21.599297, 21.596945, 21.596945, 21.594131, 21.594131, 21.59183, 21.59183, 21.585135, 21.579954, 21.577581, 21.576797, 21.56904, 21.567211, 21.567211, 21.565228, 21.563784, 21.563784, 21.563784, 21.561998, 21.561998, 21.552324, 21.547289, 21.543327, 21.524683, 21.519455, 21.515285, 21.513971, 21.513105, 21.502312, 21.50109, 21.48903, 21.48903, 21.48867, 21.483196, 21.481636, 21.475687, 21.474712, 21.468655, 21.45371, 21.45371, 21.446798, 21.446045, 21.446045, 21.42047, 21.416073, 21.410002, 21.409092, 21.403732, 21.401447, 21.394886, 21.394886, 21.390985, 21.380302, 21.368374, 21.368374, 21.367085, 21.36449, 21.36094, 21.35788, 21.35275, 21.34936, 21.34936, 21.342613, 21.334719, 21.319878, 21.31951, 21.314632, 21.312222, 21.292671, 21.292671, 21.287882, 21.282516, 21.27929, 21.278534, 21.275028, 21.27323, 21.266382, 21.266382, 21.266382, 21.240002, 21.240002, 21.240002, 21.234058, 21.234058, 21.228584, 21.224197, 21.22269, 21.21723, 21.215666, 21.210178, 21.205757, 21.191853, 21.187836, 21.174368, 21.171947, 21.171532, 21.166523, 21.166523, 21.16129, 21.157536, 21.137133, 21.134857, 21.132206, 21.124983, 21.12178, 21.119785, 21.11955, 21.11755, 21.109709, 21.101389, 21.09163, 21.091475, 21.08353, 21.08196, 21.07506, 21.06866, 21.06832, 21.067337, 21.05907, 21.058651, 21.057095, 21.050808, 21.0486, 21.046604, 21.04609, 21.044601, 21.044601, 21.042038, 21.040724, 21.039406, 21.036745, 21.036745, 21.034042, 21.033735, 21.032524, 21.031559, 21.018068, 21.004356, 21.003223, 20.997637, 20.997637, 20.994825, 20.992353, 20.990976, 20.990276, 20.972933, 20.964369, 20.959162, 20.959162, 20.957615, 20.949224, 20.944351, 20.944159, 20.943739, 20.941025, 20.938648, 20.937382, 20.935406, 20.9172, 20.915316, 20.914928, 20.909044, 20.908934, 20.902805, 20.901567, 20.901567, 20.901567, 20.898539, 20.898016, 20.894373, 20.88932, 20.888466, 20.881569, 20.87181, 20.860008, 20.859993, 20.859785, 20.859747, 20.85921, 20.848186, 20.83726, 20.826462, 20.82169, 20.814854, 20.812157, 20.812012, 20.812012, 20.812012, 20.812012, 20.812012, 20.812012, 20.801329, 20.79701, 20.789255, 20.78162, 20.769194, 20.76701, 20.759867, 20.758984, 20.75654, 20.755278, 20.75127, 20.748037, 20.748037, 20.744987, 20.74293, 20.741514, 20.739592, 20.737457, 20.737457, 20.737457, 20.731264, 20.724209, 20.72057, 20.718023, 20.715872, 20.70799, 20.707924, 20.70038, 20.698822, 20.698822, 20.696827, 20.696146, 20.694603, 20.679651, 20.678265, 20.670881, 20.658918, 20.658886, 20.653957, 20.643822, 20.640608, 20.629515, 20.626652, 20.626398, 20.624687, 20.6237, 20.613222, 20.613222, 20.601606, 20.590899, 20.586288, 20.576641, 20.576641, 20.575994, 20.575924, 20.575924, 20.573307, 20.568073, 20.567158, 20.563072, 20.562483, 20.547241, 20.539497, 20.539047, 20.535185, 20.535185, 20.532383, 20.532368, 20.531015, 20.524305, 20.523758, 20.520521, 20.518501, 20.517893, 20.51683, 20.511225, 20.510881, 20.50893, 20.488255, 20.48783, 20.485662, 20.4802, 20.479982, 20.477682, 20.473715, 20.464373, 20.459911, 20.458458, 20.457945, 20.457945, 20.457132, 20.454247, 20.454247, 20.454247, 20.434732, 20.434284, 20.431875, 20.429743, 20.416904, 20.416904, 20.4134, 20.402075, 20.40167, 20.397112, 20.395489, 20.39183, 20.390093, 20.387606, 20.376652, 20.376652, 20.376652, 20.374424, 20.374117, 20.374117, 20.367453, 20.367453, 20.366856, 20.364342, 20.362564, 20.361908, 20.351889, 20.351274, 20.348053, 20.34707, 20.342966, 20.341244, 20.341244, 20.341244, 20.339489, 20.332941, 20.33233, 20.330322, 20.321777, 20.31451, 20.31366, 20.309536, 20.304798, 20.298395, 20.290346, 20.286976, 20.286497, 20.286497, 20.284311, 20.284, 20.277092, 20.26705, 20.265764, 20.260345, 20.258965, 20.25894, 20.258596, 20.25821, 20.253784, 20.248411, 20.24721, 20.245619, 20.23823, 20.238045, 20.236979, 20.234627, 20.229023, 20.228764, 20.227953, 20.225098, 20.218582, 20.215555, 20.215555, 20.214556, 20.205938, 20.204134, 20.19946, 20.198566, 20.198566, 20.197956, 20.197433, 20.197037, 20.187197, 20.168705, 20.166183, 20.16573, 20.16573, 20.16573, 20.16573, 20.16156, 20.160437, 20.150663, 20.14675, 20.144732, 20.144732, 20.144623, 20.13362, 20.13362, 20.121353, 20.107449, 20.106184, 20.106184, 20.106184, 20.106184, 20.106184, 20.106184, 20.103203, 20.090235, 20.089964, 20.08787, 20.085386, 20.081768, 20.081205, 20.077244, 20.07289, 20.07289, 20.072653, 20.059443, 20.059443, 20.059443, 20.057365, 20.056816, 20.05444, 20.05428, 20.051441, 20.048378, 20.041897, 20.038359, 20.038097, 20.038038, 20.037884, 20.029554, 20.029554, 20.028873, 20.028742, 20.025984, 20.025915, 20.025766, 20.021065, 20.020117, 20.013424, 20.011717]}, {'id': '2344138609', 'text': 'couple deep autoencoder single image super resolution', 'timestamp': 1483225200, 'rel_doc_ids': ['2010070459', '2017832276', '2025768430', '2047920195', '2049237558', '2087436818', '2088254198', '2097200430', '2098506229', '2111634407', '2118963448', '2120824855', '2129696493', '2148312679', '2149669120', '2160558632', '2161516371', '2164551808', '2167191464', '2293264500', '54257720'], 'user_id': '2104129307', 'user_doc_ids': ['9976222', '24269932', '50236904', '86042615', '87994792', '92091516', '95072457', '119906723', '141623697', '143393982', '195410969', '197893777', '219545488', '236254921', '242877468', '243961370', '281205060', '379823112', '584359966', '612386257', '774383936', '1193984273', '1494198676', '1500842814', '1502638562', '1506543833', '1514406729', '1514928307', '1517525610', '1551199372', '1560790359', '1566386155', '1571658558', '1572368544', '1574780945', '1575889909', '1581766538', '1589734464', '1589737505', '1599412172', '1600534621', '1609306619', '1628595345', '1670132599', '1672347394', '1672966589', '1675155851', '1693214838', '1757873053', '1790336448', '1794760533', '1818395637', '1836353685', '1849710887', '1850888726', '1861074110', '1880193759', '1886901842', '1894767581', '1897029691', '1901075642', '1903001680', '1915599933', '1925417509', '1937059634', '1944092705', '1948657956', '1951319388', '1964860887', '1965793029', '1966999409', '1968952662', '1969934308', '1970779216', '1972084483', '1972702299', '1972908405', '1973651542', '1973711324', '1974573780', '1974587077', '1977355761', '1977572230', '1977814411', '1977888156', '1978163914', '1979981096', '1980069326', '1981902088', '1982554717', '1982849076', '1983474530', '1983681808', '1984907405', '1985133440', '1985243543', '1985436611', '1985572608', '1985583020', '1986738057', '1987286789', '1987559927', '1987670974', '1987890534', '1988170517', '1988378196', '1988542814', '1988652596', '1988719066', '1988870180', '1991598519', '1992051107', '1992127150', '1992229061', '1992408872', '1992942689', '1993589664', '1995390334', '1996449957', '1998528166', '1999478721', '2000144367', '2001045763', '2002155132', '2002849025', '2003684104', '2003731944', '2003941100', '2004080111', '2004331293', '2004362630', '2005106632', '2005588269', '2006613284', '2006815170', '2006904655', '2009363625', '2009672203', '2009732942', '2009939901', '2010037226', '2010262905', '2010430323', '2011295372', '2011494630', '2011952414', '2012260009', '2012422921', '2012619263', '2012964815', '2012989253', '2013916543', '2014764728', '2015725301', '2017453188', '2017865634', '2018510373', '2019185225', '2019338222', '2021003221', '2021029591', '2021874460', '2023714745', '2024082504', '2024387934', '2024712482', '2026188082', '2026255638', '2028312814', '2028338040', '2028433924', '2029234388', '2029859592', '2031049553', '2031326491', '2031596661', '2032427587', '2032707239', '2032884192', '2033783862', '2035299679', '2036713601', '2037360998', '2038857347', '2040812261', '2041196946', '2042120685', '2042289698', '2042576467', '2042929821', '2042943498', '2045610202', '2046708745', '2046769852', '2048647542', '2049538695', '2050201048', '2050218035', '2051019429', '2051067740', '2051111292', '2051285356', '2051290159', '2052355654', '2052545764', '2052703616', '2054210502', '2054846461', '2055603434', '2055928229', '2056027358', '2057817757', '2058252138', '2058834037', '2059283460', '2059497048', '2059589342', '2059827653', '2061017849', '2061626571', '2062375191', '2062558125', '2062668526', '2063208467', '2063506423', '2065387086', '2067022487', '2067625321', '2068078373', '2069338601', '2070501200', '2070926294', '2071175064', '2072697225', '2073416270', '2075387502', '2076091393', '2080917896', '2080943899', '2081059587', '2081178133', '2084445849', '2086452927', '2086564093', '2091711928', '2091942479', '2092377492', '2093093856', '2093582128', '2094090780', '2094100186', '2094579227', '2094919853', '2096503181', '2096561263', '2097139832', '2097200430', '2097239983', '2097415537', '2097627380', '2097703723', '2097711399', '2097872774', '2098711149', '2099891036', '2100094703', '2100299422', '2102330047', '2102591820', '2102742858', '2103325943', '2103754534', '2103844245', '2104619158', '2104834807', '2105053450', '2105610271', '2105622852', '2105732058', '2105894645', '2106426828', '2106844779', '2107626950', '2108013467', '2108090678', '2108193616', '2110112487', '2110314108', '2110329341', '2110967761', '2111303182', '2111310758', '2112109082', '2112737239', '2113426222', '2113817964', '2114264417', '2116816735', '2117513046', '2118036996', '2118301633', '2119908741', '2119939615', '2120362657', '2120380470', '2120577785', '2120865781', '2121281940', '2121378474', '2121417558', '2121568632', '2121772968', '2123028890', '2123111020', '2123432324', '2123497041', '2123613719', '2124083668', '2124298430', '2124325819', '2125721850', '2126740276', '2126888709', '2127089855', '2127975252', '2128489591', '2128707556', '2129672212', '2129945466', '2130638881', '2130922351', '2131824731', '2132096534', '2132822263', '2132925565', '2133699849', '2134104904', '2134123547', '2135059100', '2136002239', '2136228842', '2136637876', '2136948085', '2137747622', '2138516811', '2138764541', '2139849636', '2140803156', '2141172981', '2141581971', '2141583337', '2142036616', '2142077116', '2142314651', '2142955877', '2143332281', '2144245116', '2144512280', '2144719328', '2145357920', '2147846978', '2147940013', '2148374979', '2149048621', '2149364056', '2150383810', '2150688176', '2151208853', '2151548973', '2151736844', '2152009710', '2152073437', '2152404931', '2152756885', '2152964853', '2153937399', '2154624311', '2156571432', '2156743847', '2157323954', '2158501888', '2158829898', '2159209388', '2159248397', '2159917172', '2160540105', '2160558632', '2161746326', '2161851558', '2162136823', '2162854380', '2163084005', '2163721823', '2164799551', '2165014883', '2165112272', '2165343651', '2166083072', '2166168249', '2166782149', '2167175671', '2167388003', '2168114633', '2169242504', '2169711633', '2170426384', '2170947705', '2171859866', '2188151942', '2189540548', '2196615847', '2200675780', '2202499615', '2210580011', '2213118230', '2219295639', '2219752407', '2220422437', '2221735899', '2223051473', '2234056538', '2242222096', '2243437193', '2243549423', '2246782854', '2247839338', '2247935935', '2248800885', '2256362396', '2273376582', '2274787793', '2278086079', '2290309368', '2292056273', '2294233550', '2294478261', '2296034168', '2300234500', '2318553756', '2319358180', '2327049400', '2327669213', '2336406062', '2341916285', '2342751813', '2343166405', '2343745409', '2345837149', '2359108789', '2399812666', '2400262625', '2401075073', '2401484561', '2415339614', '2428551659', '2436544366', '2444115989', '2463241543', '2463598282', '2469023256', '2473844757', '2479750863', '2485522583', '2497571620', '2507011716', '2507855991', '2510520237', '2513443267', '2520049838', '2522148122', '2526428212', '2527454481', '2527636115', '2532295275', '2549850705', '2554242204', '2559954935', '2563212024', '2566720494', '2571958202', '2574983961', '2577436636', '2583938035', '2949373531', '2950035127', '2950434771', '2950643904', '2950680045', '2951153209', '2951965309', '2952596593', '2953334077', '2963698912'], 'bm25_doc_ids': ['2527417498', '1546200464', '2405933695', '2503186844', '54257720', '62153270', '2050602175', '2523714292', '2290089146', '2243595562', '1885185971', '2949064199', '2494890720', '2345557152', '2056527989', '2512214556', '1920235975', '2006654193', '2509277247', '2354253112', '2951997238', '1919542679', '2042357879', '2138616927', '2476548250', '2949128343', '1970563617', '2569929202', '2414132572', '2437557374', '2507609509', '2148312679', '2088254198', '2583800510', '2530144925', '2952194607', '2137974653', '2239879258', '1587563279', '1999366827', '1595923625', '135113724', '2014771814', '2320725294', '2515699280', '2160144202', '2292254049', '2004882387', '2950560720', '2221899823', '2166537789', '2275893684', '1982251998', '2145094598', '2050198433', '1601729531', '1984575295', '1521543644', '2534320940', '1484539719', '2067625321', '947897035', '2196919019', '2248620004', '2189422931', '2064945773', '2593697158', '1519506695', '2073354302', '2012485971', '1991150826', '1550498963', '2335930829', '1975965915', '1995228944', '2148600052', '2214802144', '2949079773', '7682646', '2507582775', '2411032597', '2611338391', '2138011923', '2032194209', '2129696493', '2066941135', '138643349', '2133700636', '2018810158', '2174358748', '2039794255', '2157087088', '2085573253', '2174473997', '1950038212', '2036649333', '2079746973', '2134433631', '2005395179', '2144135123', '2210480155', '2482139312', '1558536312', '2118723395', '1873032297', '2219845087', '2153341028', '2099273082', '2094811265', '2276239023', '2147062276', '72279657', '2021306562', '2617585083', '2131686948', '2269675680', '2414249090', '2123431518', '2010430323', '2001353114', '2949422991', '2055929705', '2077061986', '2009691887', '1950594372', '2561766433', '2014677380', '1973536567', '2534859633', '2952558436', '2509938168', '2010400289', '2091670669', '1970185025', '2232647241', '2110633874', '2196707239', '1721184332', '2766947113', '2074039689', '2165870274', '1974616302', '2159284541', '2077756867', '1874498466', '2097116510', '2033021901', '2154466813', '2131965686', '2092712606', '2242218935', '1973681148', '1965560754', '2000978763', '2060076542', '2404704342', '2088813249', '2294676272', '1534795867', '2088777113', '2065133960', '2229689369', '2022198602', '1999129663', '2059650736', '1589189156', '2103751387', '2066858836', '2202027014', '1997695576', '2039539938', '2962869147', '2015204168', '2057065563', '2094907170', '2054681904', '1750803150', '2002121894', '2952077927', '2115895912', '1984662878', '2116268653', '1982892462', '2294780673', '2161958585', '2067108728', '2049237558', '2527569769', '2158657094', '2157466038', '2093008421', '1994204330', '1991192972', '1977581467', '2184016288', '1813620924', '2076959472', '1992102478', '2120824855', '2079624778', '2037712231', '2086453121', '2093726451', '2105250910', '2121137496', '22300684', '1929324367', '1999780244', '2410332788', '2105777420', '1481157510', '2081145135', '2087436818', '2083331495', '2261310161', '2140887227', '2473963253', '2163039579', '2756523879', '2028869821', '2512505748', '2084512488', '2158046142', '2132669346', '1594005383', '1971373099', '2103824707', '2303897497', '2062317708', '2295862745', '2154897856', '1559193917', '2076413147', '2102291794', '2290736026', '2111634407', '2024168163', '2063425286', '2534524192', '2339749455', '2161516371', '2160907592', '2020799659', '2053484197', '2135460496', '2154789478', '2049694261', '2160635556', '1989560527', '2025172455', '2046933782', '2028998721', '1999212085', '2100113359', '2282268007', '2211843587', '2110467405', '2118963448', '2952209150', '2110181061', '2145936614', '2526148220', '2098506229', '2096555493', '814272769', '2106602818', '1817990152', '2112666355', '2952838738', '1866230956', '2074958792', '2489082147', '2069597015', '2005316647', '2074112114', '2097307151', '2015408707', '806261972', '2331619054', '1991275931', '2290061803', '2408985603', '2163747519', '2038830612', '2424665364', '2322112093', '2093456296', '2178761095', '2130518641', '2011703074', '2106260930', '2963102887', '2295634579', '1856435895', '2550894969', '1996278890', '2096626549', '2070774492', '2261067923', '2107115785', '2129680231', '2076783291', '2518141884', '2964046669', '2140891521', '2146439580', '2143057788', '2099861032', '2136545230', '2137495321', '2142866567', '2123796344', '1913007689', '2163950289', '2130638881', '2168302623', '2012160771', '2513083919', '2112926985', '2022546853', '2503458650', '1968121410', '2078008442', '1136074057', '2207374674', '2169076226', '2056609315', '165060941', '2017832276', '2567961463', '1539200859', '2542055599', '1604391234', '2078954012', '2111249617', '2405940898', '2030480732', '2030428805', '2065972666', '2302522700', '2006008489', '2016213557', '2050874697', '1678246745', '2520164769', '2065842538', '2184360182', '2070849764', '2527114552', '2129445665', '2105728138', '2118097920', '2080142539', '2013380636', '2589086700', '2017070998', '2503339013', '2050117303', '2158688931', '1974973438', '2580080206', '1564496423', '1995390109', '2134149431', '935139217', '2005877525', '1562948138', '2142566218', '2183521529', '2093696856', '2120707422', '2949634581', '2300779272', '2118048031', '2151503710', '2244206261', '2065277358', '2552742471', '2566291216', '2138449156', '1607376735', '2042970394', '1956853614', '2093292578', '2518227085', '2018459209', '2200125188', '1989293853', '2025768430', '2097200430', '2061575734', '2169945149', '2163974684', '2149669120', '2075224820', '2041333007', '2963299038', '1968830188', '2126578859', '2053319420', '2522681120', '2014843408', '2124907686', '2179290474', '2520322935', '2112152806', '2167191464', '2191540403', '2028338040', '2319470980', '2161521745', '2194590514', '2054781681', '2103338344', '2254691124', '1897123318', '2145809219', '2140611763', '2026319643', '2273328137', '2169336016', '2298587072', '1584280808', '2098397292', '1637824528', '2182308895', '2017054849', '2167635673', '2301006124', '2160816406', '2398578479', '2249467874', '1984553492', '2547400848', '2108108630', '2371936065', '1575262777', '1981299881', '2319655619', '2046843072', '1561936935', '1590245106', '2132717427', '2219715551', '1970828469', '2674412558', '2544400020', '2013511761', '2030707839', '2065195327', '2537656575', '2103071014', '2302840847', '2035643522', '2069208282', '2148700636', '2037039580', '2079351309', '2165350898', '2085664274', '838143937', '1981599260', '2003921122', '2022163189', '2037234307', '2164551808', '1991329314', '2099496146', '2010070459', '2001393004', '2212985400', '2047920195', '2064451467', '2016250367', '2070977875', '2136995369', '13929034', '1969187791', '1934344900', '2041844466', '2530367476', '2084055865', '2022165014', '2103346247', '2045991273', '2148815921', '2950202780', '1594359125', '2510850936', '134193804', '2079609842', '2126653386', '2492018752', '1999264913', '2412425194', '2035651355', '2474173670', '2170603012', '2289807533', '1858693634', '2048023787', '2027591819', '2539092257', '2418353079', '1973794531', '2089017688', '2222661056', '2088656037', '2085684912', '1500576017', '1974618110', '2045715004', '2558415726', '2163836320', '2070800503', '2243067292', '2111454493', '2066678776', '2756304191', '1523716861', '2075252610', '2184011619', '306280504', '2120432039', '1987017523', '2110003257', '2075729794', '1979325788', '1982507297', '2243768923', '2492869694', '2285985042', '2097074225', '1972142352', '2094333790', '1994545243', '1979715606', '2025689805', '2135032339', '2160633441', '1883420340', '2952773148', '2037515702', '2067826505', '1630757601', '2018625345', '2114396656', '2121122889', '2062820291', '2820804594', '2030463878', '90610118', '1938071281', '2026847083', '784113155', '2196256193', '1904687420', '1982780932', '2963135265', '2399306074', '23240714', '1978970275', '1977536489', '2115252781', '2161686820', '2084818694', '1914830576', '2151619100', '2042245263', '1898468339', '2089967755', '2137577104', '2539309965', '2185811375', '1544483978', '2344552205', '1987225540', '1488380911', '2145896187', '2137163039', '2514526063', '2080602228', '2096120349', '1964073652', '1883107094', '2169019762', '2148498317', '2012448262', '2049747560', '18669060', '2148349004', '2952342326', '1531103298', '2548779750', '1520250264', '1516678082', '2569920930', '2123019155', '2157962884', '1492408785', '2111098411', '2537829830', '2112902973', '2056218566', '2110320844', '138368652', '2102226020', '2404208881', '2068699225', '2140120900', '2395103039', '2066370446', '2216949605', '1576462183', '2556013083', '2576452040', '2087110670', '2432643382', '603536130', '2107589634', '2269892441', '2004232299', '2064761415', '2052588627', '1993221954', '2120771670', '2043770409', '2086479975', '1989936475', '1550607661', '2071871719', '2050299492', '2134542730', '2056891298', '2015497428', '2305099079', '38463216', '1579118046', '2135184018', '2296458386', '2132844636', '150568077', '1979721912', '2306506499', '2014439379', '2111215091', '1964916631', '1965703556', '2076503096', '2359099468', '1973788353', '2256030505', '2050692499', '2069539014', '2065033168', '2019359269', '2008523386', '201101826', '1557533979', '2096607215', '1806901794', '2058880390', '2157665330', '2110152839', '43726425', '2144065255', '2116113317', '1591337819', '2107970510', '1522199561', '2081835714', '2111746730', '2056573726', '1964672368', '2088480274', '1969794797', '2088081944', '2071910939', '1487152357', '2108002231', '2047280020', '2229992274', '2002503554', '2534710061', '2162594098', '2013035813', '2140257560', '2121332115', '2336204166', '2280455352', '2072972848', '2299789843', '2148983690', '2104599718', '2475461338', '2061216685', '1518801581', '2098552096', '2056189002', '2126739786', '2271120545', '2238917930', '2100378669', '2087922977', '1582236847', '2167663112', '2016312399', '2076399550', '1508652512', '2075652274', '1525006309', '2104876002', '1517722522', '2060221529', '5940284', '2034419752', '1973397887', '1981748102', '1977509770', '2152404872', '2031662247', '1594839621', '2543616106', '2061901549', '387326119', '2428551659', '2117787067', '1919814523', '2264471314', '2396697831', '1190155712', '2121058967', '2028811652', '2134122233', '1756845810', '184699070', '1992365893', '2327081270', '23777879', '2179428711', '2397249253', '2963830389', '2296138801', '2047837012', '2469023256', '2007971802', '1973517736', '1724235419', '2160558632', '2149108531', '2539291380', '2310809832', '2137136552', '2031837063', '2214506707', '2157613008', '1992408872', '1981341515', '1805889745', '2121670710', '988885538', '2166262046', '1989510016', '2004351654', '2437838797', '1990416059', '2085463895', '1908796597', '2855858408', '2067042811', '1606818879', '1499777718', '2096960845', '1480262878', '2379948024', '1967212196', '1480648780', '2139140317', '2116633244', '1491911871', '1999821839', '2029316659', '2015044344', '1583463209', '1483871765', '2397476359', '2020570883', '2292560765', '2952857781', '2263468737', '2055225600', '2565015337', '2088282940', '2003104372', '1586298956', '1510555902', '2167980585', '2055945866', '2034564742', '1972901231', '2107358240', '2566133564', '2129613727', '2113598499', '2515143609', '2160065436', '2124438045', '2258669628', '2280154230', '1620381137', '1789333338', '1568325678', '1985777924', '1482227353', '2097594185', '2162596999', '2117998419', '2552964202', '2125704842', '2587586954', '2215765065', '2949311623', '2414857070', '125324442', '20660258', '2282115031', '2165939075', '1507417347', '2162106460', '1479651931', '2464755555', '1972955366', '1991716681', '1973060145', '2101044008', '2057609679', '1542039189', '2414985432', '2043824135', '2083835515', '2094684509', '2107591416', '2165783425', '2217217682', '2147047527', '1981860348', '2081020026', '2232901134', '2148040994', '2514777505', '2951570301', '2195231623', '2545025984', '2034265989', '1970425698', '2098239312', '2417463121', '2093500899', '2308288752', '2058452082', '1973375055', '2399812469', '2096399195', '2405056221', '2046361076', '2475285080', '2485714625', '1544785557', '2475020042', '2101146502', '2014424292', '2150509167', '2011149131', '1976387432', '2060643298', '1998605501', '2058990475', '2152521038', '2146238954', '2399409792', '2070174295', '2248382736', '2148450250', '2471801048', '2127306343', '1962110432', '2116295350', '1985473611', '2041982225', '2150626098', '1988974846', '2114380981', '2138743012', '2474601476', '588242912', '1536766384', '2047692471', '2025872532', '2293264500', '2145967412', '2283018310', '1970708550', '2155233049', '2039495012', '2107272455', '2137485840', '436865409', '2124154798', '2164044612', '2141373100', '2068047946', '2515131597', '1883346539', '2001513489', '2963634205', '2210483910', '2044728175', '2155365450', '2327669213', '2076923157', '2015127038', '2048882492', '1976542945', '2093070546', '2018747920', '1923186697', '2950787732', '2123591123', '2417306259', '2140875772', '2096683987', '3460322', '2152759637', '2028007684', '2155891363', '2111327023', '2128148283', '2131375801', '2140218857', '1989335386', '2170057902', '1485082902', '2170573007', '2145695289', '1596562677', '2013101727', '1616310794', '2031171456', '2102207191', '2275733078', '2039632702', '2262039552', '2070840516', '2155400421', '2220123745', '2027731851', '2311857205', '2021783758', '2120275945', '2042244106', '2107231419', '2540742548', '2098987432', '2096067495', '2525699706', '1503406846', '2027794313', '1916874600', '2137749994', '40940082', '2098995229', '2212470574', '1611569943', '2107399164', '2000166524', '2134080236', '2016267523', '2082823765', '2399155180', '2123613719', '1498842291', '2123821166', '1566494032', '1999666556', '2019770943', '2278054625', '2608989529', '2086503865', '1540664832', '2137290314', '3008944035', '1977783786', '2533287689', '2068183717', '2513385807', '1516074693', '2264837042', '2743034328', '1965085171', '2103717678'], 'bm25_doc_scores': [35.300793, 31.169071, 29.92206, 29.8992, 29.560362, 29.317644, 29.18898, 29.033213, 28.93797, 28.843973, 28.5874, 28.5874, 28.186926, 27.720268, 27.704674, 27.431524, 27.403221, 27.245792, 27.209808, 26.984488, 26.61795, 26.586098, 26.453144, 26.440014, 26.41922, 26.41922, 26.41259, 26.37899, 26.353207, 26.342884, 26.298088, 26.244942, 26.08954, 26.01001, 26.001045, 25.825727, 25.825727, 25.762007, 25.745192, 25.687832, 25.633986, 25.619524, 25.551495, 25.551495, 25.53049, 25.51449, 25.484434, 25.436924, 25.419888, 25.362274, 25.352043, 25.329176, 25.327637, 25.30519, 25.26307, 25.252132, 25.228552, 25.157957, 25.134747, 25.124496, 25.112453, 25.105429, 25.084217, 25.053617, 25.019503, 24.986734, 24.966156, 24.938423, 24.928074, 24.922155, 24.913183, 24.854053, 24.852062, 24.794985, 24.765944, 24.753132, 24.672348, 24.672348, 24.64725, 24.638187, 24.636648, 24.561277, 24.55002, 24.516476, 24.493057, 24.478176, 24.458773, 24.453833, 24.41866, 24.345789, 24.333275, 24.297806, 24.290268, 24.289845, 24.275074, 24.2591, 24.236124, 24.217165, 24.184635, 24.176012, 24.149841, 24.149117, 24.147009, 24.120852, 24.111591, 24.108553, 24.04911, 24.029806, 23.999624, 23.989676, 23.988512, 23.97829, 23.967978, 23.960007, 23.947203, 23.923378, 23.87936, 23.84012, 23.803516, 23.80298, 23.80298, 23.802734, 23.796188, 23.79407, 23.787287, 23.784105, 23.779905, 23.77873, 23.766552, 23.7622, 23.752853, 23.74787, 23.719772, 23.710669, 23.704985, 23.704487, 23.698116, 23.690418, 23.684881, 23.67021, 23.658276, 23.652016, 23.64129, 23.62521, 23.619644, 23.61173, 23.587812, 23.583323, 23.5565, 23.552876, 23.551733, 23.526577, 23.526104, 23.507343, 23.506046, 23.50336, 23.48186, 23.45821, 23.45821, 23.453209, 23.447952, 23.445808, 23.413727, 23.365044, 23.333193, 23.323727, 23.295973, 23.272526, 23.271763, 23.269215, 23.251085, 23.248287, 23.235048, 23.229683, 23.228062, 23.216871, 23.21619, 23.203531, 23.176928, 23.173471, 23.168041, 23.151037, 23.14362, 23.140919, 23.127447, 23.120052, 23.099876, 23.079529, 23.07229, 23.069607, 23.054409, 23.049028, 23.024088, 23.017128, 23.012333, 22.993153, 22.99303, 22.984898, 22.982676, 22.970268, 22.95849, 22.95694, 22.95644, 22.947535, 22.94597, 22.928177, 22.913261, 22.91177, 22.898851, 22.89291, 22.88684, 22.886694, 22.88479, 22.879824, 22.878918, 22.87648, 22.868484, 22.861141, 22.85876, 22.858276, 22.858162, 22.846003, 22.836817, 22.81583, 22.813745, 22.778528, 22.747192, 22.74513, 22.739288, 22.736376, 22.714481, 22.713583, 22.713278, 22.709671, 22.691498, 22.686375, 22.67565, 22.67547, 22.672894, 22.669662, 22.665762, 22.640596, 22.618313, 22.616285, 22.607578, 22.597857, 22.597038, 22.592426, 22.587675, 22.586975, 22.575735, 22.569777, 22.567698, 22.562124, 22.557005, 22.550928, 22.537996, 22.526047, 22.525852, 22.498472, 22.496767, 22.488575, 22.480595, 22.471405, 22.446608, 22.423855, 22.42124, 22.417665, 22.416277, 22.416277, 22.415577, 22.412094, 22.40887, 22.407314, 22.398432, 22.39203, 22.369812, 22.361797, 22.352192, 22.35215, 22.34689, 22.33996, 22.338114, 22.336391, 22.336182, 22.333584, 22.317783, 22.317402, 22.304626, 22.298136, 22.297813, 22.296322, 22.296322, 22.292849, 22.28301, 22.282562, 22.269825, 22.260859, 22.260414, 22.247969, 22.241547, 22.239962, 22.238165, 22.237099, 22.193768, 22.190035, 22.185097, 22.185078, 22.183022, 22.1706, 22.160784, 22.1575, 22.12825, 22.128054, 22.127499, 22.113808, 22.11008, 22.11008, 22.11008, 22.082268, 22.075554, 22.050608, 22.038792, 22.033154, 22.030283, 22.017893, 22.01401, 21.993435, 21.993435, 21.992485, 21.985912, 21.975708, 21.963753, 21.96139, 21.956337, 21.954603, 21.934584, 21.934584, 21.934584, 21.934391, 21.92974, 21.929485, 21.920189, 21.918602, 21.918224, 21.887259, 21.8755, 21.872486, 21.865633, 21.86193, 21.843267, 21.829111, 21.829111, 21.825851, 21.819332, 21.814606, 21.813519, 21.804913, 21.802855, 21.787766, 21.78614, 21.785294, 21.774387, 21.766869, 21.762367, 21.756, 21.748487, 21.744848, 21.742907, 21.734905, 21.734358, 21.723299, 21.723299, 21.723146, 21.720764, 21.711388, 21.705902, 21.7031, 21.695267, 21.683855, 21.68348, 21.68163, 21.67947, 21.677671, 21.675545, 21.670452, 21.665453, 21.665434, 21.662174, 21.656944, 21.654877, 21.65112, 21.65112, 21.643984, 21.643984, 21.643473, 21.609138, 21.608795, 21.599373, 21.58779, 21.582865, 21.578009, 21.57454, 21.571383, 21.56593, 21.564331, 21.561535, 21.559761, 21.5587, 21.557102, 21.550888, 21.548105, 21.544724, 21.540205, 21.520971, 21.517328, 21.504824, 21.49972, 21.498655, 21.488674, 21.480446, 21.46081, 21.458912, 21.455658, 21.442215, 21.433392, 21.41483, 21.396217, 21.390816, 21.386953, 21.386406, 21.366386, 21.364803, 21.362928, 21.362377, 21.360737, 21.359001, 21.331375, 21.329817, 21.329136, 21.32653, 21.324461, 21.322899, 21.31712, 21.304031, 21.30318, 21.303038, 21.275318, 21.27349, 21.263887, 21.255224, 21.255003, 21.251207, 21.24951, 21.246199, 21.242561, 21.239914, 21.232296, 21.22427, 21.223553, 21.220718, 21.207731, 21.203983, 21.200731, 21.19928, 21.197632, 21.197577, 21.195732, 21.18482, 21.180023, 21.177914, 21.165554, 21.161673, 21.15262, 21.150433, 21.149132, 21.146214, 21.145859, 21.145157, 21.144138, 21.143223, 21.142115, 21.134165, 21.133572, 21.131756, 21.120312, 21.11969, 21.11969, 21.116947, 21.11615, 21.11615, 21.114178, 21.113672, 21.11302, 21.112091, 21.111696, 21.108973, 21.107351, 21.105888, 21.10317, 21.101875, 21.099796, 21.084278, 21.079353, 21.07664, 21.070374, 21.069473, 21.061203, 21.047768, 21.031105, 21.02, 21.014723, 21.014454, 21.011517, 21.0086, 21.005272, 21.00249, 21.000044, 20.997025, 20.990572, 20.988686, 20.987072, 20.986183, 20.985533, 20.984436, 20.979374, 20.976213, 20.968153, 20.964048, 20.96296, 20.957817, 20.957464, 20.956287, 20.95374, 20.939318, 20.938797, 20.935965, 20.933025, 20.932995, 20.927454, 20.925528, 20.915432, 20.91385, 20.912292, 20.911087, 20.909792, 20.900269, 20.900225, 20.892971, 20.879738, 20.877777, 20.872036, 20.871359, 20.86655, 20.86068, 20.85934, 20.855911, 20.855686, 20.855436, 20.855436, 20.83574, 20.821836, 20.815649, 20.813463, 20.80224, 20.799255, 20.798996, 20.797485, 20.793783, 20.793636, 20.790617, 20.790144, 20.789532, 20.789387, 20.788967, 20.785757, 20.782967, 20.781542, 20.775227, 20.772797, 20.771479, 20.770245, 20.765932, 20.758556, 20.748562, 20.74464, 20.74453, 20.74453, 20.733679, 20.732765, 20.730448, 20.730341, 20.730341, 20.728289, 20.722776, 20.722717, 20.72115, 20.705618, 20.703808, 20.703459, 20.700964, 20.700964, 20.684418, 20.684233, 20.682253, 20.675383, 20.662014, 20.657211, 20.657198, 20.652367, 20.64943, 20.644033, 20.641048, 20.640491, 20.640491, 20.616814, 20.614552, 20.611473, 20.608816, 20.608252, 20.606884, 20.604343, 20.595947, 20.595549, 20.594992, 20.594353, 20.586006, 20.585297, 20.583803, 20.581322, 20.578661, 20.57685, 20.576347, 20.574804, 20.569914, 20.569143, 20.567574, 20.566107, 20.560411, 20.552387, 20.547564, 20.54546, 20.535278, 20.528063, 20.527426, 20.52326, 20.521727, 20.518055, 20.5169, 20.5169, 20.515192, 20.51409, 20.5083, 20.505356, 20.495884, 20.492514, 20.488297, 20.487785, 20.487785, 20.47312, 20.469625, 20.462873, 20.460535, 20.460451, 20.456478, 20.452442, 20.452232, 20.45126, 20.44616, 20.445602, 20.444624, 20.44028, 20.439812, 20.438604, 20.436571, 20.43474, 20.427637, 20.424585, 20.41916, 20.414421, 20.412916, 20.408493, 20.40569, 20.402843, 20.391024, 20.383705, 20.380104, 20.378387, 20.3735, 20.369707, 20.36845, 20.365559, 20.364895, 20.363579, 20.363174, 20.36188, 20.35743, 20.357218, 20.357086, 20.355839, 20.354877, 20.351173, 20.346552, 20.346546, 20.346088, 20.346088, 20.334494, 20.333397, 20.321516, 20.320282, 20.31689, 20.314281, 20.309536, 20.308332, 20.307148, 20.30098, 20.297379, 20.295479, 20.293776, 20.286552, 20.286058, 20.285637, 20.28525, 20.283792, 20.282408, 20.280863, 20.279799, 20.279196, 20.277557, 20.277369, 20.275782, 20.269602, 20.261389, 20.26105, 20.258757, 20.257477, 20.25727, 20.256811, 20.252045, 20.245514, 20.243591, 20.243591, 20.23838, 20.237251, 20.235283, 20.22713, 20.22216, 20.22204, 20.209423, 20.206367, 20.203026, 20.199986, 20.199505, 20.19667, 20.196474, 20.195536, 20.192318, 20.18735, 20.184704, 20.184704, 20.184704, 20.183422, 20.178345, 20.17535, 20.173206, 20.170416, 20.166208, 20.159817, 20.159239, 20.156065, 20.150965, 20.143562, 20.135696, 20.134415, 20.133448, 20.129992, 20.128212, 20.125076, 20.120163, 20.11942, 20.118988, 20.11448, 20.114372, 20.112194, 20.111713, 20.106129, 20.104906, 20.099056, 20.095488, 20.095488, 20.087276, 20.086784, 20.086689, 20.083912, 20.083912, 20.078781, 20.078117, 20.07773, 20.071388, 20.070063, 20.069561, 20.06831, 20.068306, 20.06331, 20.060251, 20.059355, 20.059195, 20.058662, 20.05769, 20.056776, 20.056776, 20.056776, 20.054031, 20.052973, 20.046406, 20.044127, 20.039248, 20.035173, 20.033941, 20.03151, 20.03151, 20.03151, 20.03151, 20.03151, 20.027195, 20.025198, 20.024532, 20.022257, 20.02083, 20.01537, 20.008741, 20.007769, 20.004827, 19.995468, 19.995274, 19.994225, 19.983204, 19.97647, 19.972118, 19.964836, 19.958673, 19.9561, 19.949095, 19.946503, 19.94347, 19.934551, 19.933735, 19.930807, 19.927338, 19.922977, 19.92039, 19.918667, 19.915848, 19.906319, 19.905008, 19.89916, 19.898073, 19.896317, 19.894121, 19.894121, 19.88754, 19.88546, 19.882261, 19.881979, 19.873861, 19.873844, 19.87221, 19.870892, 19.86448, 19.864197, 19.859938, 19.859938, 19.859215, 19.859215, 19.852875, 19.852507, 19.852507, 19.848902, 19.847555, 19.846048, 19.845663, 19.84499, 19.844093, 19.841345, 19.841003, 19.833393, 19.82452, 19.82093, 19.818409, 19.815441, 19.813478, 19.813307, 19.802855, 19.801376, 19.801153, 19.798613, 19.796688, 19.786205, 19.783764, 19.781378, 19.78126, 19.780428, 19.779516, 19.779263, 19.774153, 19.772657, 19.772324, 19.771708, 19.761412, 19.761337, 19.751759, 19.748217, 19.748217, 19.74191, 19.738836, 19.738836, 19.737003, 19.737003, 19.728737, 19.724031, 19.716791, 19.713882, 19.710789, 19.708984, 19.708984, 19.702118, 19.702118, 19.699612, 19.6925, 19.692389, 19.689024, 19.687695, 19.687695, 19.687614, 19.68343, 19.68075, 19.670713, 19.669884, 19.669641, 19.666845, 19.660885, 19.660276, 19.656528, 19.65454, 19.64918, 19.645569, 19.640734, 19.635685, 19.634232, 19.632156, 19.628496, 19.627575, 19.627096, 19.607117, 19.60528, 19.605206, 19.605024, 19.60425, 19.603596, 19.597502, 19.597502, 19.596642, 19.596642, 19.596016, 19.584171, 19.583025, 19.582989, 19.582602, 19.580048, 19.578777, 19.572323, 19.570707, 19.569067, 19.569067, 19.568901, 19.567974, 19.567226, 19.567226, 19.56488, 19.56383, 19.562014, 19.552853, 19.548422, 19.538076, 19.537712, 19.536833, 19.531115, 19.530289, 19.528221, 19.526352, 19.524254, 19.522495, 19.522406, 19.522318, 19.52058, 19.519808, 19.515339, 19.51487, 19.513937, 19.509352]}, {'id': '2419555779', 'text': 'integrated heart--coupling multiscale multiphysic model simulation cardiac function', 'timestamp': 1485903600, 'rel_doc_ids': ['1987533249', '2005993072', '2045840642', '2077590638', '2078242986', '2097819905', '2115087864', '2137001160', '2139132227', '2145827038', '2151777635', '2160241374', '2169839169', '2184488534', '2240155340', '2283958186', '2291401466', '342730161'], 'user_id': '150772663', 'user_doc_ids': ['18366864', '141759635', '160136759', '1578355164', '1591915543', '1735823337', '1738812763', '1791067423', '1794936996', '1895645629', '1910956624', '1911004364', '1941276016', '1971722909', '1987377268', '1995498988', '2009176130', '2020920275', '2037675107', '2041657637', '2051567707', '2059088502', '2060887891', '2078286087', '2081746320', '2116292166', '2122494265', '2131092468', '2131154387', '2142168519', '2150069959', '2156149812', '2157130548', '2159994350', '2162650555', '2165901372', '2283958186'], 'bm25_doc_ids': ['2042087811', '2168240094', '2291401466', '2939997', '2108027741', '342730161', '1511916253', '1971577916', '2116810549', '2041918441', '2014331478', '2037516277', '2016431493', '2005198977', '2076792489', '2127240158', '2011877498', '2102485765', '2012078774', '2108623448', '1970969013', '1998689016', '2053515988', '2215163094', '1970919211', '2091133161', '2049419548', '2042809186', '1596244030', '1502385629', '2327519846', '1965199354', '2124987231', '965503987', '1966543040', '2016399920', '2156137422', '2000006673', '2146376723', '1537765608', '1581361120', '2325243900', '2012135016', '2098256494', '1588352056', '2090469206', '2058749831', '1976110908', '2155413932', '2097842356', '2000481344', '2129879127', '2064263180', '1993234585', '2514371318', '1969089270', '1972838779', '2138929119', '2102194765', '1996377749', '2114011216', '2124686196', '2465489578', '2045919784', '2012514315', '1900242079', '2011674305', '1525337079', '2044790700', '2081645064', '2170613553', '2114235390', '1730851447', '2063659910', '943621318', '2084929239', '2150973453', '2112197312', '1984459862', '2169038227', '2005993072', '2116880843', '2287937249', '124881962', '1863711105', '1572030961', '121274080', '2091860224', '2061862777', '2119688180', '1993738370', '2059191611', '2151777635', '2101710519', '2008383281', '2074387731', '2031770198', '2137477111', '2052811036', '2076288716', '2122808763', '2020929776', '2152724633', '2120890179', '1973589758', '1992654982', '2080215837', '2164897783', '2055780636', '2410445967', '149626477', '1547755009', '2056464833', '2107476523', '2592518457', '2159293185', '1556247673', '2055608717', '2154985523', '2139687606', '2102456200', '2035921161', '2142532265', '1544151147', '2065833031', '2105811964', '2169150819', '2148038248', '2002634573', '2171695688', '2180950025', '2158442743', '2063314523', '2039766412', '1978754647', '2137001160', '2019012010', '2244526909', '2000389398', '2143941386', '2164630694', '1981702986', '2023366003', '1502300720', '2030001335', '2166179838', '2098563402', '2030221331', '1997610317', '2170234911', '2043649542', '2492391282', '2148954218', '1988666133', '2031409368', '2335695330', '11653831', '2127737606', '2592251127', '2145815805', '2089194947', '2342607183', '1978588246', '2601712078', '1535466631', '2052848505', '1947740174', '2107986059', '1986755410', '1989502965', '2112384267', '1565777923', '1587474003', '2018920576', '2235491783', '2142177891', '2147540444', '2028815369', '2124889867', '2139508923', '1484609840', '2919078399', '2540169846', '2158705509', '1530372710', '1978252319', '2335118199', '2061379360', '2052786465', '2126768587', '1983119015', '2533079833', '2029527177', '2130505007', '28482778', '2094764414', '2042172897', '1978574671', '2104081710', '2081974879', '2092757808', '1964861604', '1998172733', '2044910634', '2533700181', '2169959732', '2078323210', '162760937', '2128910368', '2094195846', '2038238199', '2538547209', '1988130274', '2056451096', '1978160580', '2060172487', '2078119018', '2028296379', '1992863948', '1484777998', '2064740288', '2058312364', '2040372812', '2127262798', '2153579011', '2056299084', '1523740859', '2149002598', '1579109572', '2072212815', '2084553830', '2189036137', '2024268169', '2123960243', '158044000', '2124384272', '2163786414', '2018792437', '2269932692', '2112211175', '2964221710', '2007648666', '1162481492', '2136850370', '2070140147', '2145323332', '1917602020', '2046934253', '2031525098', '2238361499', '2114277372', '2022851351', '2029826558', '2124286181', '1973756207', '2088822485', '2097819905', '2031760723', '2134942905', '2165188319', '2114083918', '1592284865', '2095187885', '2165922402', '2169545386', '1846518452', '95717809', '1585278738', '1986808125', '2033219316', '1977072721', '2122009976', '2098593145', '2015616284', '2130779047', '17261976', '2061285587', '2087292325', '2056486520', '2050551508', '2177864975', '1968753443', '2141351157', '2165301141', '2055943941', '2536165414', '2020460294', '2134256758', '1506546117', '2510736440', '2152295640', '1574623013', '1663293642', '2109656481', '1974222580', '2030851264', '2167204789', '2143721230', '2073367883', '2092172247', '1578073890', '1964525020', '598797396', '2119488021', '1000394224', '2128298772', '1993536307', '1972693740', '2104622807', '1980133340', '1985226352', '2009190518', '167612029', '1973391164', '2043504541', '2027014939', '2952118606', '2158189140', '2344666443', '2110089899', '2047658479', '1969882224', '2146166796', '1886380108', '2046839347', '1990245416', '2067071938', '2031344785', '2079489881', '2092248363', '2267757974', '1986623300', '2130493617', '2027111729', '2117175183', '1993774732', '2166386925', '2159299504', '2147991903', '2296537313', '2066801885', '2090237405', '2096427898', '2111609195', '2002801448', '2091557449', '2129716882', '2072839368', '2042099692', '2006953198', '158438992', '1986836297', '2124302662', '2120204925', '1995358087', '2046396382', '2167363044', '2049613322', '2035880036', '2076567530', '2078916411', '168619327', '2523590678', '1955228981', '2311044675', '2072615766', '2160241374', '2088957247', '2114408179', '3011413250', '2076760198', '2171043106', '2114414013', '1523896403', '2952411993', '1806228610', '2049854874', '2063674012', '2963710982', '1980394605', '2119820406', '2162007185', '1986356457', '2003964428', '2068458829', '2103769764', '1976162546', '2018772114', '2098029486', '2004647483', '2162062207', '1999773996', '2102696211', '2003564091', '2047434891', '2100909458', '2169839169', '1562165823', '1847657166', '64655713', '2142669026', '2143585675', '174015286', '2082365888', '1970614773', '1513236357', '3022428191', '2020190995', '1835351569', '2109839400', '2152308146', '1986420517', '1971533812', '2081508613', '105435840', '2007784306', '2153777818', '1855744943', '1523118747', '2318278720', '1990806142', '2041387489', '2103876722', '2140332303', '2042266516', '2135239400', '1592933783', '2014415700', '2120451917', '2157231504', '2096118053', '2294911451', '2098627046', '2118818261', '2024044790', '2083801566', '2155542084', '1949618367', '2023618071', '2038622158', '2055880852', '2125346217', '2008657923', '2169349775', '1994160545', '2005078182', '2116135678', '2026670441', '33908953', '2034532318', '2172236985', '2087871837', '2169054892', '2123021817', '2273172449', '2077590638', '2029864817', '2090450513', '1979183669', '1977943543', '2064330500', '2079797700', '2152539566', '2013193043', '2410433995', '2532822250', '2152913557', '2119279572', '2145140666', '2139132227', '1993417849', '2062709770', '2004660874', '2118587791', '352500163', '1972390461', '2142070691', '2101511363', '2064548742', '2040163651', '2030001020', '1600023214', '59427560', '2107214135', '2019291954', '1986406591', '1967218094', '2021344187', '2095873412', '2147188488', '2025859527', '1570229582', '2082010119', '1962537036', '1529052753', '2106641241', '2109764939', '1996407797', '2046931297', '2103512909', '2074584600', '2133848493', '2544018251', '2058603131', '2118499503', '21813396', '1972141480', '2283958186', '2186364680', '2072203157', '1968835146', '2031682261', '2063783858', '2114127830', '1500072073', '2018437246', '2012001579', '2169796238', '2085939132', '190812387', '1994867826', '2909114891', '207228923', '1980191464', '2112771147', '1578002743', '2021509762', '2014104660', '2290404289', '2125407721', '1619587174', '2032384637', '2027962610', '2184849980', '1991347248', '2067264931', '2137867316', '2051210587', '2029367459', '2052723505', '2063092376', '2136697195', '2050621855', '2119348902', '2082333210', '2024965566', '1867221958', '2050113551', '1874623802', '161011754', '2307689898', '1983064238', '181865331', '2078804895', '2002760615', '2059995865', '1990414858', '2171613704', '1999184428', '2075087565', '2535129702', '2040582515', '2129755766', '2139194272', '2602372774', '2035571037', '2153827227', '1978425155', '2199823730', '2438561999', '1966977432', '2100345558', '2159939892', '674670', '2141496033', '2023707004', '2051161463', '2084489466', '2170646598', '60911303', '2006229647', '1906002767', '2124427351', '2165606761', '1992533348', '1998845044', '1583832643', '2051970205', '2002673781', '2124440167', '2011652866', '1996166757', '2156037398', '2122439447', '1992317947', '2109527175', '2104410938', '2078242986', '2016721687', '2159865809', '2125049442', '2239231616', '2534489451', '1966581503', '1881292487', '2484882962', '171850257', '2078276473', '44147525', '2332360555', '2065154817', '2222321154', '2078750390', '2095501572', '2066129063', '2615805087', '2013537947', '2012223895', '2037613535', '145483289', '2083188144', '2075204291', '1512969257', '2290679245', '2415726085', '2125342534', '2221240764', '2103877420', '1587373819', '2069590379', '2045840642', '2013111423', '2065672727', '2113342867', '2051920868', '2167454509', '2059851411', '2074630101', '2031288167', '2151400286', '2326506273', '1970687419', '1553733534', '1994527015', '2128365184', '2155464739', '2163832864', '1972519108', '2026087549', '2081394635', '2178337699', '2136620165', '2155554580', '2172185563', '1995476345', '2027082381', '2001117924', '1963710891', '1966322936', '2120800529', '2163415341', '2143842017', '2136131661', '1555642721', '2060637162', '1979123451', '2087467696', '1981050310', '2252307109', '2030217064', '2115539792', '2076426701', '2143911014', '1972260128', '2023538552', '2000761206', '2004560375', '1588161861', '2148704306', '2078827936', '2162234580', '1515227948', '2127654654', '2126708228', '1685310863', '2092755171', '2165562993', '2044889765', '2081744561', '2150263122', '2070546168', '2163334102', '2050614728', '2144176736', '2154317699', '2017778997', '2072845789', '175467639', '1989639070', '2027659707', '2095623699', '1533731629', '2028685719', '2046156160', '2117051139', '1964061879', '2043425026', '2963116096', '2032596064', '2069638239', '2128909281', '2248875179', '2150337102', '2121204181', '2537626110', '2012226138', '1970872300', '1582716130', '1993585491', '2125439136', '2035280682', '2029372046', '2070159602', '2013140934', '1996840531', '1572108067', '2100230160', '130097682', '2091448716', '2068711045', '2089366740', '2058277075', '1591071783', '2120130455', '2014351048', '2240155340', '1547259510', '2278999915', '2009131885', '1987533249', '2108101411', '2132439709', '9614526', '1584326584', '2158794309', '972424519', '2047944367', '2150987830', '2139243879', '2119310418', '2015207398', '2030531052', '2124436671', '2031631592', '2008362741', '2106221033', '2088140214', '1915970248', '1963679837', '2097997825', '1763066752', '2165445845', '1992137225', '2122085371', '2058458232', '2002466642', '2117797677', '2062698844', '2160790216', '2018010614', '1978100298', '2121925203', '1998727374', '1963353149', '2020638179', '2336389281', '2052384452', '2094308116', '2046905283', '1972656614', '2112596836', '2057654787', '1979401055', '2219094525', '2130820341', '2015970843', '2024697390', '2103615349', '2137342910', '2151026775', '1564206159', '94624080', '2122760001', '2170211873', '2106280788', '1531977904', '2084990719', '1497856944', '2001032894', '2344293268', '2117077211', '1561061300', '2167044696', '2165067994', '2070500455', '2115087864', '2101224116', '2125356473', '2168896685', '2079059155', '2157296702', '2102397036', '1982200457', '1970837159', '2542901069', '1902126572', '2019346659', '2100604996', '2171790646', '2226819868', '2120144611', '2127929982', '2145840688', '2543584958', '2213354465', '1965507121', '2261011542', '2100695320', '2094242952', '2081338610', '2022887551', '2035178327', '2064306347', '2139448469', '2013811182', '2142289574', '1563459556', '2000142208', '1971065470', '1553588862', '1521246456', '2066249639', '2096211527', '2043968589', '2170305692', '1999722266', '2067488678', '2057649846', '2093250899', '2047901538', '2007674250', '2153901189', '1634592155', '1976147036', '2075718469', '1989552414', '2067511555', '2045940139', '2123037718', '2161023180', '2315769953', '2043388684', '2102181507', '2481676618', '2146190031', '2089000836', '2070743564', '1963809003', '2104881731', '1994007484', '2016252483', '2106579739', '1551060558', '2088409920', '2339803386', '2070033465', '2166472306', '2160905344', '2127617353', '1980982089', '2117138850', '1985381099', '2162292215', '2141879489', '2012065441', '2109996997', '2047219622', '1524412882', '1982713078', '2101006818', '2053237333', '2548173572', '1425972884', '2151999466', '2161857951', '2466865207', '2169558980', '2120118530', '1965468395', '149709301', '2137616060', '1866040239', '2145827038', '2000062311', '2184488534', '2091958606', '1878949075', '2096139409', '2141714379', '2095786789', '2023125185', '2056193931', '1985814347', '2217539148', '2147161648', '1984776228', '2010293459', '2167772797', '2105506964', '2167850198', '48553765', '1443747714', '1833560002', '2133636697', '1994037588', '2107101643', '2125946516', '2018433261', '2008566392', '2087991658', '2175872967', '2169851238', '1587895276', '2095016595', '2096430891', '1991163944', '2000402521', '2165893041', '2030182955', '1977203608', '938422861', '2112081602', '2085840942', '2172117292', '2019475991', '2158767632', '1583310869', '2105714141', '2163993958', '2048754774', '2031868184', '2108550488', '2146510985', '2015457603', '2132791882', '2100605856', '17639551', '2035093145', '2000329606', '2101758271', '2035891870', '2105990125', '2113191012', '2146762168', '1556421167', '1981098235', '2135660903', '2155852006', '2002639489', '2155025614', '2133822325', '2012085656', '2114109068', '1505377038', '2069845860', '2056815812', '2113416565', '1872336954', '2071302718', '2136827072', '1969085832', '2105573149', '1573191461', '1988965277', '2089533993', '2064979916', '2132516776', '2164460482', '2163327279', '2085638229', '5724136', '2127270655', '2025553519', '1985356425', '2055243081', '1984288883', '2037371523', '2171969361', '2139591598', '1521218102'], 'bm25_doc_scores': [34.49251, 33.02857, 32.800377, 31.19019, 31.135008, 30.485397, 29.763319, 29.691528, 28.584017, 28.492277, 28.489456, 27.949362, 27.847763, 27.223269, 26.699364, 26.548431, 26.459618, 26.185965, 26.167381, 26.13911, 26.034142, 25.844818, 25.765167, 25.75905, 25.548185, 25.330399, 25.282383, 25.279764, 25.182001, 25.112585, 24.538403, 24.534573, 24.488234, 24.485785, 24.413546, 24.389475, 24.28789, 24.238188, 24.194942, 24.187338, 23.981234, 23.92677, 23.876125, 23.824732, 23.785276, 23.743603, 23.545565, 23.508657, 23.421247, 23.28764, 22.975521, 22.781689, 22.72495, 22.71446, 22.677103, 22.579834, 22.550322, 22.464592, 22.27665, 22.121832, 22.06303, 22.039017, 22.001263, 21.687311, 21.681288, 21.60613, 21.367575, 21.31886, 21.298588, 21.258307, 21.235218, 21.221333, 21.161484, 21.127686, 21.069555, 21.05676, 20.98159, 20.98045, 20.958633, 20.95423, 20.897846, 20.8827, 20.842129, 20.759926, 20.752232, 20.729847, 20.717178, 20.691813, 20.676567, 20.639484, 20.604855, 20.597284, 20.548117, 20.50665, 20.437376, 20.432156, 20.427122, 20.41486, 20.290848, 20.287724, 20.27665, 20.27446, 20.221634, 20.190361, 20.11912, 20.103008, 20.092226, 20.061628, 20.037783, 19.990824, 19.92745, 19.852037, 19.786812, 19.70527, 19.702686, 19.702631, 19.653923, 19.616978, 19.598902, 19.572943, 19.565887, 19.509626, 19.499447, 19.461998, 19.455555, 19.45451, 19.454464, 19.44351, 19.42915, 19.405993, 19.4019, 19.391317, 19.343222, 19.326721, 19.317604, 19.274178, 19.270191, 19.258629, 19.250671, 19.207895, 19.16579, 19.143816, 19.094862, 19.084236, 19.081427, 19.017393, 19.011557, 18.978012, 18.974878, 18.933298, 18.921974, 18.882095, 18.878506, 18.870607, 18.840145, 18.807627, 18.782633, 18.762718, 18.695215, 18.691757, 18.685143, 18.667583, 18.657568, 18.650822, 18.623716, 18.620485, 18.612392, 18.59262, 18.58254, 18.580883, 18.567062, 18.514486, 18.504044, 18.491877, 18.490992, 18.457592, 18.454565, 18.433064, 18.426468, 18.405928, 18.405666, 18.374643, 18.346233, 18.345552, 18.327927, 18.321608, 18.306973, 18.292152, 18.285637, 18.266989, 18.237488, 18.230288, 18.226923, 18.21664, 18.21339, 18.171875, 18.160042, 18.153908, 18.148623, 18.112831, 18.081573, 18.04554, 18.037008, 18.02991, 18.019136, 18.007408, 17.976051, 17.97222, 17.968761, 17.96516, 17.959448, 17.959068, 17.957792, 17.948425, 17.945415, 17.941603, 17.941252, 17.936241, 17.91685, 17.91523, 17.905657, 17.899458, 17.89943, 17.874388, 17.857174, 17.81847, 17.808544, 17.804426, 17.793247, 17.782354, 17.770248, 17.764473, 17.760202, 17.754509, 17.75394, 17.74923, 17.73679, 17.734486, 17.72694, 17.719639, 17.71958, 17.714003, 17.697117, 17.69406, 17.649063, 17.641987, 17.631458, 17.625935, 17.619389, 17.603838, 17.603218, 17.578022, 17.575478, 17.569332, 17.553524, 17.536978, 17.521034, 17.511585, 17.507797, 17.501526, 17.499002, 17.496836, 17.48573, 17.476141, 17.456314, 17.446531, 17.446531, 17.44479, 17.44172, 17.43381, 17.425564, 17.420464, 17.402088, 17.397291, 17.39069, 17.379755, 17.377787, 17.375599, 17.356499, 17.352026, 17.336294, 17.330276, 17.31855, 17.303322, 17.294323, 17.281553, 17.276924, 17.264984, 17.261765, 17.251291, 17.24782, 17.240349, 17.231216, 17.221218, 17.214006, 17.208284, 17.205986, 17.204712, 17.194613, 17.187086, 17.173908, 17.172197, 17.1679, 17.157831, 17.15618, 17.148989, 17.14456, 17.140491, 17.140491, 17.130308, 17.125357, 17.11648, 17.109812, 17.084711, 17.07831, 17.059227, 17.049885, 17.049885, 17.039679, 17.032997, 17.023378, 17.018759, 17.018251, 17.017897, 17.011473, 17.004967, 17.003212, 16.992151, 16.978962, 16.973293, 16.966095, 16.955149, 16.943558, 16.932274, 16.927116, 16.927116, 16.915062, 16.914667, 16.907848, 16.900782, 16.885298, 16.877995, 16.87257, 16.869337, 16.868595, 16.862171, 16.857744, 16.85722, 16.846355, 16.835594, 16.826107, 16.822859, 16.812048, 16.79521, 16.79521, 16.792168, 16.782476, 16.77708, 16.772057, 16.757595, 16.75628, 16.746078, 16.740433, 16.738087, 16.725182, 16.717402, 16.711596, 16.708776, 16.691097, 16.65199, 16.65124, 16.650343, 16.649052, 16.647491, 16.628916, 16.628916, 16.620995, 16.611973, 16.610533, 16.610266, 16.587141, 16.583002, 16.570835, 16.562578, 16.54921, 16.51848, 16.507015, 16.499779, 16.49958, 16.49465, 16.478054, 16.477375, 16.457024, 16.456842, 16.45082, 16.448076, 16.43917, 16.427902, 16.427902, 16.427902, 16.418226, 16.412956, 16.410599, 16.407244, 16.40682, 16.40254, 16.401415, 16.398987, 16.388596, 16.379074, 16.37692, 16.37523, 16.371487, 16.362926, 16.359827, 16.349144, 16.346912, 16.341331, 16.334116, 16.320415, 16.31714, 16.309114, 16.30487, 16.304344, 16.303417, 16.303009, 16.302408, 16.296995, 16.28913, 16.277527, 16.277084, 16.27687, 16.269905, 16.26944, 16.26573, 16.262363, 16.253319, 16.252949, 16.23591, 16.232496, 16.232485, 16.22915, 16.222061, 16.21952, 16.21464, 16.21181, 16.206324, 16.19408, 16.1937, 16.193624, 16.183031, 16.175932, 16.173998, 16.169445, 16.16355, 16.15893, 16.157848, 16.119835, 16.113827, 16.108324, 16.104584, 16.10397, 16.100805, 16.099241, 16.06932, 16.068855, 16.062687, 16.053673, 16.049627, 16.046907, 16.040014, 16.030567, 16.02688, 16.02235, 16.022047, 16.02104, 16.014908, 16.011297, 16.010038, 16.006546, 16.006102, 16.004993, 15.997978, 15.991586, 15.987041, 15.978443, 15.978119, 15.964215, 15.962343, 15.953536, 15.953061, 15.950666, 15.948094, 15.938797, 15.938797, 15.935926, 15.935196, 15.925158, 15.907007, 15.906656, 15.905204, 15.902535, 15.88773, 15.879761, 15.878557, 15.8749485, 15.871739, 15.870233, 15.869412, 15.868527, 15.861439, 15.86083, 15.857111, 15.856644, 15.85609, 15.849209, 15.848343, 15.848114, 15.845919, 15.8449545, 15.831413, 15.826188, 15.823341, 15.822428, 15.81486, 15.814303, 15.813882, 15.810935, 15.809566, 15.807996, 15.806948, 15.8052435, 15.8011265, 15.799631, 15.799505, 15.786524, 15.768425, 15.765463, 15.760288, 15.757374, 15.755528, 15.752656, 15.74461, 15.7441845, 15.742016, 15.742016, 15.733212, 15.726624, 15.725887, 15.725257, 15.715788, 15.714587, 15.706854, 15.702436, 15.700818, 15.694547, 15.685307, 15.685284, 15.681869, 15.663091, 15.660761, 15.659344, 15.653648, 15.651619, 15.651556, 15.64996, 15.645878, 15.644934, 15.63947, 15.634964, 15.634178, 15.634178, 15.630865, 15.614766, 15.614497, 15.6142235, 15.606063, 15.600146, 15.599064, 15.599064, 15.597207, 15.593705, 15.589749, 15.588362, 15.582827, 15.577006, 15.574818, 15.573478, 15.570943, 15.56818, 15.568063, 15.567443, 15.562014, 15.561764, 15.561764, 15.55122, 15.543736, 15.542593, 15.541157, 15.537152, 15.533251, 15.532741, 15.525219, 15.5174675, 15.514624, 15.514346, 15.511472, 15.510809, 15.509401, 15.507952, 15.498991, 15.4936, 15.489709, 15.487667, 15.484316, 15.484114, 15.482472, 15.482197, 15.481414, 15.480812, 15.477913, 15.472536, 15.47072, 15.464767, 15.462998, 15.461614, 15.460733, 15.458528, 15.457869, 15.449621, 15.4486885, 15.439615, 15.4354925, 15.433617, 15.429029, 15.427669, 15.427071, 15.423699, 15.421143, 15.419806, 15.419806, 15.416238, 15.411528, 15.408964, 15.408127, 15.402997, 15.402975, 15.399165, 15.399165, 15.397909, 15.385267, 15.38428, 15.38373, 15.382864, 15.38155, 15.380162, 15.3784, 15.373554, 15.366761, 15.365837, 15.359147, 15.358369, 15.355022, 15.351225, 15.349778, 15.346843, 15.3457985, 15.340942, 15.337354, 15.334756, 15.333134, 15.326607, 15.318177, 15.310777, 15.310361, 15.309971, 15.308619, 15.30288, 15.296234, 15.291264, 15.287491, 15.2874565, 15.284574, 15.282508, 15.2767105, 15.274715, 15.274572, 15.269703, 15.268457, 15.264044, 15.263897, 15.263606, 15.259371, 15.259054, 15.255962, 15.254799, 15.252734, 15.2510605, 15.250891, 15.249229, 15.24843, 15.245309, 15.242336, 15.238674, 15.2340975, 15.231788, 15.229697, 15.219528, 15.218288, 15.217195, 15.210776, 15.20838, 15.206785, 15.204922, 15.20237, 15.199764, 15.1991625, 15.19688, 15.195901, 15.188599, 15.187423, 15.184049, 15.175828, 15.174671, 15.164626, 15.16407, 15.15779, 15.142462, 15.141071, 15.138266, 15.132397, 15.132264, 15.131551, 15.127262, 15.12478, 15.116513, 15.114054, 15.113121, 15.108331, 15.09428, 15.093481, 15.09339, 15.089762, 15.089492, 15.083163, 15.077764, 15.075007, 15.073866, 15.071718, 15.071718, 15.070635, 15.063005, 15.060784, 15.058419, 15.055457, 15.05324, 15.05265, 15.048132, 15.038373, 15.034791, 15.033697, 15.031937, 15.031162, 15.030795, 15.027318, 15.027052, 15.024418, 15.016819, 15.015293, 15.015203, 15.015115, 15.014567, 15.014203, 15.0121155, 15.009663, 15.002686, 15.0016575, 15.001231, 15.000994, 14.999485, 14.991175, 14.986818, 14.981156, 14.978683, 14.974179, 14.97406, 14.9722595, 14.969939, 14.969375, 14.968363, 14.967543, 14.964175, 14.964127, 14.96335, 14.963276, 14.953646, 14.953197, 14.953197, 14.950392, 14.948472, 14.948022, 14.94716, 14.946768, 14.940188, 14.940052, 14.939338, 14.939027, 14.937979, 14.935749, 14.934205, 14.925743, 14.922211, 14.922066, 14.918293, 14.918111, 14.917539, 14.916068, 14.909624, 14.896751, 14.889627, 14.888642, 14.888642, 14.885626, 14.883866, 14.882772, 14.88125, 14.880038, 14.878343, 14.876898, 14.876537, 14.873785, 14.868048, 14.868044, 14.867417, 14.860298, 14.85698, 14.856885, 14.854545, 14.854128, 14.853053, 14.850552, 14.850479, 14.846895, 14.846895, 14.845886, 14.843571, 14.838465, 14.837623, 14.833803, 14.831201, 14.83056, 14.830145, 14.827552, 14.823802, 14.817504, 14.817504, 14.813048, 14.805551, 14.8037, 14.797153, 14.79613, 14.795916, 14.794163, 14.790331, 14.783192, 14.781997, 14.780898, 14.766571, 14.764925, 14.763032, 14.760435, 14.760028, 14.75978, 14.754465, 14.754194, 14.750906, 14.74569, 14.744917, 14.738682, 14.7370205, 14.732175, 14.730955, 14.729438, 14.729057, 14.723698, 14.720385, 14.711519, 14.707258, 14.702195, 14.70001, 14.693181, 14.689338, 14.687121, 14.685791, 14.683568, 14.680029, 14.678186, 14.6775875, 14.6775875, 14.67708, 14.672661, 14.672631, 14.672358, 14.671956, 14.67037, 14.667787, 14.667414, 14.664322, 14.661232, 14.657501, 14.657085, 14.65628, 14.651487, 14.650875, 14.649588, 14.646608, 14.642546, 14.640372, 14.638451, 14.637794, 14.636463, 14.635997, 14.632015, 14.628105, 14.626806, 14.625961, 14.625075, 14.623982, 14.620733, 14.616835, 14.615129, 14.613614, 14.612524, 14.611495, 14.611332, 14.60708, 14.606548, 14.606041, 14.597961, 14.591806, 14.585934, 14.585509, 14.582346, 14.581778, 14.5806675, 14.579592, 14.572586, 14.572084, 14.570152, 14.568944, 14.5636, 14.563282, 14.563282, 14.561522, 14.559494, 14.554469, 14.550589, 14.548325, 14.544285, 14.54418, 14.538752, 14.537325, 14.531515, 14.531109, 14.528137, 14.527338, 14.519256, 14.516212, 14.515995, 14.515324, 14.514458, 14.51284, 14.498504, 14.496023, 14.495283, 14.493701, 14.4924135, 14.491908, 14.491308, 14.48731, 14.485116, 14.481238, 14.480192, 14.480149, 14.480133, 14.479715, 14.4793415, 14.478874, 14.475789, 14.475239, 14.474947, 14.474947, 14.474643, 14.461727, 14.4593, 14.45837, 14.456105, 14.454979, 14.45157, 14.451488]}, {'id': '2501861162', 'text': 'automatic detection microaneurysm retinal fundu image', 'timestamp': 1483225200, 'rel_doc_ids': ['1762163599', '1982761740', '1989786678', '1996151979', '2003624418', '2009329982', '2060749088', '2061276338', '2084318809', '2089615939', '2089969656', '2122644916', '2149430368', '2161342819', '2168112110', '2239579562'], 'user_id': '2135591858', 'user_doc_ids': ['81499379', '122715087', '124240369', '157191393', '1515593122', '1545889339', '1547002199', '1553162758', '1555159745', '1563262380', '1571515740', '1589791259', '1593107063', '1848925522', '1881919258', '1910102824', '1973592416', '1974356107', '1974842966', '1976896194', '1979102712', '1990035632', '2024233232', '2028936330', '2035002824', '2036324927', '2038607477', '2072086181', '2072950347', '2099300497', '2104289358', '2107953342', '2111744777', '2115560616', '2116830841', '2119926202', '2122538199', '2131065627', '2132830939', '2133047176', '2138047924', '2139463777', '2149335720', '2153269637', '2163773367', '2300511734', '2316075884', '2342476115', '2504874736'], 'bm25_doc_ids': ['2146927999', '2037832150', '1972851499', '2079656102', '1990219668', '172594560', '1970149764', '2534419511', '2044566025', '2074883517', '1976861102', '2220261417', '2153954621', '2293498001', '575503930', '1651341085', '1502277866', '1972333286', '2031033658', '1995019176', '2532617767', '2298690736', '2069878879', '2540986690', '1488868672', '205080650', '2014429143', '2048021006', '2116273337', '2137936662', '2512148493', '1981577676', '2165952271', '1600778991', '2144737531', '2039744358', '2107871626', '1915650656', '2142866653', '2015389221', '2289573798', '1539798066', '2239579562', '2342949904', '2101384127', '2433157206', '1967660412', '2060749088', '1965366670', '2145695422', '2049437564', '2122827247', '2509890384', '2087919959', '1974634913', '2121703462', '2026641873', '1866357777', '2099478840', '1974708751', '2132016355', '1975322353', '2166214224', '2301681138', '1999954174', '2061276338', '1930275192', '165597098', '2086848813', '1996151979', '2051096525', '2296755154', '204234776', '1831894763', '2137127990', '2018658051', '2124459495', '2575248783', '2009851794', '20040508', '2565139801', '2175751043', '2095215022', '2038473515', '2031867636', '2118818120', '2089969656', '2078681411', '2010049940', '2610913943', '114204748', '2086916172', '2114063873', '2161342819', '1994833429', '2005022091', '1864524337', '2036391583', '2149430368', '2060819847', '2000114707', '2156422447', '2011428156', '1970015880', '2104492227', '1997183308', '2160773244', '2107910786', '2109683734', '2047234801', '2104507524', '2100743035', '1999354443', '147511327', '2009263890', '2151842157', '2161851082', '2168113473', '2103063185', '1523151681', '2010373608', '2133194556', '1629328159', '2156811101', '2431197474', '2113119033', '2026427067', '2327793514', '2089615939', '2086106167', '127102904', '2126197688', '2126891598', '2069543979', '2052710217', '2015271030', '2067074351', '2049996624', '2414079435', '261459436', '2161795232', '2118548945', '185858101', '2167963303', '1983197241', '2101546453', '2543182157', '2169318059', '2154138928', '2092106903', '2084318809', '2130679772', '2152922676', '2169736040', '2038137113', '1762163599', '2001422514', '2146898399', '1982034955', '2255583379', '2010466185', '2442349545', '2110786147', '2149280451', '2078789324', '2301001905', '2270367735', '2260906889', '2093545979', '2222796588', '257049640', '2267762288', '1753463689', '2119155092', '1679358830', '2226754619', '2102799105', '2107306261', '2121233747', '2140331352', '2134670624', '2123422255', '2338203391', '2078659958', '1486269824', '2395635227', '2117089674', '1980375532', '2576531595', '1547115578', '2120389036', '2121737033', '2005772228', '2091248650', '2157984366', '2096333975', '2104223983', '2296064604', '2082543130', '2010962711', '1976647582', '2058914116', '1986492571', '198750002', '192929063', '1978038874', '1981942733', '2070079537', '2117274523', '1982115876', '1989786678', '2001955655', '1980571360', '1855261921', '2537253116', '1494799991', '2150337917', '2091271128', '2144897994', '2092150800', '2037760632', '2086053970', '2148989106', '2147432771', '2611207491', '2148040994', '2021586830', '1663704817', '2532576713', '2398347515', '1993295430', '2078316046', '2205871084', '2151745025', '62132766', '2050352291', '1968498343', '2480987982', '2258939777', '2151238137', '1977855901', '2157682860', '2100756624', '2285597517', '1987947134', '2177654152', '1178861821', '2097954762', '2114593155', '2024978699', '2126623645', '1896969699', '2492332077', '2004996897', '2005970950', '2463610651', '2128848886', '1500191525', '2014374651', '297330039', '2080389430', '2081093511', '2168868438', '2113247245', '2532928309', '1891626539', '1572336300', '2168932490', '166338369', '1507470219', '2104023616', '2054184672', '2107569086', '2015312811', '1581552333', '2397746333', '2089479074', '2056425777', '2060140621', '2544875997', '2105393075', '2152118884', '2271045920', '1486660381', '2136679353', '2152040166', '1585932041', '2142652821', '1592418956', '2089769759', '2562566663', '2034786340', '2042109072', '2083158334', '2060680491', '2073911907', '2095405787', '2032424373', '1521137403', '2037413884', '2136103933', '1008612578', '2052627443', '2142412853', '2024550852', '2192592811', '2168468500', '1949479385', '2044769106', '2288039401', '2009404629', '2045626752', '2143551962', '2105311464', '1798363820', '2156080102', '1971938781', '2088422930', '2041230595', '2140431812', '1587136368', '348278759', '2162970164', '2058816799', '2099768981', '1972759071', '2159423988', '2293842029', '2154934742', '2297761292', '2094646395', '2400525874', '2156119810', '2088468266', '1975753658', '2434017844', '3258327', '2089393349', '2169824714', '1999391531', '2108969748', '2155558397', '2159395868', '2097570228', '1603141456', '2030853058', '2405402222', '2409099933', '2294926082', '74702703', '2003517812', '2130826886', '1912184907', '1884944813', '2135062808', '2167086999', '2142512089', '2748191316', '2059736991', '2823238661', '2091585872', '2026562747', '2092006374', '1897125036', '2088252342', '1984242098', '1999483037', '2247456277', '2051942570', '1762194249', '2403579479', '2227075449', '2296102276', '2142627556', '2015604886', '2080749271', '2160064874', '2317686780', '1497070616', '2084100156', '2038079321', '40505795', '2038488197', '2138001711', '2161989935', '2105839704', '2040806095', '1990380740', '2169842403', '1480785676', '1932840270', '1993138166', '114328460', '2105655269', '2046553925', '2135221009', '1872507226', '2513326255', '1983483233', '2143730236', '1983155562', '1482523208', '2112248380', '2198235674', '1514571846', '1967372890', '1751547976', '132729571', '2081641419', '1977958573', '2169375969', '2161111234', '1542617061', '1533859153', '2169164902', '2156723723', '2129917030', '2280256208', '1977125677', '1983374065', '2161530198', '2545404630', '2043492194', '2033554041', '1825382665', '2325395071', '2134374023', '2080590359', '2089802691', '2033688669', '2019683893', '2102290599', '2169428761', '1543399908', '2106883050', '2061676243', '1966305775', '2103820732', '2009128342', '2084585745', '2001353247', '1901361661', '1827864374', '2003821440', '1538463832', '1969169314', '2085656914', '2045227075', '1513940210', '2143014003', '2330592348', '2107823820', '1967762685', '1964681770', '2024859286', '153596868', '1987351961', '2165258886', '2546828105', '2950147620', '1605527094', '2576961874', '1983001804', '1991611945', '2098142346', '1988926132', '1549120046', '2087221324', '2045372703', '1995340881', '2395842378', '2105733783', '1571891713', '2023260945', '1917574908', '2004905374', '2069330237', '2139031088', '1823673455', '2124690815', '1486669370', '2082256640', '1921052677', '2164948443', '2109221146', '2065856652', '2020249638', '2006487828', '1997778270', '1532257638', '2081716693', '2054383075', '2073971146', '2048721378', '1971654131', '1841324543', '1972376260', '1536270514', '2471736554', '2154201679', '2049729972', '2060212233', '2042988683', '1998106502', '2187731819', '2168205497', '2529844114', '2009329982', '2018103501', '2150105292', '2083335615', '1974969377', '2130235524', '2258285047', '2016519643', '2008087598', '2003809075', '2083337717', '2543991396', '2538462707', '121917011', '999983672', '2078239496', '1991045225', '2142821373', '2140776918', '1971414336', '1982980715', '312794020', '2079160211', '2110450173', '1121561169', '2026457357', '2542304285', '1777993770', '2023237866', '2057998163', '2141575807', '2052646460', '1996611778', '2117023799', '2169242575', '2137009054', '2038126331', '2081466246', '2079984111', '1550531685', '2102457348', '2324226986', '2011375137', '2556615514', '2015854443', '2538579510', '1970025873', '2207798371', '296045363', '1976400065', '2100040279', '2018138595', '2150486368', '2218277910', '1880508059', '2433259561', '2041655614', '1967023689', '2130723048', '2309022991', '2316348143', '1588983445', '2223922412', '2756138156', '2029875010', '2135908022', '2159314398', '2144900711', '2115298507', '2118917314', '1823656996', '2029255095', '2032254649', '2082571018', '2330237340', '1535703902', '2166463820', '2125837678', '1999725904', '2749117513', '2165014883', '2023065165', '2529321959', '1971327342', '90131806', '2539852306', '2531866780', '1979176126', '2534129789', '1994024320', '2090695180', '2066041491', '1193943752', '2020491509', '1993060683', '2023584141', '2099031125', '2004426356', '2162097093', '1972438296', '2119324198', '2112974048', '1998762225', '2152410740', '129084174', '1970100896', '2170616084', '2144810021', '2104501205', '2259857371', '1529699969', '2035858328', '2123350525', '2002276422', '1878483291', '2544114213', '1997894594', '1977972806', '2197072555', '2031612938', '2150180562', '1981613277', '2534504587', '1996671990', '2130920828', '2080379074', '2557738935', '2114404799', '1957191894', '2041137496', '1608377285', '2008115515', '2035573343', '2136289855', '1976049336', '2137071183', '2270168057', '2033748823', '2072643456', '2540236341', '822860665', '2042497109', '2422973293', '1982996680', '1967871052', '2043334495', '2061556698', '2034708693', '2088338269', '2065423531', '185699638', '2109037308', '209964180', '2161198735', '2149274338', '2158322422', '2041423964', '2059036143', '2057771956', '1804882088', '2321097941', '2144396085', '1895732026', '2168112110', '1541431731', '2152407813', '2075108010', '1980010757', '53361100', '2038198878', '2071383689', '2019829573', '2082112311', '207690533', '1924768908', '2404623897', '1997124998', '1999194192', '2141211128', '2187524180', '1600969471', '1994205414', '1601476374', '2029616009', '1506578720', '2126924587', '1994635901', '2125660481', '2174711832', '2474489285', '2119790910', '1914943439', '2610010235', '2097092081', '938233457', '2987605868', '2318452939', '2400696114', '2254144287', '1544290124', '2142095525', '2100072940', '2010099973', '1770826372', '2106442510', '2343943864', '2049701347', '2118726633', '2100993030', '2113950977', '2095990136', '2116255423', '2033736290', '2152050449', '2050759855', '52035241', '105232971', '2136539671', '62865632', '2110090447', '2040278009', '1878694625', '2135914471', '2020339941', '2882572478', '2532616559', '2175684014', '2010782951', '2059888006', '2539693590', '2100529622', '1589126114', '2003022131', '1902064527', '1991669891', '2001811885', '1181445873', '2034993983', '2085012886', '2159212388', '2117825948', '2135732838', '2521319891', '2167821469', '2035174542', '2090043544', '2032052565', '2064376476', '1979276409', '2059120001', '2132831991', '1550881344', '2133919334', '2134363734', '1999327895', '2010868592', '2157669435', '2022136966', '2047383141', '2041693724', '2122574810', '2221821740', '2047063132', '1587300351', '2164635737', '2169719349', '60104851', '2091467020', '2096546861', '2490191101', '2142161285', '1526519290', '2110642225', '2034659498', '1973086081', '2010262414', '2015393534', '2197648668', '2061891677', '2104787155', '1550450874', '2535118350', '2112112817', '2016194585', '2151349744', '151130048', '1540846520', '2288628943', '2075636440', '2153372904', '2109357612', '1707018060', '2106314403', '2086986013', '2104578644', '2049184890', '1989458396', '2122644916', '2029728726', '2520186357', '2548337268', '2160779691', '2133371060', '2083325330', '2026787873', '2117953248', '1994268319', '2037776979', '2025008906', '2037953114', '1979595091', '1595431502', '2005312687', '2067379168', '2107885708', '2573507073', '1937832096', '2564596280', '1473767589', '2032477478', '1982761740', '2328371157', '1977880724', '2240472106', '2068179630', '2095303849', '2124130318', '2152990105', '2060091892', '2139697833', '2028724631', '2164748764', '2123146336', '32702003', '2143504228', '1974979298', '2128492366', '2000043638', '2034598891', '1841377117', '1915073489', '1999377661', '1598145779', '1930872584', '2018593404', '2027694967', '1993920983', '2165462885', '1557271295', '2071530063', '2001457150', '2273059372', '2103828547', '2151082289', '2122160601', '2075374539', '2053638745', '2148900397', '2037065998', '2528556198', '1836321400', '2533466254', '2107415846', '90652424', '2155122478', '2746082187', '2072850750', '2166545411', '186782046', '2085202667', '2093948535', '2288495312', '2155955002', '2112859404', '2071073174', '2119112562', '2115150825', '2128247256', '2246991053', '2039675777', '2068250244', '2116458070', '2089940549', '2098038197', '1940810297', '2084807920', '197424697', '1991845085', '2404019942', '1566241495', '2175234323', '2155626145', '2753439130', '2035887722', '2028786911', '2523612729', '1872487577', '2228802744', '2003624418', '2038375429', '2131327864', '2576587620', '2108370292', '2036018570', '2078403184', '2005224707', '2171077533', '2112637299', '1991558902', '2162426307', '2018649230', '1986139367', '2148497673', '2077800962', '2087072126', '2084700217', '2065385897', '2302236336', '1993053266', '2127302541', '2040495365', '1966247839', '2127339568', '2398866467', '1977437451', '2463184571', '2146336484', '1574891941', '2116628223', '2280525290', '2145061499', '2017544115', '2120751402', '2062131296', '2294729137', '1520347847', '2105923875', '2168681409', '2165987290', '2104324599', '1515854699', '2273205327', '843969030', '2027416947', '2088714123', '1968281506', '2296798226', '2516208417', '2063603043', '118981789', '2123041898', '50583629', '2288283259', '2165376948', '2217118232', '2096274929', '2035052246', '1579240109', '1964217512', '2062659628', '1982672043', '101843986', '2254264209', '2008101778', '2116353806', '2095294713', '2153940148', '2283983651', '2102721154', '2274159472', '2090272372', '2115176208', '2064202402', '2120172698', '2074677260', '2040584809', '1970063352', '181775189', '1857308984', '2055690146', '1996035596', '2168660178', '2021212824', '2543877586', '2036569342', '2151830467', '2168091729', '2086632989', '2131349903', '2549338107', '1987843764', '1963739915', '2000015918'], 'bm25_doc_scores': [52.865273, 47.864655, 47.850864, 47.836555, 47.436127, 46.432827, 45.646088, 45.483356, 45.38869, 45.323902, 44.19518, 43.56214, 43.373478, 43.260044, 43.164833, 42.78143, 41.583755, 41.43191, 41.14499, 41.11116, 41.065643, 41.0463, 41.035595, 40.999382, 40.929314, 40.58997, 40.575058, 40.408077, 40.355732, 40.27225, 40.24369, 40.108547, 39.84198, 39.722946, 39.26847, 39.161163, 39.04068, 38.832314, 38.401764, 38.34306, 38.073067, 38.059044, 37.873627, 37.692173, 37.67495, 37.48007, 37.35925, 37.303913, 37.246353, 37.17477, 37.140694, 36.972755, 36.84292, 36.779243, 36.464783, 36.43235, 36.37505, 36.32924, 36.03939, 35.79694, 35.73715, 35.693153, 35.470726, 35.444675, 35.423084, 35.319897, 35.28, 35.139812, 34.883915, 34.833008, 34.768314, 34.702938, 34.67309, 34.353523, 34.340626, 34.300877, 34.241447, 34.22121, 34.17406, 34.164124, 34.128567, 34.104347, 33.999454, 33.94144, 33.80974, 33.792427, 33.763443, 33.64856, 33.447605, 33.376484, 33.215267, 33.184227, 33.14713, 32.937157, 32.927784, 32.74793, 32.709423, 32.66462, 32.561398, 32.538826, 32.522644, 32.508713, 32.485157, 32.38535, 32.37893, 32.329475, 32.322998, 32.2562, 32.23377, 32.18563, 32.133984, 32.100525, 32.05081, 32.017162, 32.014122, 32.01173, 31.840702, 31.77346, 31.74902, 31.723137, 31.712206, 31.685753, 31.645325, 31.582666, 31.570194, 31.559969, 31.508556, 31.508091, 31.464922, 31.437115, 31.434492, 31.36482, 31.35236, 31.332108, 31.327353, 31.15364, 31.101974, 31.084679, 31.057154, 30.95231, 30.874277, 30.872805, 30.86147, 30.80352, 30.790388, 30.77851, 30.685501, 30.672894, 30.656174, 30.507826, 30.437752, 30.394783, 30.377396, 30.345455, 30.308212, 30.252108, 30.248655, 30.24865, 30.240135, 30.238943, 30.179615, 30.139242, 30.12528, 30.116592, 30.011055, 29.991018, 29.978773, 29.973522, 29.963772, 29.957449, 29.947647, 29.940723, 29.935873, 29.928621, 29.912642, 29.890467, 29.855536, 29.837002, 29.717281, 29.707153, 29.68675, 29.683634, 29.658096, 29.652971, 29.633377, 29.62225, 29.613132, 29.603895, 29.567585, 29.548496, 29.5097, 29.493929, 29.484194, 29.470346, 29.458483, 29.452887, 29.445984, 29.401886, 29.354374, 29.347883, 29.313828, 29.313711, 29.305677, 29.290123, 29.286743, 29.212803, 29.202063, 29.196934, 29.145378, 29.140789, 29.071108, 29.070333, 29.068476, 29.026205, 29.023216, 29.020548, 28.922747, 28.921375, 28.918999, 28.880531, 28.86853, 28.85883, 28.84955, 28.831718, 28.830257, 28.826271, 28.814915, 28.77721, 28.773924, 28.762394, 28.758745, 28.73306, 28.729805, 28.694101, 28.643831, 28.640902, 28.589687, 28.5835, 28.582424, 28.556673, 28.540482, 28.532974, 28.515072, 28.48577, 28.473366, 28.45872, 28.446304, 28.393198, 28.391876, 28.388498, 28.372738, 28.369812, 28.341831, 28.332882, 28.326494, 28.32075, 28.315117, 28.307478, 28.306383, 28.306042, 28.265135, 28.228289, 28.198797, 28.190458, 28.162485, 28.144466, 28.121412, 28.086937, 28.08343, 28.07757, 28.06888, 28.06353, 28.037231, 28.001102, 27.94056, 27.915491, 27.901194, 27.890392, 27.856781, 27.828318, 27.806225, 27.773163, 27.760082, 27.75615, 27.730719, 27.715614, 27.67928, 27.678314, 27.668167, 27.666304, 27.612186, 27.608105, 27.600231, 27.59251, 27.589542, 27.561026, 27.553062, 27.536354, 27.515158, 27.48379, 27.478027, 27.477924, 27.46234, 27.457617, 27.445421, 27.433828, 27.39998, 27.378614, 27.3762, 27.352755, 27.347178, 27.344532, 27.333456, 27.328024, 27.299541, 27.291803, 27.291126, 27.28018, 27.270342, 27.244453, 27.244453, 27.238548, 27.235435, 27.233389, 27.226503, 27.218977, 27.209835, 27.191256, 27.173668, 27.16842, 27.156614, 27.120398, 27.114815, 27.068733, 27.066063, 27.036858, 27.010973, 26.99473, 26.97685, 26.947922, 26.935743, 26.911194, 26.89899, 26.889864, 26.865288, 26.854141, 26.85373, 26.85373, 26.829178, 26.829042, 26.824347, 26.799717, 26.775219, 26.772131, 26.769947, 26.768673, 26.744596, 26.729202, 26.714767, 26.70265, 26.681168, 26.675823, 26.673376, 26.626068, 26.616724, 26.616222, 26.605585, 26.586258, 26.58256, 26.562483, 26.553532, 26.528618, 26.518797, 26.510935, 26.507963, 26.502037, 26.501282, 26.488947, 26.47149, 26.464153, 26.44675, 26.443325, 26.440495, 26.421326, 26.421143, 26.401901, 26.391678, 26.368675, 26.36755, 26.357172, 26.343819, 26.316744, 26.314848, 26.288765, 26.279638, 26.266445, 26.26478, 26.241879, 26.229755, 26.226631, 26.209677, 26.203499, 26.12299, 26.089546, 26.051779, 26.048752, 26.028044, 26.023478, 26.014982, 26.004557, 26.003262, 26.0009, 26.000896, 25.99352, 25.983719, 25.982697, 25.980455, 25.970768, 25.95643, 25.925133, 25.924185, 25.900835, 25.896532, 25.891901, 25.867159, 25.859442, 25.857231, 25.854252, 25.832857, 25.832027, 25.80088, 25.732834, 25.703405, 25.703363, 25.694311, 25.682795, 25.67657, 25.669674, 25.666998, 25.660788, 25.646109, 25.646097, 25.642866, 25.635403, 25.599724, 25.595562, 25.591679, 25.584352, 25.579298, 25.57862, 25.543123, 25.542269, 25.53208, 25.524347, 25.521217, 25.520334, 25.496462, 25.484272, 25.483183, 25.48105, 25.478596, 25.468788, 25.44613, 25.39389, 25.393328, 25.386393, 25.361204, 25.321064, 25.303734, 25.296377, 25.283585, 25.27652, 25.272964, 25.266602, 25.244967, 25.230692, 25.219843, 25.213709, 25.206348, 25.206041, 25.177753, 25.153864, 25.141449, 25.130432, 25.11396, 25.102846, 25.073608, 25.068674, 25.068508, 25.064972, 25.049791, 25.041597, 25.035412, 25.033543, 25.025967, 25.00258, 25.001122, 24.975931, 24.966356, 24.956142, 24.9506, 24.929482, 24.911423, 24.908583, 24.886524, 24.874727, 24.874727, 24.862099, 24.854752, 24.829786, 24.812056, 24.773579, 24.766996, 24.765089, 24.762941, 24.762024, 24.75816, 24.75585, 24.739609, 24.73727, 24.730553, 24.714815, 24.707283, 24.69874, 24.682384, 24.66285, 24.652071, 24.651537, 24.651276, 24.645947, 24.636562, 24.610332, 24.590103, 24.57307, 24.569399, 24.558172, 24.552902, 24.545383, 24.541712, 24.536339, 24.516743, 24.50296, 24.501629, 24.474735, 24.464703, 24.425213, 24.395212, 24.381657, 24.381657, 24.374565, 24.362177, 24.35199, 24.339823, 24.332518, 24.318201, 24.3126, 24.303759, 24.292736, 24.282413, 24.282112, 24.252539, 24.237045, 24.23179, 24.217999, 24.216028, 24.207226, 24.205957, 24.193937, 24.186045, 24.179527, 24.176447, 24.160513, 24.147831, 24.140007, 24.138523, 24.115192, 24.109167, 24.099722, 24.07132, 24.06939, 24.062162, 24.061937, 24.044998, 24.027637, 24.023163, 24.00373, 23.9923, 23.99042, 23.985819, 23.959354, 23.958439, 23.939331, 23.935196, 23.931694, 23.914366, 23.894516, 23.88943, 23.870846, 23.864342, 23.861914, 23.854761, 23.842854, 23.841303, 23.835121, 23.806398, 23.805077, 23.804905, 23.795769, 23.780554, 23.7683, 23.743189, 23.732294, 23.702477, 23.697916, 23.692259, 23.689623, 23.684702, 23.673279, 23.663357, 23.655642, 23.643892, 23.638184, 23.625526, 23.622538, 23.622328, 23.619944, 23.6192, 23.606306, 23.581772, 23.579145, 23.578619, 23.571966, 23.566086, 23.540695, 23.532017, 23.471773, 23.470072, 23.45759, 23.446377, 23.416306, 23.392563, 23.370249, 23.366901, 23.343605, 23.340826, 23.339375, 23.337723, 23.290783, 23.286316, 23.282732, 23.281239, 23.264477, 23.205013, 23.178911, 23.171253, 23.17002, 23.166584, 23.14698, 23.142567, 23.13272, 23.113659, 23.091087, 23.088491, 23.08669, 23.081196, 23.062534, 23.05896, 23.056427, 23.040428, 23.038307, 23.017824, 23.012978, 23.011524, 22.99326, 22.992245, 22.979061, 22.978928, 22.97173, 22.96717, 22.960005, 22.954348, 22.953295, 22.949924, 22.94672, 22.9456, 22.943096, 22.932947, 22.924112, 22.87552, 22.868637, 22.867199, 22.86618, 22.84937, 22.835, 22.817823, 22.817823, 22.815899, 22.813368, 22.792765, 22.792765, 22.784027, 22.781073, 22.781073, 22.772854, 22.768429, 22.76228, 22.759888, 22.749159, 22.74681, 22.728828, 22.723722, 22.721401, 22.718418, 22.702961, 22.699581, 22.687191, 22.67958, 22.66695, 22.664005, 22.664005, 22.663425, 22.661072, 22.65957, 22.627262, 22.62098, 22.619701, 22.618925, 22.612944, 22.60856, 22.60856, 22.60856, 22.593933, 22.583294, 22.58094, 22.573498, 22.54719, 22.543036, 22.520521, 22.516523, 22.501505, 22.458527, 22.453823, 22.44226, 22.436773, 22.419695, 22.41948, 22.416399, 22.414024, 22.409397, 22.404749, 22.404749, 22.403936, 22.396576, 22.3956, 22.372898, 22.363487, 22.359837, 22.354027, 22.348305, 22.337265, 22.295712, 22.277544, 22.265383, 22.263775, 22.251173, 22.246422, 22.244083, 22.232718, 22.22562, 22.220974, 22.213226, 22.21042, 22.207962, 22.20647, 22.203299, 22.19549, 22.18709, 22.179552, 22.14512, 22.139872, 22.137173, 22.136621, 22.123598, 22.073982, 22.066572, 22.061111, 22.05278, 22.034643, 22.015472, 22.006954, 21.996008, 21.98797, 21.98797, 21.985176, 21.98463, 21.976093, 21.976093, 21.966524, 21.945374, 21.938528, 21.938528, 21.931215, 21.90769, 21.899326, 21.877174, 21.87091, 21.853395, 21.847555, 21.844929, 21.843302, 21.8175, 21.813692, 21.775616, 21.77389, 21.75958, 21.747187, 21.745846, 21.74571, 21.738987, 21.737242, 21.733246, 21.729294, 21.715055, 21.700542, 21.699444, 21.69346, 21.677025, 21.675009, 21.660645, 21.64985, 21.64242, 21.63674, 21.626936, 21.609089, 21.600449, 21.600449, 21.591545, 21.576334, 21.559345, 21.557287, 21.551266, 21.53736, 21.530935, 21.51775, 21.464441, 21.463657, 21.458563, 21.457024, 21.451097, 21.450235, 21.44626, 21.426521, 21.40965, 21.407171, 21.395065, 21.379847, 21.37745, 21.377384, 21.376833, 21.366081, 21.35918, 21.342655, 21.342121, 21.339684, 21.339344, 21.31731, 21.309988, 21.309881, 21.303799, 21.299192, 21.295555, 21.29328, 21.288767, 21.278967, 21.278967, 21.27485, 21.27052, 21.269348, 21.230225, 21.22646, 21.222513, 21.222513, 21.206318, 21.200489, 21.196672, 21.194178, 21.144016, 21.140583, 21.122753, 21.122753, 21.121119, 21.11925, 21.11925, 21.115332, 21.082407, 21.08173, 21.079556, 21.060244, 21.057703, 21.05263, 21.047644, 21.02853, 21.010136, 20.988932, 20.972223, 20.969088, 20.967875, 20.965328, 20.96152, 20.961391, 20.953514, 20.952345, 20.94614, 20.939436, 20.916256, 20.915213, 20.913546, 20.904768, 20.899422, 20.89349, 20.89349, 20.886587, 20.880661, 20.87841, 20.86761, 20.864164, 20.819193, 20.814095, 20.814095, 20.810806, 20.80825, 20.799528, 20.797562, 20.796448, 20.794155, 20.790554, 20.776756, 20.77614, 20.759386, 20.75397, 20.750988, 20.748062, 20.735346, 20.725653, 20.724554, 20.722061, 20.713055, 20.709368, 20.707678, 20.67699, 20.658972, 20.653734, 20.647276, 20.642258, 20.640554, 20.637112, 20.636292, 20.631887, 20.620691, 20.599583, 20.586231, 20.586178, 20.585844, 20.585844, 20.58273, 20.57591, 20.564835, 20.562384, 20.561916, 20.549099, 20.548168, 20.546799, 20.54573, 20.54357, 20.531116, 20.528927, 20.525274, 20.521032, 20.518137, 20.505413, 20.505053, 20.499792, 20.488873, 20.475737, 20.47252, 20.47225, 20.47032, 20.47032, 20.46961, 20.457788, 20.457096, 20.447683, 20.432896, 20.432556, 20.43233, 20.417122, 20.41401, 20.411137, 20.397503, 20.395906]}, {'id': '2514571180', 'text': 'forensic investigate p2p cloud storage service backbone iot network', 'timestamp': 1485903600, 'rel_doc_ids': ['1722180796', '1755477624', '1969240345', '1970440495', '1991458033', '1998957169', '2001913551', '2015410758', '2020315857', '2022108140', '2025529089', '2056271845', '2078469142', '2093283549', '2122122011', '2124387823', '2263140893', '2334541668', '2462696620'], 'user_id': '2318895841', 'user_doc_ids': ['15506636', '74808775', '77651440', '103932535', '116395549', '126031720', '193377621', '561145213', '755265946', '828137986', '840751038', '919859439', '1507367034', '1508659626', '1524453022', '1525738306', '1533552014', '1542001427', '1546532811', '1559087881', '1594759064', '1725493522', '1749282018', '1755477624', '1781678696', '1808013424', '1814718128', '1833790872', '1837048997', '1941804596', '1966027500', '1969240345', '1970440495', '1978455222', '1979667920', '1991109940', '1995965340', '1998957169', '2004218389', '2006778800', '2008256412', '2014113877', '2015410758', '2016229865', '2020315857', '2022108140', '2024316961', '2025179284', '2027839170', '2028914478', '2031389367', '2032240808', '2033360454', '2034493311', '2045327705', '2046411953', '2048190753', '2048311211', '2055500615', '2056271845', '2058393210', '2058586878', '2066565675', '2067756468', '2076377913', '2078469142', '2078975757', '2081747399', '2083631268', '2084467408', '2085676741', '2099750454', '2100006179', '2103274830', '2103335850', '2103769032', '2103829340', '2103964960', '2112372902', '2114359803', '2117024508', '2117701112', '2119497716', '2120999547', '2122122011', '2128409613', '2132449440', '2133186418', '2137916520', '2144393725', '2155672489', '2155685591', '2174733357', '2181458060', '2181652435', '2184498111', '2186021972', '2187571250', '2211900011', '2254433317', '2257970068', '2272206587', '2280570678', '2281126320', '2281202115', '2285563693', '2289010250', '2291549475', '2293466296', '2295776812', '2299150109', '2299450638', '2300707846', '2303287830', '2323106813', '2331595182', '2333725978', '2334541668', '2345464210', '2346331195', '2365436337', '2394587725', '2397981708', '2401497809', '2404184955', '2404676241', '2405356014', '2417282157', '2418806285', '2460920259', '2463871910', '2469927444', '2474414057', '2478352276', '2481640251', '2483024783', '2498630325', '2500298352', '2502928996', '2507351359', '2508781145', '2508998626', '2511121934', '2518265829', '2520022511', '2520574184', '2522052520', '2522263928', '2522972287', '2523744263', '2527818263', '2529059446', '2550198472', '2560411358', '2562535746', '2564914932', '2567582603', '2625099568', '2626862036', '2749305344', '2963254318', '2963347091', '2963982550'], 'bm25_doc_ids': ['1014033202', '2263140893', '2951591428', '2039233071', '3023453665', '1991458033', '2081930626', '1521737313', '2001913551', '1962096123', '1824047601', '2536604331', '140692303', '2272194572', '2591728639', '2069999933', '2030482452', '2183563210', '2103964960', '1969240345', '2044448129', '2073709132', '650945507', '2550198472', '2279176866', '2115478218', '1589190226', '2766647272', '1906264264', '2011460925', '2083533476', '2296139781', '1548029709', '2146520495', '23717803', '1993586921', '2078469142', '126608434', '2310240969', '1555036802', '2009426851', '3023832899', '561145213', '2562108840', '89872232', '1969238379', '2519118766', '2544843976', '2498630325', '1998957169', '2594674933', '2526107990', '2094104365', '2085455137', '2070117955', '2093283549', '2525228524', '2294804235', '2121942750', '2020315857', '2586645166', '16482292', '2259470974', '2054315779', '2548601994', '2161882986', '1558159027', '2076639539', '2047398363', '2079565248', '1968989207', '2171926816', '2542700464', '2481050413', '2101429074', '2019453699', '2169249759', '2066973059', '2520993891', '1985267018', '2564745288', '2077126419', '2065062408', '2073008201', '1757283378', '2581697219', '1987164007', '2158913515', '2040383559', '2295582926', '2396001458', '2072647790', '1983454454', '2000801802', '2024713489', '2033360454', '2022108140', '2114769244', '2242546181', '141772518', '2125176383', '1531316950', '2338828827', '1507241541', '2246286794', '1596694673', '2362941995', '2334541668', '2019421251', '2099427174', '2016764778', '2025813159', '2157006116', '2158403736', '2025529089', '2292398915', '2024468903', '1481931599', '2037172006', '2345685777', '2336677785', '1933219619', '1982950856', '2272764376', '2101673977', '2168074924', '2505533491', '2062254040', '2345301610', '2003807909', '2521799916', '2161946648', '2116047956', '1594759064', '126291697', '2055204629', '2472333518', '2057293489', '2009132875', '2001996001', '2397657997', '1939548024', '2137187288', '2933254399', '2087648028', '2152670919', '2010094050', '2079345704', '1515052829', '2060988608', '2129840796', '2557469050', '2080134081', '2513931985', '2122122011', '1733918233', '2401687993', '1806759150', '1781545590', '2148642667', '1570235902', '2020041214', '2009090887', '2016374480', '1994506898', '2586762583', '2285650816', '1966836595', '1561047780', '2060006673', '2034493311', '2507288794', '2279956321', '2132756441', '2095259430', '2478910470', '2145278891', '614531281', '1971580317', '2587720827', '2521824271', '306946146', '2051229648', '2335223849', '1997182427', '2103309748', '1097120739', '2163831694', '2087433244', '1506597781', '2251418013', '2010965336', '2267724964', '2055283320', '1971875452', '349371204', '1976538445', '1963489908', '1722180796', '1997010178', '1749525484', '2242824577', '2069974347', '2084136933', '1633318179', '1550815238', '2953357142', '2516011357', '2186299612', '2569531417', '2512234118', '97403240', '2598412860', '1594193016', '2321345742', '2189407270', '2510244456', '2548415449', '2479288622', '2491209115', '1449986297', '1574533905', '2134584914', '2346029216', '2611565958', '2160227992', '2146986300', '2039687910', '2580686730', '2528201848', '2332886073', '133380263', '1837048997', '2314627612', '2120392452', '2086389113', '2105855893', '2130900504', '2586094944', '2246561543', '143942840', '2015548170', '2094065396', '1607809991', '2529163980', '2326007942', '1792167539', '2054619664', '2581822242', '1969690570', '2056271845', '138782056', '2392235604', '2380820373', '2002964709', '2146255547', '1981975711', '1561135059', '2052792720', '2341120131', '2527148314', '2062921720', '2571158126', '2047088201', '1835322790', '1994920437', '69162483', '2010254123', '1937828813', '2464295643', '1562742302', '2481640251', '2150250348', '2103200909', '2545328382', '2022448864', '2099958966', '1939443437', '2290447772', '2003849731', '2081209861', '1454782005', '2098120415', '1490898536', '2065668400', '2383927028', '1970814671', '2016575016', '2532098783', '2462696620', '2535753625', '2293095660', '2329992726', '2022014971', '2111457491', '2069514153', '2326016897', '1994202968', '2109191804', '2543346459', '2002653397', '1550746896', '152132211', '1996802761', '2326123011', '1971283588', '2093387648', '2534668683', '2289258947', '2281906662', '1553023320', '1500437522', '1999458275', '2020837817', '2053321886', '2124387823', '2133919537', '2399098898', '991808601', '2568364175', '2277310894', '2092910425', '2400191671', '2243451334', '2086323652', '1855709057', '2280053510', '2460765705', '2496161010', '1987006781', '2281202115', '2149452919', '2527567355', '2204681158', '2010880161', '2099210732', '2135610854', '2007511616', '1954111899', '2503731017', '2565589313', '2014113877', '2038003148', '2103019234', '1981163003', '2556705104', '2525959385', '1565538618', '2963347091', '1755477624', '647479589', '2062160421', '2080746216', '2154652414', '2086113250', '2052340408', '2063068607', '2116297707', '2100354155', '2121062723', '1545693834', '1584208130', '2036286551', '2266632848', '2243728652', '2164827494', '2166009805', '2286488013', '2418689914', '1947610140', '56798659', '2095141520', '2949561916', '2099815523', '2381405996', '1606395467', '2100153424', '2085582029', '1979030901', '1986294857', '2294664745', '2055951785', '2548646153', '1979334633', '2161646900', '2561346842', '1580538119', '2039258924', '2519963258', '2099236609', '2227642511', '1945867156', '2049429509', '2056625091', '2551496437', '2097072038', '2546606151', '1583361925', '2050222294', '2027569758', '2405353940', '2059842310', '2545431703', '1995833791', '2169923262', '641732948', '2091198105', '2209220621', '1994721878', '2532971777', '1995464719', '2011274866', '2011700720', '2271081388', '2499976535', '2492722871', '2560732611', '1563206715', '155786561', '1128340951', '2518665378', '1999275268', '2048659471', '2077664395', '2037151160', '947027414', '2281834151', '1968787503', '177539255', '2118964134', '2013317495', '2022273053', '2101908874', '2180638899', '2039078487', '2329156628', '2111928617', '2052184323', '2051555003', '2262286122', '2162852006', '6312178', '167374357', '1910867531', '2528300505', '2010604937', '1638185289', '1975947453', '2072345601', '2025784223', '2461872906', '2016608146', '1924994176', '2016585230', '2098836777', '2335526684', '1431736832', '2014989534', '2084829994', '2027781998', '2156092677', '2155617481', '2118188975', '2280592985', '2289149676', '1992204376', '2154209107', '1970440495', '2154769279', '2161548297', '2514148140', '2063120361', '1606912675', '1599916625', '2208633762', '1044079485', '1992393680', '2337977175', '2543644059', '2067080724', '1982744441', '832685209', '2602445533', '2277377765', '2150978022', '620975999', '1778282005', '1986831441', '2114992415', '1991491875', '403364479', '1988506727', '2020409598', '2002277552', '2064741734', '1966797943', '2403713996', '2322987107', '2577162623', '1978418837', '2400763571', '2157254570', '2268485603', '1965742133', '2562535746', '2095815177', '2094416037', '2041783805', '2107928909', '1836040104', '801599454', '2207117320', '2101281803', '819037725', '2169489339', '2473031807', '2567333176', '2077719093', '2533866537', '2019026371', '2017528085', '2030287259', '2077996621', '2483097313', '2170535964', '2085936320', '2099692426', '2168941875', '2585236569', '163738642', '1592526126', '1833212623', '2064593095', '1977577978', '2276764389', '2032082575', '2221031400', '2122068153', '2342778009', '2033607782', '2546235368', '2234254772', '2162031302', '1991674094', '2280922791', '2009133786', '2069760905', '2549107717', '2541527520', '2079255150', '2181487992', '2129368140', '2101843742', '2517633426', '1964418470', '1511346087', '2104724066', '1851752624', '2294624902', '614318664', '1971541601', '2231796304', '1987749451', '2074709832', '2107236952', '2488936582', '2113469892', '2163544205', '936439116', '2517373000', '1949315818', '1944395747', '2139206243', '2332151664', '1992284100', '2074768989', '2142332462', '2555541806', '2274254580', '2587429714', '2138577441', '1981398867', '2187397317', '1963935316', '2039194338', '2153423941', '2042576402', '2122227216', '1975813011', '2072531829', '1590053617', '2053714016', '2081272457', '2548115664', '2277120543', '1563811846', '2038106871', '2008682053', '2531717957', '1539338302', '2564767638', '1599443115', '2498358642', '1456662665', '2203418400', '2566913480', '2394933229', '2485172669', '2111231604', '2047803968', '163440109', '2019421619', '2034582500', '2088899317', '2013633301', '262754714', '2281822004', '2472682372', '1966558473', '1992936368', '2953060200', '2111619626', '2103851297', '2016669115', '2113132620', '1976806437', '1986096467', '2167505895', '2109213829', '1837982953', '2008094782', '2289823189', '1998964974', '2146117736', '2147282265', '2430001852', '1636364634', '1980418810', '2047352628', '2289644782', '2518319056', '2253906049', '2468803319', '1993693882', '2510781223', '1753817908', '1652082496', '2132479554', '2523946387', '2078433240', '1974083310', '2124968132', '2586466887', '908181160', '2521114648', '2057751865', '2007801672', '2279756469', '2147472587', '2131674814', '2519105949', '1986488795', '2143617739', '2560665895', '307786802', '2132731787', '2562625140', '2092756533', '2104903448', '2306156530', '2019462775', '2087982500', '1988154365', '2007121330', '2078732990', '1955090539', '1987124439', '2085749377', '2052982204', '2112172938', '2066785918', '1935487859', '1483221362', '2098239585', '1540903002', '1460694375', '2132043817', '2069583953', '2134295053', '2343300676', '1562366460', '1489578560', '2059168327', '385276318', '170143881', '114940871', '2611619861', '2160388979', '2087603199', '2064212344', '2183201255', '2017391854', '2511472933', '1872254807', '2033945109', '2143587547', '948939867', '2295872724', '2172620437', '2281609470', '1573894309', '2016689908', '2566097740', '2156721776', '2089452274', '2015786848', '2015410758', '1604990430', '1975778996', '2119631914', '1987436594', '1480272951', '2248028015', '2153508037', '2952502000', '2288485085', '33658911', '2517733757', '1598463081', '1020864014', '2056412352', '1486949119', '169466422', '2002027317', '2133884582', '2395952832', '1994276481', '2019269619', '2001780124', '2063958980', '2000472351', '2021534869', '2462952137', '1966100307', '21427492', '2117894254', '2108535050', '2067730440', '2120417567', '1957654261', '2537407247', '2307993216', '2159499277', '2756215125', '2035321839', '2222625043', '2130924213', '2231095945', '2001089727', '2533427303', '2286773538', '2288018813', '2115606594', '1497241928', '1983332116', '2570617441', '2127955210', '2472970276', '2026289525', '2123317341', '69151002', '2127294517', '2496617652', '2154334082', '2116075922', '2174076370', '2586073387', '2029992670', '2528960837', '2094665807', '2275211813', '2143829139', '2000069982', '1828746609', '2115237615', '2139098192', '1015774226', '2154077823', '2059154578', '2084854878', '2025823130', '1562794398', '2243988086', '2053509435', '954501512', '2564810971', '2280816377', '2991610625', '2061288034', '2510451544', '2133130767', '1526859224', '2049421380', '2016589762', '2186339937', '2534795981', '2466563759', '2055477748', '2007886029', '2145695733', '2101712107', '2140680019', '2077100676', '2003452289', '1669150381', '2303323316', '1964952516', '2013298460', '2059258175', '1989306606', '1964425773', '2145676449', '2021603238', '2195415882', '2327802721', '2130033371', '2963000581', '2081825692', '2525607610', '2129582520', '1979072344', '2220179734', '2017543213', '2059258748', '955603516', '2541711883', '2140669960', '2153457047', '2083811542', '1073581072', '1987778589', '2098884664', '2128559682', '160922935', '2152997435', '2500132898', '2142293984', '2285750015', '1971602293', '2038055165', '2227087907', '2198252115', '2167886234', '2134444029', '2469481058', '2106861843', '2032151215', '1976479301', '2470257024', '2057676176', '2170512963', '1982246970', '2289950708', '1981223869', '1967315140', '97539848', '2306447822', '2204967300', '2111228194', '2029909716', '1958009659', '138623172', '2171511963', '2200921637', '2414923796', '1572773265', '1943858613', '1975809061', '2480734751', '1989447671', '2077671437', '2059951390', '2344770124', '1572621272', '1572083294', '1556736518', '1790074971', '2026892459', '2157437046', '2105975290', '2112034660', '1561360575', '155758722', '2229168773', '2123649966', '2018087302', '1524921823', '2509643850', '2403005236', '2148844988', '2118822287', '938641848', '1602537754', '2293288627', '2032739611', '2474071019', '2383413053', '1844490947', '2543419896', '1580001677', '2417221886', '2078217448', '1971080668', '1539722554', '2048669134', '2153863195', '2584077101', '2140206229', '2110567548', '1987269572', '1834161701', '1996619035', '2071220027', '1992236679', '2098866098', '2102162364', '2008427934', '1638163870', '2116435078', '2226898239', '1914846392', '2862405528', '2129815617', '2030560149', '2005945323', '1998840566', '2149477921', '2294271434', '2127904651', '1014328319', '181714688', '2090839316', '2136751840', '1863140851', '2265201304', '1977765379', '2467311395', '1559991558', '1459074731', '2128970445', '2065046698', '2506019673', '2345800547', '2545388817', '2125944453', '605475480', '2063765897', '2107933099', '2545323722', '2080659780', '1965674633', '2041025920', '1509754650', '1997557646', '1518563014', '1975686767', '2045813140', '2148434818', '2003861089', '2587832086', '2080956388', '1966892630', '1570211022', '2284129584', '2401673893', '2327842617', '2279588110', '2113786635', '1029018918', '977857428', '2137269541', '168522250', '7892163', '2025009877', '2561015838', '1534584878', '2289178885', '2415753807', '2113005368', '2514524524', '2345080593', '2292375639', '2547402947', '2085210068', '2025723714', '1038473064', '1806380925', '1997034776', '2153578102', '1788419102'], 'bm25_doc_scores': [36.464058, 35.924576, 33.475914, 33.475914, 33.300266, 33.294117, 33.26253, 32.622482, 32.393074, 32.374657, 32.228706, 32.215305, 32.16769, 32.1226, 31.913593, 31.772646, 31.76134, 31.723549, 31.31052, 31.164783, 31.041721, 30.963207, 30.89344, 30.827456, 30.561115, 30.481552, 30.465847, 30.40892, 30.402952, 30.267338, 30.260765, 30.244373, 30.006481, 29.839396, 29.83569, 29.826414, 29.741459, 29.705053, 29.544966, 29.472286, 29.36478, 29.359074, 29.359074, 29.308306, 29.234217, 29.030296, 28.994345, 28.970407, 28.921106, 28.816404, 28.79819, 28.751108, 28.736984, 28.696823, 28.64159, 28.63683, 28.536293, 28.533834, 28.522085, 28.450087, 28.407318, 28.382994, 28.36401, 28.32106, 28.318079, 28.104675, 27.998367, 27.977425, 27.940317, 27.92233, 27.921028, 27.83645, 27.831533, 27.823652, 27.81555, 27.798729, 27.797947, 27.721914, 27.692417, 27.675585, 27.640945, 27.617252, 27.57412, 27.570621, 27.556683, 27.433924, 27.424105, 27.417282, 27.400791, 27.328285, 27.289524, 27.264807, 27.259754, 27.202074, 27.165455, 27.116484, 27.082726, 26.997988, 26.9432, 26.942692, 26.9395, 26.843237, 26.775211, 26.6934, 26.664265, 26.661362, 26.660152, 26.65529, 26.64736, 26.64422, 26.590796, 26.583714, 26.57363, 26.544823, 26.481459, 26.447397, 26.444725, 26.392405, 26.37457, 26.360714, 26.305035, 26.300426, 26.28609, 26.272215, 26.230743, 26.22798, 26.213947, 26.213436, 26.115602, 26.053947, 25.985159, 25.984211, 25.969824, 25.964975, 25.96082, 25.94732, 25.930271, 25.889303, 25.88855, 25.881062, 25.868687, 25.85298, 25.850086, 25.850086, 25.83532, 25.796673, 25.762487, 25.75592, 25.749907, 25.738426, 25.737537, 25.733671, 25.726343, 25.715446, 25.699045, 25.649893, 25.609753, 25.603277, 25.601233, 25.600977, 25.575167, 25.540958, 25.517256, 25.515835, 25.501265, 25.485603, 25.483276, 25.479223, 25.440002, 25.431046, 25.41414, 25.399677, 25.386623, 25.380226, 25.375572, 25.36901, 25.362324, 25.361343, 25.359772, 25.347534, 25.330765, 25.316135, 25.312916, 25.298082, 25.282015, 25.269884, 25.243162, 25.218643, 25.21185, 25.192287, 25.179047, 25.157183, 25.127834, 25.10558, 25.088327, 25.068184, 25.05523, 25.032545, 25.031492, 25.029, 25.013313, 25.012901, 25.001, 24.980473, 24.973106, 24.957924, 24.956194, 24.89146, 24.864414, 24.85753, 24.837585, 24.815434, 24.814278, 24.814138, 24.81041, 24.798231, 24.785212, 24.78058, 24.776665, 24.722412, 24.721472, 24.717693, 24.697546, 24.672098, 24.670135, 24.653715, 24.627075, 24.623129, 24.614132, 24.61379, 24.6104, 24.607004, 24.552778, 24.545792, 24.539557, 24.53668, 24.512438, 24.501549, 24.485947, 24.481565, 24.46843, 24.467478, 24.443466, 24.43609, 24.42286, 24.376116, 24.37155, 24.359827, 24.35888, 24.31645, 24.254826, 24.245129, 24.238438, 24.230955, 24.211704, 24.210129, 24.207737, 24.163925, 24.156292, 24.155413, 24.146769, 24.140713, 24.089085, 24.068306, 24.049976, 24.045778, 24.020624, 24.007833, 23.995321, 23.991602, 23.975357, 23.972973, 23.959335, 23.923012, 23.92243, 23.917814, 23.916105, 23.902912, 23.901913, 23.89708, 23.878002, 23.862812, 23.849672, 23.808992, 23.798452, 23.795725, 23.788767, 23.779528, 23.778196, 23.762323, 23.761038, 23.75765, 23.741697, 23.727097, 23.72409, 23.69547, 23.691105, 23.684534, 23.664547, 23.663416, 23.654024, 23.646849, 23.641285, 23.63408, 23.625572, 23.618088, 23.609686, 23.609034, 23.552555, 23.525267, 23.489006, 23.48589, 23.477379, 23.465557, 23.465124, 23.458673, 23.452047, 23.425251, 23.419147, 23.402298, 23.400055, 23.365017, 23.362183, 23.361128, 23.346539, 23.343098, 23.328115, 23.32143, 23.318115, 23.302555, 23.300488, 23.271017, 23.260508, 23.239271, 23.21878, 23.217945, 23.179184, 23.148808, 23.147543, 23.144579, 23.11995, 23.107992, 23.09931, 23.094387, 23.09166, 23.086903, 23.08676, 23.07796, 23.060867, 23.060867, 23.054916, 23.05468, 23.048063, 23.032822, 23.01546, 23.014965, 23.002495, 22.996235, 22.991812, 22.9699, 22.966455, 22.946873, 22.94652, 22.937113, 22.9339, 22.909061, 22.904903, 22.903002, 22.895273, 22.894958, 22.881222, 22.880249, 22.874582, 22.873821, 22.86574, 22.815891, 22.809734, 22.791935, 22.79103, 22.789963, 22.78481, 22.762245, 22.761045, 22.754223, 22.750862, 22.744207, 22.7353, 22.727814, 22.727528, 22.71723, 22.714867, 22.703577, 22.702694, 22.701046, 22.692585, 22.686481, 22.662184, 22.649746, 22.642082, 22.641403, 22.62915, 22.606983, 22.605892, 22.594618, 22.594526, 22.565882, 22.560251, 22.558266, 22.55735, 22.553616, 22.544147, 22.527206, 22.48448, 22.484478, 22.471216, 22.468307, 22.4664, 22.45719, 22.453789, 22.449001, 22.42831, 22.427477, 22.420816, 22.41375, 22.413591, 22.40658, 22.406551, 22.404953, 22.400023, 22.386843, 22.371607, 22.370056, 22.369148, 22.369125, 22.34888, 22.346664, 22.344135, 22.33955, 22.332249, 22.318676, 22.313889, 22.311056, 22.287271, 22.286684, 22.282999, 22.274996, 22.26716, 22.262104, 22.26159, 22.252495, 22.243376, 22.234806, 22.23166, 22.204514, 22.194025, 22.193092, 22.189636, 22.188854, 22.186398, 22.182898, 22.170593, 22.162891, 22.155125, 22.151594, 22.112349, 22.109327, 22.109255, 22.10409, 22.103085, 22.084621, 22.0617, 22.05487, 22.048752, 22.026377, 22.023232, 22.00724, 22.007133, 22.006046, 22.005589, 22.004456, 22.002525, 21.964565, 21.954845, 21.92735, 21.917997, 21.901104, 21.895855, 21.89468, 21.894651, 21.88004, 21.87036, 21.867353, 21.856771, 21.854452, 21.853292, 21.846125, 21.845442, 21.845442, 21.841265, 21.83979, 21.826782, 21.824146, 21.821247, 21.80511, 21.802128, 21.801655, 21.790838, 21.788425, 21.7793, 21.775543, 21.76034, 21.745043, 21.738256, 21.73043, 21.708082, 21.70553, 21.70287, 21.702747, 21.697691, 21.697336, 21.69595, 21.695793, 21.692455, 21.680355, 21.673464, 21.667547, 21.662634, 21.63876, 21.634426, 21.6168, 21.606983, 21.592983, 21.592024, 21.56158, 21.561241, 21.552805, 21.550394, 21.548635, 21.537323, 21.530539, 21.522743, 21.514149, 21.50989, 21.506773, 21.500937, 21.492641, 21.474165, 21.471813, 21.468275, 21.458889, 21.447851, 21.444, 21.43512, 21.427305, 21.423264, 21.422272, 21.420883, 21.420574, 21.416723, 21.411257, 21.40217, 21.387695, 21.374088, 21.36678, 21.354523, 21.352036, 21.33721, 21.33268, 21.324305, 21.323221, 21.319736, 21.318316, 21.304663, 21.30107, 21.295773, 21.290907, 21.267958, 21.247862, 21.244965, 21.24106, 21.237875, 21.228788, 21.225508, 21.216875, 21.215359, 21.194443, 21.176388, 21.15189, 21.150543, 21.14927, 21.12822, 21.121561, 21.117233, 21.108446, 21.107447, 21.106035, 21.105055, 21.096802, 21.095037, 21.09161, 21.083216, 21.081198, 21.081081, 21.078653, 21.078653, 21.07599, 21.069004, 21.067066, 21.063126, 21.062664, 21.058313, 21.054522, 21.054325, 21.042763, 21.035173, 21.02762, 21.025265, 21.024448, 21.009464, 20.98616, 20.982752, 20.982752, 20.98125, 20.978598, 20.968712, 20.961697, 20.960163, 20.959812, 20.950586, 20.91074, 20.904816, 20.90321, 20.899014, 20.893682, 20.891024, 20.890594, 20.889679, 20.888899, 20.887342, 20.886349, 20.88481, 20.873827, 20.866272, 20.864988, 20.863983, 20.863655, 20.86259, 20.858372, 20.854782, 20.852709, 20.838326, 20.829706, 20.829456, 20.828121, 20.820017, 20.816189, 20.793201, 20.791294, 20.789833, 20.789524, 20.789427, 20.784197, 20.76983, 20.766577, 20.740704, 20.73971, 20.738033, 20.737185, 20.732864, 20.73003, 20.728765, 20.72206, 20.721924, 20.715853, 20.708061, 20.70787, 20.703571, 20.702477, 20.6902, 20.6873, 20.67017, 20.663467, 20.663383, 20.66114, 20.654911, 20.654057, 20.649736, 20.648361, 20.641045, 20.639315, 20.624977, 20.62276, 20.618767, 20.618137, 20.610945, 20.60392, 20.590107, 20.589909, 20.585772, 20.582762, 20.582218, 20.578997, 20.557667, 20.53765, 20.533312, 20.532232, 20.530926, 20.527344, 20.52567, 20.517271, 20.511751, 20.5102, 20.495739, 20.490759, 20.485605, 20.48441, 20.46927, 20.469265, 20.46746, 20.449596, 20.447836, 20.418142, 20.414375, 20.4076, 20.397186, 20.39638, 20.381836, 20.377903, 20.3759, 20.375263, 20.373245, 20.373241, 20.366703, 20.365093, 20.362537, 20.356335, 20.35556, 20.34659, 20.344181, 20.341463, 20.336357, 20.335476, 20.334806, 20.332233, 20.328178, 20.320911, 20.31765, 20.3162, 20.30927, 20.30663, 20.30339, 20.302525, 20.301666, 20.298107, 20.29689, 20.295437, 20.29418, 20.291504, 20.287401, 20.286816, 20.282333, 20.279827, 20.277554, 20.276764, 20.274885, 20.270906, 20.267992, 20.26775, 20.266342, 20.26495, 20.263226, 20.247997, 20.24615, 20.239683, 20.236906, 20.233273, 20.233034, 20.231829, 20.223782, 20.221107, 20.220278, 20.218023, 20.216997, 20.198244, 20.190918, 20.190454, 20.187569, 20.187527, 20.182919, 20.174557, 20.172625, 20.169445, 20.16731, 20.167282, 20.166178, 20.165943, 20.164722, 20.156975, 20.150993, 20.14731, 20.141203, 20.140799, 20.137447, 20.133923, 20.131508, 20.12496, 20.120155, 20.116287, 20.11326, 20.11288, 20.111217, 20.109163, 20.092243, 20.091143, 20.090698, 20.08537, 20.077534, 20.0709, 20.070074, 20.069305, 20.068453, 20.066011, 20.059212, 20.052618, 20.052227, 20.047915, 20.04503, 20.042803, 20.04244, 20.04152, 20.038183, 20.03596, 20.023703, 20.021235, 20.019918, 20.015785, 20.015596, 20.013845, 20.011602, 20.006374, 20.001627, 20.00099, 19.997906, 19.997555, 19.979153, 19.978104, 19.977905, 19.97168, 19.97115, 19.970997, 19.969254, 19.958658, 19.952637, 19.94613, 19.944437, 19.94391, 19.932678, 19.932503, 19.930922, 19.921755, 19.920752, 19.9174, 19.917038, 19.915815, 19.913593, 19.912706, 19.903513, 19.892982, 19.892544, 19.891378, 19.889677, 19.885525, 19.885311, 19.883743, 19.878065, 19.872086, 19.870768, 19.868176, 19.866537, 19.864565, 19.864239, 19.86307, 19.859404, 19.856781, 19.85621, 19.85621, 19.850166, 19.84844, 19.84659, 19.843485, 19.84229, 19.840067, 19.834686, 19.833675, 19.83028, 19.824076, 19.818687, 19.816612, 19.81528, 19.814497, 19.81356, 19.811977, 19.811771, 19.808931, 19.79263, 19.785957, 19.785234, 19.784159, 19.781525, 19.78031, 19.779903, 19.778362, 19.77362, 19.763227, 19.751743, 19.743328, 19.739962, 19.739782, 19.737476, 19.736185, 19.734364, 19.73275, 19.730804, 19.73071, 19.73007, 19.726847, 19.71781, 19.71405, 19.709677, 19.708313, 19.698744, 19.69098, 19.690407, 19.6902, 19.689772, 19.688545, 19.68798, 19.68361, 19.68329, 19.682009, 19.68113, 19.677008, 19.672825, 19.670872, 19.670662, 19.66754, 19.66742, 19.662182, 19.659624, 19.6561, 19.647156, 19.643171, 19.641678, 19.635529, 19.63454, 19.634378, 19.634125, 19.632332, 19.63207, 19.630146, 19.626472, 19.614714, 19.614666, 19.613752, 19.61107, 19.605825, 19.60524, 19.601618, 19.591427, 19.591019, 19.589405, 19.588821, 19.58374, 19.58237, 19.58062, 19.576807, 19.57232, 19.572262, 19.56964, 19.569304, 19.567055, 19.565834, 19.564451, 19.562414, 19.557428, 19.556005, 19.555788, 19.55148, 19.544672, 19.541714, 19.537354, 19.528013, 19.524923, 19.522467, 19.522142, 19.521343, 19.520948, 19.517097, 19.516706, 19.512823]}, {'id': '2515844138', 'text': 'multiobjective cooperative coevolutionary algorithm hyperspectral sparse unmixe', 'timestamp': 1490997600, 'rel_doc_ids': ['1964570608', '2004207873', '2022383869', '2030347721', '2039827010', '2084252873', '2125298866', '2126527280', '2147573707', '2157321686', '2295820431'], 'user_id': '2139563917', 'user_doc_ids': ['330616951', '1033655201', '1416942850', '1492535402', '1501873207', '1509481941', '1517400677', '1538629284', '1550604781', '1553804644', '1578012740', '1587552950', '1593094410', '1760775197', '1761267170', '1837902948', '1843801636', '1902936532', '1957145452', '1964069486', '1964967908', '1965126626', '1965607176', '1965882418', '1967414515', '1970483889', '1972731536', '1972744876', '1976659815', '1978373800', '1978567114', '1979550826', '1979822131', '1981263080', '1982666353', '1983257422', '1983489604', '1988723788', '1994661745', '1999218527', '2002189912', '2004651858', '2006313131', '2006570817', '2007005198', '2007250943', '2009186229', '2010790293', '2015216757', '2016322375', '2024367395', '2027499598', '2028087711', '2028151808', '2032079649', '2032338419', '2032552004', '2033656756', '2037152374', '2042141552', '2043993508', '2046861976', '2047243114', '2049585214', '2050953881', '2054564636', '2056281790', '2060446948', '2065905935', '2067501358', '2068898064', '2070813603', '2076639588', '2077433479', '2077744467', '2078412950', '2079461163', '2080073912', '2080858591', '2081539367', '2083114270', '2085687954', '2086860003', '2089277894', '2092091239', '2092892263', '2095306059', '2096180460', '2097464974', '2098777834', '2108615722', '2119859584', '2123047699', '2132036416', '2135107882', '2140619591', '2142755862', '2156330930', '2157156326', '2160018910', '2160021903', '2163928855', '2164800951', '2164911019', '2168549419', '2174636480', '2183148175', '2206604233', '2216748439', '2221448138', '2295862745', '2314669335', '2315503695', '2320144048', '2323772467', '2329749247', '2336953982', '2343098366', '2414744193', '2419125634', '2460312455', '2464384554', '2469003029', '2479340554', '2505484073', '2507974220', '2516616494', '2517579290', '2535605529', '2547390926', '2556806358', '2564843073'], 'bm25_doc_ids': ['2126527280', '2063069198', '1993335267', '2004594443', '2048056733', '1964570608', '2765455392', '1979634789', '2039827010', '2464427950', '2004207873', '2342750892', '2147573707', '2049418899', '2030347721', '1631938343', '2061937963', '2331319323', '2019190440', '2106642558', '1937669826', '1969238202', '2110990923', '2467741384', '2506064001', '1902936532', '2054854521', '2323672905', '2534774117', '2073455368', '1996001182', '2277598686', '2118112862', '2418263100', '1994726448', '2318512420', '2046875278', '2032944446', '2519375576', '2071898616', '817041652', '1974448798', '2326013034', '2328456116', '1992622214', '2169466597', '1531818640', '2119792302', '2137165245', '2092116045', '2029786966', '2148531828', '2081834705', '2021194005', '2027878671', '2109629159', '1988868707', '1995659258', '2293524743', '2159819721', '1991900070', '2056793468', '2327364376', '2084252873', '1560324652', '2163886442', '2033050675', '1968952662', '2028291849', '2949464013', '139212506', '2052954580', '240039221', '2533034223', '2116743594', '2059468996', '2137410567', '2344025572', '2119938444', '2547263678', '2123040824', '2085656297', '2125298866', '1902016676', '2043711943', '2318553756', '1988854580', '72355561', '1974512955', '2031485426', '1499945524', '2040399008', '2138235599', '2124781164', '2011276785', '2058484777', '1981939910', '2547840382', '1576357034', '2000882531', '2015548667', '2324845311', '2541520249', '2077239519', '1923622064', '2143265137', '1925324463', '1556363313', '1964409388', '2050041778', '1971168710', '2311995462', '2072924910', '2474792379', '2470203776', '1808742542', '2113976600', '1816429301', '2165407860', '2036745147', '2277068403', '2113125088', '2521669256', '2229654487', '2152513153', '2547812178', '2113596135', '2109361751', '2085733018', '1915139133', '2140629888', '2009576740', '2021523534', '1990057795', '2027357158', '5896900', '2042521440', '2109193644', '2161344247', '2516893097', '2097295283', '1965888395', '2536673347', '2119886624', '2119449590', '2000785725', '2562611250', '1993151028', '2158099176', '2004886822', '2256634236', '2324815542', '2099997159', '1961730324', '2949896994', '2135329030', '2162888866', '2490201121', '2066976481', '2061699712', '2077098488', '2127827417', '2097790609', '2494274259', '2031268467', '2067936154', '2054645422', '2167244571', '2020384796', '2020769413', '2008311097', '1524648708', '2045746004', '2137143026', '1969634684', '1555407049', '2076332129', '2148977782', '1977884755', '2162328319', '1983326850', '1966175096', '2004651858', '2150379994', '1514356443', '2047705660', '2300441005', '2055440460', '1785552625', '2156458885', '2170078675', '2020249414', '2068021559', '2312917021', '2003752781', '2073650477', '2041310692', '2048419331', '1898446294', '2078064725', '2032237655', '2039191759', '2250124065', '2113753426', '2003896345', '2317049507', '1545613946', '2015894711', '2032345535', '2034268775', '1995347266', '2517982728', '2025315075', '2057545537', '2963156861', '1988936098', '1976615758', '1976701003', '2114111347', '1520008972', '1990231296', '2129099774', '2063952947', '1494218887', '2084830040', '2141314769', '2160907264', '2019772884', '2144492104', '2088069360', '2129936995', '2034035291', '2095343758', '2174086746', '2042626896', '2119317657', '2081555128', '2025907697', '2051130913', '2005818237', '2100503315', '1988117583', '2112790824', '2032273682', '1683821609', '1524571335', '1558485752', '2008924446', '2044051588', '1485989326', '3012452093', '2058024781', '2076462645', '2060147006', '2342695286', '1989441657', '1971520148', '2098751572', '2177296861', '2329928123', '2169118302', '2133115251', '2207490041', '2041256455', '2963265857', '1982448609', '2170609420', '177097005', '2011315899', '2117176777', '2062532315', '2112008476', '2545779638', '2171996226', '2128869518', '2013132362', '2944774743', '2126616379', '2107219953', '2062050542', '1588264526', '2034528033', '2120248066', '2040190537', '2161219071', '2128990403', '2139987077', '2023518723', '2239643428', '2088543719', '1985627555', '2138418872', '2112873047', '2156262512', '2150473677', '2032809664', '2121955539', '2075962781', '2060629443', '1975295194', '1999660298', '2115573141', '2040325979', '1965340435', '2054973482', '2148282317', '2118038900', '2116639333', '264067352', '2037328426', '1974733207', '2510660981', '2092840066', '2124821796', '2127523161', '2045049599', '2061176056', '2127934268', '1967981290', '2032861771', '1990118276', '2059876025', '1982618228', '1841053374', '2102272775', '2055282848', '2152825021', '2121205058', '1998415360', '2089298795', '1555155554', '2043651068', '2041463442', '1999731081', '2096296424', '2146148434', '2019589768', '2099396514', '2174740601', '2155447189', '2151352870', '1516948749', '2127968282', '2024623434', '2002086429', '2172492417', '15752843', '1896927632', '2134098137', '2125118959', '2085830967', '1987891568', '2963069169', '1900867691', '1562163564', '2120859255', '1603120003', '1997529414', '2163721270', '2103782255', '2012452381', '2071438878', '1536019064', '2548343949', '1999331929', '2046729680', '2063790512', '2072329731', '2154970996', '1574723274', '1523282099', '1995770467', '1990151006', '2260132511', '1974563921', '2090820189', '1812472129', '1994285949', '2151640445', '1963659868', '2108917905', '2080696007', '2075394305', '2467427493', '2114222203', '2100169396', '2110093134', '2022202941', '2247723941', '2073355897', '2143500192', '2482912082', '2003701329', '2137055319', '2115665136', '2025090977', '2295820431', '2033529478', '2100186518', '2285790097', '2019268445', '2766591767', '2084724634', '1982307148', '2071747462', '1181348626', '2000114968', '2004184783', '2007921299', '285039866', '2070748536', '1987373274', '2314465365', '1989113490', '1988428080', '2548252967', '1676079609', '2030114983', '1985690611', '1985728473', '2088259770', '2141991709', '1483342903', '2146344928', '1989336400', '2060027068', '2022063180', '2097211423', '2080920313', '2033918435', '2136392120', '2168594727', '1986909036', '1938476487', '2038266628', '2148983294', '2049783455', '2042642342', '2245855345', '2140501674', '2075830350', '2109343601', '2139766511', '2054892132', '2167408677', '2020685381', '2145597631', '1579515978', '2020320008', '2121879440', '1584305781', '2078033978', '2169877245', '2324784787', '2060403006', '2113138229', '2562867554', '2140982131', '2035057253', '2165167536', '2221899823', '2111560151', '1949713295', '2169974854', '2073849130', '2086903604', '2142202560', '2063755563', '2338894643', '2049444988', '2083170051', '1992367803', '2547826358', '2109533493', '1968272468', '2031591932', '2097915756', '2153122975', '2020128215', '2140619591', '1973944513', '2018375997', '1562256965', '2070424424', '2097808723', '2107200702', '2305655979', '2146707163', '2155052878', '2150677150', '2048365972', '2134317247', '2140267146', '2144348684', '1830857935', '2168647154', '1991480333', '2153885347', '2083462063', '2019149505', '2056196874', '2131659181', '2163957348', '2125678373', '2012810082', '2208952826', '2106506049', '2171314936', '2108230487', '2115484215', '2046849232', '2059031774', '2327302159', '2084975575', '1982611432', '2140692918', '2014135451', '2041320831', '2099703033', '2140823438', '2037206506', '2156598774', '2138366237', '1979970913', '2082827499', '2067466063', '2031249287', '2135350092', '2117027921', '1727416960', '2122880092', '2193409108', '2097777682', '2120273653', '2157321686', '2165542909', '1965853007', '2007735486', '1989408781', '2070504962', '2030585344', '798608864', '2096295989', '2010215118', '1974090011', '2415341181', '2061356527', '1973217443', '2070781258', '2149768178', '2410437504', '2032837901', '2136531170', '2078183677', '2156517622', '2080081021', '2167256583', '1969455435', '2122702721', '2058590043', '2539574252', '2111288219', '2964257934', '1980525259', '2154536501', '137381211', '2032117087', '2040812261', '2134613818', '2194818953', '1535575336', '2047048633', '1986870493', '1504426916', '2074594734', '2055077910', '2069561558', '2044636610', '2066554465', '1988802668', '2513923277', '1569799663', '1492461934', '2060467239', '2096163458', '2112656986', '2044837670', '2098811477', '2545029332', '2162842940', '2000590751', '1993694937', '2136078175', '1755724306', '1989367256', '2116153733', '2027281017', '1968219458', '1668783296', '2087649910', '2187485243', '2149936180', '1963881070', '2164954691', '2322796807', '2053836552', '2089669097', '2008089719', '1585518786', '2089044377', '2033146057', '2406076604', '2507855991', '2540114266', '2015489041', '2169210068', '2253140532', '2315100696', '2170659515', '2020084268', '2019120960', '92719493', '2157590492', '1981166859', '2039596145', '2112771280', '2130815286', '2140039936', '2021415354', '2011149328', '2003525759', '2110844826', '2548730065', '2065548527', '2107222994', '1996699294', '2058840890', '2128323493', '2538480301', '2100975942', '2963646316', '1928929295', '2132791360', '2171992352', '2088657558', '2043923646', '2089076201', '1570312920', '2016666536', '2122520938', '2125356185', '2074527119', '821066772', '2121123096', '1555462461', '2046491030', '2089381226', '1573690241', '2016201611', '2044745327', '1995299967', '2090862158', '2085821381', '2028654068', '2048281487', '2005905802', '2025511177', '2013645073', '2185811375', '2059650581', '2011631302', '2271716409', '2413911167', '2073936830', '2066984615', '2129272928', '2104391405', '2019274094', '2167586448', '2092363223', '2486845456', '1987483041', '2029440826', '2545286754', '2008337397', '1977355761', '2053675509', '2087263574', '2236801581', '2762869430', '1975845054', '1984210642', '2037143195', '2029470596', '2291033752', '2122171774', '2078316667', '2010359881', '2094264433', '2026462813', '2077406229', '2085469741', '2296544499', '2290946060', '2050728456', '2313932751', '2103414672', '2138304554', '2089769267', '2112281011', '2054865467', '1971335521', '2517481871', '2085589130', '2021919383', '2076123250', '2127077132', '2547896425', '2089892215', '2002266849', '2070598848', '2022523171', '188646088', '2161587018', '2136530094', '2082568135', '2166040020', '1921147317', '2169924573', '2170407643', '188692745', '2625172473', '2082619589', '2083384375', '2141494774', '2141544611', '2134025368', '1503912227', '2132956823', '2157313449', '2021527468', '2012002893', '2123886106', '2009286595', '2022383869', '2041753695', '2168014890', '2161946867', '2033667660', '2007250943', '2050173686', '2079390300', '1526470323', '2140792529', '2102203042', '625476304', '1976935203', '2060704570', '1979956016', '2006105982', '2465611810', '172791972', '2117678289', '2069563014', '2006419537', '2025884142', '1541317945', '2048625826', '2766547421', '2087661330', '2171600748', '2079358075', '2518045224', '2024564856', '2543410723', '2061833653', '2093059173', '2013079618', '2151164824', '1978567114', '2106029803', '2161831948', '2342009893', '2170848195', '1987531056', '2126025632', '2117359632', '2168963512', '42050771', '2471333453', '2517660364', '2051358843', '2099961512', '1985774233', '1965548396', '2079539078', '2051160666', '2071980259', '2001096515', '2135109884', '2004811928', '2539569152', '1512435724', '40671998', '1985551005', '2005445218', '2053631917', '2111963787', '1988627994', '2114194579', '2069113380', '2000929155', '2171566342', '2767194849', '1593447321', '2141143788', '1557114726', '1564659586', '1519610060', '2463773089', '2552171562', '2021086890', '1480663026', '1991099438', '2048685544', '1938187040', '1825488653', '2120524916', '1503530273', '2149591382', '2016147261', '2116776498', '1997565609', '2118090562', '2100740156', '2140702875', '2542048516', '2414009677', '2039025707', '2048832139', '580864916', '2029754495', '206024', '1991360699', '2213389309', '2126241286', '2024078374', '1512089703', '2095745840', '2143416511', '2141630781', '1981710090', '2047348370', '2496981643', '2139173628', '2037590387', '2167542034', '2200474412', '2153753491', '2101442390', '2136804075', '1957094454', '2009064434', '2131959287', '2064736001', '1596882128', '2015826095', '2073774279', '2105139690', '2083480069', '1537224597', '2170406089', '1968536280', '2173008755', '2322575893', '1741870144', '2475283175', '2100738313', '2087537126', '2152438175', '2041733866', '2293682645', '1995295300', '2334023959', '2056302425', '2131725398', '2323917763', '1490228308', '2125126993', '1897878572', '2115465207', '2105002599', '2087175427', '2315347323', '2192010520', '2021755911', '2085919540', '2166864699', '2099675430', '1974933922', '2411930473', '2509021534', '2118816472', '1916874600', '2951243444', '2029124499', '2086758789', '2122367764', '1987924114', '2095665557', '2083016737', '1973046251', '2121737160', '2126972815', '2012255037', '2086505477', '2072425335', '2060600859', '2063071687', '2964216914', '2108218391', '2131614384', '2142458747', '1483737994', '851774843', '220610738', '2091983493', '1968845542', '2155745915', '2134973369', '2160258952', '2130267836', '2061443113', '2002701929', '2130835014', '1507792554', '2024328388', '1998700518', '2332761596', '2165021728', '2288329408', '2118587583', '2164488484', '2151196296', '2075335910', '1963408805', '2149217960', '2132086463', '2295576075', '1965734557', '2151992816', '2085529604', '2048649979', '2169781749', '2005459214', '2008607892', '1979552216', '2221166316', '2056621966', '2062480014', '2145883888', '2201114316', '2134337254', '2289756263', '2056605897', '1995762162', '2008402696', '2506106863', '2067815145', '2168534096', '2056242352', '2031007444', '2073897310', '2074871763', '229472839', '2430545387', '2087459822', '1556200398', '2520430674', '2006210775', '2129698522', '1971891445', '2201252054', '2335306109', '2102614393', '2041307939', '2034643293', '2159988591', '2031735032', '2190646811', '1983140909', '2120832559', '2009109609', '1980763639', '2128090514', '1997210330', '2462946880', '1980391875', '2054005353', '2166106466'], 'bm25_doc_scores': [37.254684, 36.649765, 36.514156, 35.749275, 35.482986, 35.466633, 35.122993, 35.07591, 35.01129, 34.917847, 34.897427, 34.727276, 34.724613, 34.66961, 34.49928, 34.49525, 34.446404, 34.13731, 33.94901, 33.936203, 33.85742, 33.70123, 33.640373, 33.57481, 33.547966, 33.536934, 33.527607, 33.354355, 33.35323, 33.350994, 33.339344, 33.26927, 33.197197, 33.119015, 33.1157, 33.11493, 33.103577, 33.073235, 32.798748, 32.744343, 32.62648, 32.57539, 32.381004, 32.290543, 32.284843, 32.232475, 32.1926, 32.185013, 32.150246, 32.056538, 31.9871, 31.894062, 31.880854, 31.867912, 31.808659, 31.701502, 31.568829, 31.535324, 31.531559, 31.408463, 31.166496, 31.131142, 31.119204, 31.091843, 30.999567, 30.850628, 30.836569, 30.796284, 30.786814, 30.744146, 30.720879, 30.670967, 30.56566, 30.422474, 30.417446, 30.391214, 30.385117, 30.269218, 30.266243, 30.187515, 30.022303, 29.913685, 29.850735, 29.822388, 29.71741, 29.559992, 29.47279, 29.414278, 29.34378, 29.299255, 29.251461, 29.159573, 29.15067, 28.929523, 28.89317, 28.889635, 28.874182, 28.865282, 28.768452, 28.735863, 28.710564, 28.666016, 28.590805, 28.545729, 28.485651, 28.373558, 28.309942, 28.27876, 28.24243, 28.111523, 28.014107, 27.897442, 27.827843, 27.824635, 27.813915, 27.75032, 27.697515, 27.655815, 27.631954, 27.625261, 27.528143, 27.512539, 27.512054, 27.511702, 27.510227, 27.450903, 27.428417, 27.413157, 27.398699, 27.389416, 27.378513, 27.354862, 27.313398, 27.290981, 27.286316, 27.15219, 27.09096, 27.056986, 27.031044, 26.977823, 26.923119, 26.859512, 26.851587, 26.845566, 26.837109, 26.778675, 26.76559, 26.633043, 26.628801, 26.59818, 26.577991, 26.523659, 26.514284, 26.51172, 26.51172, 26.48947, 26.488813, 26.462696, 26.461338, 26.452682, 26.409683, 26.387806, 26.376144, 26.342754, 26.321468, 26.300776, 26.25131, 26.225897, 26.224562, 26.18926, 26.173553, 26.173553, 26.172426, 26.169031, 26.156162, 26.112852, 26.06426, 26.042751, 26.009995, 26.00095, 25.98788, 25.949692, 25.937168, 25.918362, 25.91584, 25.904215, 25.88849, 25.885168, 25.880207, 25.871283, 25.839867, 25.831276, 25.82994, 25.799343, 25.783447, 25.766054, 25.760653, 25.657959, 25.648384, 25.587502, 25.57386, 25.57386, 25.56931, 25.499624, 25.492462, 25.490074, 25.489239, 25.464405, 25.44837, 25.444365, 25.38998, 25.389484, 25.362787, 25.335537, 25.32962, 25.32962, 25.298962, 25.269947, 25.245842, 25.23982, 25.207975, 25.192774, 25.16699, 25.146463, 25.140854, 25.094563, 25.0597, 25.05801, 25.025515, 24.99517, 24.963453, 24.93324, 24.931131, 24.929909, 24.910873, 24.879263, 24.877026, 24.847216, 24.803917, 24.787474, 24.76824, 24.765203, 24.762842, 24.74401, 24.740576, 24.70277, 24.69205, 24.691576, 24.691576, 24.691576, 24.683794, 24.683794, 24.663553, 24.6601, 24.637188, 24.622042, 24.622042, 24.593063, 24.58095, 24.58095, 24.577715, 24.57321, 24.57321, 24.561983, 24.559776, 24.555956, 24.551928, 24.532621, 24.52078, 24.498287, 24.496168, 24.487457, 24.487385, 24.47785, 24.433907, 24.4232, 24.374363, 24.37048, 24.367085, 24.36252, 24.360376, 24.343397, 24.342052, 24.341526, 24.315859, 24.312187, 24.310392, 24.30425, 24.263823, 24.258636, 24.239239, 24.233967, 24.2285, 24.18847, 24.185307, 24.183926, 24.183418, 24.177855, 24.163954, 24.142843, 24.13994, 24.138988, 24.133667, 24.130804, 24.099163, 24.09036, 24.09036, 24.082102, 24.0797, 24.071754, 24.065174, 24.065174, 24.050713, 24.025936, 24.018871, 23.999563, 23.999563, 23.986917, 23.959154, 23.944788, 23.944626, 23.943354, 23.943354, 23.92987, 23.910881, 23.89736, 23.8876, 23.874245, 23.859114, 23.838463, 23.824419, 23.81762, 23.782894, 23.77868, 23.776157, 23.776157, 23.767551, 23.74915, 23.717165, 23.717165, 23.693192, 23.693192, 23.67853, 23.666044, 23.659876, 23.659874, 23.62775, 23.627308, 23.623188, 23.62308, 23.606703, 23.583704, 23.576172, 23.561325, 23.551159, 23.51213, 23.508442, 23.493164, 23.480194, 23.453442, 23.449516, 23.449308, 23.439232, 23.4151, 23.410397, 23.374556, 23.365803, 23.345161, 23.324505, 23.302095, 23.258415, 23.258224, 23.224365, 23.219534, 23.215027, 23.18733, 23.185562, 23.17896, 23.165668, 23.163113, 23.163113, 23.146362, 23.14005, 23.12788, 23.123264, 23.113014, 23.089718, 23.079899, 23.062944, 23.052181, 23.039911, 23.03829, 23.0279, 23.01323, 23.01323, 23.01323, 22.99607, 22.99607, 22.985168, 22.981705, 22.976793, 22.962595, 22.924557, 22.907234, 22.898344, 22.894878, 22.861761, 22.855232, 22.848171, 22.843317, 22.79412, 22.790058, 22.789505, 22.78814, 22.778748, 22.767193, 22.762856, 22.738077, 22.73315, 22.731596, 22.729702, 22.706953, 22.706533, 22.695051, 22.674189, 22.671684, 22.638586, 22.617342, 22.617342, 22.582655, 22.575089, 22.564453, 22.554188, 22.55073, 22.49424, 22.476967, 22.474257, 22.445303, 22.41898, 22.417938, 22.413664, 22.413664, 22.412338, 22.412338, 22.411795, 22.403275, 22.385937, 22.367807, 22.367096, 22.318298, 22.307148, 22.303991, 22.303991, 22.299122, 22.297716, 22.297716, 22.295052, 22.289463, 22.289463, 22.285019, 22.28341, 22.258614, 22.257004, 22.238613, 22.170582, 22.166168, 22.106974, 22.106974, 22.106974, 22.097837, 22.089622, 22.087196, 22.087196, 22.085491, 22.085491, 22.064163, 22.042267, 22.039116, 22.007816, 21.97144, 21.97073, 21.96887, 21.947819, 21.93064, 21.904953, 21.892242, 21.87474, 21.872736, 21.870562, 21.870562, 21.862297, 21.858353, 21.838709, 21.835627, 21.821964, 21.81857, 21.804323, 21.793335, 21.787792, 21.787792, 21.787792, 21.784504, 21.764072, 21.762989, 21.757126, 21.740803, 21.74038, 21.740133, 21.736383, 21.720016, 21.720016, 21.720016, 21.694153, 21.67723, 21.67723, 21.67723, 21.650791, 21.647165, 21.628132, 21.626755, 21.599993, 21.574308, 21.552525, 21.552265, 21.54655, 21.540226, 21.528389, 21.509115, 21.509115, 21.504517, 21.489319, 21.484581, 21.484444, 21.476837, 21.473112, 21.456936, 21.44947, 21.443764, 21.441175, 21.435268, 21.421478, 21.415527, 21.410095, 21.400082, 21.39426, 21.388596, 21.384632, 21.376656, 21.372185, 21.353367, 21.351122, 21.342651, 21.319231, 21.319231, 21.319231, 21.312592, 21.30333, 21.291605, 21.275845, 21.257774, 21.246061, 21.224598, 21.224598, 21.224598, 21.206448, 21.204811, 21.20001, 21.195011, 21.185179, 21.18051, 21.180346, 21.174728, 21.164814, 21.162876, 21.156586, 21.138035, 21.13007, 21.120653, 21.096909, 21.075228, 21.069405, 21.063297, 21.062803, 21.060839, 21.060839, 21.059042, 21.053825, 21.038668, 21.038668, 21.011, 20.950031, 20.933537, 20.92875, 20.92875, 20.908176, 20.90793, 20.904642, 20.902979, 20.892288, 20.890528, 20.882164, 20.86776, 20.857779, 20.85302, 20.84693, 20.816235, 20.816235, 20.811516, 20.797913, 20.795376, 20.795376, 20.792969, 20.781797, 20.760658, 20.757912, 20.749891, 20.727667, 20.71263, 20.7112, 20.7112, 20.705404, 20.705404, 20.705404, 20.693523, 20.682392, 20.678825, 20.653713, 20.643627, 20.637192, 20.618286, 20.615187, 20.610355, 20.610355, 20.604214, 20.604214, 20.600952, 20.600952, 20.599949, 20.599949, 20.571167, 20.571167, 20.559797, 20.535805, 20.51683, 20.511639, 20.509836, 20.501911, 20.501911, 20.49744, 20.48275, 20.469347, 20.469122, 20.461515, 20.461493, 20.45333, 20.437668, 20.415417, 20.393772, 20.3915, 20.3915, 20.380653, 20.380484, 20.378216, 20.372997, 20.357018, 20.35124, 20.35124, 20.349882, 20.320377, 20.314943, 20.314404, 20.313993, 20.290062, 20.289265, 20.27525, 20.274164, 20.24112, 20.238678, 20.238678, 20.214357, 20.19424, 20.191204, 20.187407, 20.186281, 20.1749, 20.1749, 20.1749, 20.1749, 20.157715, 20.154198, 20.152311, 20.144875, 20.141973, 20.132824, 20.123169, 20.121914, 20.121914, 20.11652, 20.116034, 20.115988, 20.086151, 20.086151, 20.084505, 20.074076, 20.073835, 20.064995, 20.034248, 20.032457, 20.025537, 19.99707, 19.98514, 19.979591, 19.975128, 19.935461, 19.913374, 19.912764, 19.883015, 19.87833, 19.8707, 19.863308, 19.863308, 19.857727, 19.849186, 19.847845, 19.803886, 19.79895, 19.798777, 19.790787, 19.787466, 19.787466, 19.779709, 19.776176, 19.772726, 19.767948, 19.752413, 19.752413, 19.734716, 19.715633, 19.711843, 19.70921, 19.67629, 19.669998, 19.669998, 19.669668, 19.669548, 19.664429, 19.664429, 19.663445, 19.655754, 19.64623, 19.64297, 19.641197, 19.641197, 19.631367, 19.608492, 19.600752, 19.600159, 19.599907, 19.585608, 19.584692, 19.581789, 19.578579, 19.572409, 19.570484, 19.570045, 19.566208, 19.557056, 19.553055, 19.552977, 19.548906, 19.538311, 19.537949, 19.53601, 19.510319, 19.49907, 19.494205, 19.491383, 19.485361, 19.473476, 19.471642, 19.465034, 19.465034, 19.465034, 19.464863, 19.452055, 19.417599, 19.40941, 19.403011, 19.385448, 19.361221, 19.356714, 19.355492, 19.355492, 19.350113, 19.348606, 19.342405, 19.34078, 19.33291, 19.320705, 19.318407, 19.31763, 19.310953, 19.305334, 19.291256, 19.283375, 19.275997, 19.274183, 19.274183, 19.259037, 19.250898, 19.249462, 19.241373, 19.21884, 19.21346, 19.201794, 19.194204, 19.194204, 19.191723, 19.187363, 19.17847, 19.17833, 19.163038, 19.161036, 19.15697, 19.141727, 19.136105, 19.133135, 19.129387, 19.129387, 19.125923, 19.112867, 19.112867, 19.105774, 19.101316, 19.098103, 19.082916, 19.07798, 19.07646, 19.071854, 19.057573, 19.05618, 19.050383, 19.043783, 19.042149, 19.03318, 19.020014, 19.01627, 19.01086, 19.003244, 19.000174, 18.986118, 18.98405, 18.969862, 18.957071, 18.948082, 18.948082, 18.935661, 18.92698, 18.907833, 18.904322, 18.895958, 18.895958, 18.895958, 18.894554, 18.894312, 18.893902, 18.88978, 18.889061, 18.886631, 18.882385, 18.882385, 18.871298, 18.869587, 18.85356, 18.852114, 18.84726, 18.846607, 18.843401, 18.837502, 18.830784, 18.827984, 18.817026, 18.81055, 18.808315, 18.799988, 18.798737, 18.797112, 18.796337, 18.795586, 18.794575, 18.792484, 18.790949, 18.784607, 18.780035, 18.757505, 18.745434, 18.736893, 18.722332, 18.705685, 18.702555, 18.690918, 18.68888, 18.673265, 18.645935, 18.645935, 18.643452, 18.643452, 18.632088, 18.627865, 18.622568, 18.6022, 18.594418, 18.57582, 18.57582, 18.573725, 18.573725, 18.572908, 18.568829, 18.560768, 18.549316, 18.537521, 18.536486, 18.531736, 18.527458, 18.527134, 18.510979, 18.509007, 18.483744, 18.476204, 18.474209, 18.474072, 18.472242, 18.470812, 18.46668, 18.44435, 18.444273, 18.433434, 18.432882, 18.422354, 18.416395, 18.416395, 18.40543, 18.368137, 18.36408, 18.362438, 18.362333, 18.357826, 18.352282, 18.350266, 18.347336, 18.342102, 18.338251, 18.326244, 18.319489, 18.305794, 18.303696, 18.298456, 18.298456, 18.29229, 18.29132, 18.28264, 18.255888, 18.232376, 18.223942, 18.219196, 18.218931, 18.214094, 18.209818, 18.209818, 18.209818, 18.207447, 18.197319, 18.194923, 18.189583, 18.177807, 18.169523, 18.162834, 18.159555, 18.158983, 18.150003, 18.141136, 18.12179, 18.12179, 18.118322, 18.101292, 18.101292, 18.098713, 18.09496, 18.071505, 18.064547, 18.062792, 18.060242, 18.052153, 18.052153, 18.050076, 18.04935, 18.044786, 18.026218, 18.025734, 18.02009]}, {'id': '2523498403', 'text': 'forecast daily stock market return use dimension reduction', 'timestamp': 1483225200, 'rel_doc_ids': ['1086310491', '1995456904', '2004463884', '2005346797', '2021938316', '2059852492', '2070181657', '2073912178', '2078368457', '2083036265', '2112549957', '2120911092', '2263406265', '2345563409'], 'user_id': '2075225989', 'user_doc_ids': ['1672771681', '1945166973', '2002376945', '2005346797', '2006596904', '2011782945', '2013675302', '2019176006', '2042105482', '2042779839', '2065466048', '2071128954', '2073172233', '2088946350', '2092849574', '2120911092', '2140360763', '2160810870', '2172288312', '2178921228', '2182293839', '2345563409', '2345835628', '2540411432'], 'bm25_doc_ids': ['2142717356', '85467525', '44976017', '2019678334', '2304517173', '2038299699', '1679881948', '1980458013', '1924561663', '2263406265', '2085210082', '1740533156', '2149159862', '2149640509', '2004463884', '2156706050', '334228768', '2400770063', '1980486587', '2129294676', '1535737509', '2016801461', '2168816734', '1523177310', '1536349692', '2160183711', '2270100336', '1978520392', '1608670167', '1967647846', '1831255594', '2064157633', '1979395212', '2151107171', '2136225293', '2126048770', '2016116986', '1581492247', '2028827524', '2126617136', '2005424446', '1875160079', '2049916782', '2111741143', '2309329010', '2148774343', '2086384288', '10307964', '2053615983', '2008803927', '1969865884', '2064709414', '2110612772', '2112549957', '2110348891', '2124259195', '2112880279', '1988715797', '2111732212', '2487144183', '157959680', '1977229415', '92282897', '307184769', '77341351', '1972734500', '2127278225', '2035942174', '2412923674', '1603700568', '1905147732', '1815264562', '2152743378', '1995844635', '2140360763', '2059135023', '2085150224', '1979083669', '56865669', '1504076621', '111527205', '2032771991', '2164955860', '2463080129', '2271008577', '2088380833', '1571138948', '1866279363', '2000653979', '2330443210', '1973965353', '2092225085', '2081085020', '2163273654', '2275516988', '1584895601', '1978775764', '2099370025', '1983430993', '2010040079', '2282463101', '2429027831', '47153464', '2140308831', '285159233', '1564933795', '2000729445', '1508061413', '1966922786', '2137167428', '2202071898', '2094392844', '1443712405', '1934559266', '2082854060', '2092130541', '1984315581', '2134927658', '1885899143', '2027632322', '2171544957', '1999282625', '2160642935', '1507191851', '2082078263', '1999324399', '1996059829', '2599534045', '1982159761', '1896657160', '1577915501', '2062090301', '2268394201', '2130586113', '2543479447', '2035403469', '2011782945', '2001405494', '2056981468', '2120911092', '1601960377', '1982547605', '2035023489', '2040088269', '2347175495', '2105772438', '1996602931', '2514033443', '1979384757', '2017201611', '2059852492', '2150703107', '2169533279', '1991346019', '2080152090', '1965100508', '175678083', '2001280190', '2134183041', '2017812666', '2001759806', '1858254751', '1967444025', '1976580043', '1945609842', '1963606230', '204857277', '2385866669', '2094882603', '2473416588', '2038221107', '2104785403', '2024681729', '2026182499', '1995132292', '2128467430', '2156219032', '1977607860', '2033949374', '2043406008', '2007614242', '2151986948', '1499931993', '1970411806', '2044828825', '2061550403', '2086961759', '2095284702', '2121592620', '2065119570', '2032597011', '2024883025', '2170854601', '2021693371', '1527421868', '1984500452', '2110329955', '2043626568', '46513916', '2045014179', '2206918413', '1966093719', '2152255499', '2026320748', '2034941470', '2046321381', '1918248777', '1971619052', '1983965282', '2044828741', '1493779890', '1599085411', '1533654746', '2144120967', '1536371749', '2097536910', '2292541499', '2038844583', '2166788055', '2094304287', '1980223370', '1541145016', '1982879133', '2311584084', '2046346480', '2117391284', '2394680095', '2055405938', '2043583889', '1571908831', '1579680186', '2018786003', '1990430259', '1836066174', '2108360946', '2033799950', '2017055324', '1977182634', '2029545326', '2273569893', '2009880058', '2131132750', '141543995', '2125826044', '2128399825', '2086799485', '1086310491', '2056704409', '2059477555', '2153941920', '78204330', '2145097934', '1512609712', '2085926963', '2155031998', '2116242695', '2065706705', '2140728436', '2281767479', '2993716753', '2056955279', '1641999427', '1763828535', '2018725600', '1997994299', '2090408119', '2017234030', '2149942751', '2542516223', '2111204393', '2007682563', '2323186289', '2108591703', '2002071019', '2103864935', '2291683584', '2054118498', '2010628845', '2157850792', '1599836326', '2165995949', '1982796656', '2006942814', '1996480982', '2077447589', '2315752500', '1531956449', '2089247440', '2270937275', '1966577984', '843040289', '2019272463', '1993116502', '181010784', '2051931130', '2026241661', '2144217557', '2063759248', '2169868011', '2101466958', '1535343944', '2037730387', '2125561531', '2124640598', '2063341363', '2071825902', '2043805990', '2027212376', '1971484679', '2070181657', '2066916223', '2102438757', '1990235732', '1968447126', '1935825981', '1511595559', '2036774117', '67735642', '2107094664', '2300916226', '2043964177', '1521294532', '1984829154', '2027735148', '1993924666', '2080210651', '1969428964', '2035526095', '2063784014', '2153627311', '2059351869', '1878833954', '2093464379', '1577305894', '2052084490', '1979082890', '2962818906', '2036714088', '2018384741', '2087412183', '2036895384', '1995141167', '2121367231', '1973384092', '2180407328', '1978049199', '2157366207', '2094515478', '2029727871', '1494411021', '1527813886', '1521558591', '2168392282', '1575726819', '2150068769', '2111556359', '2037659946', '107640567', '2107257765', '2181402761', '2101420345', '2138129811', '1995306760', '1968380673', '2080465999', '41130631', '2086694651', '2040274108', '2056539886', '644764664', '2033390701', '2068451972', '2021435663', '2100270817', '2161889846', '2041730429', '2021938316', '762566604', '1974559920', '2149613662', '2006861519', '99151699', '2181345418', '2067712825', '2131773610', '2289642014', '2078368457', '2053844660', '2103997983', '2106517448', '2172213518', '2054062736', '2006514975', '1491654721', '2015975090', '2115839797', '2289850322', '2101455925', '1980001610', '1995572748', '2057070467', '2011527669', '2542188015', '2149223397', '2148338846', '2168567680', '166789083', '2139205400', '2087186620', '2028919963', '2091084277', '1789983628', '1588805258', '2073912178', '2067147912', '2082558154', '1568276906', '1994873482', '1806663855', '1603872091', '2163952404', '2272642392', '2162936541', '2046499088', '1976202613', '2041906115', '2094257667', '2950127671', '2002203871', '2053612364', '2109181216', '2171567624', '2014341469', '2397149446', '2165926399', '1495461120', '205689123', '2333424759', '2095430958', '247222457', '2121526280', '2089809028', '2035330366', '2060180654', '2251147127', '2117921312', '1580469992', '1967234480', '109563165', '2236200778', '2094741156', '1571948804', '2092583488', '1963732168', '2133572338', '1657409404', '1670923411', '855508711', '2053296404', '1494292579', '2007424755', '78022471', '2024220749', '633676582', '2094042860', '2115435858', '1999233885', '2172147451', '2171865949', '2107420211', '2508626809', '2169889651', '2135229256', '2014394327', '415659269', '2056539411', '2128545275', '2042287092', '2124408169', '2093332904', '2273538302', '2013722099', '2078309414', '2231496021', '2038227460', '1997049341', '2533443122', '2083036265', '1980850818', '2548950555', '1690327809', '2093804588', '2108570858', '1484700953', '2110620429', '1973051354', '2049451480', '2074737245', '2035260928', '2516768646', '1968904637', '2058115783', '2164933932', '1989780968', '150277858', '2165335923', '1843063765', '200045268', '2035525938', '1532696436', '2094882164', '2216930980', '2083075007', '2131146916', '2108947007', '2034192144', '1976578922', '2027239050', '2142964630', '2167010855', '1640622893', '2088018054', '2063008284', '2154558287', '14152741', '99918501', '1602613182', '1895650610', '1482672013', '2101269184', '2043433345', '1500717766', '2220775719', '2038370770', '1986890389', '1542975340', '2063009876', '2034849402', '2052381604', '2110322343', '2517580759', '2059229600', '2069454342', '2159456841', '2064395535', '2084353205', '2135540367', '2142859226', '1508463275', '1044059734', '2005405732', '1499848614', '2096797953', '2114754400', '1971860259', '2171916840', '1937425859', '2084264176', '1968425925', '2121683996', '2016590704', '2027770345', '2063630798', '1979637725', '2034556731', '2063656543', '2017715655', '2031591846', '1995456904', '1547387272', '2104096268', '2053997475', '2125804487', '2021947909', '2100526182', '2155321204', '1998856889', '2187808597', '2117818052', '114054022', '2159475781', '2006354011', '2118750342', '2003942338', '2006742092', '2032292402', '2045585046', '1555167785', '2163349176', '2009930896', '2001185716', '2126388159', '1899697322', '1597792062', '2071437943', '2022597957', '2065928975', '1688', '2115451563', '2170448660', '2135349957', '2057457088', '2139810397', '2071831003', '2171943708', '2027431813', '1879441965', '2141673418', '2047279556', '2103359683', '1607023315', '2156980348', '2171191425', '2140773067', '2024119477', '1508819654', '1992660275', '2011394589', '1513004489', '1983770023', '2147054261', '2089594275', '2087885584', '2140305729', '2104384848', '2138246095', '2117801445', '2103416405', '1969336021', '1988242608', '1973152450', '2135791301', '2161908013', '1971995734', '2087463866', '2070034262', '2040413550', '2117057028', '2017537474', '2167404758', '2057648740', '1566741890', '2012041576', '2088205326', '2171315676', '2266743991', '1579499405', '1987035897', '2103879222', '2134377618', '2064850675', '2132578118', '1558502133', '1993887080', '2001395117', '1981878077', '2101527929', '1556091388', '2104417648', '2090152599', '2072664345', '1981975946', '1643877990', '2324196090', '1974891822', '1993565439', '2163707987', '2285855825', '2121690610', '2135998982', '2008165846', '2140257550', '1994254749', '2132635375', '1516192242', '1519308093', '2164438627', '2148929890', '2152710485', '2049878772', '2167290928', '2166737125', '2008938188', '1913351238', '1968914532', '2059047370', '1481132062', '2079747853', '1984335229', '2214106063', '2159265135', '1809255060', '1563759807', '2171745074', '2158689233', '1965806369', '2094875253', '2117678451', '2049731335', '2275806125', '1958819020', '2080984056', '136721157', '2156619850', '2166253460', '1600080087', '2260348130', '1190326552', '2021821837', '2065455158', '1734243050', '2076333559', '1483492079', '1623020006', '2050937273', '2129413312', '2003735580', '2040705100', '2090258464', '1969040644', '2116428378', '2029586474', '2003647412', '2541075267', '789578048', '2024282289', '2001751530', '1981531537', '2150706345', '330429230', '1965484229', '2062030044', '415945829', '2066995518', '1973324839', '1997342558', '2032762054', '2124204568', '1663569564', '1999021852', '2003815806', '2001895632', '2116751249', '2063705592', '2040148652', '2063971729', '1995139569', '2187531982', '2017270018', '1986998340', '1793701335', '2005011583', '2034135522', '2119661364', '2536643855', '2028660455', '2136062212', '2073757338', '2143084353', '2008788993', '1975961566', '2148360625', '2018414711', '2116282447', '2057367767', '1992278941', '2327576113', '1999042945', '2120802447', '1638469948', '1918893774', '1977047419', '2055953851', '2026477331', '1991770442', '1604617494', '2086607005', '1713078372', '2005789857', '2501851833', '2036392750', '1992671619', '2139997280', '2130896076', '103470643', '1963991736', '2073738814', '2037768171', '1983153610', '82712395', '2006163801', '2139581356', '2090818941', '2301106258', '2141702887', '2063871438', '2084850848', '2041723890', '2104846125', '2515598829', '1984186425', '2482615287', '2035600039', '2012079387', '2119712504', '1620781023', '2021039871', '2057484835', '2067465375', '2537457478', '770907932', '2147914811', '2064112214', '2399219715', '1512081066', '2539543354', '2461412690', '1565071190', '2015065119', '2051564300', '1968359385', '1997537847', '2178677884', '1705826416', '2049404332', '2022765296', '2019397395', '1553716673', '1980513749', '2134773907', '1643777747', '1583084448', '2207756506', '2005421479', '2148458864', '2076938597', '1976892599', '2138434827', '2051330115', '2061625802', '1921732602', '2085444045', '2125287803', '1527692262', '1975550749', '2013144489', '2161323654', '2163828179', '1976151220', '2205910341', '2065121307', '2127662037', '73554123', '2092155530', '2066289234', '2134353770', '1514498283', '2027658217', '2025983492', '2122671960', '2345563409', '2164315238', '2156115641', '746481651', '2100283300', '2133901018', '2042536367', '28966451', '2149330402', '1583396642', '1974681651', '2000033743', '13049418', '1998454512', '1978637569', '1560951688', '1547039882', '2179043097', '805717466', '2150690316', '2014555407', '2156766645', '2148734070', '2033466599', '2515298219', '776347474', '1993963921', '2134793169', '2164277132', '1533188365', '2069803859', '1963789760', '1507992233', '2466438279', '1886894250', '2125776183', '2250629460', '2139581634', '2094978649', '2116202864', '2136995162', '2164205891', '1909899148', '1926023772', '1973825495', '1665556569', '2082160633', '2072755596', '2125885346', '2027574342', '1981031595', '2079978831', '2050801485', '2117362195', '1525967574', '2062449769', '2088115262', '1555451424', '2155798417', '2158650045', '1605898583', '999141513', '2075661246', '1973043741', '1975934974', '2116027419', '2079005304', '2149577048', '1963652416', '1127603374', '1965876239', '1566639555', '2063583831', '2087058869', '2017333734', '2202089267', '2148640219', '2005346797', '2130519559', '2090692745', '2054993337', '1516720554', '2060501093', '2048395053', '1973440097', '2079560891', '1979364655', '2017096159', '8242173', '2148555761', '2037641724', '1920661432', '2015174807', '2078605286', '1990345901', '1988514978', '2105764387', '2115331025', '1592122091', '1967885203', '2129301527', '619758126', '1979192143', '2124535468', '2017216310', '2165841090', '2166730694', '2024355307', '1983239532', '2150575030', '2052547672', '2056489884', '2788057238', '2048551161', '1953512529', '2225574294', '1991214761', '74470255', '1525151549', '2078702165', '2042759796', '2147220398', '2016733409', '2055213915', '1974369590', '1985876103', '2014116058', '2101037287', '2008534646', '2544736451', '2148196859', '1498989225', '2012745663', '1670369968', '2062800290', '2122789673', '2055194861', '1595706759', '2126707462', '2027154950'], 'bm25_doc_scores': [39.11329, 37.14475, 35.990074, 35.728638, 35.188705, 34.479454, 33.91003, 33.41993, 33.366573, 33.22847, 32.906933, 32.41816, 32.14759, 32.042522, 31.972483, 31.617245, 31.559282, 31.353214, 31.268538, 31.096605, 30.907347, 30.81508, 30.740423, 30.697521, 30.382835, 30.355032, 30.27018, 30.2277, 30.054373, 29.92753, 29.902021, 29.797583, 29.716063, 29.672163, 29.62356, 29.497831, 29.495691, 29.423668, 29.38787, 29.234064, 29.212793, 29.143177, 29.053915, 29.04036, 29.033314, 29.029478, 29.005898, 28.955555, 28.918953, 28.893515, 28.829294, 28.811357, 28.715511, 28.687567, 28.64941, 28.630213, 28.612497, 28.574606, 28.55084, 28.50467, 28.471527, 28.41968, 28.383396, 28.368979, 28.255674, 28.238655, 28.166372, 28.13397, 28.100695, 28.063875, 27.998909, 27.957602, 27.895288, 27.876623, 27.841944, 27.816074, 27.780191, 27.754852, 27.735209, 27.682915, 27.667274, 27.64572, 27.63892, 27.576893, 27.480022, 27.456757, 27.455141, 27.451965, 27.44617, 27.422722, 27.42146, 27.32233, 27.290092, 27.27535, 27.239174, 27.185123, 27.150885, 27.147074, 27.124928, 27.082512, 27.07308, 27.00246, 26.995766, 26.983229, 26.958212, 26.95321, 26.935616, 26.84313, 26.819092, 26.809885, 26.788412, 26.73399, 26.724655, 26.721502, 26.6885, 26.679682, 26.6787, 26.668653, 26.645876, 26.63742, 26.615057, 26.54792, 26.5345, 26.528835, 26.516075, 26.514381, 26.505718, 26.505615, 26.481653, 26.438046, 26.429585, 26.421888, 26.348484, 26.335674, 26.332409, 26.327688, 26.29892, 26.283169, 26.264252, 26.264065, 26.256054, 26.244665, 26.236637, 26.232353, 26.221863, 26.203602, 26.203125, 26.181602, 26.175354, 26.157148, 26.156319, 26.123272, 26.069836, 26.062132, 26.047052, 26.041492, 26.038992, 26.035454, 25.942947, 25.922535, 25.915527, 25.890026, 25.883991, 25.883293, 25.845308, 25.811405, 25.795263, 25.79499, 25.781487, 25.771255, 25.766138, 25.723291, 25.71519, 25.668337, 25.642004, 25.607372, 25.598974, 25.59095, 25.578781, 25.571064, 25.567291, 25.556206, 25.538677, 25.531809, 25.520876, 25.502434, 25.480545, 25.474213, 25.460987, 25.459305, 25.412409, 25.396065, 25.391535, 25.391075, 25.364824, 25.350912, 25.344688, 25.330933, 25.321957, 25.299276, 25.298143, 25.270935, 25.25002, 25.234768, 25.233408, 25.225649, 25.224447, 25.20881, 25.19666, 25.194134, 25.1845, 25.18436, 25.178465, 25.16245, 25.155228, 25.144068, 25.130268, 25.123205, 25.122234, 25.114882, 25.096106, 25.09138, 25.083048, 25.08035, 25.078712, 25.057507, 25.05057, 25.027634, 24.969513, 24.967903, 24.967464, 24.959839, 24.952278, 24.95168, 24.94873, 24.94614, 24.940842, 24.931686, 24.928051, 24.911228, 24.88935, 24.888906, 24.885708, 24.88166, 24.880709, 24.858198, 24.840988, 24.83797, 24.831821, 24.826616, 24.823885, 24.812792, 24.803253, 24.788103, 24.780893, 24.772924, 24.703514, 24.69772, 24.680908, 24.679483, 24.673363, 24.662933, 24.662367, 24.648771, 24.632153, 24.622395, 24.618896, 24.607302, 24.601372, 24.588123, 24.585264, 24.554344, 24.552088, 24.550337, 24.539297, 24.538372, 24.533857, 24.509336, 24.509336, 24.479645, 24.473291, 24.470858, 24.470116, 24.466383, 24.459187, 24.42826, 24.41639, 24.399508, 24.3963, 24.395958, 24.384682, 24.359169, 24.336887, 24.316725, 24.309948, 24.30808, 24.247292, 24.243471, 24.234823, 24.22896, 24.227657, 24.186975, 24.168018, 24.15603, 24.150467, 24.14227, 24.124489, 24.118025, 24.11696, 24.091736, 24.081892, 24.076208, 24.067509, 24.053194, 24.047968, 24.040968, 24.035196, 24.029316, 24.025267, 24.025267, 24.022474, 24.021778, 24.019018, 24.007812, 23.995779, 23.957392, 23.954144, 23.948236, 23.942356, 23.935759, 23.935257, 23.933535, 23.923573, 23.90965, 23.885452, 23.875017, 23.85513, 23.847038, 23.81937, 23.815348, 23.813099, 23.804766, 23.78452, 23.783802, 23.779074, 23.759258, 23.73974, 23.735968, 23.729822, 23.722395, 23.696344, 23.69446, 23.6456, 23.640213, 23.635576, 23.621422, 23.606407, 23.601753, 23.588818, 23.553734, 23.550018, 23.545122, 23.5222, 23.518312, 23.512596, 23.502377, 23.502308, 23.498798, 23.492125, 23.484842, 23.468817, 23.466915, 23.462769, 23.440712, 23.434622, 23.424858, 23.417309, 23.408386, 23.382645, 23.373507, 23.355143, 23.347166, 23.336931, 23.32747, 23.32526, 23.304783, 23.301489, 23.297966, 23.297703, 23.292112, 23.288921, 23.28603, 23.285059, 23.28217, 23.279867, 23.27506, 23.246119, 23.239902, 23.239426, 23.239424, 23.23863, 23.238369, 23.22242, 23.220856, 23.21275, 23.205227, 23.200516, 23.199188, 23.194204, 23.193247, 23.19151, 23.186491, 23.168278, 23.155693, 23.144604, 23.143549, 23.1414, 23.132534, 23.130157, 23.120375, 23.115732, 23.112135, 23.111317, 23.111067, 23.102024, 23.084492, 23.070223, 23.060717, 23.058992, 23.044907, 23.043344, 23.023691, 23.004147, 22.99929, 22.994194, 22.987268, 22.980478, 22.97177, 22.96719, 22.964909, 22.955486, 22.937284, 22.932606, 22.924929, 22.918488, 22.893465, 22.88711, 22.883688, 22.878624, 22.876915, 22.874317, 22.870153, 22.869339, 22.860216, 22.858267, 22.842127, 22.839201, 22.837986, 22.828257, 22.82289, 22.816244, 22.808102, 22.807281, 22.80672, 22.802906, 22.791492, 22.790384, 22.789736, 22.787199, 22.782892, 22.780737, 22.779978, 22.770796, 22.769281, 22.749847, 22.749302, 22.746216, 22.743578, 22.743008, 22.72702, 22.726524, 22.72561, 22.723331, 22.722973, 22.722956, 22.721884, 22.715805, 22.714527, 22.713032, 22.70261, 22.69624, 22.68642, 22.673838, 22.670858, 22.663815, 22.663013, 22.662716, 22.66097, 22.658833, 22.651594, 22.649158, 22.64738, 22.644356, 22.635693, 22.63313, 22.627228, 22.614132, 22.607618, 22.596724, 22.583054, 22.579172, 22.578426, 22.575325, 22.563198, 22.552765, 22.551098, 22.547577, 22.535078, 22.531488, 22.531136, 22.52533, 22.497852, 22.495173, 22.491785, 22.489092, 22.476667, 22.476345, 22.473097, 22.471123, 22.46979, 22.469393, 22.456144, 22.448812, 22.422302, 22.414139, 22.404718, 22.391584, 22.383219, 22.376411, 22.370218, 22.3597, 22.351498, 22.34742, 22.343245, 22.339443, 22.336922, 22.333836, 22.315735, 22.314629, 22.313026, 22.304945, 22.289011, 22.287022, 22.28232, 22.28021, 22.269932, 22.259682, 22.247995, 22.239246, 22.239218, 22.235678, 22.235434, 22.23224, 22.224499, 22.220205, 22.213566, 22.18563, 22.182837, 22.176851, 22.176123, 22.17173, 22.16479, 22.161947, 22.152811, 22.150196, 22.148659, 22.148167, 22.13925, 22.13549, 22.132175, 22.118454, 22.113947, 22.113663, 22.112406, 22.108059, 22.101358, 22.08934, 22.081251, 22.077686, 22.067945, 22.065386, 22.056458, 22.050888, 22.046318, 22.044584, 22.030258, 22.025911, 22.023651, 22.022213, 22.021301, 22.020721, 22.016472, 22.013767, 22.003048, 21.999437, 21.992073, 21.986692, 21.97982, 21.978062, 21.963612, 21.962208, 21.960695, 21.95852, 21.95701, 21.942865, 21.936184, 21.93518, 21.933825, 21.933533, 21.925108, 21.920357, 21.91635, 21.909264, 21.906881, 21.906172, 21.894962, 21.8944, 21.885506, 21.875738, 21.872425, 21.866241, 21.865791, 21.861082, 21.858036, 21.85771, 21.855164, 21.852974, 21.852732, 21.850035, 21.846668, 21.84514, 21.841944, 21.840672, 21.838572, 21.83466, 21.833612, 21.831978, 21.827356, 21.824121, 21.823221, 21.822731, 21.818005, 21.816242, 21.806156, 21.804407, 21.798023, 21.792149, 21.778305, 21.774593, 21.77335, 21.753096, 21.748486, 21.733576, 21.719917, 21.705307, 21.701735, 21.695093, 21.690563, 21.687428, 21.675398, 21.666039, 21.65951, 21.659014, 21.65266, 21.647272, 21.639696, 21.639385, 21.6377, 21.636368, 21.630056, 21.6248, 21.622528, 21.615273, 21.613285, 21.611092, 21.605217, 21.5923, 21.58926, 21.58842, 21.585434, 21.581448, 21.581366, 21.572256, 21.569925, 21.563646, 21.557375, 21.551243, 21.539959, 21.53342, 21.533325, 21.526333, 21.525372, 21.518013, 21.518013, 21.508978, 21.501183, 21.495054, 21.491291, 21.48732, 21.485966, 21.480766, 21.476847, 21.473335, 21.473227, 21.472626, 21.471983, 21.461216, 21.45744, 21.456514, 21.449959, 21.44976, 21.44618, 21.44412, 21.433208, 21.420406, 21.4048, 21.402409, 21.389967, 21.380234, 21.376362, 21.37262, 21.372547, 21.37202, 21.368542, 21.365828, 21.365202, 21.363298, 21.363298, 21.353485, 21.349342, 21.347404, 21.342806, 21.339739, 21.330502, 21.32662, 21.32513, 21.324474, 21.324314, 21.31599, 21.31349, 21.308708, 21.292534, 21.289022, 21.283297, 21.269129, 21.26893, 21.267195, 21.262985, 21.251553, 21.243092, 21.239134, 21.235722, 21.235653, 21.234276, 21.224033, 21.217865, 21.216475, 21.207077, 21.20233, 21.198753, 21.198753, 21.197252, 21.184532, 21.179163, 21.176964, 21.176325, 21.176003, 21.174814, 21.17197, 21.16934, 21.168417, 21.163805, 21.15848, 21.14663, 21.143488, 21.140083, 21.123335, 21.115475, 21.106718, 21.103884, 21.100931, 21.097486, 21.095312, 21.084785, 21.07276, 21.06722, 21.063112, 21.061354, 21.061354, 21.060583, 21.059433, 21.055218, 21.049097, 21.045782, 21.045166, 21.04046, 21.040306, 21.039669, 21.037287, 21.03059, 21.026028, 21.02452, 21.020344, 21.014807, 21.00476, 21.000599, 20.996902, 20.996363, 20.983526, 20.977303, 20.973646, 20.964035, 20.960293, 20.956217, 20.95105, 20.945753, 20.94558, 20.939415, 20.93186, 20.930515, 20.927258, 20.918802, 20.90952, 20.907349, 20.905327, 20.89864, 20.898453, 20.895765, 20.895393, 20.88743, 20.886248, 20.885881, 20.861158, 20.861158, 20.856823, 20.855589, 20.851757, 20.85068, 20.85062, 20.844042, 20.842487, 20.839548, 20.83393, 20.827572, 20.821344, 20.812456, 20.810865, 20.807392, 20.804123, 20.799421, 20.787334, 20.784512, 20.778725, 20.764477, 20.752378, 20.750923, 20.749985, 20.749985, 20.742085, 20.729982, 20.729042, 20.728868, 20.727108, 20.726845, 20.723787, 20.720348, 20.718546, 20.716095, 20.714956, 20.714624, 20.711643, 20.711384, 20.710844, 20.703747, 20.694511, 20.693623, 20.692928, 20.691051, 20.685896, 20.683655, 20.68158, 20.68149, 20.671106, 20.670296, 20.66937, 20.66066, 20.656467, 20.653252, 20.646906, 20.643665, 20.641573, 20.641573, 20.629133, 20.627216, 20.622454, 20.619556, 20.614872, 20.611456, 20.61044, 20.584877, 20.580418, 20.575783, 20.56821, 20.564318, 20.562368, 20.560116, 20.55854, 20.554043, 20.551214, 20.547258, 20.54201, 20.540445, 20.535294, 20.534859, 20.534859, 20.523893, 20.522484, 20.521465, 20.5152, 20.504725, 20.50144, 20.493956, 20.484854, 20.483065, 20.481512, 20.474422, 20.473654, 20.468395, 20.454355, 20.454193, 20.45351, 20.447418, 20.446404, 20.444004, 20.443193, 20.437666, 20.435793, 20.432816, 20.430643, 20.42848, 20.4252, 20.424187, 20.408205, 20.408161, 20.40299, 20.4007, 20.397558, 20.394897, 20.388546, 20.38585, 20.378727, 20.37814, 20.377586, 20.373146, 20.369879, 20.365864, 20.360603, 20.357933, 20.357399, 20.350248, 20.344194, 20.330233, 20.327578, 20.327578, 20.319742, 20.316347, 20.306896, 20.297739, 20.29172, 20.290634, 20.289246, 20.27379, 20.273663, 20.273611, 20.272882, 20.264936, 20.254904, 20.25233, 20.251837, 20.245487, 20.24005, 20.23024, 20.221046, 20.221046, 20.209787, 20.201185, 20.19958, 20.197798, 20.190046, 20.180614]}, {'id': '2524249505', 'text': 'direction arrival estimation spherical harmonic domain use subspace pseudointens vector', 'timestamp': 1483225200, 'rel_doc_ids': ['1980903300', '1982687260', '2116346963', '2129501075', '2152632507', '2158941010', '2160525176', '2163994287', '2166543153', '2204091679', '2205176333', '2398746191', '2399672035', '2563608090'], 'user_id': '2096449736', 'user_doc_ids': ['3189630', '79696884', '934453593', '1041051684', '1482315657', '1488702213', '1511044115', '1518272374', '1534770910', '1575926703', '1579726707', '1603544686', '1866989267', '1906917904', '1950416721', '1969110495', '1982687260', '1985693260', '1989016082', '1995164087', '1997175528', '2001402966', '2001476715', '2008335297', '2011283848', '2012348064', '2014779724', '2016730668', '2017607271', '2028043639', '2033288740', '2034112919', '2038730156', '2040734299', '2041851330', '2042212675', '2042682033', '2049415884', '2057936121', '2060008332', '2060513454', '2068275783', '2068632357', '2072604753', '2073895660', '2078895922', '2080766307', '2091836239', '2092169519', '2098294667', '2103607865', '2104204079', '2109199922', '2112417179', '2113259617', '2115344082', '2117426145', '2117939881', '2119088686', '2119367741', '2120349113', '2121237609', '2121359356', '2122312225', '2123038074', '2125042854', '2126584544', '2129568383', '2130883038', '2135969423', '2141761187', '2141761826', '2143294515', '2144074397', '2145939212', '2148013426', '2149818355', '2149966458', '2150419720', '2150764842', '2152567223', '2152697528', '2152865625', '2153077516', '2153167244', '2153275868', '2154345302', '2154860095', '2155295839', '2155952401', '2156826341', '2157578001', '2161096157', '2163732756', '2163895401', '2165432241', '2169474952', '2171858789', '2171963039', '2172116171', '2172169143', '2172491908', '2175303068', '2213904342', '2306323236', '2394627318', '2398746191', '2538965586', '2544697576', '2563608090', '2911394281', '2949659076'], 'bm25_doc_ids': ['2107195280', '2399672035', '2563608090', '2116346963', '2204091679', '2152632507', '2155646418', '1990055747', '2105067039', '2170512959', '2327701832', '2160788363', '1982687260', '2171908478', '1606167728', '2168299838', '2086865077', '2123701557', '1990072752', '2148270200', '2038088554', '2039724859', '1971211045', '2039965957', '1974214069', '1996371435', '2134875082', '2251976524', '2018860100', '329296917', '2142071174', '1963699169', '2068740773', '1968350054', '2155840984', '1985609120', '1544035235', '2140801454', '2081934505', '2124306988', '131946912', '2072866464', '2031105940', '80739183', '1980903300', '2004697363', '2128051112', '2164215467', '2165548826', '2133404884', '2011283848', '2133954120', '2298296384', '1569164057', '2117170433', '2962917348', '2028301714', '2089727948', '1983991135', '2085156437', '1557979299', '2543666419', '2145675350', '1898338067', '2201155701', '2114285496', '1998301743', '2050701083', '2129550627', '2106520758', '251315133', '2038625156', '2062602757', '2133586013', '2091816760', '2030229267', '2171853589', '2104050961', '2139612634', '2171917193', '2038730156', '2111999986', '2075216062', '1524677060', '1986413538', '2123053226', '2148142857', '2026374942', '2168896363', '254698674', '2045042809', '2069930369', '2074458630', '2107527200', '2139743994', '1963337227', '1566935401', '2075107327', '2108428911', '2137151229', '2120241308', '2030255233', '2008462695', '2158236008', '2088617070', '2121529575', '2040822780', '2261858229', '2112583420', '2102756853', '2163623445', '2171990039', '2118096836', '1609941950', '1898269248', '1968495228', '2109710284', '2144883546', '1536753135', '2013139135', '2126433208', '1995822412', '2171999233', '2111468789', '2056263005', '2111627180', '1986583619', '2167710683', '1873389479', '2116255268', '2058864484', '2171787095', '2159741524', '2109004302', '2020814973', '2174115052', '2144424618', '1972646926', '128222928', '2164495812', '2125874614', '1579000240', '2345417717', '1981259238', '2099645443', '2036667177', '1505851013', '1613297483', '2133507081', '1993088102', '2058002405', '1967378991', '2135400807', '2019224567', '2063059671', '2105983337', '2106205831', '2107029795', '1999804259', '2160500433', '2047671773', '1986439111', '2045566218', '1979219848', '2964033919', '2142757188', '2028772184', '2138392608', '2112686847', '1575433626', '2059073543', '2201181482', '2155493559', '2009291374', '2106693929', '2158725112', '2167282658', '2022416308', '2146765594', '2007716752', '1941370846', '2113071053', '2004621162', '2002381575', '1980379294', '2016086832', '2078598312', '2169073093', '2126556471', '2009850603', '1933412717', '2075815580', '1995370222', '2065931256', '2123071279', '2150009939', '2167773605', '1987450470', '2146085064', '2131927769', '2130342534', '2148563609', '1992300140', '2129188305', '995769704', '1970584764', '2146098349', '1537475529', '1975995306', '1968983568', '2321362635', '2092179111', '2134155478', '2031274324', '2003481810', '2138320838', '2148886316', '2132743051', '2155781330', '2091061359', '1965380755', '2169635990', '2170585716', '2116365951', '2096638964', '1504716680', '1480442660', '2133071021', '2091373577', '2537400661', '1970314780', '2122266149', '2148241102', '2160409960', '2078671025', '2104644455', '2103893382', '2028882748', '2163994287', '2532631716', '2076554046', '63352313', '2106335966', '2010150747', '2068556767', '2055838441', '2241079503', '2034930724', '1559820134', '1991069881', '2249759737', '2167529044', '2148770873', '2104371723', '2146930300', '2067380624', '2127912193', '1999170002', '1981913121', '1979692260', '2151105244', '2014692038', '2019865478', '2114421903', '1993812259', '1543828044', '2019271105', '1994739323', '2144524571', '2111302345', '1975680912', '1509769887', '1969535540', '2046063597', '2103664973', '2097177383', '2134248105', '2159328531', '46179679', '2129013763', '2069452218', '2094680103', '2089550744', '2171629296', '2027607884', '2107710410', '2404145708', '2109123274', '2158539962', '2240185166', '2138235240', '2003402856', '2089858709', '2003064452', '2104121661', '2067293606', '2035247460', '2075406189', '2022395096', '2059359446', '2147345370', '2266780708', '567105040', '2161147105', '2110789499', '2949914043', '1970595491', '2255275540', '2308565424', '2107297119', '1991590219', '2064894286', '1574858636', '1775800598', '2096855653', '186082612', '2213904342', '2003733299', '1604654285', '2096417022', '1497516129', '2061893205', '2225303823', '2037891800', '1543954733', '2263619728', '2398746191', '2097525122', '2125566886', '1980617447', '2138962724', '2106481279', '2104266700', '2162528647', '627495730', '2017547032', '1978275592', '1486855082', '2136731103', '2121398386', '2196668564', '2040832769', '2012270713', '2108617900', '2149708440', '2097584365', '2143114085', '1949398423', '2048978695', '1491826464', '2151972970', '2103914324', '2051600537', '2105923168', '2117295349', '2161385196', '2082501189', '1989237782', '2102701524', '2046855287', '1967138585', '1545951160', '2146729014', '2149186753', '2116635101', '2076964216', '1967671896', '1946110011', '2033608482', '1991170630', '1519471938', '2011768471', '2128182753', '2003564384', '2123471406', '2112146830', '2137283820', '2070195050', '2124930868', '2106598309', '1606887812', '1605132321', '2127224001', '1939355338', '2122045484', '1971030600', '1995843795', '2077022309', '2141714149', '2021856733', '2169260271', '1517749972', '1980455271', '2031043588', '2158769234', '2083499784', '2103665886', '2541928073', '2097239906', '2126962853', '2143460307', '1562910560', '2143166606', '2155392321', '2131264803', '2127695678', '2097657341', '2082318030', '2099421358', '2128131274', '2113817303', '2116005102', '2047130136', '2122924984', '2045248339', '1679944936', '2118620951', '2126566632', '2140166042', '2110984024', '2039359654', '2004011607', '1999867771', '974511872', '1936817103', '2143716125', '2547951313', '2159999556', '2168719080', '2166541206', '1992766081', '2003035166', '2092820540', '2141567657', '2061478509', '2098150671', '2045966997', '2024772675', '2119933098', '1509829222', '2141638501', '1603075283', '2098445930', '2157779349', '2143283319', '1599148263', '2132844480', '1976879621', '2171428399', '1666613736', '2035680234', '1986659997', '1846249046', '2150101794', '2135629581', '2064821401', '2134830181', '2078745071', '2165045112', '2043487075', '2064810928', '2163175707', '2142189809', '2272374066', '53658504', '2097207905', '2143341862', '2122431481', '1857754476', '2052448522', '2081270133', '2017374081', '2146589072', '2101376505', '1998924366', '1953378757', '810161188', '2051725963', '2141969242', '2142750353', '2326153866', '2101464388', '2161938105', '2103405773', '1979165578', '2007289542', '1865776139', '2084055929', '2124724628', '2113679114', '2046039020', '1606095553', '2047179193', '2111194413', '1574934438', '2034358488', '2085644962', '2155145307', '2128722732', '2059053466', '1488029913', '1989876652', '2103263585', '2137492554', '1998958723', '2395865476', '1987359230', '2131708962', '2209627325', '2066278925', '2134246816', '2083835631', '2009619338', '2065825228', '1539726260', '1984793157', '2130202098', '1591758211', '2054411957', '2093016268', '1592354327', '2171278455', '1981750300', '2152541300', '2949687630', '2099761206', '2161894231', '2080330073', '2153939185', '2023857294', '2096464064', '1574099848', '2045627586', '2156781258', '2032056322', '1993495347', '2038907618', '2062860621', '2119903608', '2035267477', '2136377405', '2104773653', '2131090450', '1990406323', '157698429', '2100574887', '1999751397', '1981560313', '2023792469', '2131797282', '2153791945', '1802300744', '2077785134', '2079269116', '2042508878', '2025622016', '2025824277', '2534674033', '2157006235', '65329221', '1880963629', '1596749014', '1566809597', '2075737186', '2963881844', '1900210448', '2164438967', '1977158198', '1986051947', '1980731550', '1972009849', '2494253314', '2021773261', '2101293488', '1526870684', '1859276392', '2158285215', '2038211897', '1999849826', '2090951282', '2131010183', '2149693982', '2110473269', '2027708938', '2205176333', '2142820633', '2142017648', '980485324', '2071197225', '2112874951', '2109319356', '2547806318', '2255884342', '1991928242', '2117692638', '2165615111', '1652867070', '1989704465', '2064811928', '2534518344', '2025997552', '1983899241', '2120954484', '2155809672', '2052009247', '2149171904', '2010906471', '2033332370', '1988106785', '2044944587', '2167511587', '2110070138', '2054345612', '1790954942', '2152959713', '2050970186', '1986148183', '2549450702', '2153535555', '2097670707', '2106840227', '2167694344', '2211589247', '2540714589', '1858667010', '2111280390', '2172024328', '2135969586', '2038069731', '2103757191', '2033945550', '2016506370', '1584814649', '2144332006', '2273155963', '1606440527', '1549123728', '2064713618', '117229680', '2086992494', '2149523072', '2139615161', '2132394949', '1545302321', '2056229585', '2126771657', '2046053831', '2110074146', '2165916500', '1504196308', '1965113512', '1850714194', '10464604', '1986467063', '1987850385', '1577053538', '2119890508', '2036645923', '2063644988', '2149686197', '2096329835', '2075332319', '2139468139', '1883766569', '2134901320', '2010238960', '2069806160', '2194451348', '2118084397', '1969253908', '1816862936', '2000358255', '1991533283', '2102660957', '2077600808', '1541537218', '2619462624', '1830360151', '2055069670', '2170476366', '2126923742', '2123678129', '1553234079', '2090932768', '2109629160', '2116915566', '2167599467', '2160525176', '2226744698', '380814507', '1987083071', '2159280840', '2153628356', '2046302980', '1846339416', '2014309208', '2296261412', '2086501978', '1979972347', '2011053773', '2043731250', '151548437', '2099076058', '2049805819', '2099628270', '2048084056', '2072575351', '1567126994', '2027926994', '2950478873', '2108061386', '2115675554', '2081050624', '2167076377', '2049466097', '2110999089', '2298325421', '2081199393', '2112323376', '2159718202', '2060898603', '1853554770', '2143056127', '2050134757', '2103603787', '2028599779', '2107387379', '1550922062', '963904880', '2168223861', '1971875565', '1999528110', '1999970066', '2083803084', '1972839404', '2097727109', '2169310078', '2122702797', '1571103352', '1988603202', '2048515166', '2153505610', '1966434362', '2109205099', '1905245267', '2149732804', '2088440563', '2130094920', '2047931094', '2258011769', '2129501075', '2092048411', '1989466721', '2112222342', '2135945984', '2096608892', '2165886362', '2166543153', '2123577089', '2031652243', '2045923097', '2569750427', '1523831565', '26685064', '2014622714', '2045551387', '1992121753', '2076400391', '1952527677', '270035182', '2103054609', '2152005770', '1755578129', '2130693290', '2089173587', '401259478', '2153487479', '2127657009', '2040256188', '2136322154', '1526558152', '2115770043', '1994743492', '2098736008', '1980239101', '1970896269', '2125625651', '1492038285', '1513822213', '1025006772', '2000428205', '2010066971', '2154169260', '1484905911', '2168335662', '2109835730', '1965041242', '2204219076', '1986301155', '2112612355', '2001673939', '2130294743', '1986058453', '2171952424', '1802115832', '1526377902', '1996509964', '2277324284', '2011017646', '2127267652', '2050113095', '2166167085', '2002475465', '1986967345', '2148905020', '2130739159', '2134968362', '2168479245', '2552987523', '2013353527', '2078774889', '2539902822', '2126432613', '2045254928', '2120101160', '2043468346', '1560668657', '2026447062', '2149066318', '2099630326', '2064015190', '2284743441', '2152508672', '2105168508', '2088770626', '2053859454', '2545903769', '2102736780', '2161469700', '1124870649', '2034511746', '1913494087', '2155368689', '2130429288', '1491890440', '2120802460', '2120049249', '1574267945', '2090020898', '1966458579', '2276756418', '2113221539', '1897979046', '2031121127', '1541575757', '2290215683', '2131853781', '2106417065', '2142388040', '1983684301', '2099216015', '1987972213', '2052419574', '2070884827', '2165358154', '2032749529', '2170452323', '2244219423', '81851533', '2539180713', '2028240716', '2110564308', '2085588995', '2046330884', '2164117175', '2085111629', '2149673298', '2101673608', '2151028357', '2137223120', '1985882638', '2329672906', '2155299317', '2078031705', '2137370905', '2913985088', '1608916426', '2159755794', '2096051656', '1574877207', '1971519787', '633298760', '2138217679', '1799812590', '1970094958', '2158206351', '2164927172', '2114289535', '2141227106', '1492024128', '2162848374', '1519213162', '1601293496', '2109300581', '2073351574', '2407463531', '2141586563', '1974950561', '2115757479', '2079302539', '2106322418', '2105621717', '1518877122', '2129060197', '2022847412', '2116045131', '2009535986', '2110877491', '2134018578', '2061908989', '2027602198', '1983104971', '1980486598', '2002656300', '2155392969', '2081637407', '1966112421', '2123962454', '2096229075', '2067480525', '2126686722', '2136728531', '2165916220', '2066622786', '2104413851', '1883423627', '1985936705', '1964722127', '2161546193', '1995047622', '2085527494', '2614962167', '2160205863', '2115877163', '2283626204', '2046069074', '2037150186', '1965887040', '1985473420', '1502226082', '1971299415', '1726885280', '2158941010', '1992619157', '2165655637', '1968792879', '2052487768', '2547259537', '2398715318', '2008769701', '2153782554', '1522877892', '1600598401', '1983600173', '2157364958', '2114402068', '1854124967', '2106207920', '2501242162', '2148985304', '2043114941', '2166312569', '1838561982', '2105391183', '1985145881', '2093328671', '2170272983', '2132279897', '2144377658', '2123409455', '2101772227', '2135175880', '2544973567', '1972556919', '2144787994', '2160435417', '1996897861', '2164787613', '1996285030', '2257689208', '2130503630', '2149551443', '2097279410', '2036549641', '1536972474', '2949919540', '1687232949', '1685152806', '2143746649', '2002879366'], 'bm25_doc_scores': [47.076225, 41.528038, 40.8862, 40.687965, 39.00239, 38.45826, 35.303192, 34.97972, 34.68743, 34.652927, 34.530636, 34.05526, 33.734123, 33.0483, 33.002903, 32.99024, 32.746082, 32.655064, 32.048344, 32.035183, 31.720213, 31.188856, 31.114, 30.928284, 30.692932, 30.359173, 30.255945, 30.253792, 30.15419, 30.047726, 29.902748, 29.871323, 29.828043, 29.640703, 29.50859, 29.500446, 29.451607, 29.2929, 29.040981, 28.78474, 28.74924, 28.626135, 28.622631, 28.544634, 28.542225, 28.47712, 28.324648, 28.313017, 28.293484, 28.226833, 28.226414, 28.200825, 28.138046, 28.136387, 28.06208, 28.060179, 28.03238, 27.982092, 27.946661, 27.917465, 27.838678, 27.683796, 27.603727, 27.536678, 27.498678, 27.457735, 27.440481, 27.426983, 27.385792, 27.357544, 27.29481, 27.291649, 27.269436, 27.259258, 27.25565, 27.242064, 27.23139, 27.22874, 27.222425, 27.216227, 27.143066, 27.07374, 27.056206, 27.027964, 27.027905, 27.015417, 26.922752, 26.916613, 26.9021, 26.85815, 26.857647, 26.83845, 26.826794, 26.826794, 26.801994, 26.748653, 26.695566, 26.652046, 26.611515, 26.521938, 26.512852, 26.389685, 26.389061, 26.357508, 26.345871, 26.303545, 26.252846, 26.229643, 26.228283, 26.211388, 26.165852, 26.017916, 26.004717, 26.002779, 25.98384, 25.977797, 25.952007, 25.949669, 25.884352, 25.849695, 25.82571, 25.786278, 25.773876, 25.770092, 25.749462, 25.730726, 25.71028, 25.70831, 25.707905, 25.668327, 25.642935, 25.605387, 25.60315, 25.590042, 25.511576, 25.478308, 25.472122, 25.454956, 25.453043, 25.425766, 25.420748, 25.416859, 25.410479, 25.40661, 25.38188, 25.374094, 25.364172, 25.339857, 25.32723, 25.322813, 25.29542, 25.275501, 25.25815, 25.22542, 25.210712, 25.197475, 25.179789, 25.136372, 25.135366, 25.124052, 25.123247, 25.088999, 25.063702, 25.057804, 25.02757, 25.021591, 25.017157, 25.013973, 25.000937, 24.988653, 24.963413, 24.962385, 24.948505, 24.947609, 24.929605, 24.928677, 24.915209, 24.905407, 24.90304, 24.901333, 24.899595, 24.8877, 24.866388, 24.84112, 24.798395, 24.792524, 24.786232, 24.784592, 24.779291, 24.772705, 24.766651, 24.757902, 24.747572, 24.724064, 24.72121, 24.712982, 24.687744, 24.668093, 24.635408, 24.630573, 24.614748, 24.594513, 24.56815, 24.564686, 24.527462, 24.514782, 24.513609, 24.493786, 24.493786, 24.487625, 24.467295, 24.43233, 24.396675, 24.39575, 24.345127, 24.342844, 24.31041, 24.306978, 24.287567, 24.280235, 24.250246, 24.246698, 24.237194, 24.233482, 24.233269, 24.213242, 24.202843, 24.197882, 24.194786, 24.193178, 24.157278, 24.15628, 24.155249, 24.135695, 24.135216, 24.1324, 24.128267, 24.128187, 24.1258, 24.095917, 24.077347, 24.06969, 24.06144, 24.049843, 24.041697, 24.038202, 24.016201, 23.99719, 23.956385, 23.9497, 23.943663, 23.911312, 23.9094, 23.893612, 23.892084, 23.885729, 23.885355, 23.8711, 23.867203, 23.864752, 23.842878, 23.826012, 23.82123, 23.77885, 23.761477, 23.729332, 23.723269, 23.721252, 23.718876, 23.714544, 23.7032, 23.701332, 23.700138, 23.69597, 23.695644, 23.681978, 23.667624, 23.66543, 23.66267, 23.621384, 23.609053, 23.606756, 23.606441, 23.599089, 23.589405, 23.561047, 23.51319, 23.509583, 23.509512, 23.495361, 23.457987, 23.457243, 23.453337, 23.446047, 23.443737, 23.441017, 23.440128, 23.429947, 23.427116, 23.418144, 23.392298, 23.391531, 23.36939, 23.366915, 23.366613, 23.363846, 23.363846, 23.36346, 23.362617, 23.353376, 23.339308, 23.33782, 23.33372, 23.322605, 23.293623, 23.290903, 23.276735, 23.25477, 23.229355, 23.229225, 23.223976, 23.195808, 23.185982, 23.18335, 23.181856, 23.178917, 23.17865, 23.17292, 23.141064, 23.121353, 23.106754, 23.097952, 23.089182, 23.06892, 23.066952, 23.049114, 23.044004, 23.001406, 22.996004, 22.990623, 22.989326, 22.981398, 22.97003, 22.963009, 22.959637, 22.936388, 22.934763, 22.92918, 22.928623, 22.924139, 22.922535, 22.897898, 22.89443, 22.86949, 22.868038, 22.864431, 22.860989, 22.851524, 22.849613, 22.849586, 22.845242, 22.836323, 22.82653, 22.813652, 22.802467, 22.802046, 22.777084, 22.767326, 22.767057, 22.76642, 22.765806, 22.754835, 22.736687, 22.725239, 22.712538, 22.706493, 22.687794, 22.686201, 22.683617, 22.682108, 22.680622, 22.67936, 22.642155, 22.640467, 22.639584, 22.622923, 22.614132, 22.613071, 22.610386, 22.606056, 22.5991, 22.592974, 22.585814, 22.57812, 22.571686, 22.529493, 22.50392, 22.487024, 22.435167, 22.428493, 22.39553, 22.39419, 22.392897, 22.39153, 22.389969, 22.385777, 22.383133, 22.33559, 22.326977, 22.320808, 22.320747, 22.301346, 22.301344, 22.295685, 22.27658, 22.264368, 22.26102, 22.25898, 22.25898, 22.243568, 22.226618, 22.21373, 22.201786, 22.198023, 22.193708, 22.18726, 22.179548, 22.16837, 22.156918, 22.153816, 22.141747, 22.132084, 22.12755, 22.11644, 22.107342, 22.102812, 22.09644, 22.095469, 22.095205, 22.081135, 22.067198, 22.06274, 22.056353, 22.056353, 22.053288, 22.047903, 22.046158, 22.04531, 22.039412, 22.023321, 22.016125, 21.993187, 21.991781, 21.96454, 21.959955, 21.948877, 21.94607, 21.93718, 21.936928, 21.931551, 21.905838, 21.89899, 21.885086, 21.860191, 21.828442, 21.826838, 21.82405, 21.788086, 21.786785, 21.785847, 21.785145, 21.782473, 21.775812, 21.774452, 21.76123, 21.73064, 21.724867, 21.710978, 21.697908, 21.69261, 21.687733, 21.686909, 21.68465, 21.682066, 21.679272, 21.676262, 21.661959, 21.65301, 21.65111, 21.647007, 21.645254, 21.632885, 21.632381, 21.630482, 21.617693, 21.617672, 21.613974, 21.613066, 21.612759, 21.611088, 21.607447, 21.604837, 21.60082, 21.595617, 21.594387, 21.591503, 21.589067, 21.58229, 21.580032, 21.573257, 21.53838, 21.518032, 21.512295, 21.506329, 21.501799, 21.496176, 21.492456, 21.479134, 21.478218, 21.475142, 21.465567, 21.461931, 21.446306, 21.442919, 21.440434, 21.440434, 21.440434, 21.436676, 21.435444, 21.43319, 21.420864, 21.419222, 21.414614, 21.403282, 21.400467, 21.396044, 21.39595, 21.394787, 21.394787, 21.387814, 21.379951, 21.378115, 21.369864, 21.355145, 21.345095, 21.310776, 21.30992, 21.299334, 21.298279, 21.291918, 21.278683, 21.278265, 21.26532, 21.26532, 21.26303, 21.253214, 21.251429, 21.23545, 21.233389, 21.221684, 21.219782, 21.191193, 21.18862, 21.185543, 21.171982, 21.171982, 21.171915, 21.171534, 21.16796, 21.163862, 21.162832, 21.158068, 21.153233, 21.152199, 21.149733, 21.148197, 21.131535, 21.128334, 21.117458, 21.116255, 21.112612, 21.108286, 21.106583, 21.097734, 21.096952, 21.096252, 21.09525, 21.088186, 21.079636, 21.07597, 21.066437, 21.063934, 21.055296, 21.043486, 21.03512, 21.028866, 21.02836, 21.024076, 21.019384, 21.011574, 20.982082, 20.971018, 20.968452, 20.967476, 20.964651, 20.961878, 20.959501, 20.958927, 20.947632, 20.942719, 20.94136, 20.936646, 20.929863, 20.92486, 20.921751, 20.92049, 20.904552, 20.89357, 20.885183, 20.883589, 20.872383, 20.870928, 20.867405, 20.862738, 20.852097, 20.850124, 20.831411, 20.817877, 20.811077, 20.80139, 20.794222, 20.78769, 20.787575, 20.784016, 20.780338, 20.768581, 20.765942, 20.765717, 20.765665, 20.755066, 20.749695, 20.74819, 20.739676, 20.73922, 20.730663, 20.728142, 20.706656, 20.702497, 20.698812, 20.697504, 20.696926, 20.696877, 20.695358, 20.691805, 20.689117, 20.686451, 20.685627, 20.685595, 20.683933, 20.681955, 20.678642, 20.678373, 20.676737, 20.675701, 20.675154, 20.672043, 20.671423, 20.659666, 20.650265, 20.631512, 20.630499, 20.629486, 20.620245, 20.614016, 20.606192, 20.599428, 20.593628, 20.585672, 20.584875, 20.581135, 20.575333, 20.560362, 20.559145, 20.558727, 20.555994, 20.541103, 20.540773, 20.539711, 20.5325, 20.530546, 20.52618, 20.523046, 20.520031, 20.520031, 20.516386, 20.515953, 20.51074, 20.50511, 20.504457, 20.504457, 20.49622, 20.477005, 20.470577, 20.4689, 20.463993, 20.459196, 20.458721, 20.456846, 20.451368, 20.438671, 20.431675, 20.421843, 20.419792, 20.419537, 20.414188, 20.401014, 20.396807, 20.396137, 20.391714, 20.390993, 20.383947, 20.3817, 20.376844, 20.37307, 20.352024, 20.347858, 20.346062, 20.345875, 20.345242, 20.34516, 20.340298, 20.334576, 20.331478, 20.328001, 20.309334, 20.308327, 20.308327, 20.297886, 20.289736, 20.281406, 20.277494, 20.271587, 20.267143, 20.25956, 20.259054, 20.257076, 20.245457, 20.239336, 20.237799, 20.230753, 20.230532, 20.230013, 20.220367, 20.218077, 20.215282, 20.211357, 20.208494, 20.204004, 20.195868, 20.18863, 20.187216, 20.187216, 20.181795, 20.180565, 20.17917, 20.17241, 20.169062, 20.158653, 20.156437, 20.150286, 20.143236, 20.143236, 20.142319, 20.133114, 20.13303, 20.122742, 20.1219, 20.121801, 20.120504, 20.113405, 20.112999, 20.11036, 20.11036, 20.11027, 20.106127, 20.097034, 20.086884, 20.086058, 20.07867, 20.06982, 20.066797, 20.065567, 20.059937, 20.057987, 20.057838, 20.054274, 20.046677, 20.042433, 20.042278, 20.03915, 20.036638, 20.025757, 20.024103, 20.022852, 20.007626, 20.007446, 20.002628, 19.997711, 19.993788, 19.99106, 19.97606, 19.968208, 19.967602, 19.961267, 19.961239, 19.961124, 19.95517, 19.95363, 19.948215, 19.94305, 19.94297, 19.942965, 19.942377, 19.942375, 19.94127, 19.940762, 19.93832, 19.928545, 19.922306, 19.914879, 19.911814, 19.90217, 19.897955, 19.89414, 19.890965, 19.887838, 19.886448, 19.884787, 19.88166, 19.88057, 19.87268, 19.868546, 19.86639, 19.866364, 19.860039, 19.859358, 19.853065, 19.850803, 19.843798, 19.841574, 19.835028, 19.832703, 19.83189, 19.829845, 19.829533, 19.82653, 19.817255, 19.812225, 19.810465, 19.80985, 19.809593, 19.808697, 19.793915, 19.793032, 19.790146, 19.774305, 19.774256, 19.774256, 19.762308, 19.759432, 19.752602, 19.749058, 19.748085, 19.743975, 19.740896, 19.738464, 19.731565, 19.726183, 19.724833, 19.714426, 19.714426, 19.714426, 19.707851, 19.705933, 19.701801, 19.700684, 19.700384, 19.700384, 19.697792, 19.67881, 19.675148, 19.673302, 19.668074, 19.662315, 19.656305, 19.646982, 19.64679, 19.644983, 19.644844, 19.64327, 19.62993, 19.620203, 19.6129, 19.610851, 19.608091, 19.606876, 19.605875, 19.605516, 19.603966, 19.60344, 19.600372, 19.595177, 19.587448, 19.586792, 19.58147, 19.569805, 19.56932, 19.56763, 19.566599, 19.563612, 19.563334, 19.562511, 19.55618, 19.555174, 19.553503, 19.551682, 19.54571, 19.535912, 19.535713, 19.534882, 19.52905, 19.528433, 19.5163, 19.513538, 19.510822, 19.504314, 19.504045, 19.503431, 19.492643, 19.491158, 19.487587, 19.485193, 19.480787, 19.479261, 19.479115, 19.479115, 19.478256, 19.477364, 19.474098, 19.472921, 19.472744, 19.47148, 19.469368, 19.466455, 19.461193, 19.456097, 19.454952, 19.453445, 19.44344, 19.442133, 19.435621, 19.43513, 19.434721, 19.43103, 19.430317, 19.422985, 19.42094, 19.398436, 19.395508, 19.392012, 19.389736, 19.38956, 19.37997, 19.376827, 19.373293, 19.36982, 19.366041, 19.364038, 19.362434, 19.362288, 19.358732, 19.352745, 19.352745, 19.345293, 19.341253, 19.336687, 19.334776, 19.330763, 19.326908, 19.32206, 19.316563, 19.315027, 19.309767, 19.306862, 19.304178, 19.304178, 19.303204, 19.302551, 19.296242, 19.292957]}, {'id': '2530073819', 'text': 'massive mimo communications', 'timestamp': 1483225200, 'rel_doc_ids': ['1576083089', '1654832238', '1977207404', '2021699271', '2061134945', '2105392125', '2141682101', '2142238976', '2218620158', '2952232172'], 'user_id': '2029994413', 'user_doc_ids': ['254932647', '599363857', '838780744', '1480712522', '1487762291', '1489592013', '1519184722', '1552633185', '1565450917', '1567975822', '1585931726', '1590480603', '1645823567', '1692944083', '1764607045', '1914285235', '1963830396', '1979176817', '1985450853', '1986482555', '1991386995', '2011687201', '2016541714', '2017190463', '2020154264', '2023840728', '2023903825', '2036686439', '2038329885', '2040788409', '2044075512', '2049081193', '2053489212', '2054281205', '2055350048', '2059311692', '2061134945', '2063973835', '2072599023', '2090094073', '2093893649', '2104962311', '2105392125', '2110715360', '2111122268', '2111717285', '2111737938', '2113745016', '2114258467', '2115324684', '2121085736', '2124568462', '2129780803', '2133666456', '2133826250', '2139227769', '2142238976', '2142724750', '2147056723', '2152053270', '2152157622', '2157498345', '2162246853', '2168679175', '2168744525', '2169227282', '2182231018', '2195255315', '2205821735', '2208500189', '2218620158', '2231452787', '2238937638', '2271007160', '2273732191', '2287320610', '2292153931', '2293884929', '2341675250', '2346625958', '2397897286', '2408600879', '2503114466', '2517020940', '2520289511', '2561711915', '2949147150', '2949831433', '2949881731', '2950412592', '2950931348', '2952198840', '2952871608', '2952961039', '2953047992', '2963422853', '2963545705', '2963722216', '2963742327', '2963767492'], 'bm25_doc_ids': ['2161007399', '2023535408', '1973171928', '2466216970', '2293884929', '2048318885', '2291537983', '2291457367', '2520394698', '2076393910', '2038227506', '1969508636', '1563429468', '2288599753', '1582872561', '2963820714', '1986681576', '2963452556', '2226622727', '2238937638', '2523358175', '1565450917', '2534128482', '2245062803', '2964324869', '2185277770', '2272774410', '1905129071', '2215340352', '2279641318', '1918848090', '2294766866', '1628525439', '1482690055', '2460496349', '2090124632', '1625407446', '2132048364', '1508988232', '2183165041', '2022464413', '2072339605', '2047515592', '1573332369', '2463053673', '1972176711', '2282281953', '1600909290', '2237891383', '1946587052', '2465569272', '825726496', '2962682231', '2016999686', '2092533696', '1901339769', '1926244710', '2952996586', '1576083089', '2014447384', '2344487751', '1683036431', '2489416480', '2963898896', '2151595823', '2032667779', '2152419614', '2044320304', '2090195299', '2963869708', '2319865523', '2535781046', '1900130540', '1553383431', '2343491855', '2583847974', '2189415357', '2012733443', '2141682101', '2219680690', '2560012396', '1635873181', '2293295048', '2013539280', '2190375219', '2510254469', '1997036878', '1579745827', '2048758495', '1557740912', '2011687201', '2339152847', '2402205534', '2007438484', '2951842360', '2195014314', '2083946815', '170985090', '1608044471', '2962876875', '2196484074', '1981835967', '2546654018', '1547376378', '2518464570', '2407430223', '2037107335', '1987826945', '2080348569', '1580564268', '1985120115', '2963569604', '2231976888', '2085468476', '2501970484', '1892892095', '2393478743', '2963692432', '1982976550', '2296440432', '2081793951', '2460175177', '2052190282', '2078726927', '2328815417', '2466927000', '2291923445', '2403379548', '2120549790', '2059957218', '2234643609', '2024143370', '2484878832', '2188942936', '2441080375', '1562920898', '2028575156', '2096776475', '2952871608', '1552633185', '2128763799', '1966618289', '2088076874', '1972255293', '2520000847', '2550997788', '2493462078', '2952961039', '1481455657', '2182231018', '2294127476', '2502225875', '2171066806', '1581372935', '2179893729', '2341050407', '2511080939', '2070887824', '1761295751', '2021699271', '1540562499', '2952232172', '2075856645', '2059663341', '2292732892', '2963287991', '2334927989', '2041800218', '1563244061', '1966542038', '2091244310', '2117787771', '2290144479', '2076038390', '2061237050', '2111917993', '2518974416', '2036477244', '2144021954', '2261777092', '1524013832', '2292289916', '2950102769', '2259323981', '2144251041', '2509733445', '2586590129', '2519232304', '2043088666', '2326913951', '2616676881', '2289552691', '2531823880', '1982151194', '2198693549', '2289191545', '2100623689', '2139317044', '2340073803', '2078352566', '1490600586', '2121153072', '2040209771', '2291159204', '2031107356', '2076457767', '1952897221', '2949494324', '2228048552', '1981906322', '2952703527', '2509434436', '2218101785', '2950052913', '2293467174', '2040978064', '2131744661', '2548702916', '2185724119', '2201689199', '1697846079', '2057922050', '1692944083', '2288802913', '2045303140', '1763302039', '2147531358', '2518545741', '2077265546', '1855653084', '2560671460', '2963652505', '2007496851', '1870121198', '1939715090', '2566940354', '2482869343', '1557578388', '2548919989', '2290739911', '2059749635', '2262962221', '2963722216', '2024598931', '1976506934', '2520068277', '2438522066', '2076415656', '940678180', '2527435379', '1977207404', '2952800221', '2335610008', '2240983499', '2091618406', '2023633390', '2166064468', '2508523245', '2262641688', '2189347381', '2080036787', '2585411909', '2953237615', '2049736249', '2025622016', '2001715686', '2963411606', '2004857286', '2499409397', '2293899286', '2557898533', '2015563778', '2492051176', '2607642866', '2344765023', '2952174639', '2522190652', '2036030019', '2951419721', '2057565382', '2339895629', '2538876428', '2463526659', '1974849590', '1987998902', '1895366383', '1423052447', '2064511203', '2187681750', '2114393037', '1999955092', '1570487913', '1971619525', '1636094761', '2507439055', '2346272184', '2963437736', '2963742327', '2096705625', '2069151980', '2558235288', '1986586714', '1545619830', '1588797481', '2582685227', '2080872533', '2019012533', '2486860031', '1902846346', '1776974347', '2103972037', '1598430125', '2274594953', '1967286561', '2011652892', '2953189015', '2280339973', '2233020785', '2399162316', '1913949318', '2516393568', '2317308077', '2133797416', '2182417006', '2495930396', '1515886242', '2497490689', '2512789564', '2398051781', '2146038629', '2190983331', '2314487664', '1976675630', '2083435166', '2295289609', '2083547810', '2218593314', '2239825991', '2253544485', '2027500963', '569699014', '2292759272', '2084634022', '2396914874', '2118659723', '1966317186', '2110715360', '2270514833', '2511383248', '1547154698', '2193576262', '2258247785', '2124568462', '2243489190', '2000784360', '2342114053', '2089121898', '2082796814', '2189272674', '2550203133', '2524918773', '2018348074', '2547663239', '2567695990', '2137898377', '2548842562', '1663885374', '1991379467', '2544795213', '2583408947', '1590582473', '2005516521', '2121085736', '2092050237', '2092734317', '2286225772', '2559774973', '2963680448', '2528530289', '1980560153', '1703697374', '1694802015', '2045312961', '2472183167', '2094543712', '2199557176', '1595035639', '2561492764', '580007683', '2147584524', '2119092896', '2016576276', '2963433686', '2402975996', '2004613568', '2542286118', '2293184686', '2157370137', '2056544853', '2054925503', '2135742182', '2286671472', '2520466259', '2183269049', '2342690689', '1975378333', '2061134945', '2320098994', '2486852881', '2952631185', '2485340518', '2100548565', '1974956497', '2147035723', '1982600349', '2542549766', '2017907646', '1910232789', '2508602144', '2964187112', '2467524786', '2527234754', '2072109245', '2241684080', '2520438672', '1645827502', '2561711915', '2568331865', '1964188748', '2460318769', '2090082636', '2254580574', '2413992983', '2010006279', '2205314305', '2963306130', '1964974987', '2062629891', '2291311362', '2195255315', '2142015064', '2231026252', '2963599964', '2045245929', '2553090309', '2256188983', '2963989430', '2016994934', '1505449036', '1550212317', '2049024492', '1978423571', '2094116203', '2539180713', '2140270695', '2134503975', '2322337574', '2067605736', '2200045585', '1553336399', '1977658075', '1520888585', '2016672139', '2325314016', '2292756547', '2004671569', '2271217063', '2113625159', '2142565247', '2126615546', '2152847926', '2156965898', '2736205338', '1999848302', '2142238976', '751677133', '2344555256', '2951519619', '1938636034', '2015358848', '2396887712', '2147056723', '2949831433', '1975073552', '2192659810', '2472080793', '2109549551', '2161106981', '2052622881', '2289872978', '2295370248', '2048098688', '2033509082', '2465125666', '2336403911', '2156343734', '1510911730', '1007743689', '2215920531', '2603051828', '2084336203', '2026500813', '2098814788', '2298279259', '2287320610', '2196078015', '1988817313', '2560450996', '2500148834', '2243705763', '2223557134', '2203659613', '2086412585', '2479087166', '2319752963', '2065395581', '2156373522', '951502412', '2087998432', '1977999980', '2248542721', '1598317042', '2100907248', '1898348575', '2292448732', '1590480603', '2521993221', '2953047992', '2963422853', '2962932059', '2342701392', '986667027', '2208500189', '2048598640', '1609941950', '2344169480', '2314562768', '2295166138', '2368975782', '1975694057', '1903699831', '1976666273', '2074265894', '2093031827', '2023903825', '2963726076', '2041805598', '2949147150', '2519378799', '1516185633', '2593996319', '1907170265', '2288353181', '2288609809', '1700248997', '2215418220', '2206155246', '171518659', '1559521884', '2018891861', '1711964499', '2486164642', '2963159303', '2562028632', '2271871887', '1964813759', '1493000414', '1973823961', '2492669438', '1634253120', '1945176479', '2295830844', '2515483793', '2950412592', '1535655465', '2068395281', '1487762291', '2512019831', '2048629421', '2488753138', '2020154264', '2336038098', '1507824477', '2022628978', '2026207094', '2341675250', '2245233530', '2938488567', '1991878931', '1986261838', '2213657724', '1797104640', '2597031856', '2291813880', '2473343262', '2950143710', '2124040657', '2100933735', '2059545631', '1990524061', '2111939009', '1965724314', '2083437687', '2151771412', '1581485791', '2103851250', '2120333512', '1971486693', '2294775169', '1522932132', '1539934993', '2291936128', '2291665038', '1481036637', '2963809637', '2019412361', '2400825964', '2477606399', '2400729854', '2294665748', '2610221079', '2257955630', '1515962983', '1906838983', '2000949190', '1976787557', '1585931726', '1704719739', '1987280012', '2093226015', '1543446660', '2098777011', '1980088507', '2166145766', '2082473541', '1988176243', '2136289309', '2082117566', '2034035111', '2132317618', '2042411497', '1921940856', '2067279462', '2504613859', '2009022382', '2200905890', '968230761', '2416807117', '1987031311', '2246689711', '2534409167', '1995042902', '2039501150', '2279472773', '1920202025', '2104764386', '2148921431', '2170078282', '2095875830', '2203104858', '2112785353', '2128146199', '2055923209', '855061766', '2276981860', '2296627564', '2963545705', '2218620158', '1988637071', '2520766356', '2762160885', '2074188112', '2963042019', '2963820164', '2539692090', '2529057847', '2162642454', '2511549126', '1970113402', '1747072732', '1520013525', '1587872684', '1558077575', '1502325265', '2144848739', '2122616116', '2511827679', '2124274640', '2086204859', '1999833500', '2297832764', '2511985069', '2163736641', '1489182777', '2038329885', '2145187705', '2093877440', '1867703244', '2164436981', '229759986', '2053850706', '2042842731', '2488265751', '2075726076', '2274218125', '2072048111', '2441581728', '2292161501', '2079067784', '2583119666', '2316308291', '2296498262', '2344299770', '2185100331', '2046462440', '2182495519', '2520289511', '2499387191', '2194569170', '2156918229', '2240342643', '1606470095', '2002660669', '2407603800', '2471673817', '1975609575', '2213586231', '2341774112', '2397858385', '2267049979', '2299729451', '2149221419', '1575393389', '1949032649', '2126195975', '2085405697', '1494749573', '2033752851', '2530517520', '2494218130', '2292140884', '2056012330', '1518438959', '1598618986', '1588635273', '2132135582', '2014759965', '1579985442', '2097800432', '2075427741', '2163637915', '2092397817', '1667688530', '1602720105', '2540485954', '2122567460', '1980235469', '2111534748', '2162324018', '2112300062', '1495650669', '1602205353', '2055568125', '838780744', '1787689942', '2166627902', '2475831358', '2963887063', '2950931348', '2964182871', '2020771020', '1902678874', '2088717366', '2240613635', '2289434109', '2077717186', '2145367705', '2951017956', '2088212435', '2090271790', '1979905340', '2951377799', '2081563253', '2167648760', '2088754180', '2430451911', '2098444394', '2102521360', '2406321701', '2241289263', '2127104921', '2557668865', '2014075533', '2171613969', '2055200363', '1974823549', '2005287749', '2592929647', '2160644488', '2067990194', '1608520144', '1872314928', '2407427184', '2281629663', '2029682362', '2311514773', '2042741029', '2000348124', '2054178666', '2163770389', '2101893936', '2406697160', '1523526729', '2054856910', '962555085', '2396559331', '2160494039', '227904293', '2407842861', '1985478128', '1515234589', '2131693249', '1870272984', '2047709837', '2216544441', '2090397075', '2149577913', '2291810350', '2091012876', '2963933003', '2949881731', '2018252898', '1985811162', '2026017062', '2105392125', '2109288811', '1603841033', '1765929735', '1505000263', '1996092374', '2269519812', '610891907', '2083506609', '2063396228', '2010993231', '2083020163', '2584238769', '836396136', '2141949717', '2210356422', '2158871785', '1512342304', '942682199', '2089274856', '2157053920', '1982839138', '1601396701', '2098257653', '2029884227', '2137465514', '2032558313', '2148812503', '2343321058', '2108375932', '2044806061', '1954502265', '2529211288', '1622810908', '2093295575', '2257006526', '2316629361', '2158057034', '2114510658', '2052247134', '2052310292', '2139015614', '2142931787', '2021459301', '1525277621', '2171652334', '2541169019', '2399091500', '2064567807', '2374986748', '1654832238', '2005470419', '2509202454', '2262813238', '2951052324', '1975478173', '2086214525', '2324552872', '1585508658', '966173886', '2065489737', '2288308878', '2290660983', '2120510547', '2008353351', '2517020940', '2928792846', '2069477970', '2142149569', '2131310293', '2142433466', '1992901080', '2122416065', '2122909807', '2490530049', '2251086141', '2168036408', '1840195231', '2104141128', '1747682845', '2028648808', '2119549704', '2277452608', '2137662449', '2119207405', '2158273901', '1971662949', '1491490124', '2086913878', '2153168526', '2104499965', '2246255603', '2057501921', '2249542180', '2276066255', '1950170246', '2168381034', '1603624148', '2043903120', '2123471405', '2127401968', '2171603359', '2124233298', '2345562294', '1754865996', '2120335985', '1541153678', '1534469893', '2065785654', '1488640028', '2084150570', '2098719542', '2058994941', '2033645022', '1895970761', '2063196025', '2290142157', '2278832915', '2047459700', '2343589627', '1560563382', '2520873811', '1999162150', '254932647', '2247161821', '2201575775', '1557872956', '2033453086', '2013506486', '994102517', '2120128807', '2047998065', '2257467327', '2596843293', '2187164504', '2164834527', '2150764210', '2041277817', '2250630283', '2066628341', '2045862607', '1980766085', '1698719573', '2963080915', '2061159444', '2141253333', '2033423684', '2155346106', '2033858082', '2104039986', '2270273871', '1965357332', '2023443307', '1974395279', '2404595987', '2139803198', '2157758160', '2049624983', '2137434758', '1966725577', '2540018060', '2511486895', '2108266973', '77462886', '2138278210', '2129370088'], 'bm25_doc_scores': [24.412066, 24.137304, 23.466352, 23.140022, 22.958736, 22.900997, 22.897522, 22.68777, 22.550488, 21.987331, 21.74718, 21.74665, 21.73182, 21.73182, 21.728598, 21.728598, 21.38786, 21.199873, 21.16306, 21.157913, 21.08444, 20.861141, 20.824612, 20.791367, 20.791367, 20.716415, 20.700502, 20.611689, 20.580662, 19.904222, 19.879185, 19.819391, 19.810507, 19.810507, 19.75505, 19.71514, 19.705944, 19.705944, 19.66171, 19.65863, 19.589228, 19.562239, 19.379808, 19.372837, 19.372414, 19.36259, 19.331703, 19.323435, 19.323435, 19.317596, 19.279976, 19.26659, 19.26659, 19.2519, 19.247873, 19.247873, 19.247654, 19.186352, 19.064981, 19.062735, 19.04747, 19.011852, 19.005028, 19.004457, 19.004457, 19.004457, 19.004457, 19.004457, 18.911764, 18.885643, 18.864613, 18.846115, 18.826073, 18.81281, 18.791061, 18.790653, 18.790653, 18.751339, 18.711254, 18.684708, 18.663843, 18.650768, 18.650768, 18.649944, 18.571442, 18.563578, 18.512955, 18.512955, 18.512955, 18.478006, 18.448313, 18.432507, 18.394442, 18.394442, 18.393223, 18.393223, 18.393223, 18.393223, 18.37716, 18.37716, 18.37716, 18.37716, 18.37716, 18.309212, 18.309212, 18.309212, 18.24478, 18.24478, 18.24478, 18.243343, 18.243343, 18.24086, 18.239626, 18.225964, 18.220337, 18.202219, 18.14347, 18.14347, 18.14347, 18.14347, 18.138268, 18.111462, 18.097202, 18.079178, 18.04729, 18.018063, 18.013607, 18.005142, 17.981472, 17.980705, 17.980705, 17.980705, 17.976494, 17.964535, 17.952702, 17.952702, 17.952702, 17.939688, 17.929455, 17.929455, 17.925827, 17.905693, 17.872543, 17.853336, 17.853336, 17.853336, 17.820833, 17.820833, 17.820833, 17.820833, 17.820833, 17.81435, 17.81017, 17.81017, 17.766924, 17.759342, 17.746317, 17.746317, 17.73823, 17.669895, 17.663778, 17.663778, 17.663778, 17.663778, 17.663778, 17.663778, 17.663778, 17.663778, 17.663778, 17.662544, 17.656158, 17.656158, 17.650364, 17.650364, 17.650364, 17.64288, 17.566925, 17.559385, 17.559385, 17.559385, 17.559385, 17.541424, 17.531834, 17.531834, 17.531834, 17.511944, 17.509468, 17.509468, 17.509468, 17.509468, 17.509468, 17.509468, 17.491493, 17.47918, 17.457424, 17.457424, 17.429115, 17.424736, 17.395926, 17.391169, 17.391169, 17.391169, 17.37569, 17.365091, 17.365091, 17.358547, 17.358547, 17.358547, 17.358547, 17.358547, 17.357834, 17.357834, 17.357834, 17.357834, 17.357834, 17.357834, 17.35664, 17.35664, 17.327847, 17.283627, 17.281433, 17.257013, 17.257013, 17.257013, 17.218933, 17.218933, 17.210354, 17.210354, 17.208797, 17.208797, 17.208797, 17.208797, 17.208797, 17.208797, 17.208797, 17.208797, 17.208797, 17.208797, 17.208797, 17.193754, 17.19231, 17.192223, 17.158524, 17.158524, 17.158524, 17.121298, 17.109497, 17.109497, 17.08043, 17.08038, 17.063791, 17.062304, 17.062304, 17.062304, 17.061152, 17.061152, 17.061152, 17.061152, 17.061152, 17.061152, 17.061152, 17.050116, 17.050116, 17.050116, 17.050116, 17.042086, 16.964878, 16.964878, 16.964878, 16.964878, 16.964878, 16.964878, 16.964878, 16.957363, 16.924088, 16.918282, 16.918282, 16.918282, 16.918282, 16.918282, 16.918282, 16.918282, 16.910553, 16.884613, 16.884613, 16.884613, 16.884613, 16.884613, 16.884613, 16.880386, 16.834888, 16.81496, 16.786829, 16.775555, 16.775555, 16.775555, 16.775555, 16.775555, 16.775555, 16.775555, 16.775555, 16.775555, 16.775555, 16.775555, 16.722328, 16.722328, 16.694218, 16.658173, 16.63741, 16.63741, 16.63184, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.59041, 16.580559, 16.580559, 16.565372, 16.563164, 16.563164, 16.53538, 16.524158, 16.49721, 16.437656, 16.429594, 16.40931, 16.40931, 16.40931, 16.40931, 16.40931, 16.40931, 16.40931, 16.40931, 16.40931, 16.40931, 16.40931, 16.40931, 16.40931, 16.407036, 16.407036, 16.407036, 16.400091, 16.36571, 16.36571, 16.36571, 16.352169, 16.31774, 16.31774, 16.316814, 16.297508, 16.253853, 16.234226, 16.234226, 16.234226, 16.234226, 16.232119, 16.232119, 16.232119, 16.232119, 16.232119, 16.232119, 16.232119, 16.232119, 16.232119, 16.230034, 16.230034, 16.221292, 16.159725, 16.115276, 16.115276, 16.102741, 16.079504, 16.059368, 16.058712, 16.058712, 16.058712, 16.058712, 16.058712, 16.058712, 16.02903, 16.010662, 16.010662, 15.99917, 15.973852, 15.968456, 15.968456, 15.955996, 15.955996, 15.91514, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.888974, 15.873583, 15.845111, 15.845111, 15.845111, 15.815018, 15.805538, 15.805538, 15.805538, 15.805538, 15.805538, 15.79567, 15.771542, 15.771542, 15.729284, 15.722786, 15.722786, 15.722786, 15.722786, 15.722786, 15.722786, 15.722786, 15.722786, 15.722786, 15.722786, 15.72083, 15.72083, 15.72083, 15.72083, 15.693666, 15.6689625, 15.658363, 15.657523, 15.657523, 15.635408, 15.621517, 15.605703, 15.605703, 15.605703, 15.605703, 15.598482, 15.598482, 15.598482, 15.598482, 15.596043, 15.584787, 15.582805, 15.570694, 15.564502, 15.560037, 15.560037, 15.560037, 15.560037, 15.560037, 15.560037, 15.560037, 15.560037, 15.555318, 15.529486, 15.529486, 15.529486, 15.529486, 15.529486, 15.492502, 15.489229, 15.488599, 15.478024, 15.478024, 15.478024, 15.478024, 15.478024, 15.478024, 15.478024, 15.478024, 15.478024, 15.410952, 15.410952, 15.410952, 15.410952, 15.368052, 15.366793, 15.359413, 15.359413, 15.359413, 15.359413, 15.359413, 15.359413, 15.359413, 15.359413, 15.359413, 15.359413, 15.329777, 15.318706, 15.313509, 15.310794, 15.290757, 15.288124, 15.286208, 15.271627, 15.269243, 15.26569, 15.244444, 15.244444, 15.244444, 15.242605, 15.242605, 15.242605, 15.242605, 15.242605, 15.242605, 15.230314, 15.221088, 15.221088, 15.221088, 15.20634, 15.190572, 15.176369, 15.176369, 15.173591, 15.148242, 15.127561, 15.127561, 15.127561, 15.127561, 15.127561, 15.127561, 15.127561, 15.127561, 15.127561, 15.127561, 15.127561, 15.127561, 15.060448, 15.060448, 15.057035, 15.05511, 15.05511, 15.05511, 15.035928, 15.035928, 15.035928, 15.035928, 15.035802, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.01424, 15.009798, 14.990688, 14.962928, 14.946968, 14.94635, 14.945164, 14.945164, 14.941397, 14.941397, 14.941397, 14.923009, 14.916704, 14.916704, 14.907768, 14.907768, 14.891508, 14.889563, 14.861813, 14.840517, 14.840517, 14.837285, 14.837285, 14.81713, 14.80962, 14.796295, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.792616, 14.767828, 14.767828, 14.767686, 14.7579365, 14.7579365, 14.749388, 14.746172, 14.744816, 14.728989, 14.72889, 14.725488, 14.716967, 14.695299, 14.670271, 14.669014, 14.669014, 14.668953, 14.665687, 14.664795, 14.657129, 14.650166, 14.646119, 14.632856, 14.632856, 14.629047, 14.614059, 14.611433, 14.605548, 14.582931, 14.582931, 14.582931, 14.581878, 14.581739, 14.581739, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.57744, 14.560753, 14.544133, 14.526831, 14.525918, 14.524849, 14.506954, 14.506954, 14.506954, 14.504641, 14.503092, 14.460375, 14.453503, 14.442538, 14.420189, 14.420189, 14.420189, 14.414742, 14.414742, 14.401522, 14.398563, 14.375232, 14.369141, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.368435, 14.358196, 14.3579645, 14.357689, 14.344199, 14.344199, 14.344199, 14.344199, 14.332279, 14.331229, 14.331229, 14.327936, 14.317066, 14.294653, 14.26447, 14.26447, 14.26104, 14.26104, 14.24733, 14.24733, 14.239749, 14.217476, 14.199485, 14.199485, 14.19072, 14.19072, 14.187487, 14.187487, 14.187487, 14.174828, 14.17221, 14.17221, 14.17221, 14.1653385, 14.1653385, 14.1653385, 14.1653385, 14.1653385, 14.1653385, 14.1653385, 14.1653385, 14.1653385, 14.1653385, 14.1653385, 14.1653385, 14.1653385, 14.1653385, 14.1653385, 14.160494, 14.157188, 14.150918, 14.150918, 14.144853, 14.141825, 14.141825, 14.139752, 14.126942, 14.126942, 14.113716, 14.105366, 14.105366, 14.105366, 14.105366, 14.1028805, 14.100481, 14.09203, 14.08116, 14.08116, 14.070619, 14.069151, 14.069151, 14.069151, 14.068723, 14.067921, 14.065155, 14.065155, 14.065155, 14.057779, 14.057208, 14.038801, 14.034363, 14.034363, 14.034363, 14.034363, 14.034363, 14.034363, 14.034363, 14.010344, 14.0079775, 14.0079775, 14.007267, 14.001413, 14.001413, 13.997573, 13.991296, 13.991296, 13.991296, 13.991296, 13.991296, 13.970154, 13.967903, 13.967903, 13.967903, 13.967903, 13.967903, 13.967903, 13.967903, 13.967903, 13.960054, 13.960054, 13.951382, 13.951382, 13.9512615, 13.935913, 13.929792, 13.928284, 13.928284, 13.911219, 13.911219, 13.909482, 13.907656, 13.907656, 13.902594, 13.902594, 13.902594, 13.895004, 13.894702, 13.894702, 13.890704, 13.884704, 13.884704, 13.884704, 13.884704, 13.869286, 13.864718, 13.857794, 13.853529, 13.853529, 13.841332, 13.841332, 13.839197, 13.827595, 13.827595, 13.81935, 13.81935, 13.812874, 13.81021, 13.81021, 13.803995, 13.796551, 13.79597, 13.79597, 13.79597, 13.790215, 13.783838, 13.780317, 13.775894, 13.775894, 13.775894, 13.775894, 13.775894, 13.775894, 13.775894, 13.775894, 13.775894, 13.7707, 13.7707, 13.754698, 13.754698, 13.751823, 13.751823, 13.751063, 13.751063, 13.751063, 13.751063, 13.732986, 13.728586, 13.728586, 13.728586, 13.728586, 13.728586, 13.728586, 13.728586, 13.728586, 13.727074, 13.727074, 13.719225, 13.7162, 13.7162, 13.704496, 13.704496, 13.704496, 13.704496, 13.702706, 13.702706, 13.702706, 13.699174, 13.699174, 13.699174, 13.676764, 13.672703, 13.66192, 13.656174, 13.656174, 13.645254, 13.645254, 13.645254, 13.645254, 13.645254, 13.637583, 13.635962, 13.622134, 13.621065, 13.620383, 13.620383, 13.608038, 13.605837, 13.598215, 13.595286, 13.595286, 13.594559, 13.594559, 13.594559, 13.589096, 13.589096, 13.589096, 13.589096, 13.589096, 13.589096, 13.589096, 13.589096, 13.589096, 13.589096, 13.589096, 13.586508, 13.586508, 13.564713, 13.564713, 13.564713, 13.564713, 13.564713, 13.564713, 13.559905, 13.559905, 13.559905, 13.559905, 13.558964, 13.558964, 13.558964, 13.55483, 13.540015, 13.526398, 13.526398, 13.525942, 13.525942, 13.525942, 13.515234, 13.514052, 13.514052, 13.513552, 13.513552, 13.513552, 13.513552, 13.513552, 13.5112095, 13.505842, 13.501019, 13.501019, 13.501019, 13.500378, 13.497918, 13.495647, 13.486702]}, {'id': '2540977243', 'text': 'statistic features base real time detection drift twitter spam', 'timestamp': 1490997600, 'rel_doc_ids': ['1526831942', '1554862173', '1573258035', '1986678144', '1993081839', '1996802155', '1998871422', '2098395374', '2107541186', '2125347318', '2158568356', '2163764145', '2165701072', '2553878259', '2963208026'], 'user_id': '2133814358', 'user_doc_ids': ['203709580', '300703538', '1040508638', '1505773145', '1535144957', '1542182453', '1545443952', '1555336707', '1558453077', '1576160170', '1800052238', '1875839673', '1942358607', '1966491983', '1967136009', '1969579989', '1971610995', '1972878411', '1975322997', '1978898436', '1982463884', '1982685002', '1982694274', '1984601143', '1984984326', '1986190836', '1987364304', '1987519072', '1989193672', '1990543788', '1991511814', '1992220535', '1992727250', '1993876244', '1994025096', '1994345874', '1995703300', '2001760830', '2002476668', '2003842956', '2006725573', '2007707818', '2007725839', '2009777652', '2011279403', '2011604344', '2012175200', '2016164545', '2017004545', '2018067627', '2018485550', '2019584880', '2022974505', '2025665684', '2026744045', '2027784199', '2034244766', '2035343789', '2036285251', '2038463591', '2040493347', '2045283442', '2045884766', '2046799051', '2047781619', '2055467941', '2055495204', '2057867790', '2058660427', '2059992938', '2062011073', '2062488906', '2063652806', '2067093797', '2068420673', '2069564486', '2072600107', '2074057010', '2076284194', '2076947796', '2077903042', '2078740191', '2081842460', '2082122839', '2082612354', '2082897135', '2086466332', '2087406327', '2089932827', '2093945522', '2096962951', '2097417601', '2097463071', '2098344851', '2098927482', '2099563412', '2100121800', '2101783672', '2103243101', '2104923237', '2105317041', '2107477285', '2109507695', '2111503558', '2112849183', '2114114938', '2116663226', '2116970365', '2118569014', '2119706853', '2120259385', '2120839652', '2123657394', '2126463735', '2129701190', '2130844264', '2131252712', '2132035635', '2132988513', '2133781811', '2134076886', '2134253333', '2135808715', '2135960355', '2138702677', '2139947400', '2143494522', '2143958251', '2146227720', '2146333641', '2147468145', '2148289319', '2149707865', '2151037416', '2151620488', '2151891161', '2153159014', '2153749319', '2159453242', '2162535717', '2162952134', '2165339374', '2165425095', '2165730492', '2166961163', '2167017872', '2168130745', '2168677400', '2202405147', '2240141800', '2303544238', '2328195623', '2461171666', '2504797124', '2522453496', '2527840905', '2528038511'], 'bm25_doc_ids': ['2398905593', '1554862173', '2107541186', '2090182207', '2158568356', '2295464403', '1526831942', '1929765742', '2962680986', '1517046895', '2028475679', '2547879817', '1998871422', '1993081839', '2209809854', '2505992194', '2398757235', '1233141674', '2158076717', '1850562331', '2474087371', '143803448', '2556657590', '1573258035', '146417747', '2020826871', '2020754036', '1600782690', '1989573911', '1561558549', '2163764145', '2070302372', '2241850672', '2087619260', '2030233494', '1998859960', '2077916364', '2346278349', '2268219748', '1993871915', '2243295299', '2166781513', '1968314519', '2083951363', '2251925957', '2030833701', '2396651124', '281005445', '1996802155', '2044709159', '2125347318', '2404821121', '2008347539', '1987732684', '1997974141', '2136971460', '269675895', '2005556331', '2118599520', '2165701072', '1651784994', '2001706110', '2102058195', '1813583885', '2000908731', '2417850884', '2051681257', '2064237629', '2099502475', '2072715695', '46636081', '2098395374', '2159511354', '141775348', '2510717748', '2491512594', '115173597', '2046245151', '2066935271', '1931723352', '196347582', '2158270125', '1545645313', '2205844474', '8444177', '1986678144', '113059239', '2012974247', '2085704997', '1979332355', '1980298600', '2082891496', '2100718462', '2160952429', '2103516631', '2286465138', '2136917637', '2230176481', '1854430333', '2113136835', '2346875348', '201259697', '2086416261', '51312703', '1973668723', '1494372926', '2521164685', '1849719402', '2102223555', '2339765045', '1797988549', '1873936197', '1988814344', '2062152380', '2296560196', '1662284448', '2099924982', '2135727005', '1590729858', '822069289', '2146500291', '637393809', '2088162362', '2198186897', '2124499489', '1832529897', '1562141712', '2097356025', '2105521254', '614737293', '110420324', '2144911507', '1890727290', '2125361505', '2064494041', '2129487958', '1563333130', '2106476394', '2047522174', '2399652602', '1578390167', '2045458087', '1661536979', '2015587001', '2160612539', '2089159695', '1970801611', '2141127666', '2306432270', '2128509431', '73651556', '2078427117', '2008445509', '2166202924', '1999768179', '2002349220', '2013734030', '205828981', '2408112618', '2163867371', '2052250245', '2293219949', '2011949237', '2337150356', '2013177973', '2023271960', '1994940238', '2131076952', '1883304999', '1943015726', '1991839217', '2106755564', '2105480048', '2124969208', '2254816288', '1938800651', '401229429', '2248413252', '1908960008', '2082050349', '1993808777', '1994424971', '2124758299', '1873218251', '2039390459', '2039190653', '2094462980', '960613343', '2217754596', '1983915931', '75938866', '1997199381', '2099755974', '2125390010', '1974647321', '1653166509', '1600653717', '2054869233', '119509267', '2080083917', '1532350976', '2287223902', '2963523292', '2071719040', '2028679974', '2102626934', '1998344004', '2295063801', '1927571234', '1612095785', '2160654919', '1976866112', '2023670764', '1918031619', '1027438488', '2052283750', '2079799661', '2112358949', '2261337388', '2216726495', '2461345536', '5916089', '2081591787', '2204759918', '2080913999', '1994817155', '1818695835', '1565083473', '2102629895', '134314029', '2335942905', '2021711480', '1767160258', '2053241453', '2039707439', '2089077498', '2137168471', '2052549182', '2963208026', '2583516892', '1976607364', '965920791', '2121430617', '2296094432', '2112213600', '2052427829', '2028611501', '1984160751', '2963588880', '1674228750', '2111043420', '2130196635', '2096425971', '1551975479', '2014245071', '2396450147', '1987429824', '2152775624', '2092982915', '1802331303', '2023917709', '2204537309', '2160265010', '2026140487', '2124462883', '2014454002', '2173744076', '1820138', '2014398636', '2515161830', '2162877415', '2060382346', '2086504900', '2156223715', '2103881442', '2569250462', '2963208797', '2500666914', '1780541288', '2063197985', '1905065283', '1724358049', '2107469357', '2062088920', '2069128736', '848902177', '2167071579', '2151673632', '2407316094', '346733163', '2016627120', '1866408647', '2100081831', '1682770403', '176139705', '1572662288', '2140316968', '2027116318', '2053915267', '2039641042', '2310485631', '884764891', '2018150842', '1688698383', '79354004', '2164238124', '2141413155', '2165283569', '2057340441', '2039281672', '1962414156', '1969568357', '1863656644', '1989597542', '2126675855', '2033420644', '2150106467', '2105173205', '2138894494', '2152936979', '2022616004', '2047537658', '2017317305', '1973895586', '2249072679', '1860364451', '2128650797', '2293769660', '2047386129', '1529944579', '2067636034', '73634652', '2029551688', '2236870451', '1978865100', '1980345777', '2052312461', '2104821394', '2009119083', '2120968007', '2114402392', '61081441', '1498333281', '1758505613', '2037242621', '2952142603', '2168377083', '1553273090', '2043162934', '2146265555', '1988394183', '2532357529', '2033960944', '1993784310', '2257283960', '2058088532', '2102232904', '2296452361', '1516801063', '2055043826', '2142542731', '1634549304', '2241524359', '2553766993', '1683040692', '1979233855', '2174920082', '106172520', '2071998402', '2293296324', '2011746316', '2173089013', '2087211161', '2038276563', '2062332855', '110633000', '1815362064', '2244133696', '2046210419', '2235038291', '1585854823', '2038221484', '2127241327', '1496526573', '1975016259', '1980672078', '2206554098', '1558753140', '2123449924', '236300676', '1999373354', '1529004212', '2039724772', '1975223096', '1987303754', '2411986398', '2168192700', '1968418810', '2151648721', '2016744715', '2416164492', '1976467781', '1964442799', '2015001165', '2143046236', '2139164688', '2463319373', '919122619', '2949693138', '1979012928', '2553878259', '2031796265', '2105424043', '2049395791', '89590731', '2009748653', '1965829639', '2142863739', '2290443121', '2141971016', '1895662064', '2056076776', '2142869398', '2024278742', '189250639', '1510642132', '2156805595', '2091299110', '2130022202', '1553542546', '2067344105', '2244645434', '2096476632', '1589802283', '2005008227', '1594228157', '1564243574', '2041709326', '2953124317', '1978717800', '2168069066', '2205436487', '2138872119', '1989079872', '2018165284', '2008664567', '2949406927', '1988307605', '1880223052', '2144043567', '1449208847', '1982958099', '196350606', '1892820855', '2145485370', '2301943589', '1481472066', '2146859361', '2131873610', '148554337', '2097350013', '113187444', '2149621472', '2084611828', '1983151163', '1911786799', '2162979314', '2132535758', '301116043', '2293330917', '2070241949', '2060012275', '2166770220', '1553253930', '1484357154', '2046124188', '2020791430', '2060009247', '2042909046', '2293496281', '2283302759', '2003798735', '1983777919', '1659760629', '2037214977', '2102238388', '1924650641', '2016144722', '2050099642', '2017270267', '2065433353', '1599514765', '800461572', '1813480731', '1987365175', '2251567709', '2049023511', '23949839', '2553895521', '1851422430', '1683501046', '2136710010', '2041597619', '1987323290', '2019077935', '1906699800', '2047605254', '2283429330', '2299033095', '1788524306', '2115817283', '1604388503', '2139750075', '159036455', '1590146294', '1796766288', '2062614982', '2250323255', '1971994673', '50940840', '2182578714', '1507821632', '134335166', '101093485', '1970024574', '1566930617', '2099937822', '2200230606', '2187541412', '1973644085', '2094257886', '1493233065', '2166490929', '1966778625', '24428692', '1998299412', '1956666414', '1994344180', '2108059628', '2108462593', '1518922009', '1757967305', '2099302642', '2559256361', '2060174807', '2106321773', '2028712136', '2099633070', '658995882', '2028772504', '2064298023', '1783786644', '2231695724', '2035621475', '2026713428', '2964313162', '1794826094', '2187846933', '2734483151', '2103456180', '2875651713', '1793148911', '314701596', '2038616484', '1541201746', '2388245502', '2007864137', '2006239241', '2951366044', '2117443688', '2066191317', '2107094345', '2131826876', '1931083906', '126636327', '2477147993', '2136648591', '1605750912', '1787335459', '2344028288', '2080290417', '2149995699', '2165797144', '2142173635', '2162348860', '1828930387', '2065576748', '2156654243', '2142526639', '1991804188', '2112537565', '2156253296', '2037930066', '2018025646', '2081334572', '1977058264', '1605096432', '1991121779', '2293402593', '1983012012', '2023361923', '2235780046', '2121343916', '2008624687', '2011893683', '68141439', '2052978498', '2132429870', '2283621191', '2062287158', '2046131484', '2313952855', '2037355127', '2122438174', '2163396107', '1712123015', '1892881989', '207661326', '1586487077', '1571070816', '2518175279', '1986006443', '2136828682', '2255897954', '2156930013', '2113064978', '1999838466', '2134969428', '2070522631', '1983849556', '1539019481', '2136559381', '2044122205', '1970925404', '2099396986', '2053469918', '2042699667', '15988238', '2592335424', '2169627230', '2152592596', '2164397078', '2030008840', '2017258348', '2141243388', '1775665607', '2015063698', '1833209954', '1025075416', '2167318910', '2406261957', '2299239789', '2110519346', '1558411928', '2018115967', '92117805', '1584393767', '2113235416', '2246463822', '2150540492', '2053906954', '2124375849', '2220196580', '2018558592', '2044173330', '2047844561', '1994136430', '2163737512', '2282414926', '1561289889', '2094627020', '1964829577', '2147876157', '2013785969', '2074719413', '2049541992', '2128184786', '160891001', '1501783657', '2105531169', '2092633423', '1981566365', '2135481138', '1660343246', '2509304078', '2952485460', '1672242362', '2105918731', '2114748216', '1650037363', '2550035969', '2197552074', '2217381693', '2138647480', '2153684295', '1731771962', '2271887370', '1997884482', '2170722887', '1995632843', '1907548369', '2387314750', '1561650654', '2124049481', '2131485224', '2152761101', '2541087607', '2130958646', '2120907035', '1976146430', '286890319', '2484086829', '2126127511', '1999281295', '2140050211', '2160134441', '2116413397', '2117138276', '1547743371', '1981605153', '1999458239', '1988354443', '2296704245', '2042835751', '327991062', '1593149011', '1970733677', '204835077', '2125388892', '2135327149', '1481151770', '2103837390', '2332153499', '2145959026', '2028900141', '1600977692', '1518640223', '2052097906', '2247977100', '2123177879', '2043266130', '64724365', '2270741278', '2136698325', '2411456514', '2100738695', '2009359153', '2106766484', '1488640715', '2247971863', '2167265881', '2012322694', '2140427797', '2301815932', '2224794515', '2167384224', '1951294597', '2117372041', '38774257', '2051405935', '2216769490', '2064058256', '22989850', '1657704256', '2114750518', '1970464092', '2157317834', '568396791', '1871142974', '2161537925', '964346738', '2205910341', '2131485284', '2323537218', '2155367144', '1996677075', '1742402942', '2913854892', '2148121208', '1520011673', '2021246779', '2221735899', '2084538762', '1973000493', '2107164345', '2406063490', '2129752272', '2415022573', '2547869030', '2333444811', '1971735434', '1983324541', '2063984881', '1613672546', '2132178626', '2154680954', '2079224581', '2540438180', '2159030150', '2084224313', '2401140887', '1839559394', '2169924934', '2051095246', '2134571681', '2242853700', '2021659075', '2099447390', '2411767884', '1569596443', '2043037645', '2083839627', '2097178527', '2088154795', '1998032016', '2204526832', '2088553752', '2023666594', '1514461580', '132707840', '2120332129', '2081323371', '92684717', '87386348', '2066682493', '2022063488', '2156284506', '2171170967', '2109734180', '1479916289', '2166069449', '2047756776', '2200959369', '1499281013', '2533046532', '2141932001', '1993592567', '2107141100', '2005007001', '2162990900', '1601518299', '2145139979', '2132023035', '2072219467', '1977764012', '2151591744', '1613768052', '1707806712', '2039256651', '2115962466', '2107771570', '2221975861', '2074129227', '1982029265', '1791333417', '2163731957', '2477032014', '1480101260', '2612903413', '2289843914', '2527436263', '2155362843', '2084983099', '2003461943', '843622778', '2043936873', '2167528138', '1590700995', '2104180589', '1774499654', '2952982100', '1996531611', '2172118938', '2077669887', '2060565099', '2240194419', '2050911860', '2063816676', '2171124597', '1574412104', '1979269557', '2099789730', '2188117874', '2042022007', '1722546875', '2004838129', '2131007986', '2037193886', '3805906', '2174714397', '1988189801', '2095376951', '635562148', '2251080896', '2105400363', '1968685081', '2098612466', '2139725602', '2050544466', '1503892994', '1986427413', '2061535005', '2020421778', '2059067870', '2124156373', '1977339759', '2079452002', '2055625904', '2026324356', '2407666331', '1840636859', '2110623368', '2131431452', '2296803955', '2135471293', '2016051345', '1644417361', '1988135700', '331396756', '1984457329', '4776438', '2110446210', '2086401735', '37118932', '2108885157', '2090659861', '2251210340', '2529421958', '2072159774', '1548973753', '1530636824', '1976446088', '2077020916', '2002517322', '2231815655', '2116320027', '2289385123', '2001426180', '2039521808', '2078425582', '1968259071', '2125532150', '2094579808', '2034855885', '2139632979', '1996986584', '2107450959', '67224652', '1840033715', '1925668703', '1999276895', '1480035260', '2274237444', '2016266039', '1570098300', '1541525934', '1687618488', '1937136103', '2951398914', '2104290684', '2099200944', '2168009256', '36725181', '2001883299', '802478723', '2544896492', '2195164399', '2039063855', '2264549447', '2157213381', '2038002297', '2144152199', '992121316', '2021655674', '2116445151', '2184009014', '2167303973', '2063655617', '2120707422', '1574249958', '2415154714', '2508822914', '2141814394', '2185735639', '2171013856', '2170709423', '1973043693', '2121701967', '1539562568', '2100114754', '231212003', '1578622023', '2042467140', '2340204089', '2018388486', '2157103390', '1777095367', '2130586177', '2171410332', '2007628554'], 'bm25_doc_scores': [51.13399, 44.54148, 40.512066, 40.428604, 39.507244, 38.32277, 37.96093, 37.901966, 36.726463, 36.09367, 35.07294, 34.934757, 34.916332, 34.602, 34.50584, 34.45184, 34.345615, 34.25563, 34.178974, 34.076187, 33.991196, 32.8573, 32.577065, 32.193798, 31.927448, 31.635294, 31.533796, 30.761415, 30.623892, 30.320398, 30.113483, 28.89153, 28.879969, 28.783552, 28.748798, 28.73739, 28.622507, 28.618736, 28.591238, 28.565392, 28.328005, 28.302673, 28.218279, 27.988787, 27.96525, 27.822851, 27.807878, 27.701483, 27.66643, 27.648731, 27.627865, 27.590809, 27.583466, 27.47958, 27.43565, 27.401497, 27.38671, 27.386272, 27.340645, 27.235626, 27.122698, 27.09631, 27.019466, 26.939632, 26.857851, 26.847816, 26.80533, 26.70812, 26.69368, 26.53909, 26.528677, 26.510263, 26.3557, 26.346468, 26.277578, 26.137566, 26.120993, 26.110394, 26.047916, 25.99129, 25.982159, 25.97422, 25.943026, 25.862537, 25.720808, 25.69936, 25.68631, 25.672628, 25.666157, 25.663015, 25.661829, 25.621397, 25.587374, 25.582758, 25.56596, 25.56084, 25.550005, 25.521917, 25.500544, 25.483295, 25.46213, 25.454998, 25.409374, 25.389198, 25.369041, 25.365705, 25.315166, 25.285461, 25.257816, 25.231932, 25.08556, 24.917122, 24.915277, 24.899702, 24.87909, 24.841017, 24.822433, 24.79597, 24.786118, 24.707619, 24.706211, 24.69544, 24.67473, 24.5756, 24.574556, 24.573162, 24.53268, 24.528965, 24.525644, 24.468414, 24.459574, 24.457407, 24.432947, 24.422476, 24.390413, 24.388262, 24.382244, 24.379196, 24.341547, 24.328278, 24.273853, 24.25418, 24.152325, 24.072922, 24.04533, 24.005407, 23.98232, 23.975056, 23.971655, 23.923626, 23.917538, 23.915886, 23.903107, 23.854801, 23.840456, 23.725166, 23.694464, 23.654758, 23.55286, 23.550184, 23.543701, 23.538452, 23.518417, 23.518417, 23.446735, 23.442495, 23.43401, 23.43256, 23.42569, 23.419216, 23.398973, 23.347958, 23.328758, 23.310474, 23.284912, 23.276562, 23.252836, 23.234087, 23.22789, 23.202122, 23.198578, 23.185244, 23.182703, 23.164883, 23.091568, 23.077883, 23.065922, 23.046564, 23.015074, 23.011719, 23.008572, 22.958794, 22.949804, 22.932793, 22.909786, 22.909765, 22.896461, 22.878391, 22.87547, 22.85075, 22.848566, 22.835732, 22.811983, 22.794659, 22.787466, 22.748728, 22.728401, 22.72286, 22.688353, 22.644396, 22.641113, 22.620855, 22.620451, 22.606476, 22.599194, 22.582718, 22.54613, 22.533535, 22.533245, 22.510593, 22.498299, 22.488197, 22.487656, 22.484968, 22.477373, 22.467566, 22.450617, 22.415493, 22.405622, 22.403511, 22.399565, 22.3907, 22.387522, 22.385153, 22.370443, 22.365223, 22.350204, 22.334766, 22.305445, 22.271662, 22.257402, 22.255322, 22.234718, 22.22756, 22.22106, 22.198292, 22.194569, 22.166555, 22.156488, 22.156488, 22.14738, 22.145206, 22.134233, 22.133799, 22.124136, 22.089476, 22.082146, 22.04372, 22.035158, 22.01125, 22.010317, 21.987265, 21.987265, 21.98476, 21.983332, 21.98217, 21.959642, 21.950155, 21.946411, 21.936756, 21.922255, 21.919182, 21.915146, 21.889166, 21.876022, 21.85067, 21.806631, 21.803574, 21.785595, 21.780619, 21.774616, 21.758537, 21.758156, 21.754562, 21.73547, 21.734118, 21.729979, 21.729519, 21.72467, 21.719418, 21.71417, 21.706783, 21.697277, 21.693802, 21.679691, 21.677582, 21.664051, 21.65769, 21.64659, 21.643873, 21.642998, 21.588943, 21.58682, 21.57614, 21.571655, 21.568336, 21.560596, 21.557205, 21.556942, 21.520811, 21.513689, 21.495632, 21.492233, 21.491823, 21.480587, 21.477364, 21.47527, 21.469496, 21.45459, 21.452866, 21.442453, 21.436792, 21.417229, 21.399055, 21.389723, 21.385572, 21.365229, 21.343214, 21.343016, 21.332321, 21.330845, 21.319452, 21.288685, 21.278248, 21.277939, 21.275108, 21.2449, 21.230606, 21.230223, 21.20529, 21.202658, 21.18533, 21.165972, 21.165972, 21.161886, 21.142832, 21.135954, 21.129992, 21.127724, 21.123938, 21.11592, 21.112028, 21.092371, 21.091347, 21.077425, 21.053675, 21.046745, 21.046675, 21.045534, 21.038326, 21.032444, 21.030725, 20.990723, 20.989408, 20.988144, 20.9877, 20.979883, 20.976131, 20.975616, 20.96342, 20.958977, 20.946808, 20.93522, 20.930878, 20.92187, 20.921108, 20.917973, 20.908228, 20.899431, 20.878773, 20.870628, 20.869448, 20.869377, 20.867027, 20.864994, 20.86355, 20.859003, 20.857712, 20.850952, 20.847353, 20.844364, 20.828487, 20.824621, 20.801155, 20.772816, 20.768497, 20.751085, 20.74856, 20.738379, 20.732323, 20.7252, 20.716253, 20.705639, 20.704372, 20.70272, 20.698093, 20.68466, 20.677784, 20.67215, 20.66698, 20.659973, 20.6567, 20.654848, 20.648518, 20.644474, 20.641308, 20.638683, 20.636036, 20.60387, 20.603134, 20.600279, 20.598412, 20.595015, 20.579563, 20.551846, 20.551321, 20.55045, 20.540092, 20.527262, 20.52654, 20.513432, 20.510118, 20.508793, 20.487226, 20.486284, 20.48081, 20.4643, 20.462885, 20.446712, 20.444998, 20.439411, 20.413818, 20.413284, 20.411016, 20.388985, 20.372728, 20.369818, 20.36845, 20.351013, 20.34649, 20.332508, 20.320963, 20.314127, 20.31008, 20.285118, 20.282421, 20.27575, 20.275747, 20.274443, 20.25984, 20.254627, 20.252352, 20.24623, 20.243212, 20.235518, 20.228918, 20.21467, 20.195604, 20.19183, 20.191734, 20.190847, 20.187313, 20.1857, 20.182352, 20.178083, 20.170692, 20.17026, 20.168354, 20.166681, 20.160204, 20.158733, 20.144344, 20.13945, 20.123745, 20.114174, 20.108585, 20.091398, 20.089123, 20.08801, 20.068764, 20.0658, 20.056988, 20.055592, 20.045568, 20.029486, 20.022114, 20.01347, 19.999733, 19.997826, 19.995947, 19.993553, 19.987934, 19.987347, 19.982891, 19.982544, 19.96867, 19.964382, 19.954626, 19.953423, 19.94953, 19.940712, 19.939335, 19.927979, 19.920605, 19.916294, 19.908682, 19.908499, 19.905884, 19.892452, 19.890266, 19.878716, 19.860374, 19.858322, 19.855162, 19.84865, 19.844376, 19.837208, 19.83584, 19.82975, 19.818892, 19.818668, 19.803402, 19.802588, 19.780342, 19.777864, 19.775415, 19.766876, 19.765976, 19.762579, 19.760223, 19.75428, 19.75111, 19.74355, 19.738052, 19.73768, 19.734993, 19.732231, 19.73151, 19.728817, 19.725052, 19.716208, 19.705042, 19.702724, 19.69628, 19.692871, 19.692871, 19.690807, 19.686537, 19.676834, 19.662682, 19.659489, 19.652142, 19.651278, 19.637943, 19.63616, 19.634842, 19.627743, 19.622377, 19.622377, 19.617533, 19.614067, 19.611736, 19.609774, 19.596952, 19.596155, 19.595024, 19.58965, 19.57798, 19.575241, 19.572205, 19.570627, 19.56973, 19.56973, 19.566992, 19.565607, 19.564102, 19.560295, 19.557041, 19.556576, 19.555307, 19.533342, 19.524572, 19.52383, 19.52359, 19.522797, 19.510996, 19.5076, 19.505098, 19.495853, 19.48843, 19.48687, 19.484882, 19.48075, 19.478748, 19.478617, 19.469177, 19.467669, 19.466002, 19.462456, 19.449896, 19.447731, 19.446686, 19.442732, 19.436571, 19.42837, 19.416044, 19.414276, 19.413494, 19.413494, 19.402864, 19.402023, 19.401596, 19.400469, 19.386784, 19.382523, 19.381105, 19.379398, 19.378458, 19.3763, 19.37406, 19.372204, 19.371935, 19.369259, 19.369202, 19.36785, 19.35663, 19.356352, 19.354513, 19.35308, 19.35083, 19.349396, 19.347506, 19.347174, 19.345652, 19.34054, 19.333853, 19.330513, 19.330048, 19.31928, 19.319118, 19.318192, 19.315878, 19.31319, 19.29955, 19.297678, 19.295307, 19.294672, 19.291742, 19.283705, 19.275278, 19.27468, 19.269823, 19.269823, 19.267576, 19.262203, 19.257553, 19.25169, 19.240929, 19.238445, 19.222477, 19.222391, 19.215588, 19.211266, 19.208841, 19.203926, 19.201603, 19.184101, 19.149647, 19.149542, 19.145601, 19.144424, 19.140074, 19.13428, 19.128641, 19.127356, 19.121075, 19.121075, 19.116528, 19.112364, 19.112364, 19.104086, 19.103191, 19.101555, 19.094477, 19.092556, 19.091587, 19.086224, 19.084278, 19.0825, 19.071465, 19.07077, 19.069023, 19.069023, 19.0625, 19.060137, 19.058853, 19.05283, 19.050344, 19.04913, 19.042372, 19.033905, 19.019339, 19.016436, 19.008675, 18.995476, 18.994469, 18.991146, 18.986746, 18.98599, 18.983326, 18.980211, 18.980074, 18.978899, 18.978386, 18.97252, 18.97144, 18.970518, 18.970007, 18.969934, 18.930656, 18.929525, 18.928, 18.914, 18.909618, 18.909084, 18.908293, 18.893095, 18.890438, 18.883154, 18.877745, 18.874104, 18.870972, 18.863499, 18.856907, 18.854424, 18.846006, 18.84213, 18.832312, 18.82763, 18.823015, 18.82223, 18.819258, 18.819248, 18.813671, 18.813332, 18.812992, 18.810247, 18.79853, 18.796513, 18.79601, 18.794909, 18.791672, 18.787882, 18.787344, 18.786175, 18.779472, 18.777325, 18.771255, 18.769978, 18.767786, 18.767506, 18.765345, 18.756521, 18.74775, 18.746107, 18.734358, 18.731848, 18.727692, 18.727407, 18.722914, 18.717793, 18.715517, 18.715351, 18.703218, 18.702213, 18.701859, 18.699291, 18.697979, 18.697695, 18.690678, 18.686491, 18.685188, 18.683315, 18.680298, 18.679361, 18.6705, 18.652935, 18.65166, 18.650843, 18.647846, 18.64575, 18.64123, 18.634897, 18.631516, 18.623558, 18.62167, 18.616753, 18.614346, 18.604776, 18.59772, 18.597536, 18.585192, 18.58451, 18.582628, 18.580091, 18.56977, 18.557703, 18.555147, 18.554981, 18.546553, 18.543852, 18.536036, 18.535934, 18.524275, 18.522568, 18.517923, 18.509674, 18.509453, 18.506647, 18.504694, 18.501528, 18.500063, 18.498003, 18.484575, 18.481451, 18.478907, 18.471977, 18.44684, 18.446629, 18.444344, 18.441822, 18.433973, 18.431818, 18.43175, 18.431622, 18.428513, 18.42601, 18.425196, 18.424587, 18.421156, 18.416107, 18.415924, 18.412294, 18.411348, 18.405958, 18.400753, 18.40021, 18.39518, 18.394611, 18.382214, 18.381853, 18.376478, 18.373156, 18.367748, 18.353346, 18.348137, 18.346666, 18.345133, 18.341549, 18.338743, 18.338549, 18.338549, 18.334269, 18.332645, 18.330729, 18.330631, 18.324537, 18.323841, 18.321785, 18.321785, 18.316923, 18.31567, 18.312057, 18.307827, 18.300373, 18.297064, 18.291702, 18.291483, 18.287144, 18.278097, 18.277512, 18.27123, 18.270628, 18.26588, 18.262701, 18.260666, 18.260355, 18.252974, 18.240154, 18.232243, 18.229055, 18.228191, 18.227955, 18.222128, 18.21947, 18.21581, 18.210857, 18.20875, 18.208517, 18.208172, 18.207542, 18.206293, 18.202362, 18.201105, 18.200508, 18.200499, 18.200142, 18.198336, 18.192347, 18.188644, 18.188644, 18.182722, 18.17174, 18.165133, 18.16001, 18.159561, 18.158375, 18.15794, 18.157646, 18.146484, 18.141241, 18.140041, 18.137264, 18.131006, 18.130934, 18.130392, 18.130087, 18.12818, 18.126118, 18.118143, 18.115772, 18.115625, 18.115345, 18.109287, 18.106949, 18.106344, 18.105568, 18.101303, 18.100998, 18.096657, 18.096592, 18.08915, 18.07934, 18.078188, 18.077873, 18.077293, 18.068014, 18.06124, 18.060034, 18.05743, 18.048935, 18.04774, 18.047182, 18.04281, 18.03201, 18.025562, 18.017675, 18.015766, 18.013557, 18.011572, 18.001322, 18.000343, 17.99643, 17.996353, 17.989695, 17.988363, 17.978165, 17.977348, 17.973675, 17.971329, 17.96912, 17.966457, 17.958076, 17.9569, 17.956701, 17.956602, 17.956041, 17.95154, 17.950727, 17.948421, 17.944794, 17.942358, 17.939507, 17.93747, 17.933636, 17.92971, 17.929434, 17.928988, 17.921257, 17.918736]}, {'id': '2543995857', 'text': 'shockwave base queue estimation approach undersaturated oversaturate signalize intersection use multi source detection data', 'timestamp': 1493848800, 'rel_doc_ids': ['1980258017', '1983095804', '1989573631', '1993147024', '2000075452', '2000438016', '2005961304', '2020893175', '2024564885', '2042388588', '2060404334', '2063890507', '2098207833', '2156271471'], 'user_id': '2117849233', 'user_doc_ids': ['335644182', '341099367', '572493818', '625820732', '791265519', '837502089', '976226029', '1485811280', '1488120717', '1502790431', '1509938986', '1524178524', '1545909916', '1662941239', '1810285639', '1968483723', '1973858469', '1973940534', '1978694557', '1979329469', '1988014195', '1998210776', '2000107709', '2002818663', '2003730896', '2004353783', '2005689061', '2007043321', '2011474891', '2013083620', '2019077572', '2020020727', '2023687579', '2028470166', '2028709411', '2029103942', '2030803723', '2031233211', '2031943083', '2040242394', '2045147755', '2045593919', '2046336063', '2050138223', '2050909734', '2052917584', '2057367731', '2058532765', '2062017159', '2064767061', '2066492608', '2067442163', '2069213850', '2074794092', '2074822436', '2074936023', '2077313061', '2082414341', '2084629194', '2087713917', '2089632212', '2091168119', '2098207833', '2098541478', '2110807282', '2110884310', '2116282529', '2118885273', '2119532721', '2119650747', '2126974080', '2127339593', '2127529517', '2131639027', '2132910827', '2134763973', '2137119273', '2137248244', '2144981563', '2147431824', '2149825419', '2150975732', '2151588153', '2152226015', '2154456797', '2155170539', '2155609247', '2160491249', '2160775398', '2165032732', '2168895314', '2169941626', '2183365366', '2207254389', '2212847588', '2239009162', '2259087662', '2263031682', '2288766215', '2292410499', '2303012537', '2337281148', '2409511419', '2509813992'], 'bm25_doc_ids': ['2167864904', '2098207833', '2004443896', '2168957171', '1974322517', '1995035225', '2076129294', '2162823802', '2048549522', '1992477951', '1993279233', '55215436', '830819331', '2024564885', '2156271471', '1977274983', '1971743032', '1993147024', '2038380739', '2120559951', '1996594438', '2047251773', '1972187690', '1938755216', '2067798797', '2113715734', '2160416265', '2115628255', '2132181932', '1991796783', '2165883263', '2006715767', '2026972557', '2157585915', '639472552', '2055487548', '2532228083', '2062819803', '2146895509', '2287027336', '920737085', '1975739793', '1993517423', '2137900046', '163462317', '2031882629', '2114807486', '1995418927', '2133228654', '2023255777', '2126315467', '2055094934', '2119167238', '145561636', '594494121', '2020893175', '1997512929', '1980258017', '2171688475', '157976960', '744427352', '2011804935', '2143673062', '1986079063', '2099511546', '1982389103', '2088533512', '1977767921', '2039161920', '2167502601', '1989303933', '2071880269', '2123918836', '2088048372', '1869270636', '2037782587', '2106525099', '570060864', '2042388588', '2063890507', '2000438016', '2134601047', '2286498352', '1978766896', '2148058550', '2120331270', '2273577892', '633157428', '2122169437', '1651838950', '1967736536', '2074138489', '2072801365', '2097114872', '2003730896', '2015948277', '1571084909', '2003501942', '1966822777', '1984221130', '2050299279', '2127638177', '2178213869', '2295706353', '2121632018', '2195078934', '2022003868', '2034965610', '1988267691', '1547776073', '2077313061', '2054948310', '655529880', '1991216084', '2132549746', '2090687424', '2098567572', '130580273', '1605343948', '55391173', '2122108000', '2018629612', '1969569729', '1985250445', '2061823767', '2059447384', '2917574854', '610585034', '2064370298', '2172162708', '2046068158', '1989573631', '2079873501', '2125272559', '2036250329', '1991644477', '2112455120', '2487709291', '1532702301', '2140700596', '2107994331', '1020606542', '2163091764', '144038694', '102586634', '1945763671', '1849111333', '1815264649', '2077578573', '2082043032', '2151103273', '2069680748', '2065208703', '1985561847', '2135963782', '1988387163', '2092423004', '2060404334', '2094258552', '2070075635', '2117439804', '2059849525', '2150174275', '83029354', '2093462043', '2151253383', '2069286696', '2048082838', '609467919', '1702550735', '2215584471', '1985272391', '1993894814', '2145931160', '1569350559', '2290208944', '1000766810', '2145366695', '392045534', '1983122610', '1825684449', '2098541478', '2069162326', '2355559669', '1987502951', '2042937005', '2051612742', '2108267620', '2024814648', '2125245475', '2007894170', '1994792599', '223423789', '1969020701', '2048354258', '2009991695', '1978866256', '2097833556', '1495978105', '2164972755', '2007670483', '2246462004', '1525053686', '2112534610', '2134424933', '2049555482', '1586184632', '2020599411', '2077714732', '2146208266', '1978353383', '2148570715', '2057054498', '2067680986', '1979887813', '2223158596', '2720844729', '5525483', '2042668119', '994344872', '1968198841', '2126699969', '2163166882', '1973692732', '604363048', '2015115084', '2114516948', '2127019128', '1973222742', '1964528162', '2514358729', '642541880', '2237320281', '2003420435', '2032035790', '1984258241', '1542772568', '1515699259', '1970258011', '2048178375', '650481952', '2160296386', '2046362242', '1966917785', '2163566142', '2054292198', '2166785731', '2033620987', '2126006010', '2036486937', '2085370204', '2017018908', '2020091223', '1972499042', '1980321698', '2573621651', '2168197346', '121224234', '2080562525', '2111076125', '2161166513', '2005961304', '2049577287', '1980897460', '229133426', '2037452053', '2161725623', '2113902081', '1979966011', '2035385238', '2148911946', '2152511041', '2072990104', '2095996755', '2245900628', '2151156064', '2024884228', '1484104209', '2079365496', '1979109637', '2011868993', '1972454618', '2127810199', '2109737164', '1998681414', '2066146747', '2160604101', '1567944482', '2087921693', '2151596693', '2030315694', '1894255835', '2045475727', '1975732067', '1642713776', '2080892685', '620515405', '2109568183', '2138566015', '2132890250', '2249292445', '567203716', '2121317486', '2128554297', '2095713551', '2037142879', '2057184828', '11474316', '2161843229', '2050829553', '1560236934', '194992259', '2040681841', '2053687371', '1480380834', '1963921144', '600381753', '2002219037', '591317194', '2014473231', '2101074405', '2155609247', '566218720', '2004001345', '2018305240', '2133358558', '2037995561', '2002809702', '2094565595', '2136909554', '2079303717', '753846419', '2076447696', '2092969362', '2025765033', '2095447224', '2128114480', '2026707549', '2326029274', '1980058844', '2096881549', '2023373141', '2140395883', '2008098263', '2067963915', '2116631353', '2142271913', '2034929212', '362511916', '2135392473', '2076505509', '275586726', '2047590673', '2064660546', '2017005956', '1965631431', '2027847357', '2155051208', '2061657944', '2004131807', '627003972', '2530339316', '2089792290', '2072378947', '1968554447', '2068584765', '2091494752', '2084135319', '2071262577', '1981009076', '1909788123', '1834569531', '2161634529', '2028283543', '620045783', '2113902797', '1488311095', '1995089661', '2020614634', '2104360898', '2328473443', '1983095804', '2057991002', '2157355619', '2093726459', '603936288', '2040507667', '2074675394', '2343473117', '605481478', '2100146203', '2122633064', '1900640294', '2114346030', '1973161587', '1927219124', '1982475811', '2369953487', '332856324', '2084635275', '2040387100', '2085369991', '1496609702', '1536761250', '3655591', '2072085950', '1581827110', '2073393739', '2002752808', '2011877287', '2065748588', '616703255', '1999587568', '2019077572', '2027487681', '2006046294', '2061951146', '2043597501', '2132021174', '2093202778', '1981436090', '1972674417', '2002802905', '1969793936', '2292410499', '2556963879', '1836902361', '1584453934', '2096950984', '1964887314', '617745839', '2105857682', '2153241511', '2031688136', '2100184175', '2046184914', '2142480550', '2113481188', '1954444148', '1858232341', '94242302', '2044473918', '2123201483', '2037377171', '2057647642', '608915684', '1988352876', '139274105', '2091667667', '2093657291', '1040663801', '2037389241', '2071047057', '2077316044', '1966417199', '2133805337', '2061737056', '2087839462', '2140323676', '2097713576', '2045005003', '2134766870', '2594893535', '2014918702', '2002060564', '2161072727', '2017413396', '2535315840', '2164237850', '2119873646', '2076206578', '2082173445', '2112708539', '1903726215', '2000007307', '2107073528', '2030188039', '2091077749', '1524246982', '1998152938', '2141161509', '600359274', '2170995345', '825488872', '1977512419', '2062493963', '2162529245', '2128558188', '208024345', '2086951714', '2008297905', '2151588153', '2004482885', '2152616744', '2090045241', '2163993946', '2161992635', '1547555744', '2149685796', '2105187784', '2040983340', '2085882886', '1985697701', '2093820213', '1599361768', '2407242090', '1967925806', '1965806445', '1554938838', '1578092058', '1983495640', '2106857650', '2015740922', '2004009148', '2171043423', '2085617493', '2096372798', '2040767762', '2116859998', '2014744694', '2145834948', '2302488257', '1677421583', '2000075452', '2073129378', '611756453', '2050676627', '1585346482', '2025254312', '149322717', '2068176561', '2060465752', '1979863201', '750285360', '2012428203', '1985830773', '2085912606', '2033823613', '2122857222', '2028365295', '2464759897', '2146106238', '1921581994', '2096070584', '2483502061', '1491034057', '2245783835', '2123373013', '2135637330', '629558337', '2067297627', '2001546134', '1545780821', '1977226950', '2104067184', '2111610879', '2157499108', '2475723319', '1977663646', '2093528171', '577068363', '2067444480', '2529041510', '2162533795', '2032880816', '1999461624', '2119939266', '2120294794', '1998302245', '2107695633', '2079033543', '2082395008', '2148149125', '2026615953', '1031546370', '2164339738', '1985584043', '2019778228', '2100930595', '657880616', '2004813882', '2120206181', '2248229497', '2053258077', '2130632867', '2131107250', '1819235718', '2062055006', '1511727907', '2120545488', '1544280311', '2004260704', '1970499667', '2012715214', '2395553123', '2216760301', '2145480246', '2064808506', '2067173632', '2006923070', '2042539013', '2342875829', '2075463408', '1984642392', '2084989648', '2053670885', '1537739930', '1509307049', '2003400688', '2162488596', '1893140422', '564427815', '1846760422', '2004438061', '2032119796', '2015140690', '1987097279', '2071136258', '1966764822', '2100962921', '2035125086', '1998023304', '2008590558', '1979661199', '2048041430', '2075457700', '2206497916', '1564826086', '2111075407', '1887211934', '1980624084', '2067783422', '1875222829', '2009966727', '1994711822', '1999886137', '1587330580', '621969811', '147885512', '25716128', '1990466248', '2166755871', '2071487437', '2036469243', '2080351379', '1960198871', '626303303', '2109404225', '84951779', '2542119780', '2042719979', '2002488072', '2132634086', '833054232', '1931326631', '2089837526', '2144620622', '2116977262', '2023299122', '2082698302', '2104343777', '2004474453', '2116904573', '2087270181', '2090678894', '2068881774', '2074927091', '1993226421', '116374660', '2020224078', '2321196096', '2076598048', '1579017973', '2080782477', '1554038739', '1591013600', '2126909981', '195182186', '2167217178', '2109857510', '2136122454', '2103556319', '2105683530', '571062792', '2059463457', '1997718639', '2056169502', '1567990182', '1979112084', '2062023097', '2075497944', '2150260479', '2098042645', '2050413959', '2021469800', '1973921171', '2051249594', '1995336017', '658134011', '2037141248', '2064830897', '1974963912', '2023765456', '2094553823', '1988813867', '2161451920', '1979890228', '2062331254', '2274095041', '2118524168', '2082182538', '2037774819', '1967402798', '2102104639', '2077613353', '2248770784', '2041414778', '1982048777', '2139036365', '2049593940', '2100892800', '131450286', '2039304124', '2140130972', '2158076984', '2080935724', '1567148262', '581491446', '2085349079', '1542519400', '2048703490', '2062786619', '1491619620', '2026832280', '2023614692', '1536394283', '2004105175', '2621233240', '170573255', '2025103029', '2012897294', '2096666614', '2289646755', '2090419027', '1969783478', '2044809403', '1988180708', '129758725', '582405465', '2038115940', '232857390', '1766184944', '1963691893', '1994413287', '2098989464', '2055185269', '1980133313', '2050497632', '39332416', '1996099276', '2104297755', '2001571531', '2075792684', '1964201215', '2014059632', '2119575950', '2094634922', '67806418', '1596235953', '1977174332', '1481100740', '1546257228', '2313424194', '2316810041', '2027602208', '2113949358', '2069040401', '2322896587', '2058905288', '1545466150', '2047125182', '2107390099', '2021545481', '2034537461', '1973531167', '2149904367', '1985391442', '2004562567', '2032227970', '1686755049', '2018636913', '2005942625', '1973105043', '2048353325', '159492747', '2082945312', '1965745660', '2080905915', '1943820884', '2062702018', '2121852672', '32689304', '1975500487', '2125349918', '2102717159', '1989487717', '1985286259', '2153410050', '2014989492', '2065015290', '1978685211', '2139132965', '1974094429', '2111141290', '2170631965', '2063312204', '2057949243', '1793449267', '2017544949', '1967000285', '579442102', '575407481', '2086082648', '2105253716', '2014237319', '2090880284', '2120387195', '2001776332', '2003748365', '2167218771', '2095569737', '2291379690', '2089281715', '2159319012', '2100027140', '72705000', '1976503283', '1968270417', '2096645166', '2145898376', '2025584064', '1964616766', '2073856553', '2104126071', '2163330161', '1992831203', '2096017789', '2015197759', '2145717442', '2009825391', '2146472574', '1980656046', '2053115739', '2161960523', '1567809276', '2049034728', '2102395981', '586459813', '2060106849', '2153701318', '1603184524', '2080666451', '2005835013', '1583158134', '2000630683', '2143774080', '2132653300', '2132770633', '1995384631', '2020145796', '1979374731', '2108190795', '2144693864', '2142382652', '2125516616', '2073181534', '2104300435', '2011555750', '2069652665', '2154920554', '2114667989', '2054383823', '2093948134', '2104206029', '2112184561', '61958170', '794170814', '2146874303', '2089455197', '2029607896', '605241825', '2001247526', '1539717819', '1768240848', '2089743477', '2140089842', '2045387446', '2106988221', '2108985158', '2053776921', '2085046200', '2069851392', '2115694596', '583228623', '2164688141', '2169306346', '2019698752', '1615698802', '3000981675', '2065980578', '1974007670', '2063133311', '2091823216', '1979655209', '246751559', '637620980', '2002937539', '199535599', '2110440950', '59847873', '2095598604', '2152942473', '2017944718', '2159131763', '2168675580', '1631159389', '1834937311', '104221572', '2043626900', '232794731', '1945954700', '2000554247', '581105648', '172700794', '2113057176', '2075249612', '2125705173', '2038068435', '2288393747', '2161914662', '234604783', '2027945924', '2064395103', '2533486026', '1970035771', '2104981467', '2122798065', '2018360749', '1502239137', '2106660141', '2059836838', '2145004950', '2141901785', '2166575512', '1992892907', '1989713135', '1977757178', '2071854496', '1983651380', '1944390203', '2059530928', '1987168837', '2001593263', '768183661', '761274143', '2074500080', '2059386649', '2130241203', '2009167468', '2010968010', '1996478295', '2015350657', '1998012255', '1968765912', '2078509856', '1974800020', '1968362748', '2096881176', '2009345891', '2055290237', '2136679814', '202434829', '1970574049', '2045297782', '627047244', '572870064', '1964016515', '1500301594', '1600701175', '2199681782', '1908699318', '2052435355', '2029639182', '2523203662', '2056998261', '2042171090', '2290801524', '1978587863', '2107032790', '2007145893'], 'bm25_doc_scores': [61.41382, 56.920357, 56.829536, 53.02419, 52.920498, 52.576576, 52.555325, 51.4755, 51.457844, 51.16976, 49.72224, 49.47858, 49.048645, 47.941536, 47.221523, 46.820103, 46.162823, 45.774704, 45.460487, 44.07522, 43.670986, 43.519173, 43.469227, 42.27952, 42.2644, 41.982174, 41.94671, 41.571056, 41.147263, 41.025036, 40.25206, 40.142017, 39.43929, 39.370102, 39.23745, 38.83472, 38.833916, 38.430103, 38.112267, 38.047836, 37.98274, 37.855343, 37.317783, 37.26725, 37.028324, 36.59774, 35.938316, 35.931904, 35.86609, 35.734062, 35.587505, 35.582108, 35.472546, 35.371246, 35.360577, 35.102726, 34.911762, 34.910866, 34.85115, 34.711956, 34.70052, 34.69713, 34.5582, 34.542637, 34.434814, 34.27223, 34.248035, 33.936733, 33.893955, 33.857655, 33.774986, 33.520725, 33.50886, 33.49988, 33.383366, 33.378075, 33.334164, 33.304787, 33.243805, 33.170334, 33.133945, 32.884113, 32.827557, 32.792072, 32.684334, 32.5844, 32.551178, 32.44342, 32.290653, 32.266453, 32.14192, 32.119858, 32.071873, 31.997013, 31.96343, 31.878576, 31.838232, 31.821846, 31.752594, 31.622313, 31.505224, 31.460094, 31.416807, 31.414793, 31.329302, 31.323872, 31.315765, 31.259851, 31.216349, 31.18917, 31.06076, 30.955172, 30.928953, 30.906242, 30.835762, 30.810314, 30.805822, 30.788303, 30.76458, 30.68798, 30.653334, 30.573513, 30.571075, 30.561354, 30.500698, 30.487938, 30.470467, 30.46954, 30.466206, 30.290298, 30.222649, 30.211395, 30.090998, 30.057726, 30.037521, 30.029749, 29.958473, 29.958214, 29.911982, 29.806894, 29.798342, 29.768505, 29.736542, 29.731789, 29.562527, 29.549816, 29.501034, 29.466124, 29.460442, 29.266287, 29.264933, 29.253515, 29.194016, 29.114677, 29.104685, 29.100536, 29.085686, 29.082447, 29.081251, 29.004257, 28.981363, 28.92502, 28.912876, 28.90519, 28.893545, 28.861452, 28.85434, 28.828346, 28.814835, 28.811077, 28.808046, 28.803305, 28.682924, 28.648237, 28.64549, 28.635986, 28.607748, 28.58616, 28.584126, 28.525164, 28.524475, 28.460566, 28.455101, 28.444387, 28.395247, 28.364716, 28.347029, 28.317118, 28.285744, 28.279858, 28.224747, 28.215988, 28.203125, 28.141851, 28.117815, 28.066095, 28.053783, 28.053783, 28.04889, 28.03646, 28.033478, 27.983084, 27.96001, 27.957159, 27.948757, 27.944689, 27.934814, 27.900684, 27.864756, 27.862162, 27.847984, 27.839283, 27.82912, 27.796928, 27.750353, 27.741747, 27.739145, 27.722584, 27.707935, 27.690262, 27.68728, 27.634579, 27.628777, 27.618881, 27.608738, 27.55111, 27.539902, 27.536507, 27.492708, 27.449697, 27.429405, 27.422155, 27.421371, 27.419321, 27.414051, 27.391409, 27.37602, 27.341076, 27.32308, 27.317663, 27.317617, 27.294798, 27.289501, 27.204159, 27.175098, 27.165546, 27.146935, 27.104517, 27.095085, 27.090275, 27.085947, 27.076313, 27.06341, 27.040026, 27.033573, 27.026741, 27.02674, 26.996216, 26.99591, 26.966093, 26.943645, 26.895866, 26.811886, 26.77928, 26.770342, 26.76876, 26.752668, 26.720015, 26.718023, 26.717392, 26.708128, 26.678524, 26.678041, 26.654293, 26.650507, 26.648888, 26.647991, 26.616077, 26.559607, 26.547188, 26.54218, 26.534906, 26.520905, 26.519403, 26.5047, 26.480513, 26.465536, 26.465002, 26.46249, 26.46242, 26.456594, 26.427547, 26.391418, 26.380703, 26.367231, 26.357754, 26.28039, 26.258825, 26.241066, 26.239685, 26.19422, 26.148136, 26.147686, 26.132097, 26.130722, 26.11919, 26.114397, 26.092012, 26.085804, 26.085684, 26.08535, 26.070122, 26.0625, 26.037453, 26.010262, 25.999466, 25.975077, 25.960224, 25.959381, 25.94978, 25.94519, 25.934845, 25.92401, 25.923956, 25.910463, 25.901821, 25.87404, 25.868464, 25.862425, 25.846321, 25.81741, 25.815842, 25.81078, 25.798223, 25.792944, 25.780416, 25.758408, 25.756102, 25.715765, 25.709494, 25.703037, 25.697258, 25.697174, 25.679352, 25.676338, 25.660847, 25.621048, 25.612648, 25.588465, 25.547964, 25.494738, 25.485678, 25.455671, 25.451492, 25.448862, 25.433352, 25.431376, 25.430267, 25.429981, 25.409563, 25.40445, 25.400902, 25.391438, 25.37849, 25.372372, 25.370111, 25.369064, 25.365902, 25.35208, 25.337296, 25.33178, 25.319122, 25.31756, 25.284033, 25.274414, 25.271896, 25.248308, 25.246475, 25.242805, 25.226112, 25.21729, 25.215763, 25.214962, 25.208258, 25.206823, 25.187222, 25.176859, 25.154428, 25.15289, 25.127228, 25.111412, 25.10098, 25.093967, 25.088108, 25.081362, 25.055916, 25.039928, 25.02273, 25.00425, 24.988052, 24.968206, 24.967571, 24.938213, 24.93223, 24.930313, 24.927246, 24.920864, 24.90783, 24.905293, 24.903366, 24.897135, 24.85819, 24.848616, 24.843733, 24.814085, 24.790579, 24.779388, 24.773869, 24.765087, 24.76394, 24.763708, 24.738152, 24.727127, 24.687138, 24.673262, 24.658495, 24.654217, 24.62964, 24.624226, 24.60847, 24.607147, 24.585018, 24.569101, 24.545965, 24.536043, 24.534332, 24.524117, 24.519306, 24.507631, 24.499052, 24.494265, 24.48835, 24.481441, 24.474173, 24.44138, 24.40317, 24.40198, 24.399572, 24.396254, 24.392523, 24.386442, 24.360794, 24.35805, 24.357944, 24.354189, 24.349686, 24.329237, 24.327913, 24.305021, 24.297705, 24.284449, 24.261456, 24.250826, 24.235651, 24.221144, 24.21769, 24.213795, 24.208832, 24.203484, 24.198301, 24.182333, 24.17276, 24.164032, 24.149166, 24.135654, 24.13374, 24.129698, 24.121582, 24.115053, 24.113894, 24.096413, 24.093254, 24.089931, 24.088682, 24.083714, 24.076468, 24.06691, 24.037132, 24.00757, 23.993084, 23.98864, 23.98482, 23.97891, 23.978746, 23.975466, 23.948359, 23.945208, 23.913132, 23.902626, 23.89867, 23.888226, 23.871914, 23.86345, 23.862383, 23.86134, 23.858276, 23.851246, 23.825191, 23.820019, 23.806183, 23.798216, 23.787184, 23.782656, 23.779787, 23.776941, 23.754496, 23.7432, 23.7432, 23.736366, 23.735197, 23.73045, 23.729612, 23.726757, 23.720503, 23.716902, 23.714632, 23.70534, 23.674723, 23.665583, 23.65312, 23.652073, 23.63988, 23.637346, 23.632574, 23.604994, 23.602516, 23.60163, 23.596802, 23.593216, 23.576488, 23.568314, 23.560915, 23.552483, 23.55187, 23.542675, 23.526232, 23.512981, 23.491386, 23.488611, 23.471409, 23.462688, 23.42605, 23.409637, 23.38245, 23.365479, 23.30162, 23.272713, 23.270733, 23.25905, 23.24553, 23.244698, 23.243093, 23.238674, 23.229454, 23.22695, 23.207714, 23.195892, 23.18962, 23.18091, 23.174484, 23.172329, 23.160763, 23.155931, 23.150003, 23.129387, 23.12414, 23.121271, 23.102371, 23.09382, 23.088633, 23.051067, 23.034927, 23.028305, 23.011572, 23.003017, 22.995562, 22.990622, 22.990255, 22.98319, 22.970825, 22.95681, 22.953999, 22.949242, 22.942959, 22.91962, 22.917278, 22.914492, 22.897186, 22.895962, 22.893827, 22.880667, 22.84276, 22.842253, 22.812374, 22.802399, 22.798, 22.792498, 22.7908, 22.786255, 22.753178, 22.75081, 22.725788, 22.70211, 22.676517, 22.67444, 22.668974, 22.664536, 22.653498, 22.652048, 22.646614, 22.646614, 22.638563, 22.627586, 22.622587, 22.613455, 22.603542, 22.598448, 22.593819, 22.590342, 22.58923, 22.572422, 22.559137, 22.555056, 22.550663, 22.530529, 22.526585, 22.525463, 22.522444, 22.516827, 22.514126, 22.503244, 22.49408, 22.487831, 22.487724, 22.466692, 22.455303, 22.454227, 22.44454, 22.418133, 22.414108, 22.413069, 22.405, 22.399601, 22.35371, 22.349554, 22.347683, 22.336695, 22.323553, 22.31898, 22.300207, 22.297604, 22.290081, 22.281916, 22.276068, 22.262768, 22.258352, 22.226704, 22.224579, 22.213604, 22.210491, 22.204046, 22.189362, 22.178024, 22.175728, 22.160437, 22.158852, 22.15391, 22.149641, 22.149084, 22.140263, 22.118132, 22.117638, 22.117216, 22.107452, 22.101778, 22.078903, 22.075026, 22.060686, 22.045233, 22.032194, 22.029085, 22.014103, 22.006329, 22.003283, 21.996193, 21.984783, 21.976524, 21.97471, 21.974258, 21.953852, 21.950985, 21.933195, 21.924187, 21.919952, 21.912066, 21.911827, 21.899393, 21.89923, 21.885853, 21.880644, 21.880644, 21.870123, 21.828402, 21.790174, 21.789673, 21.782585, 21.779175, 21.75111, 21.748857, 21.746033, 21.745562, 21.739923, 21.724182, 21.72131, 21.720057, 21.715782, 21.712187, 21.693384, 21.6798, 21.67868, 21.674547, 21.671556, 21.67064, 21.665443, 21.651491, 21.645723, 21.645723, 21.633614, 21.609371, 21.601162, 21.579222, 21.579222, 21.561672, 21.528976, 21.525097, 21.509573, 21.508825, 21.50193, 21.496632, 21.45228, 21.444872, 21.44359, 21.442308, 21.441711, 21.441147, 21.434738, 21.433435, 21.431751, 21.4157, 21.41091, 21.408585, 21.39537, 21.38878, 21.38633, 21.33696, 21.33696, 21.325369, 21.320723, 21.31782, 21.317556, 21.315334, 21.285551, 21.278212, 21.25683, 21.225641, 21.222654, 21.22, 21.217216, 21.201635, 21.166748, 21.156574, 21.12689, 21.116901, 21.101849, 21.097357, 21.090305, 21.08657, 21.086378, 21.085823, 21.078083, 21.07316, 21.071423, 21.055737, 21.054327, 21.05366, 21.047525, 21.03505, 21.014137, 21.011435, 21.01031, 21.00504, 20.994455, 20.988808, 20.983574, 20.975662, 20.97444, 20.97444, 20.970053, 20.968924, 20.965752, 20.94926, 20.944662, 20.936924, 20.936567, 20.935484, 20.924006, 20.915941, 20.912449, 20.911198, 20.884638, 20.881021, 20.874306, 20.870676, 20.85651, 20.843668, 20.831377, 20.828138, 20.804596, 20.801785, 20.793041, 20.775467, 20.770643, 20.760105, 20.749933, 20.734156, 20.725088, 20.703274, 20.702946, 20.68423, 20.679274, 20.675037, 20.66735, 20.666975, 20.659725, 20.657501, 20.65633, 20.646255, 20.627398, 20.626078, 20.606699, 20.595926, 20.574162, 20.570387, 20.567099, 20.566437, 20.564936, 20.564507, 20.544601, 20.537003, 20.531885, 20.531578, 20.503973, 20.498753, 20.487532, 20.483498, 20.478605, 20.473764, 20.4637, 20.458939, 20.45831, 20.456306, 20.456146, 20.451576, 20.445616, 20.4431, 20.440298, 20.438805, 20.424755, 20.422827, 20.421988, 20.414392, 20.408966, 20.39062, 20.388565, 20.385468, 20.382238, 20.37946, 20.379326, 20.374676, 20.359016, 20.35709, 20.352474, 20.349108, 20.343288, 20.338186, 20.334074, 20.333359, 20.327213, 20.320023, 20.30612, 20.293179, 20.283571, 20.276432, 20.257376, 20.25518, 20.254007, 20.251276, 20.246151, 20.243986, 20.239769, 20.233196, 20.217386, 20.216991, 20.213314, 20.210707, 20.201748, 20.201641, 20.198406, 20.182402, 20.180851, 20.180552, 20.180109, 20.153194, 20.14803, 20.13466, 20.131006, 20.125347, 20.113665, 20.111454, 20.107508, 20.093933, 20.09304, 20.092348, 20.067316, 20.062878, 20.050692, 20.050447, 20.029228, 20.020735, 20.019003, 20.01751, 20.016098, 20.012587, 19.987823, 19.969685, 19.9618, 19.960018, 19.954449, 19.952517, 19.941708, 19.93662, 19.93596, 19.926695, 19.922674, 19.920406, 19.919176, 19.902992, 19.90199, 19.901081, 19.899374, 19.899227, 19.897774, 19.881721, 19.881557, 19.880077, 19.873331, 19.868639, 19.858273, 19.851469, 19.844526, 19.835918, 19.817387, 19.815298, 19.800352, 19.799412, 19.794437, 19.793154, 19.792562, 19.786238, 19.78327, 19.783073, 19.782421, 19.76765, 19.749903, 19.74963, 19.736002, 19.728024, 19.72778, 19.725668, 19.721893, 19.718618, 19.7113, 19.698647, 19.693653, 19.691505, 19.687069, 19.686466, 19.685747, 19.680817, 19.677814]}, {'id': '2551905807', 'text': 'dissection test driven development process really matter test first test last', 'timestamp': 1498860000, 'rel_doc_ids': ['1502857873', '1603773145', '1989734019', '2005278177', '2008831000', '2012274581', '2024714614', '2025969607', '2037799783', '2073153953', '2100656553', '2103497247', '2118604270', '2129005287', '2136800703', '2146089323'], 'user_id': '499017155', 'user_doc_ids': ['126646603', '214089764', '1428154031', '1496660967', '1570229432', '1585014875', '1608682798', '1921277503', '1967423277', '1967899462', '1969593469', '1969961903', '1977247477', '1977603982', '1979351689', '1981115943', '1983631607', '1983830972', '1985112672', '1991369112', '2001051674', '2004934460', '2009223100', '2012600829', '2015709452', '2016646235', '2022788640', '2024924320', '2025865172', '2027143917', '2030522107', '2033017483', '2033903002', '2034109986', '2036212408', '2043785305', '2044177369', '2050034017', '2050629843', '2055975932', '2063037852', '2063440581', '2063879741', '2064161267', '2067002081', '2070797233', '2071577151', '2074565091', '2084278640', '2091979577', '2100131446', '2101077949', '2108410964', '2109554916', '2110472757', '2113533445', '2114952996', '2125733548', '2127424938', '2131279079', '2132249726', '2136430857', '2136928757', '2138848067', '2146343279', '2149746528', '2153759057', '2158564933', '2158685633', '2162000403', '2219831900', '2295373235', '2301873223', '2522228258', '2915963575', '3013019166'], 'bm25_doc_ids': ['1816575955', '2077208035', '2132488875', '2025969607', '2125920815', '2012274581', '630879188', '1554594814', '2107899555', '2029080835', '2156333502', '2135263236', '1520491360', '388997579', '1581458090', '1599524332', '2135334950', '2539024465', '11165583', '2122238588', '1979997084', '1989734019', '1573741924', '1630772170', '2183086489', '2076064232', '2022198665', '1481560017', '2994602461', '2005278177', '2106807778', '2469055220', '2127774413', '1502893179', '2170090972', '2129005287', '2137511956', '2156754660', '1531600807', '1997090065', '2037799783', '1791192914', '1603773145', '2037969161', '2398826339', '2166974017', '1679129939', '2165866667', '1629670215', '1488331137', '2047398661', '2133073371', '2169184031', '2099137749', '2015841749', '1654405703', '1511212867', '26132888', '1999795373', '1527161313', '1805329343', '1493960996', '2039318812', '2118111397', '2070849878', '2092229460', '1965945013', '1922775048', '2145619906', '1579486225', '2035082876', '1519593595', '2064562930', '2097396627', '2124942162', '2098707473', '1567738769', '49830884', '2168658843', '2046439782', '2144093205', '2024714614', '1529278544', '22865002', '2023414777', '2098933948', '2160836594', '2510072788', '2131981830', '1488199540', '2092198121', '2133733232', '2114109012', '2135322262', '2145885625', '2000695132', '2116589581', '1499908223', '1981590040', '2031008494', '1499233281', '1856796825', '2113749941', '2159166446', '223920239', '2033429207', '2114164878', '1964416677', '2148590904', '2030296426', '2168188083', '2113045705', '2120517712', '2234299761', '2091118269', '1986276289', '2123051093', '2101236659', '2051105192', '2138666285', '2088066602', '1984450218', '1999919134', '2136138078', '2219277787', '2439708412', '1535656280', '2161762285', '1797135964', '1976970799', '1642142456', '1992796278', '1493162009', '2136629470', '2652374878', '1791157946', '166851811', '2098116443', '1979267233', '2019313177', '2089508232', '2027449799', '1973673901', '2129484434', '2126009364', '2272231712', '2100656553', '1986501260', '2155604018', '1542960140', '2537544621', '1994615058', '1999716524', '2035328219', '2135815781', '1516008295', '1514397751', '2207244966', '1548804231', '2144041279', '1502857873', '2112227385', '2153304886', '2140258813', '2035835786', '2146089323', '2122811495', '2001288039', '2122220008', '2044385323', '2020883722', '2073153953', '1540947947', '2732818174', '2522228258', '1885857283', '1589211450', '1974267361', '56742685', '2078670680', '141832196', '1918772487', '1971755550', '2200418470', '1976539193', '2149726420', '2146347996', '2242368322', '2063268203', '2164145770', '2108485821', '2038921609', '2037595410', '2148899257', '154367956', '2301873223', '2134384693', '1591861455', '2104836268', '292732050', '1970359182', '2124470923', '2138479620', '2150124918', '2130372245', '597710695', '2018924256', '2043547578', '2259080398', '1594703822', '2160950252', '2097862540', '2016793885', '1505397441', '1867551913', '2121855876', '1967583369', '1971139216', '2286934681', '2152737553', '1999046908', '1515851329', '2084887015', '2025888618', '2150935426', '2149236216', '2014858538', '1843704840', '1998444404', '2082723859', '2145538441', '2090301720', '2144278167', '1975388244', '2124829569', '2408209775', '2400994325', '2535129134', '2065770700', '2132917502', '2131657143', '2146631973', '2094687343', '1815064695', '2396949101', '1984401437', '2014883622', '1771323890', '2090235853', '2083208205', '2056937295', '2169924227', '2160383043', '2097057030', '2124445380', '2045685909', '2101452876', '1656311805', '2140197241', '2082751146', '2093718345', '2069406606', '2059911466', '1980081737', '962004357', '1541509149', '90032636', '41977050', '1985779099', '2012479657', '2046211462', '2017465021', '2207002211', '1554459331', '2117327529', '1606339846', '2149398825', '2051760242', '2144431996', '1495985732', '1983372963', '1509941428', '2101567935', '2160875393', '1657941394', '1518350676', '2159857642', '2165444961', '1997317077', '2083760855', '2098046929', '2168974389', '1986959926', '2116707122', '2068342111', '2125538265', '2160107041', '135229763', '2095054175', '2160686006', '1520899383', '2484237927', '1530177802', '2167196574', '659576502', '2162671052', '1989046997', '1828976159', '2001947472', '2126982864', '2143116222', '2138518908', '2159080347', '2015754600', '1576848356', '2051254008', '2109987801', '1999437464', '1576032054', '2153325106', '2280256062', '2080002385', '1583047567', '780895736', '2115655439', '1486167937', '291236306', '162887044', '2043714501', '2071705017', '2130579590', '2015351372', '1967709752', '1524074347', '2136644911', '1550235714', '2004378344', '2103497247', '1975168644', '2011562185', '2120787570', '2006892971', '2121662839', '1975057653', '2078869464', '2425836966', '2010591452', '2145875406', '1512705235', '2109921290', '2091232673', '1981329136', '797792922', '1560157848', '1520007752', '1479702163', '1970202439', '2535139643', '2610216453', '2219831900', '2037001181', '38099770', '1558618245', '2080458911', '2012616631', '2118604270', '2002638408', '2133229660', '2094901300', '1999368269', '1733018966', '2151222385', '2116729408', '1981161668', '2047574505', '2024805317', '2092147313', '2002043793', '2168551673', '1794214146', '1798238987', '2155710087', '2155765577', '2011041604', '2131820522', '1587511817', '2400230273', '2043831213', '1544145945', '985401649', '2011502941', '172264927', '2107944490', '2403476939', '1982771763', '1992192645', '2208339931', '1953724919', '1993733065', '1879064811', '93717786', '2025005118', '1988882800', '2073352622', '2167143751', '2397507413', '2098998158', '1973706408', '1531444303', '2109479745', '2167939651', '2060341638', '2013258029', '844901464', '2093677364', '2509860763', '2034659051', '2161226218', '2129963397', '2540462941', '2056952837', '2144529894', '2109777045', '2124574275', '2019721991', '1494828614', '2098516903', '1551314731', '2169048397', '1608650020', '2151349320', '150263746', '2037269932', '1572913046', '1685636323', '2101046549', '2258501970', '2106018533', '2078185709', '1884185863', '2109251172', '2045981402', '2075002278', '2161665327', '1974680349', '2143922826', '1545608738', '2128583722', '67806418', '1527148304', '105147515', '2061939755', '2107061559', '2075146632', '2137276637', '2144695481', '1976377048', '1971958047', '1554220237', '2411130544', '2019041771', '2054240416', '2179773182', '2213519078', '2130876565', '2008831000', '2249351626', '2032084032', '2128651369', '1142301944', '2156374393', '2120786355', '422696301', '1916529522', '2150390686', '2036981176', '2110149712', '2052555472', '2126379098', '2062175345', '2287534751', '2119165870', '2061904659', '1969184538', '2065773792', '1978169984', '2046146948', '2078525501', '1983403095', '2139391819', '2142262899', '2097381538', '2120686320', '1985027931', '1207234815', '2028383790', '2085541880', '2017243662', '2161908845', '1987610075', '1503484533', '2003066864', '112718097', '1594272399', '2014388423', '2023611434', '2017148991', '2023255236', '235191572', '2076618134', '2031615045', '1921408909', '2171823072', '2114927631', '213389411', '2395052532', '1580299393', '2055390894', '810535291', '166880852', '1878636045', '1480817066', '1969925289', '2105153879', '2114900930', '2004593718', '1993615876', '1576638241', '2001579713', '1999944929', '2015977181', '2126812511', '2246492748', '1928504204', '2136800703', '1593579582', '24139812', '2041458301', '2395013964', '1973991374', '2122888875', '2030263637', '1867026237', '2103982836', '2019486892', '1996047778', '2266622801', '2080728372', '1968285450', '2002808878', '2331721881', '2153100417', '2101436907', '2025914262', '1976184824', '2117248804', '1490020915', '2080481409', '2090657494', '31430627', '2179418444', '191117710', '1653657989', '2588381105', '24642671', '2129690060', '2093793791', '2083050602', '2239872429', '2238371180', '1989329871', '2152787935', '128517228', '2073182999', '1831351126', '2091429127', '1866037163', '2071020064', '2146027048', '1966936511', '2102440186', '2037650683', '1997516314', '2073114891', '198211779', '1529421300', '2104434239', '1827518509', '1571361964', '2076347685', '2145922831', '2582562221', '1974846239', '2098315011', '2054180055', '175941315', '1513153379', '2010784831', '2156566730', '2135186184', '2945079885', '2221054250', '1977516243', '2412187012', '2136932895', '2078525304', '163561728', '2080630316', '2046979147', '2043240215', '1992477284', '213369333', '1567814420', '2143174533', '2284244511', '2235980868', '2024217645', '2049645102', '32096427', '2480740344', '2041953399', '203440368', '2152459051', '2120381494', '2048753738', '2156831829', '103644786', '1822026283', '2144978341', '1428154031', '2098958844', '2133033175', '1041172202', '2049288979', '2046363937', '2047405803', '2410028312', '2070910387', '2125734885', '2039390307', '60774776', '1508616138', '1965527419', '2258147470', '1599420046', '2136720523', '1847843141', '2016476908', '2114355319', '1980194971', '1995956882', '1999813422', '1488602908', '2406333979', '1965363781', '2059624066', '2004344213', '2050289971', '2039234396', '2079402412', '2396419752', '1937259999', '2165582349', '2117881166', '1974567149', '2003639649', '2200851269', '2006884214', '2001035329', '1491155388', '1994012545', '1494597521', '1975034387', '2039391801', '2067035485', '1973128502', '2058956575', '2018161301', '1965446936', '1516365899', '1535562463', '2013248140', '2037375676', '2247766998', '2146103278', '2047063934', '2072044239', '2041628107', '51830096', '140611228', '997629992', '2186701980', '2049210024', '2235604464', '1527366985', '1642718662', '2064013226', '2158092002', '1194506051', '2401607180', '2000451649', '1975692962', '2151223430', '2325132480', '2012272168', '1978070018', '2064060385', '2401024662', '2119980272', '2069278156', '2003860956', '2046363052', '2124430468', '139286382', '1582117868', '2263406997', '2082899863', '1515021332', '2131009004', '2103151082', '204358039', '2125679167', '2131694500', '2277451945', '2986618292', '2136166492', '2071131630', '1989712291', '2277780094', '2166693749', '1579298226', '2097003080', '2040263904', '2102853656', '1614226958', '2101962638', '2164179197', '1013851364', '1504622328', '2160343175', '1497164266', '2103074295', '2130414834', '974327873', '2165471135', '2078850715', '2052164589', '2010099812', '1973840180', '2130038852', '2109676928', '1586637512', '2123552647', '2036937899', '2408318784', '2164093202', '2100799079', '2083263371', '2133959300', '2280075410', '2005441384', '2287590831', '2154237597', '2114201924', '2133530990', '2172010324', '2121122724', '2242380247', '2143058502', '1990262397', '1482670424', '1946654589', '2128061502', '2102633541', '2107010538', '2040404795', '2068341588', '2019724127', '2168877254', '1966880963', '2070406344', '2117179073', '2101829226', '1514001772', '9568420', '1997191526', '2293007040', '2075656291', '2080959623', '2067817485', '63485683', '1880794682', '2000036357', '2162044456', '2158691258', '1980577438', '2083760363', '1519718087', '2013835559', '1786758508', '1978654314', '2147253199', '2122596291', '2134902474', '1003315906', '2003612663', '83755378', '1851766400', '2099449604', '2063888051', '2059865504', '1501895670', '2157393893', '2286279958', '2322223335', '2157364891', '1525340321', '2049996525', '2162617422', '53594417', '2218121385', '2135654320', '2170353216', '2041398277', '2153246844', '2284088791', '1993220126', '2161358742', '2106263419', '2095462610', '1853584165', '2038688182', '2161252077', '2135776739', '2105023373', '1604669518', '1595588782', '1501569259', '2153824114', '2300132647', '2314596189', '2404648544', '1990803303', '2042482344', '2148965331', '2054301630', '1503627356', '29432707', '2953574651', '1582097224', '2138734279', '1557226830', '1556527633', '1904315595', '2052018036', '1518090246', '2111413199', '2149167270', '2065069092', '1971141125', '1825352576', '2163057638', '1988757916', '2265250148', '2120956731', '28957832', '2167915094', '2125270590', '2061350602', '185844946', '2568817521', '2147299386', '1987835115', '2047492721', '2094807754', '2114815581', '1894940466', '2027106352', '2067520353', '2147299689', '2078147855', '2170233910', '1511244611', '2080590160', '1995769018', '1526416037', '2143397382', '2163926395', '1563423869', '2099133991', '1990085730', '2056423718', '2161471041', '2087723110', '2017074621', '2407114814', '2070635714', '2123557284', '1975953257', '2100793025', '2285251315', '2127329614', '1998204765', '2119128590', '2150937316', '800369072', '1640382905', '88044681', '1508028241', '2000651459', '2159839830', '1486078251', '2163969896', '1982818705', '1588227575', '1968527219', '1603560790', '2090697598', '2277305703', '2140046090', '49290057', '1852604756', '2167409266', '2136396194', '1568441100', '1990181683', '2018772229', '2166467874', '1528044533', '2058221911', '2098622873', '184448442', '1495651825', '1119535782', '2121908619', '37955466', '2051616977', '2252942877', '2079038124', '2079661555', '1939397766', '1529348175', '2108953480', '1508602342', '1963646974', '2016610070', '2116257108', '1986370385', '2091854703', '2110706065', '127166526', '2092453494', '2096566710', '2023975198', '2037884882', '1503680122', '2030061565', '2000802702', '2241475286', '1986800917', '2098342854', '2077025187', '1905599301', '2144512221', '2162876581', '1991349545', '2086574022', '2144569631', '2069000151', '1576453192', '2537248713', '849629505', '2109528405', '2079521366', '834938524', '2416052431', '2128623470', '1540445010', '2043257640', '2167548712', '2242363672', '2132296233', '1990367307', '2126983359', '1508309427', '2042319619', '2139758942', '2010526386', '2138102728', '85723822', '2031417141', '1525438869', '2163805338', '10171016', '2041713059', '19366481', '2061880020', '2069386051', '2004446210', '2173959313'], 'bm25_doc_scores': [35.934208, 33.0912, 33.052948, 32.82981, 31.221657, 31.138668, 30.637032, 30.280264, 30.246641, 30.006199, 29.650976, 29.600962, 29.458899, 29.423128, 29.396383, 29.252386, 28.97963, 28.744614, 28.553558, 28.449535, 28.230831, 28.20537, 28.108746, 28.036343, 27.805397, 27.7416, 27.615288, 27.509691, 27.152931, 26.971354, 26.953972, 26.792612, 26.622711, 26.585583, 26.576391, 26.561838, 26.534153, 26.531456, 26.389961, 26.372803, 26.260038, 26.2367, 26.128359, 26.046736, 26.000954, 25.919765, 25.905426, 25.89397, 25.889349, 25.862518, 25.84758, 25.782446, 25.782108, 25.772066, 25.749983, 25.740105, 25.734177, 25.67136, 25.6417, 25.600655, 25.586643, 25.564392, 25.502743, 25.486668, 25.39682, 25.391466, 25.389513, 25.368412, 25.353844, 25.331598, 25.208284, 25.035507, 24.935112, 24.933075, 24.922352, 24.880005, 24.83186, 24.823435, 24.76606, 24.74801, 24.657333, 24.656181, 24.654373, 24.640877, 24.62772, 24.614698, 24.541113, 24.495077, 24.477936, 24.430233, 24.427765, 24.390024, 24.383505, 24.370052, 24.356829, 24.349272, 24.337822, 24.33754, 24.30703, 24.291655, 24.287285, 24.271177, 24.236553, 24.23449, 24.234182, 24.187534, 24.185907, 24.168665, 24.11908, 24.08419, 24.076414, 24.01239, 24.00346, 23.992952, 23.984213, 23.97889, 23.96978, 23.950567, 23.936377, 23.892841, 23.879719, 23.86959, 23.842613, 23.792036, 23.781345, 23.77452, 23.771168, 23.761257, 23.75739, 23.745752, 23.736496, 23.715609, 23.699654, 23.68713, 23.665176, 23.655188, 23.635347, 23.629131, 23.616434, 23.605923, 23.59792, 23.572636, 23.539883, 23.51063, 23.491856, 23.479248, 23.476023, 23.472126, 23.449116, 23.43147, 23.422958, 23.410675, 23.409853, 23.397419, 23.381878, 23.365004, 23.355286, 23.332811, 23.310333, 23.289753, 23.287508, 23.28658, 23.273685, 23.258253, 23.253532, 23.23512, 23.215395, 23.184145, 23.18366, 23.182846, 23.18198, 23.177694, 23.175404, 23.175404, 23.170946, 23.148167, 23.11913, 23.114466, 23.09844, 23.097546, 23.074566, 23.074375, 23.053598, 23.051392, 23.050333, 23.033638, 23.026625, 23.013573, 23.012274, 22.994156, 22.987076, 22.967186, 22.966778, 22.959843, 22.95264, 22.951344, 22.944908, 22.944733, 22.940592, 22.922985, 22.907644, 22.885098, 22.874857, 22.864443, 22.86076, 22.847145, 22.838133, 22.825619, 22.821749, 22.821482, 22.805862, 22.792877, 22.78935, 22.788769, 22.78029, 22.754696, 22.75459, 22.746315, 22.724825, 22.721731, 22.71849, 22.714447, 22.692514, 22.688871, 22.669415, 22.665228, 22.658909, 22.644413, 22.64376, 22.627476, 22.612251, 22.608429, 22.605967, 22.602541, 22.599232, 22.592924, 22.583776, 22.574621, 22.556482, 22.536718, 22.53612, 22.532343, 22.53094, 22.5294, 22.528772, 22.524075, 22.490747, 22.479937, 22.458261, 22.43876, 22.436947, 22.430012, 22.417429, 22.412647, 22.407293, 22.406404, 22.397448, 22.378119, 22.374674, 22.373693, 22.36611, 22.363873, 22.36352, 22.361244, 22.347786, 22.33532, 22.301645, 22.276524, 22.27167, 22.24156, 22.239601, 22.234423, 22.229424, 22.225698, 22.217299, 22.217066, 22.210003, 22.201637, 22.18629, 22.17377, 22.17303, 22.170086, 22.169548, 22.169415, 22.153446, 22.150658, 22.140888, 22.137941, 22.131197, 22.126667, 22.12191, 22.118513, 22.10296, 22.097527, 22.092583, 22.090458, 22.08398, 22.082672, 22.054611, 22.05212, 22.04553, 22.045315, 22.04013, 22.035873, 22.02377, 22.01746, 22.014019, 22.003769, 21.982973, 21.972763, 21.968018, 21.96703, 21.963825, 21.953518, 21.939058, 21.937849, 21.93552, 21.930946, 21.927607, 21.921057, 21.920563, 21.91887, 21.916716, 21.91588, 21.907177, 21.894, 21.885742, 21.877861, 21.877148, 21.86257, 21.858122, 21.849134, 21.841585, 21.841581, 21.841215, 21.804647, 21.78646, 21.786121, 21.78257, 21.779375, 21.778385, 21.759483, 21.758932, 21.7566, 21.755323, 21.746838, 21.744751, 21.742533, 21.742434, 21.742332, 21.723589, 21.722637, 21.721004, 21.718166, 21.716278, 21.71017, 21.706743, 21.701086, 21.698423, 21.693697, 21.688538, 21.6851, 21.684132, 21.68258, 21.675623, 21.670792, 21.666466, 21.657818, 21.653788, 21.644705, 21.640215, 21.630909, 21.614061, 21.610664, 21.600485, 21.594748, 21.581068, 21.571085, 21.540794, 21.539852, 21.539852, 21.539852, 21.539852, 21.538881, 21.536413, 21.533052, 21.532478, 21.52989, 21.524513, 21.513763, 21.506678, 21.501396, 21.498602, 21.49228, 21.491137, 21.490154, 21.48937, 21.485287, 21.458988, 21.453932, 21.447926, 21.44762, 21.438202, 21.434727, 21.433422, 21.429157, 21.42268, 21.42266, 21.42036, 21.4203, 21.42017, 21.418, 21.414938, 21.40197, 21.401386, 21.396494, 21.389038, 21.385782, 21.379921, 21.377619, 21.377602, 21.373325, 21.361347, 21.361347, 21.355963, 21.351423, 21.351288, 21.34755, 21.3425, 21.335442, 21.33298, 21.331419, 21.326305, 21.324743, 21.324455, 21.318733, 21.31775, 21.310406, 21.307129, 21.303614, 21.29982, 21.29404, 21.292994, 21.289547, 21.277359, 21.276035, 21.254324, 21.249659, 21.239384, 21.225183, 21.220263, 21.217659, 21.216066, 21.213879, 21.213203, 21.20842, 21.207222, 21.205124, 21.203735, 21.200335, 21.193485, 21.192118, 21.186205, 21.186077, 21.186054, 21.168957, 21.168484, 21.16378, 21.16213, 21.157269, 21.153425, 21.143032, 21.132471, 21.12845, 21.126112, 21.116608, 21.107914, 21.096157, 21.09497, 21.091372, 21.084915, 21.073294, 21.050402, 21.04869, 21.044722, 21.043999, 21.034142, 21.033976, 21.03181, 21.026072, 21.016943, 21.01083, 21.010437, 21.00583, 21.001856, 20.992142, 20.986576, 20.985598, 20.972637, 20.970432, 20.96693, 20.96648, 20.955576, 20.95502, 20.948448, 20.945797, 20.941668, 20.936941, 20.927479, 20.922943, 20.918749, 20.910759, 20.90846, 20.906815, 20.895412, 20.895254, 20.892357, 20.888252, 20.879618, 20.87764, 20.869358, 20.868956, 20.86581, 20.85942, 20.858772, 20.8577, 20.856913, 20.852766, 20.845375, 20.845058, 20.835648, 20.81947, 20.815922, 20.815557, 20.811121, 20.806097, 20.804193, 20.803682, 20.801826, 20.801046, 20.792225, 20.789867, 20.786995, 20.784641, 20.783451, 20.783388, 20.77893, 20.778852, 20.777128, 20.77555, 20.775143, 20.772854, 20.772713, 20.764774, 20.758535, 20.756691, 20.752426, 20.748758, 20.747623, 20.738447, 20.737806, 20.736212, 20.729063, 20.725025, 20.724133, 20.720112, 20.719425, 20.7168, 20.71132, 20.704536, 20.704077, 20.70308, 20.702503, 20.70136, 20.695818, 20.69262, 20.686016, 20.68384, 20.683815, 20.682262, 20.681053, 20.676981, 20.661858, 20.660511, 20.65923, 20.656273, 20.654984, 20.651062, 20.649132, 20.64295, 20.64284, 20.639824, 20.63922, 20.635635, 20.63453, 20.631683, 20.626148, 20.61369, 20.61369, 20.606655, 20.605215, 20.601198, 20.59575, 20.594337, 20.591452, 20.591312, 20.590601, 20.589092, 20.582676, 20.578485, 20.578304, 20.573473, 20.57327, 20.56209, 20.555595, 20.553986, 20.551004, 20.550955, 20.540588, 20.539526, 20.538733, 20.533543, 20.533125, 20.531534, 20.52478, 20.522348, 20.51921, 20.516659, 20.513086, 20.510895, 20.503956, 20.503597, 20.50088, 20.499214, 20.495174, 20.495165, 20.485312, 20.477182, 20.476461, 20.47098, 20.463821, 20.458605, 20.457712, 20.447388, 20.44639, 20.438248, 20.43766, 20.434778, 20.42926, 20.427452, 20.41382, 20.40137, 20.39856, 20.391706, 20.39084, 20.385603, 20.383982, 20.380749, 20.38055, 20.380232, 20.376604, 20.36461, 20.363655, 20.36245, 20.359867, 20.359224, 20.352753, 20.352238, 20.348862, 20.34754, 20.346565, 20.3439, 20.340929, 20.34027, 20.339954, 20.33289, 20.326704, 20.31581, 20.30357, 20.300016, 20.298426, 20.29815, 20.296766, 20.295277, 20.289583, 20.281727, 20.281118, 20.28088, 20.279732, 20.271929, 20.26856, 20.265333, 20.26146, 20.260004, 20.257622, 20.254284, 20.254284, 20.253994, 20.251755, 20.240196, 20.237888, 20.232737, 20.229382, 20.228556, 20.228556, 20.227585, 20.225697, 20.223078, 20.220701, 20.2206, 20.21973, 20.219574, 20.218155, 20.209454, 20.20884, 20.20884, 20.20704, 20.20257, 20.202297, 20.201588, 20.19878, 20.195984, 20.190948, 20.188375, 20.18561, 20.184597, 20.177197, 20.1761, 20.17284, 20.170322, 20.166424, 20.16421, 20.163511, 20.162603, 20.158558, 20.158237, 20.158117, 20.156805, 20.155746, 20.154871, 20.153345, 20.15124, 20.146927, 20.142387, 20.14139, 20.138466, 20.135025, 20.133175, 20.130697, 20.128181, 20.127914, 20.127012, 20.126104, 20.118874, 20.11359, 20.113081, 20.112621, 20.106953, 20.103596, 20.10257, 20.098873, 20.098068, 20.097733, 20.09704, 20.097017, 20.095585, 20.094889, 20.094255, 20.093723, 20.093723, 20.090479, 20.0895, 20.081457, 20.07975, 20.07611, 20.075424, 20.073927, 20.066156, 20.065586, 20.062922, 20.058336, 20.057617, 20.05402, 20.052792, 20.04619, 20.039703, 20.039135, 20.038479, 20.038479, 20.035858, 20.034025, 20.033525, 20.032784, 20.0312, 20.02721, 20.027107, 20.022251, 20.018782, 20.01749, 20.016342, 20.016321, 20.013006, 20.002363, 20.000887, 20.000175, 19.999655, 19.9996, 19.994919, 19.99459, 19.989573, 19.98861, 19.98858, 19.988384, 19.98763, 19.987112, 19.986319, 19.983442, 19.9828, 19.982704, 19.980778, 19.979916, 19.979256, 19.979141, 19.978882, 19.97868, 19.978481, 19.978157, 19.977995, 19.976877, 19.9725, 19.966715, 19.964579, 19.96452, 19.962004, 19.957378, 19.950155, 19.94925, 19.948574, 19.946754, 19.946754, 19.945484, 19.944412, 19.944408, 19.9364, 19.934668, 19.931637, 19.929613, 19.928112, 19.926989, 19.925661, 19.925276, 19.920818, 19.919416, 19.919394, 19.918293, 19.9078, 19.907055, 19.905489, 19.904503, 19.903337, 19.901583, 19.898687, 19.898018, 19.897894, 19.895529, 19.893976, 19.889011, 19.88496, 19.884905, 19.884274, 19.881573, 19.877953, 19.876757, 19.870663, 19.868357, 19.867222, 19.865574, 19.865067, 19.862679, 19.861551, 19.852821, 19.85168, 19.8514, 19.85032, 19.848465, 19.846233, 19.836117, 19.82839, 19.827995, 19.826845, 19.8256, 19.824259, 19.821177, 19.820248, 19.818678, 19.817657, 19.816515, 19.80316, 19.800978, 19.799536, 19.798018, 19.797678, 19.796534, 19.79422, 19.790268, 19.788445, 19.785522, 19.784529, 19.783377, 19.783274, 19.782059, 19.779459, 19.77816, 19.778004, 19.777206, 19.774458, 19.773312, 19.773252, 19.772938, 19.770283, 19.767872, 19.767872, 19.754925, 19.746216, 19.744253, 19.744253, 19.743404, 19.738457, 19.737137, 19.736946, 19.731754, 19.729431, 19.728119, 19.727358, 19.727188, 19.721506, 19.720745, 19.71995, 19.717712, 19.715689, 19.71524, 19.715214, 19.711447, 19.711447, 19.709444, 19.705832, 19.704529, 19.70175, 19.701208, 19.701208, 19.696108, 19.695105, 19.693073, 19.693073, 19.690922, 19.687304, 19.68183, 19.680967, 19.678604, 19.676313, 19.674038, 19.673866, 19.671707, 19.671684, 19.669718, 19.669594, 19.668926, 19.666058, 19.665997, 19.664293, 19.663322, 19.663317, 19.66061, 19.659836, 19.659267, 19.65814, 19.655115, 19.64635, 19.645546, 19.64479, 19.644112, 19.63935, 19.637825, 19.637493, 19.635857, 19.633768, 19.633364, 19.633364, 19.633364, 19.628685, 19.625782, 19.625782, 19.622921, 19.621992, 19.621992, 19.618954, 19.613775, 19.607819, 19.60706, 19.606518]}, {'id': '2552991372', 'text': 'community integrate strategy base improve modular density increment large scale network', 'timestamp': 1488322800, 'rel_doc_ids': ['2014411919', '2015089907', '2015953751', '2029373408', '2080909935', '2088162437', '2128366083', '2131681506', '2168346701', '2320144048'], 'user_id': '51535807', 'user_doc_ids': ['1510855954', '1511290209', '1547996656', '1672285122', '1895982061', '1963763852', '1967837839', '1967981290', '1971580912', '1975680103', '1978567114', '2000270242', '2002441386', '2004149614', '2006224698', '2009821813', '2015089907', '2018408478', '2025279364', '2042675473', '2043467118', '2044312605', '2045823581', '2056534781', '2060756050', '2065216241', '2081784348', '2082213282', '2109708716', '2115519999', '2115560026', '2118262294', '2121635989', '2128447112', '2153592003', '2167309519', '2248464603', '2279005051', '2290265371', '2290268423', '2300618187', '2314669335', '2507368687', '2518141884', '2519602334', '2520964758', '2593938603', '2788667927', '2915935370', '2916780012'], 'bm25_doc_ids': ['2545185744', '2076272170', '1976348821', '2193215203', '2012737887', '2159201125', '2088162437', '2124280521', '2003352916', '1779435530', '2015089907', '2134446884', '2288862157', '2079295974', '2084108780', '2102717275', '917918705', '1856775150', '2315775489', '2114808857', '2079599800', '1966119405', '2303924406', '2052819945', '2159030247', '2153048426', '2035188969', '2037098808', '1980971889', '2125635831', '1982504621', '2171671289', '1984689597', '2080721324', '2030586864', '2949344645', '2122515732', '2404245717', '2158562883', '1997449813', '2077715475', '2962770299', '2107308132', '2281822004', '2029373408', '1600737405', '1727189812', '2225902610', '2467745878', '1660561508', '1966529053', '2136975349', '2110815753', '2073511876', '2133939386', '15019541', '1957145452', '2103980282', '240551101', '2600086047', '2040858714', '2095646393', '2950255242', '2157797752', '2095711515', '2056254431', '2320144048', '2093067948', '2523838129', '2137274228', '1479970606', '1508161942', '2117755853', '2036989370', '1971723124', '343200749', '2008209917', '2963302977', '2098993874', '2122405502', '2952394152', '2094983534', '412572545', '2088193897', '2110765523', '1984754588', '147343022', '1562524385', '2094934391', '2154658580', '2157532285', '2018023929', '2185732128', '2152420867', '2209225614', '1519940858', '2417643473', '2050031241', '2147747142', '2062825488', '2238145932', '2227911040', '1984994038', '1981141219', '2012703340', '2116045669', '1788176652', '2156028561', '2080909935', '2414744193', '2199777851', '2042382179', '2126022585', '2044920626', '2082201811', '1546079235', '2128286995', '2069837973', '2135108309', '203457315', '2020505289', '206953779', '2267451181', '1985281257', '2041835269', '2221706442', '2032569651', '1944071515', '2950597905', '1798567573', '2096522683', '2119928281', '1993191280', '1558363837', '152738136', '2030696657', '2109586114', '2133495748', '2160249427', '2168768094', '1547103408', '1555670207', '2228765530', '1954102626', '2128366083', '2074132047', '2061850038', '2343864649', '82274671', '2167074372', '2270746387', '106504814', '2187399757', '1975606664', '2411855254', '2080073912', '2046955781', '2113619626', '2010371279', '2088502728', '2288691048', '2057088849', '2115155307', '39601649', '2044783742', '2012266853', '2022693368', '2163292376', '1602847949', '2046547407', '2079404959', '2065223082', '1974897379', '2169020312', '2244728020', '2504964546', '2131681506', '2074192627', '2022943390', '2169050909', '2100601931', '1566772816', '2558632230', '2085952081', '1990548012', '2080480351', '152012270', '2002189912', '1971325936', '2633943433', '2170768441', '2040715025', '2325697977', '1978787982', '2108197648', '2134931490', '312354631', '2051226661', '1983308770', '1904463290', '1884578711', '2022615159', '2042236541', '2099326369', '2024655237', '2507629614', '2102509262', '1987874622', '2146916979', '2159549823', '2167277568', '1987077602', '2294878026', '2114532600', '2016322375', '1693437504', '1535851109', '2022655233', '2122821480', '2169163929', '2013194136', '2164041305', '2108498827', '347783327', '2054902478', '1979262068', '1978217865', '2012369610', '2068084848', '2034337937', '1997547570', '1979971249', '2015957862', '2085143124', '2065646798', '2126446147', '1594515725', '2084432741', '641279353', '2153234270', '1979749873', '1985155531', '2088190635', '84805429', '2073818640', '2120230599', '2117526408', '2166190875', '1997527463', '2140192812', '2085080593', '2050515889', '593638888', '1848552279', '1964333868', '2949478393', '2027346915', '1481632754', '1757977160', '2150497675', '1890810769', '2037902675', '1983099884', '23704684', '2083208481', '2060739241', '2040954109', '2115167721', '1965533780', '22865002', '2158957507', '2173113387', '2028553256', '2535865604', '2084009541', '1981861400', '2113993993', '2164594745', '2342205357', '2020043601', '1969549246', '2489823182', '1971019577', '2005406219', '2046108229', '2054525149', '2051365847', '2121737596', '2056480257', '2157762631', '2170641079', '2145333791', '2032820989', '2164928285', '2043002148', '2137385671', '1966846029', '1985625141', '2170181489', '1981056503', '2157711919', '2065579600', '1979582417', '2054564636', '1993734303', '2029572536', '2027295145', '2085274465', '1629693692', '2230371868', '2963472676', '2023968567', '2151910462', '2145045200', '1983560528', '2463650778', '2003855622', '2199337243', '1491379303', '2044752430', '83075549', '2078767284', '2140271346', '134985385', '1804714834', '2021947606', '1506529443', '888575606', '2119920984', '2999397385', '2023439665', '1976160665', '2095802318', '2098276351', '1548207682', '2026095389', '2055667921', '1831113816', '2254019636', '2003186926', '2239231754', '2593320540', '2105288174', '818837996', '1968847543', '2293662676', '1993042578', '2317512139', '2067000305', '2097452216', '2089500041', '2015953751', '265307472', '1639970850', '1983113601', '1123605699', '1483501515', '1971844861', '2074990030', '2076032654', '1964083923', '1950832899', '2064514945', '2405180489', '2104252836', '2527819024', '1995109099', '2033237788', '2065005304', '2287830546', '2111861233', '2113440676', '2105956161', '1657157331', '1531938654', '1999802487', '1692425423', '2142903518', '2011784152', '1990600578', '2025597303', '2170273717', '2004389296', '2024495622', '2249577208', '2111739318', '2343361823', '2061350775', '2114618638', '2118388570', '2123905258', '2297833300', '2152091390', '2162802226', '1973318966', '2078869871', '2034643755', '2036433616', '2164882174', '2026922408', '1482567108', '1943434966', '1587356849', '2086730149', '2164143190', '2095965857', '1690396231', '1600623192', '2964229821', '2104483961', '2281242609', '2056259573', '2014618539', '2111642621', '2058819611', '2168193419', '2033246834', '175395000', '2110898590', '2031439963', '2143016205', '2077946073', '2071864549', '2468164286', '2029038613', '2169116170', '77974465', '2058725299', '2057133495', '309670289', '2560767040', '2129110272', '2164749149', '2054929864', '2094426896', '1560183540', '2055081054', '2121488209', '1011540006', '109559749', '1963923302', '1967022297', '1968994622', '2132958441', '2092433959', '1770543141', '2085752014', '2122461069', '1977770738', '2951665412', '2047252885', '2156500464', '2074851655', '2408508948', '2056035178', '2509399382', '2152558226', '2137074608', '1887500199', '2023924513', '2003880261', '238862651', '1976286767', '2027355980', '1874172579', '2129729509', '1825836619', '2053950676', '1995135151', '2963413618', '164995653', '2142783542', '2169998945', '2559331598', '1987420041', '2082520038', '2150061216', '1990027004', '2320220526', '2047099393', '2125841393', '2139694940', '2010948750', '132921321', '2084048554', '2068948402', '2161774565', '1486979786', '2133828085', '2102072242', '2314104109', '2092038191', '2949254039', '2005498212', '2204376737', '2135611582', '1584566445', '2161008018', '2028069703', '2066760478', '1986371207', '2147659374', '1502595600', '2056489215', '2102111939', '2095503032', '2134829982', '2060125619', '2127928904', '1973954552', '2563734667', '1995227744', '2108839904', '242026773', '1583104733', '1542535189', '2291701016', '1789485070', '2031158620', '2131844391', '1537387915', '2090419595', '2060616833', '1566499845', '2131717044', '2125042266', '2128941558', '2082459856', '1989299066', '2368841069', '251888053', '2159463015', '2153432248', '2119625792', '2116011221', '1987911694', '2214127349', '2095189226', '2065231869', '2196880011', '1741736375', '2320617691', '2022575851', '1756737390', '2138175288', '1972067365', '1975086969', '1972034280', '1495941941', '1551713770', '2116433500', '2169499007', '1968103139', '2014411919', '2022704179', '2019430098', '2040591494', '2029855260', '2152163260', '1998045462', '2732033745', '2169006312', '2099851008', '2082380642', '2313153393', '2025764057', '1982124329', '2082726457', '2040940701', '2145152409', '2123629856', '2255019206', '756829873', '2009608161', '2002160413', '2105303354', '2107226190', '2169002223', '1516928384', '2041244058', '1898665114', '2010907553', '1887263241', '2080024680', '1548337895', '2063228777', '2309575954', '2056897951', '2134503268', '2102056218', '2333350679', '2151538077', '1977747497', '2185459184', '2115167491', '2963104673', '1970201300', '2084040756', '1988543692', '2048465561', '1974874507', '2151686561', '2048856090', '2170827446', '1969228160', '1602675647', '2115823003', '2142928176', '2106390177', '2061357511', '2408451850', '1977442405', '2157908337', '1983672392', '2128427912', '1495304293', '2168346701', '2120191719', '1660427993', '1602550277', '1594550147', '1484722923', '2150860159', '2038876783', '1513711820', '205846957', '1973400209', '813892931', '2118109887', '2066827224', '2069763596', '1493310651', '2058532765', '2146439054', '2138700526', '2052311060', '1996829063', '2065589669', '2251887092', '2130935956', '2156414722', '2072808937', '2096450713', '2080430021', '2044293720', '1970795504', '178409488', '2087113078', '1532064498', '2088465042', '2474092821', '2336248184', '2144354402', '2121336607', '2444002369', '1989276239', '2124100711', '2296546993', '2088897443', '2171039564', '2169923338', '1953588267', '2047013860', '2139626955', '2052495386', '116716900', '1577038640', '2016835555', '1406318', '2028572315', '1978161373', '2054823577', '2263392086', '2438985978', '1977109828', '2527360652', '1989700766', '2079683703', '2047740786', '2065479644', '2077738892', '2054680727', '2115118594', '162292866', '2149857313', '2025661610', '2152419477', '2158643468', '2089179100', '218258470', '1964697245', '2963932731', '1617275037', '2963772271', '2050893034', '2124943418', '2135827982', '2112804575', '2021910166', '2004805052', '2053225008', '2170176916', '2026177951', '2097208041', '1971793621', '1993922776', '2092886985', '1983117661', '2069543694', '2593701883', '2094989204', '2149697529', '2001477717', '1150348879', '2112943931', '1975649342', '1991346315', '2100240966', '2023343065', '1896862188', '2029844691', '2092102472', '2485228835', '2095012686', '2106738728', '2147227942', '1611890622', '150230256', '2077278543', '1975988161', '2124209874', '2059632648', '2097497483', '2090866468', '1999241827', '2040227800', '2031411464', '1498198371', '1576150961', '2089048768', '2127073209', '2261926893', '2066128830', '2075269458', '2123990204', '2082204469', '1628722668', '1981410982', '2207622687', '2154502203', '2070847003', '35373105', '2136642039', '2014758016', '2070870972', '2041963575', '2049629393', '2127967068', '2148851824', '2018360887', '2110633879', '2068431645', '2015178478', '2097531141', '2129755148', '2398703788', '2010338547', '2111917647', '2001105880', '2019537770', '74392392', '1986349174', '2108019282', '2134488896', '1993028176', '2148928198', '2091158229', '2165652466', '2107531455', '1987600084', '2087612346', '2041183244', '1965441943', '2044988896', '2157837631', '1913866781', '2019009079', '2060645010', '2010748059', '146471388', '2001150155', '78390088', '1908198171', '2051250362', '2405353940', '2018982301', '2120177314', '1523866696', '1973217102', '2096702882', '1605385825', '1488587583', '2044312508', '2113283744', '2409576980', '2169589098', '2046861976', '2057218453', '2132808850', '2016338978', '2010250014', '438350557', '1597188505', '2138449516', '1515279208', '2151325115', '2007314697', '1966976589', '2163167544', '2115838342', '2122832540', '2017620474', '1988309508', '1828649173', '2049641310', '2171887467', '2167025469', '2159583675', '117096852', '2093714868', '2150380088', '2144694699', '1567531241', '2063944225', '2054371169', '778011504', '1868272012', '1971417023', '2036637541', '2073741719', '2165611133', '2057285795', '2005156851', '2106587687', '125194972', '2124125807', '1978043574', '2082911481', '1984539048', '2032273054', '2010444881', '2002528071', '2034424557', '2129660884', '2239722250', '2011211123', '2017653821', '2013472030', '1993336109', '2204346730', '2116669519', '2259640286', '2221868786', '2154953443', '2044505640', '2116277877', '2153047491', '2020103822', '1606354585', '2105724116', '2143479236', '2170858440', '2143020453', '2473052092', '2050187343', '1559135799', '2059835875', '2023004365', '2031515077', '2134829359', '2148100000', '2125981644', '1978374482', '1491200812', '2034427615', '2033326237', '2501200461', '2274540769', '2067764765', '1505854179', '1978842814', '2088085905', '2079456251', '2009251196', '1977955578', '2117002019', '2140176546', '1547574152', '1968656203', '1971097778', '2043568498', '2151346267', '2074235399', '2293875723', '2051537709', '2127724694', '2533239314', '1781719596', '2152621911', '1990186189', '2949830731', '2138356120', '1966650577', '2124321145', '2537512108', '2138622732', '1931091216', '2035084401', '2169511354', '2001098640', '2047969010', '2068345362', '1536856548', '1983549688', '2125030564', '2100791897', '2275612136', '2142919762', '2951553909', '2172281678', '2028302194', '2065812289', '2052721939', '2140109325', '1531254996', '2086821976', '1852686194', '2122558727', '2019648444', '2153602793', '2061051181', '1631014018', '895718502', '2082496345', '2950903130', '2162948416', '2125062631', '1993610838', '2112811019', '2100985998', '2086662759', '2581522679', '2107345185', '2644341057', '2333013678', '1951266153', '1982828967', '2160122869', '2220870060', '2098936920', '1976124788', '2153382583', '2011502769', '2113157281', '1633473982', '2144503767', '2062787731', '1537184953', '1966059357', '1525699072', '2063112988', '2211196523', '1543043152', '1977191041', '2131079631', '1859366975', '2156470319', '1880431677', '1989035026', '2011329484', '2091204864', '2070317808', '2104593057', '2087467283', '2467690262', '1994661745', '2099606101', '2182745711', '1991375476', '2170367332', '1970342056', '2016076077', '2171396550', '1552052669', '2400759634', '1872822355', '2146660407', '2464143715', '2133112641', '1529033498'], 'bm25_doc_scores': [30.726326, 30.50159, 30.100574, 30.018976, 30.010845, 29.767776, 29.702288, 29.408192, 28.26636, 28.223866, 28.1898, 28.156815, 28.008514, 27.976555, 27.680607, 27.41899, 27.310205, 27.294249, 27.240246, 27.23637, 27.171795, 27.10222, 27.061909, 26.867367, 26.57991, 26.226295, 26.224106, 26.165468, 26.00151, 26.000502, 25.9593, 25.922224, 25.721272, 25.663006, 25.65694, 25.488842, 25.488842, 25.454653, 25.436207, 25.403831, 25.395275, 25.392406, 25.303598, 25.244593, 25.119978, 25.098135, 24.915237, 24.902195, 24.852943, 24.8354, 24.803778, 24.765337, 24.74919, 24.69321, 24.626242, 24.484053, 24.380642, 24.246725, 24.173891, 24.120554, 24.055502, 24.054379, 24.050484, 24.050484, 24.01458, 24.010141, 23.998333, 23.997402, 23.990328, 23.951326, 23.90867, 23.904186, 23.885084, 23.854168, 23.820719, 23.798912, 23.78433, 23.78433, 23.780226, 23.777931, 23.723598, 23.723598, 23.639774, 23.58738, 23.574179, 23.572756, 23.565508, 23.530628, 23.528694, 23.490332, 23.45933, 23.431343, 23.378134, 23.376062, 23.356556, 23.3154, 23.306688, 23.257885, 23.248379, 23.233139, 23.227901, 23.193682, 23.191725, 23.153439, 23.112467, 23.07659, 23.07257, 23.06084, 23.036465, 22.969099, 22.94347, 22.904394, 22.889025, 22.882988, 22.882217, 22.874784, 22.86615, 22.854887, 22.826992, 22.820518, 22.800014, 22.78951, 22.763493, 22.730854, 22.720243, 22.682804, 22.664215, 22.658636, 22.658636, 22.649971, 22.631557, 22.605381, 22.59542, 22.589987, 22.58716, 22.583723, 22.580956, 22.547441, 22.522705, 22.522705, 22.51936, 22.508753, 22.50575, 22.48103, 22.474964, 22.473738, 22.420872, 22.41414, 22.41083, 22.410034, 22.393457, 22.387308, 22.386604, 22.386484, 22.381771, 22.37332, 22.362247, 22.359426, 22.3282, 22.316368, 22.305706, 22.295946, 22.289162, 22.243505, 22.212196, 22.209957, 22.19418, 22.194136, 22.153618, 22.150528, 22.145061, 22.125067, 22.113188, 22.113125, 22.085703, 22.082405, 22.04221, 22.01307, 22.005867, 21.99893, 21.991362, 21.986233, 21.932396, 21.925278, 21.922283, 21.917072, 21.893194, 21.862265, 21.838127, 21.835869, 21.79998, 21.792526, 21.78846, 21.7866, 21.782692, 21.77779, 21.770224, 21.720055, 21.717491, 21.68925, 21.680391, 21.670183, 21.65474, 21.651882, 21.65152, 21.641523, 21.629784, 21.622873, 21.618565, 21.616941, 21.599003, 21.594948, 21.55318, 21.542702, 21.533333, 21.522326, 21.50362, 21.495895, 21.481834, 21.471054, 21.449368, 21.429302, 21.42801, 21.426538, 21.402748, 21.398703, 21.395548, 21.387127, 21.386438, 21.379274, 21.353268, 21.346182, 21.34315, 21.337944, 21.299042, 21.28815, 21.276033, 21.256609, 21.232859, 21.231876, 21.213156, 21.206343, 21.171701, 21.134823, 21.134207, 21.125841, 21.122633, 21.088816, 21.061962, 21.05418, 21.05158, 21.042229, 21.041056, 21.039751, 21.008242, 21.001173, 20.98724, 20.985023, 20.966095, 20.959751, 20.937138, 20.93464, 20.93289, 20.9297, 20.915932, 20.907835, 20.902147, 20.89011, 20.884235, 20.878807, 20.873632, 20.873632, 20.848288, 20.831776, 20.81251, 20.809887, 20.795141, 20.788883, 20.788054, 20.787027, 20.779541, 20.777798, 20.77015, 20.750717, 20.72776, 20.714, 20.706795, 20.699852, 20.698845, 20.691935, 20.689886, 20.689886, 20.683743, 20.682377, 20.664682, 20.655783, 20.650743, 20.648912, 20.645248, 20.642439, 20.636374, 20.633186, 20.624367, 20.60689, 20.602198, 20.59879, 20.588514, 20.57673, 20.573374, 20.554804, 20.549522, 20.549522, 20.545706, 20.540047, 20.52948, 20.525743, 20.52282, 20.497463, 20.493715, 20.492834, 20.481209, 20.479542, 20.475388, 20.47264, 20.465342, 20.458048, 20.449724, 20.448755, 20.439352, 20.439352, 20.431742, 20.425241, 20.424692, 20.423637, 20.40661, 20.390541, 20.388014, 20.382414, 20.379162, 20.374851, 20.374151, 20.372276, 20.368843, 20.368454, 20.356422, 20.34445, 20.33301, 20.328182, 20.32567, 20.313196, 20.311762, 20.31034, 20.296577, 20.284197, 20.277447, 20.27233, 20.254425, 20.25364, 20.246124, 20.242672, 20.242558, 20.240269, 20.238756, 20.23807, 20.23742, 20.236973, 20.236732, 20.233685, 20.232618, 20.232548, 20.230036, 20.229578, 20.225922, 20.214699, 20.205072, 20.19676, 20.189415, 20.188822, 20.18815, 20.17686, 20.163116, 20.154932, 20.146786, 20.13937, 20.1269, 20.123592, 20.122059, 20.092985, 20.076637, 20.076294, 20.069382, 20.063677, 20.055956, 20.053612, 20.052868, 20.040379, 20.034714, 20.028479, 20.024235, 20.01623, 20.015022, 20.01244, 20.011692, 20.009808, 20.009146, 19.99933, 19.988388, 19.982355, 19.972822, 19.967579, 19.96048, 19.95392, 19.948065, 19.930489, 19.925653, 19.924107, 19.92045, 19.908924, 19.908201, 19.904043, 19.903223, 19.902834, 19.899296, 19.891468, 19.880926, 19.87906, 19.878763, 19.867407, 19.866928, 19.848494, 19.8378, 19.832062, 19.822601, 19.81545, 19.804535, 19.780277, 19.7791, 19.778484, 19.776983, 19.775162, 19.770285, 19.766365, 19.755562, 19.754278, 19.75356, 19.743807, 19.74231, 19.74103, 19.733671, 19.732847, 19.728836, 19.71303, 19.712723, 19.711311, 19.702595, 19.700619, 19.700163, 19.699112, 19.698095, 19.688303, 19.686092, 19.685438, 19.675432, 19.669422, 19.666039, 19.657356, 19.653894, 19.650934, 19.63959, 19.638206, 19.635328, 19.61766, 19.615074, 19.613117, 19.606352, 19.602137, 19.594994, 19.593891, 19.58857, 19.582409, 19.581673, 19.567173, 19.563292, 19.562706, 19.56213, 19.560873, 19.554352, 19.552317, 19.55057, 19.545889, 19.543306, 19.537071, 19.531805, 19.529192, 19.527637, 19.516611, 19.512531, 19.507616, 19.505949, 19.503286, 19.498293, 19.495794, 19.495571, 19.494402, 19.492807, 19.484205, 19.478903, 19.473127, 19.468664, 19.465727, 19.46195, 19.461462, 19.457668, 19.457539, 19.457539, 19.456581, 19.445234, 19.429173, 19.425465, 19.415222, 19.411945, 19.40518, 19.40384, 19.401537, 19.400812, 19.40078, 19.397285, 19.39482, 19.391912, 19.390799, 19.387123, 19.38381, 19.381441, 19.38056, 19.380243, 19.379225, 19.37467, 19.368702, 19.365631, 19.36314, 19.354713, 19.354422, 19.349174, 19.344524, 19.339153, 19.335417, 19.331165, 19.32903, 19.326534, 19.322563, 19.32254, 19.3198, 19.314602, 19.311956, 19.310215, 19.306437, 19.294327, 19.293478, 19.2926, 19.292086, 19.288992, 19.288084, 19.284218, 19.280373, 19.27955, 19.278261, 19.271832, 19.265995, 19.264904, 19.25187, 19.240614, 19.238884, 19.228863, 19.22741, 19.226843, 19.217728, 19.204462, 19.202005, 19.19463, 19.19237, 19.182236, 19.177338, 19.17641, 19.176216, 19.175924, 19.174519, 19.17286, 19.154606, 19.153175, 19.150877, 19.150797, 19.13781, 19.123825, 19.122143, 19.121977, 19.118929, 19.118929, 19.11764, 19.116262, 19.113441, 19.111086, 19.094206, 19.08734, 19.086891, 19.078342, 19.068073, 19.06404, 19.05423, 19.048925, 19.029354, 19.024422, 19.021507, 19.020805, 19.01987, 19.019869, 19.009539, 19.004847, 19.003435, 19.0012, 19.000622, 18.998405, 18.993723, 18.99047, 18.982445, 18.981707, 18.978909, 18.977812, 18.971691, 18.965157, 18.963135, 18.958593, 18.954134, 18.9529, 18.948364, 18.946712, 18.944386, 18.942213, 18.93793, 18.934595, 18.932606, 18.925173, 18.92354, 18.908268, 18.907085, 18.90372, 18.901392, 18.897858, 18.897758, 18.897057, 18.89524, 18.893026, 18.89151, 18.888739, 18.886707, 18.88454, 18.8842, 18.881407, 18.879614, 18.878807, 18.878502, 18.876472, 18.875198, 18.874336, 18.87239, 18.87229, 18.869476, 18.868883, 18.867195, 18.866867, 18.86349, 18.861479, 18.859312, 18.858248, 18.855232, 18.850178, 18.848068, 18.847778, 18.847126, 18.843163, 18.839546, 18.838118, 18.834766, 18.830353, 18.826757, 18.816734, 18.813799, 18.813725, 18.811428, 18.808395, 18.80738, 18.806166, 18.804693, 18.80426, 18.801973, 18.789398, 18.789373, 18.783237, 18.78275, 18.782457, 18.781645, 18.778975, 18.778742, 18.777636, 18.773205, 18.77282, 18.755453, 18.753828, 18.743423, 18.742905, 18.741467, 18.737162, 18.727266, 18.727077, 18.725113, 18.723787, 18.720062, 18.71313, 18.705822, 18.701242, 18.700966, 18.698318, 18.697557, 18.695208, 18.691402, 18.689943, 18.68993, 18.687134, 18.684946, 18.67818, 18.678038, 18.675774, 18.67473, 18.673973, 18.669764, 18.669764, 18.66872, 18.657309, 18.652893, 18.650242, 18.64826, 18.647968, 18.64427, 18.639065, 18.637423, 18.63648, 18.632421, 18.632175, 18.627548, 18.624537, 18.623058, 18.620453, 18.616526, 18.615582, 18.615582, 18.61509, 18.614191, 18.612595, 18.612497, 18.611612, 18.6103, 18.60885, 18.6084, 18.605463, 18.603798, 18.597515, 18.596912, 18.596294, 18.591873, 18.589476, 18.58857, 18.586168, 18.585428, 18.583134, 18.582874, 18.582003, 18.56878, 18.563946, 18.561829, 18.560305, 18.559582, 18.556465, 18.555927, 18.544662, 18.543463, 18.539248, 18.537651, 18.528294, 18.526632, 18.525993, 18.525492, 18.521997, 18.517435, 18.5145, 18.511883, 18.506659, 18.506538, 18.505386, 18.505182, 18.502108, 18.50079, 18.500307, 18.499706, 18.498554, 18.497997, 18.494225, 18.493214, 18.492369, 18.490738, 18.48911, 18.484655, 18.475836, 18.475435, 18.474768, 18.472109, 18.471096, 18.470781, 18.470121, 18.463068, 18.462252, 18.457294, 18.456375, 18.451, 18.45087, 18.450533, 18.449308, 18.446806, 18.442602, 18.442204, 18.438858, 18.43631, 18.43454, 18.43269, 18.432, 18.431942, 18.429329, 18.428606, 18.428598, 18.427605, 18.426075, 18.42487, 18.420795, 18.420225, 18.419426, 18.41546, 18.414104, 18.406734, 18.399569, 18.393478, 18.38826, 18.385752, 18.385168, 18.383905, 18.382557, 18.382244, 18.378761, 18.378607, 18.372498, 18.367085, 18.367085, 18.365002, 18.362213, 18.35884, 18.357409, 18.356335, 18.353762, 18.350883, 18.34863, 18.345097, 18.342316, 18.341625, 18.34066, 18.337603, 18.334198, 18.333826, 18.333727, 18.332752, 18.33173, 18.32996, 18.327438, 18.326563, 18.324833, 18.322666, 18.316795, 18.313705, 18.313175, 18.309584, 18.30743, 18.307121, 18.302647, 18.300879, 18.300415, 18.295542, 18.294977, 18.292372, 18.291004, 18.29097, 18.285433, 18.280264, 18.277315, 18.27672, 18.276068, 18.27601, 18.275852, 18.271376, 18.26207, 18.260773, 18.258947, 18.257242, 18.256962, 18.247631, 18.243603, 18.243603, 18.243017, 18.241497, 18.240324, 18.23477, 18.234232, 18.23188, 18.226734, 18.225327, 18.223198, 18.222363, 18.220892, 18.22035, 18.218515, 18.216415, 18.216028, 18.212769, 18.209885, 18.209885, 18.207724, 18.204998, 18.194347, 18.192562, 18.190348, 18.186033, 18.183609, 18.182182, 18.182081, 18.18124, 18.180973, 18.180912, 18.174534, 18.171444, 18.170647, 18.167244, 18.164055, 18.16304, 18.162476, 18.158997, 18.156359, 18.153746, 18.150623, 18.150036, 18.149958, 18.147694, 18.146662, 18.140162, 18.1385, 18.136854, 18.135681, 18.135569, 18.135439, 18.134483, 18.133883, 18.130938, 18.128216, 18.128065, 18.127615, 18.124422, 18.121647, 18.119843, 18.119535, 18.118814, 18.11171, 18.111366, 18.108665, 18.108633, 18.106258, 18.104809, 18.1011, 18.100863, 18.100101, 18.099646, 18.098646, 18.095602, 18.094692, 18.089352, 18.087963, 18.08794, 18.086134, 18.085323, 18.082615, 18.082577, 18.079649, 18.079523, 18.077173, 18.076496, 18.073784, 18.07022]}, {'id': '2554612623', 'text': 'cooperative simultaneous localize synchronize mobile agent network', 'timestamp': 1500069600, 'rel_doc_ids': ['1968755793', '1978790760', '1980332598', '1988543519', '2012197287', '2043105656', '2056445701', '2074054110', '2134746901', '2136508742', '2137536023', '2167084407', '2238394037', '2327669095'], 'user_id': '35480463', 'user_doc_ids': ['1481230477', '1488640028', '1507357925', '1509332411', '1518218870', '1567965399', '1571467474', '1610863498', '1617255403', '1666012980', '1686412076', '1822870367', '1828394408', '1891361538', '1899566178', '1902585638', '1903262428', '1910095886', '1932460561', '1938422532', '1953933639', '1953974419', '1968755793', '1976373691', '1979335629', '1982780208', '1987510166', '1989995562', '1992131582', '1994475615', '1995463076', '2001466605', '2003458572', '2003559232', '2010785055', '2011866937', '2023292241', '2026686855', '2031166143', '2032830175', '2036724129', '2037596858', '2038333373', '2043105656', '2043395430', '2044073538', '2048508162', '2050386482', '2065451540', '2067532669', '2072754351', '2074054110', '2076754364', '2090822060', '2095299272', '2096845127', '2098126400', '2098565411', '2099798205', '2101829536', '2102517395', '2103884856', '2107337755', '2108375932', '2109670122', '2112463609', '2113563015', '2114186299', '2114275391', '2116219584', '2117572871', '2118577483', '2119421929', '2119629250', '2120007881', '2120726945', '2127404108', '2127821072', '2129469131', '2130023050', '2130681819', '2130818352', '2131622065', '2133698785', '2134524318', '2137390906', '2139501252', '2139617094', '2141145243', '2141539820', '2142907343', '2143184901', '2143842714', '2144829807', '2144993203', '2145126194', '2146645906', '2147752822', '2150517392', '2150890714', '2155428246', '2156699879', '2157131424', '2158877430', '2161783022', '2163923197', '2165760083', '2166890751', '2168678583', '2168699740', '2169971138', '2170551759', '2170795890', '2171075378', '2240462715', '2327669095', '2510844205', '2516596949', '2542595939', '2953103595', '2963331401'], 'bm25_doc_ids': ['1571467474', '2120007881', '2327669095', '2240462715', '1968755793', '2043105656', '2163924212', '2146979999', '1966744451', '2038946940', '2153043014', '2144084055', '2169633886', '2074054110', '2238394037', '2006538394', '2166136386', '2097037092', '1572542963', '2080965958', '2008659280', '2157434325', '1590197579', '1983684920', '1968419300', '2019942458', '2091977036', '2022276020', '49163003', '2051923321', '2007075867', '2136508742', '2034675152', '2422489187', '2167084407', '1543428583', '1597911014', '2090567012', '1990283691', '2104955175', '2163503810', '2098897082', '2158162268', '1590027972', '2127665799', '2146110276', '2104468797', '1513963612', '2122596210', '2066323815', '1504326001', '2083337192', '2104570891', '1980102475', '2295314568', '1980332598', '1706595869', '100354579', '1986394551', '1860447447', '2108284862', '2017748805', '2147945245', '2112535481', '1991762554', '2069742260', '1581920067', '1978790760', '1934687485', '2162234765', '2069061214', '1718168955', '1981861653', '1566187271', '2046541440', '2275537915', '2046220682', '1968046596', '2106457743', '1490305410', '2026321404', '2137583602', '2046681803', '2005266188', '2132239357', '2009755086', '2163072899', '2102677942', '2096192258', '2121527363', '1508377368', '1592485920', '2170926431', '1978177236', '2045719273', '1559745546', '2068998539', '1920195879', '2120858379', '2516755824', '2151140688', '2964065394', '2073201574', '2265023494', '2049040026', '2129466927', '2118030155', '2170263081', '2133975139', '2011807397', '2561022420', '1895712707', '2052314469', '2011553598', '1555998083', '2002984289', '2012393511', '1741857846', '2123332983', '2115033790', '2125293660', '2079781151', '1995789370', '2003079265', '1523429323', '2146479324', '2028102170', '2057738925', '2111089200', '2134883458', '2145667256', '2054992363', '2039245308', '2170063786', '1589799320', '2136064096', '1824779901', '2123753155', '1530737432', '2098388535', '36294969', '2165245616', '2172036892', '2533710649', '2131761689', '1434250300', '1995276467', '2057428300', '2345094194', '1981983914', '2142888583', '1791490601', '2030176049', '1996390648', '2166237242', '2011262634', '2108383324', '1524535160', '2470330948', '2089960899', '2192922110', '2536139902', '2124275795', '2006770991', '2169652527', '1576009348', '1917816613', '1992980513', '1998129840', '2167675430', '2034491860', '2152550510', '2140790038', '1582586369', '1991222255', '2298701798', '1984032440', '2144713862', '2124753336', '2142408382', '1968398861', '1812472129', '2000703513', '2166797693', '2070969654', '2159925876', '2117993414', '2541471791', '2004830430', '2162736818', '2003858506', '2122633620', '2059859034', '1502745347', '2118813501', '2071491781', '2070238134', '1976103427', '2108194802', '1518569777', '2124003876', '2148833650', '2088554465', '2115738130', '1999519891', '2105077725', '2156944801', '2145249718', '2026922808', '1953407304', '1920086316', '2266861968', '2102210101', '1593853211', '2106823291', '2101681007', '1512631419', '2001820056', '2117359296', '132635647', '2040906549', '2104989625', '2551703383', '1566443239', '1997016884', '2236778129', '2107340437', '1601801318', '2082715409', '2101521577', '1702602994', '2152163988', '2122804513', '1572554769', '1515782789', '2074461797', '2126970920', '2036751794', '1990673109', '2301644594', '2028052285', '2171077162', '2123603012', '1523659371', '2153505803', '2297280924', '127660652', '1795749589', '2133038912', '2006424233', '2059785325', '2165369325', '2041499406', '2137942755', '2112288324', '2154087004', '2105673878', '2114102725', '2027132450', '2150910590', '2036368355', '2164591799', '1538603023', '2159267971', '1508073509', '2049914146', '2146212261', '2106445544', '2096691402', '1580060675', '2015849952', '2142569879', '1519522840', '2003184114', '2147486330', '2157542101', '2038661910', '2071373293', '2133287551', '1923558290', '2069616387', '244798489', '2515403201', '2516190737', '2149287269', '983661985', '1595520978', '2136550126', '1544865128', '2285289280', '2031267557', '1981934471', '2000635218', '2100215938', '1503377747', '2151528071', '2128800581', '1519241557', '2159477781', '2121535117', '2098445492', '2111345321', '1984759619', '2015573373', '25914417', '1974778049', '2316944287', '2111358286', '2044680988', '1967682537', '2126017179', '2141813464', '1985501499', '1862908038', '1530830104', '1933463308', '1533802813', '1998282176', '2115736701', '1565890150', '2112130937', '2170787246', '2099088270', '2112901334', '2098695001', '2120424620', '2095604527', '2114379500', '1539386741', '2159155927', '2133070463', '1911351655', '2117893011', '2020385484', '1967650098', '2199908997', '96334789', '2036149711', '2110515501', '2057918257', '2084218978', '2054616818', '2070071150', '2106110374', '2014058317', '1516539966', '2086417013', '2088707245', '2095968873', '2160293370', '2007750008', '2154211836', '1994765202', '1980295161', '1555992831', '2159502462', '2112440566', '2399943230', '2129077599', '2160349063', '2159846129', '5951450', '2114984213', '1970177271', '2072825191', '1789893203', '2080294529', '2008824810', '2164370907', '2139797461', '2201450733', '2137987693', '1984442146', '2135261995', '1967049102', '2052337418', '2029388346', '2563113203', '1564816718', '2019038973', '2076311404', '2118424305', '1983286157', '2132182116', '2156553643', '2049665561', '2098434499', '2128068291', '3003762976', '1988944493', '2158611845', '2272961302', '1534956778', '2166782176', '2151346669', '2132504062', '2165670403', '2150855917', '2964279294', '2156668452', '2168770386', '2117471590', '2121355928', '2056323750', '2109369909', '2112941156', '1599608098', '2951790123', '2430985532', '2123705801', '2040684818', '2157785478', '2222046635', '1526554180', '2123696099', '1536696877', '2163184528', '1558682822', '2047374538', '2292985786', '2178364837', '1538441114', '1970733706', '2170080756', '2075808993', '2135219634', '109646171', '2123397151', '1883424227', '2279903970', '2135166380', '1739238255', '2133536583', '2116311939', '2141394106', '2140380216', '2950127004', '2065029123', '2117359632', '2346009597', '2018063634', '645937136', '39012485', '2164668812', '2520175491', '2133607396', '1975451203', '193978402', '2114541000', '2126059761', '2052994194', '2049662238', '2310840552', '1496756735', '1983614732', '2026956576', '2156472368', '2106854722', '1515318238', '2105480835', '2015113578', '2131164291', '2135742489', '2100612304', '2116107576', '2137150110', '2056974778', '1965647175', '2113098059', '2123340003', '2020744268', '2023190175', '1891430219', '2042736421', '1418118782', '2289193901', '2108994503', '2155333555', '1974252999', '2097712740', '2012278347', '1993318927', '2048308467', '2145469294', '2145317508', '1595208394', '2132986980', '2544658323', '1644654652', '1995476316', '2165689303', '2111561140', '2020672656', '2156029527', '2073483710', '75969564', '2106821529', '2545303763', '273730232', '1544756831', '2138744638', '2020105342', '2112668583', '2168405077', '1996053282', '2241331382', '2048989081', '177152208', '2137536023', '2057762014', '2150077240', '1942157047', '2119300492', '2509339136', '2012984331', '2290933052', '2554004740', '1802393745', '1991119341', '2098031676', '2155656413', '2162658373', '2093811529', '2208872871', '2013384123', '2830665168', '1910463596', '2273465587', '2107329027', '2156394342', '2134535985', '2150251427', '2067437753', '2149508605', '2131687331', '1991160686', '2009391653', '2297158628', '30111494', '1986909126', '2089422208', '2112439824', '2133405232', '2039087548', '115340427', '1982564878', '2152446170', '2152627585', '2123604997', '1981643246', '1966270352', '1613088533', '2104195831', '2039314133', '2017106121', '646347605', '1975872381', '2167138972', '1996859282', '2094732917', '1545730124', '2018168654', '2157803027', '2072249062', '2153796759', '2028931685', '2151399378', '2105543955', '1557553419', '2297887417', '2124624510', '1980604992', '100926390', '2014947976', '2101337671', '2025631196', '2116015679', '2175511746', '2027733517', '1561553263', '2110772037', '2100649311', '1907040713', '2063463795', '2143643573', '2099901267', '2115362365', '1965225666', '2118896504', '2015231140', '2102135724', '2123710852', '2002463513', '1968886330', '2291366602', '1982603963', '2076315487', '2146081694', '1499554562', '2146616212', '2166524963', '2118429180', '2139397246', '2161543941', '1505013815', '2034592035', '2006896079', '1797412981', '2141738320', '2144412323', '2020658029', '2155280140', '2140374581', '1885135707', '2108237641', '2105281417', '878963339', '1993043418', '2299080771', '1964514440', '2024171726', '1534844487', '2072056990', '2228911404', '2215578291', '2297908611', '2289460548', '2133496555', '2331818205', '2208601672', '2088696199', '2166038957', '2065079259', '163185988', '1971461697', '2029006047', '2101538324', '2098787458', '2147538652', '2032486439', '1981544765', '2099085697', '1590610367', '2093010905', '2168321830', '2152248405', '1494504210', '1986923963', '2114946303', '2099475405', '2192098495', '2033659811', '629580309', '2069381111', '1989434420', '2059831222', '2112876834', '1992015378', '2148794117', '2008468702', '2048361914', '2952445676', '2130418717', '2139703775', '2165704188', '1988889787', '2073748320', '2161182793', '2296619050', '2107544712', '2153594987', '2096842206', '2050248496', '1967342542', '2064820528', '2171256495', '2133174435', '2538906852', '2016990092', '22988422', '2137882973', '2097431885', '2163109264', '2056588157', '2124042675', '2012197287', '2079794173', '1791522544', '1976479381', '1978510195', '2048772149', '1515373455', '2171981221', '1538627613', '2109448614', '2096382287', '2112909740', '2147883902', '230063650', '2076870888', '2048522717', '2139168129', '2098535896', '2142373055', '2055671064', '2168731346', '2026227838', '2116871897', '2128453677', '2159620024', '2400383421', '2069131139', '2169148366', '2153050584', '2046100827', '1311823', '1598986820', '1567460349', '2461026323', '2246489563', '2400334166', '2166188160', '1968399385', '2105177289', '1521502694', '2056445701', '1984572758', '2143670637', '2164358871', '2519171190', '2070577557', '2156687020', '1586635469', '2056172561', '2260825363', '1574332324', '2119014993', '2156108675', '1999909540', '2293823404', '2141895717', '1547030730', '2145897363', '2074229485', '2048898391', '2106030490', '2163807940', '2538833168', '2138364970', '2111197877', '2598106551', '2123789131', '2134409573', '2073666268', '1573308205', '2077536557', '2138255015', '2413780294', '2126363032', '2002085059', '2045295971', '2115303007', '1991876273', '2038915176', '2124235489', '1994223498', '2538232383', '1480107443', '1986804610', '116352982', '2068076641', '2059687142', '2156215770', '1458087722', '599919443', '2132742292', '1977913511', '2168097476', '2171800037', '2162058299', '2082394695', '2099141642', '2123327750', '1548945257', '2037970800', '2004653034', '2149104457', '1547642283', '2532432905', '1519249771', '2011847167', '2120135289', '2543719161', '2168023956', '2012942478', '1988543519', '182168903', '1531708315', '2134746901', '2027666260', '103173201', '2041465434', '2041942261', '2544993392', '2150336603', '2129716255', '2012160269', '1964895037', '2131421091', '2013874902', '2125814367', '231597783', '2004228092', '2129461460', '2098250124', '1484581076', '2066419634', '92809975', '2016946736', '2001874077', '1989086881', '2097328047', '2112837583', '2027756802', '1656602484', '2109704280', '2046226791', '2099887998', '2127379504', '2162359940', '2090636507', '1981434751', '2168587447', '2038976388', '1988417571', '1980148603', '2051693296', '2154943073', '2111184916', '2003817535', '2038080768', '2022795107', '2147395447', '1514749958', '2160068848', '2182062983', '2159377219', '2160516790', '1570279686', '2134777991', '2138766828', '1603426217', '2078010006', '1522060758', '2041038525', '2127977131', '2174740429', '2035665425', '2100453522', '2195769232', '1549159279', '2110849801', '1982046033', '2118717978', '1497107555', '2160361162', '2105632634', '2067611679', '2019200506', '2169944694', '2149925786', '2016071318', '1971218162', '2399445527', '2137125234', '1484344239', '69028669', '2293241517', '2002198951', '1996058298', '2950746675', '2123692503', '2060375160', '2169604715', '1988021142', '2059901592', '2002052447', '2114354198', '1526183512', '2026848732', '2231866525', '2014477168', '2118909312', '2022966479', '2023459132', '2095710116', '2289791692', '2079511563', '1977585742', '1980070402', '2139548351', '1867869925', '1850328252', '2166595847', '2117657309', '2141339812', '2080549745', '2101979455', '2511968983', '1776031812', '2095877777', '2148532403', '2124323785', '2073784988', '1664884371', '2045045212', '1888227529', '2170635816', '963558256', '2103252706', '2158530766', '2038971946', '1558574741', '2135357652', '2076938044', '2133535310', '2951752361', '2068170133', '2037230040', '2035889552', '1708204186', '2054248624', '2163198662', '2161047963', '2282604327', '1592161118', '2532112296', '2261668817', '2172115793', '2133383589', '2408602973', '2158238466', '2169823756', '2404491697', '2143672232', '2014394432', '2617196863', '2064368512', '2127634296', '2023124697', '1997017704', '2121003573', '2128491806', '2159982924', '2131200328', '1989838848', '2015557948', '2050480609', '1819129872', '2105322847', '2281874973', '2170421477', '2535560756', '1533463924', '2129662785', '2271453489', '2099524725', '2133102514', '2141112830', '311789848', '2027513585', '2034568545', '2161327631', '1567154898', '2084750447', '2076568261', '1847203455', '2121964838', '2059534106', '2311899279', '2169825914', '2164785268', '2041538588', '2135572229', '2343284207', '2047426952', '2081150257', '2533652003', '1503239049', '1930481407', '2125514430', '2133762042', '2139155418', '2241210085', '2081553695', '1530420645', '2547720120', '2049192067', '2122725466', '1849344587'], 'bm25_doc_scores': [35.124653, 31.077896, 29.529682, 29.529682, 29.475824, 29.380291, 28.347794, 27.32841, 27.182415, 26.978436, 25.825005, 25.712448, 25.525225, 25.499018, 25.42003, 25.104918, 24.680386, 24.674982, 24.61616, 24.320648, 24.309162, 24.29086, 24.268957, 24.125065, 23.996304, 23.980698, 23.944206, 23.860365, 23.809027, 23.784786, 23.783669, 23.72049, 23.680191, 23.430017, 23.401741, 23.270609, 23.155363, 23.151657, 23.128662, 23.076334, 23.031546, 22.928946, 22.84602, 22.797691, 22.789104, 22.74897, 22.746325, 22.723345, 22.64595, 22.625822, 22.568691, 22.56068, 22.543337, 22.50414, 22.495583, 22.41196, 22.379412, 22.350922, 22.251768, 22.218864, 22.165524, 22.14685, 22.143604, 22.127016, 22.094759, 22.05076, 22.038677, 21.988129, 21.970642, 21.950832, 21.950298, 21.934498, 21.927624, 21.903358, 21.89991, 21.887165, 21.85986, 21.832977, 21.790226, 21.78462, 21.75626, 21.744741, 21.742844, 21.735346, 21.734882, 21.7294, 21.702543, 21.690031, 21.675087, 21.658512, 21.654768, 21.627855, 21.61772, 21.612831, 21.59883, 21.568956, 21.568235, 21.543112, 21.518616, 21.466421, 21.421513, 21.396873, 21.369308, 21.359604, 21.352228, 21.31105, 21.30996, 21.304363, 21.299917, 21.284466, 21.24019, 21.23874, 21.22852, 21.205864, 21.20073, 21.175022, 21.1277, 21.119251, 21.079878, 21.065964, 21.056664, 21.035103, 21.020788, 21.006977, 21.006382, 20.93387, 20.922626, 20.831974, 20.79773, 20.79675, 20.78703, 20.7825, 20.771465, 20.759602, 20.746273, 20.735027, 20.711681, 20.709814, 20.703331, 20.697926, 20.69729, 20.684395, 20.669498, 20.651846, 20.638248, 20.622828, 20.620867, 20.610975, 20.589855, 20.566086, 20.557198, 20.550987, 20.548964, 20.531637, 20.513573, 20.487793, 20.4631, 20.43533, 20.358967, 20.34533, 20.334927, 20.33369, 20.33251, 20.329596, 20.305536, 20.303225, 20.274899, 20.27266, 20.269785, 20.266148, 20.259293, 20.257446, 20.25559, 20.25045, 20.24944, 20.244774, 20.243557, 20.242823, 20.222855, 20.21872, 20.198725, 20.19824, 20.196045, 20.19442, 20.184538, 20.181429, 20.163557, 20.154102, 20.152082, 20.114016, 20.112686, 20.111073, 20.108206, 20.107082, 20.097115, 20.081291, 20.06482, 20.06093, 20.054459, 20.053474, 20.046236, 20.024942, 20.015884, 19.997978, 19.992863, 19.99043, 19.98924, 19.973408, 19.96772, 19.96204, 19.929039, 19.925508, 19.921034, 19.900574, 19.875858, 19.87476, 19.871824, 19.858042, 19.856167, 19.854721, 19.84203, 19.833372, 19.826683, 19.822138, 19.814787, 19.805437, 19.782812, 19.774904, 19.7747, 19.751068, 19.750286, 19.73255, 19.718037, 19.70778, 19.70778, 19.693392, 19.690825, 19.67523, 19.674427, 19.672527, 19.668144, 19.666214, 19.647087, 19.639292, 19.636719, 19.629469, 19.619305, 19.618874, 19.618874, 19.616476, 19.611338, 19.60938, 19.606997, 19.590252, 19.581326, 19.574781, 19.574383, 19.571903, 19.571735, 19.568892, 19.557816, 19.554634, 19.553547, 19.537119, 19.533848, 19.53015, 19.52881, 19.528526, 19.516685, 19.514153, 19.509453, 19.507282, 19.497038, 19.495323, 19.48674, 19.46482, 19.46114, 19.444004, 19.439848, 19.427568, 19.399708, 19.372639, 19.362574, 19.362211, 19.360449, 19.3489, 19.336952, 19.334896, 19.317905, 19.317518, 19.314922, 19.31334, 19.311558, 19.28873, 19.286938, 19.27412, 19.269337, 19.262043, 19.261515, 19.252846, 19.244333, 19.240017, 19.238764, 19.2315, 19.231026, 19.220943, 19.197838, 19.191673, 19.168356, 19.152073, 19.147831, 19.144297, 19.129734, 19.118189, 19.113337, 19.111156, 19.11015, 19.083641, 19.083395, 19.082151, 19.07853, 19.07726, 19.075615, 19.071072, 19.06462, 19.059147, 19.055498, 19.049915, 19.045372, 19.042688, 19.035843, 19.029863, 19.027184, 19.027184, 19.025412, 19.024094, 19.017998, 19.015842, 19.010386, 19.010126, 19.00669, 18.982658, 18.978498, 18.969904, 18.96952, 18.951992, 18.948807, 18.947433, 18.94729, 18.946732, 18.935383, 18.92677, 18.918, 18.918, 18.913393, 18.913116, 18.90816, 18.906406, 18.901232, 18.898, 18.896206, 18.895983, 18.89202, 18.885096, 18.880493, 18.872528, 18.866814, 18.862919, 18.861452, 18.852057, 18.849354, 18.846502, 18.838709, 18.834032, 18.832886, 18.830116, 18.828718, 18.821104, 18.752546, 18.747194, 18.746996, 18.743206, 18.740587, 18.739967, 18.738432, 18.736717, 18.730183, 18.721558, 18.717934, 18.715897, 18.715574, 18.714684, 18.710869, 18.71016, 18.703773, 18.702557, 18.702421, 18.695658, 18.693935, 18.689991, 18.678865, 18.667776, 18.665985, 18.66516, 18.663979, 18.66122, 18.66021, 18.657995, 18.65728, 18.65578, 18.649687, 18.640467, 18.633703, 18.632984, 18.631903, 18.623522, 18.619303, 18.61041, 18.59744, 18.585901, 18.573503, 18.570354, 18.562613, 18.561264, 18.56077, 18.55658, 18.55122, 18.54638, 18.544363, 18.542114, 18.541395, 18.538895, 18.53513, 18.534018, 18.533863, 18.533794, 18.533794, 18.530392, 18.5269, 18.524946, 18.523708, 18.515652, 18.510956, 18.507425, 18.501467, 18.496668, 18.494408, 18.493477, 18.491177, 18.490885, 18.487776, 18.482859, 18.479027, 18.475897, 18.44495, 18.431286, 18.421688, 18.421211, 18.414312, 18.406015, 18.401978, 18.400312, 18.400015, 18.395683, 18.390453, 18.379429, 18.378532, 18.374447, 18.368847, 18.356737, 18.354782, 18.347578, 18.346878, 18.346878, 18.344955, 18.34082, 18.339504, 18.337791, 18.337116, 18.335934, 18.33053, 18.323418, 18.309679, 18.30353, 18.303259, 18.302242, 18.29581, 18.29581, 18.295132, 18.295132, 18.294777, 18.29197, 18.288927, 18.282722, 18.276976, 18.274155, 18.255552, 18.251305, 18.250208, 18.246399, 18.240932, 18.236916, 18.228394, 18.22699, 18.224592, 18.22356, 18.220398, 18.204342, 18.203123, 18.199993, 18.19874, 18.192682, 18.190018, 18.189802, 18.186775, 18.185305, 18.178236, 18.176891, 18.174782, 18.173561, 18.167425, 18.158573, 18.156895, 18.1557, 18.15345, 18.152311, 18.145178, 18.143995, 18.14394, 18.138361, 18.132574, 18.126642, 18.115208, 18.112848, 18.112253, 18.106579, 18.104832, 18.104654, 18.104477, 18.09649, 18.095774, 18.092354, 18.091846, 18.083227, 18.080414, 18.074942, 18.060434, 18.052305, 18.051456, 18.048977, 18.047115, 18.039597, 18.035847, 18.035046, 18.02533, 18.024612, 18.021244, 18.020777, 18.020267, 18.018581, 18.018442, 18.015535, 18.011805, 18.010273, 18.007772, 18.00239, 17.998861, 17.998783, 17.99472, 17.991978, 17.985176, 17.982101, 17.969707, 17.969095, 17.960617, 17.957085, 17.950523, 17.943232, 17.94014, 17.939915, 17.923393, 17.922005, 17.919683, 17.917074, 17.916435, 17.914047, 17.911964, 17.904314, 17.902649, 17.88907, 17.888004, 17.887064, 17.88363, 17.88258, 17.877636, 17.875025, 17.870007, 17.867893, 17.855732, 17.853445, 17.853104, 17.852125, 17.850128, 17.846085, 17.843925, 17.841053, 17.830755, 17.829899, 17.824432, 17.823442, 17.821878, 17.819248, 17.81739, 17.81424, 17.813307, 17.811388, 17.802784, 17.796795, 17.795864, 17.793674, 17.790682, 17.784447, 17.783777, 17.783468, 17.78058, 17.778305, 17.77557, 17.773083, 17.772295, 17.766678, 17.7651, 17.760492, 17.759884, 17.75894, 17.756794, 17.7567, 17.755402, 17.75384, 17.74753, 17.745653, 17.74517, 17.744295, 17.738836, 17.735252, 17.734556, 17.725758, 17.724495, 17.7205, 17.71972, 17.719303, 17.714855, 17.693235, 17.692814, 17.68383, 17.682045, 17.679424, 17.678518, 17.665758, 17.664103, 17.663687, 17.663647, 17.661934, 17.661678, 17.659079, 17.65903, 17.649708, 17.64932, 17.644667, 17.643465, 17.640266, 17.638344, 17.635576, 17.632866, 17.631536, 17.6286, 17.620987, 17.619911, 17.619911, 17.618368, 17.614319, 17.613796, 17.61302, 17.606308, 17.605772, 17.604866, 17.599112, 17.59898, 17.592997, 17.592693, 17.591349, 17.59079, 17.59079, 17.587637, 17.58522, 17.576677, 17.569191, 17.568586, 17.564785, 17.56327, 17.561527, 17.560467, 17.560228, 17.55918, 17.548498, 17.544634, 17.541859, 17.541067, 17.539076, 17.535984, 17.531565, 17.530077, 17.529364, 17.524607, 17.522057, 17.519413, 17.519115, 17.519115, 17.519115, 17.515247, 17.513252, 17.511246, 17.510578, 17.500462, 17.497093, 17.495655, 17.494345, 17.493896, 17.490849, 17.485863, 17.483292, 17.479538, 17.47768, 17.475105, 17.471954, 17.471704, 17.469355, 17.465778, 17.465351, 17.464233, 17.457396, 17.45634, 17.4562, 17.451664, 17.45124, 17.448997, 17.447573, 17.445448, 17.445448, 17.44127, 17.439922, 17.437004, 17.432848, 17.431883, 17.428944, 17.425789, 17.425045, 17.423622, 17.422636, 17.422031, 17.41944, 17.413353, 17.413311, 17.412691, 17.408867, 17.407616, 17.405558, 17.40546, 17.404938, 17.40364, 17.403137, 17.40301, 17.400372, 17.395569, 17.395388, 17.394974, 17.389969, 17.387197, 17.38638, 17.38364, 17.383482, 17.377796, 17.372614, 17.37258, 17.372425, 17.367634, 17.364864, 17.35706, 17.356663, 17.355204, 17.355152, 17.353085, 17.344364, 17.343714, 17.341724, 17.340912, 17.340118, 17.33897, 17.337515, 17.32838, 17.326244, 17.325533, 17.324804, 17.323666, 17.32219, 17.320658, 17.320658, 17.319038, 17.31816, 17.317272, 17.315903, 17.30615, 17.304699, 17.299059, 17.290735, 17.287195, 17.281065, 17.278873, 17.27125, 17.269857, 17.268589, 17.266478, 17.266064, 17.265968, 17.259113, 17.25903, 17.254505, 17.245756, 17.242891, 17.241484, 17.238983, 17.238924, 17.229418, 17.22524, 17.222559, 17.221264, 17.219212, 17.216461, 17.21622, 17.216, 17.208101, 17.205698, 17.20423, 17.199745, 17.19897, 17.19613, 17.193668, 17.193111, 17.192513, 17.188002, 17.187721, 17.182852, 17.18269, 17.182196, 17.179096, 17.17868, 17.178106, 17.176111, 17.176027, 17.17021, 17.163424, 17.158285, 17.15808, 17.148123, 17.146528, 17.144659, 17.13501, 17.133928, 17.131298, 17.128437, 17.12714, 17.126936, 17.125278, 17.125278, 17.12029, 17.117847, 17.116146, 17.11427, 17.113346, 17.111729, 17.110645, 17.110645, 17.10896, 17.102852, 17.10216, 17.099289, 17.098362, 17.093918, 17.093431, 17.088936, 17.08889, 17.086674, 17.084185, 17.075693, 17.074966, 17.07483, 17.074146, 17.073435, 17.069279, 17.068768, 17.066484, 17.06499, 17.064466, 17.063993, 17.063185, 17.056385, 17.055504, 17.052952, 17.049591, 17.048065, 17.04704, 17.045944, 17.04496, 17.04418, 17.043442, 17.042526, 17.039291, 17.035263, 17.02987, 17.028221, 17.026844, 17.02657, 17.025236, 17.022484, 17.01817, 17.016703, 17.013945, 17.013721, 17.01202, 17.011768, 17.011335, 17.008493, 17.008226, 17.004812, 17.003603, 17.001991, 17.00196, 16.997822, 16.996511, 16.994167, 16.99308, 16.989956, 16.98863, 16.98808, 16.986683, 16.986446, 16.983637, 16.976334, 16.973587, 16.970306, 16.969887, 16.96802, 16.968012, 16.968012, 16.967522, 16.967342, 16.9655, 16.961294, 16.960585, 16.959013, 16.957912, 16.956999, 16.95314, 16.952684, 16.95063, 16.949358, 16.948534, 16.948534, 16.94846, 16.945799, 16.945276, 16.943441, 16.942953, 16.942427, 16.941942, 16.93818, 16.935978, 16.927149, 16.926193, 16.924076, 16.92273, 16.919306, 16.91883, 16.91624, 16.908968, 16.90643, 16.903507, 16.90333, 16.89753, 16.897509, 16.896893, 16.89607, 16.895489, 16.895216, 16.894653, 16.890865, 16.890596, 16.890438, 16.889038, 16.887712, 16.885628]}, {'id': '2555373976', 'text': 'open set face recognition across look alike face real world scenario', 'timestamp': 1483225200, 'rel_doc_ids': ['1577100866', '1776363246', '1956585817', '1965947362', '1967373529', '2006396739', '2011607031', '2033541862', '2039375240', '2105087447', '2119880843', '2133670146', '2138451337', '2152809841', '2536626143'], 'user_id': '61618545', 'user_doc_ids': ['58929444', '82598671', '1473192048', '1497534704', '1500408360', '1503828248', '1504134405', '1507769123', '1507888107', '1527562769', '1528516056', '1528518717', '1528908694', '1533942381', '1544678220', '1551916531', '1579984105', '1592129581', '1605332893', '1635632429', '1673359695', '1858102912', '1864708655', '1884234530', '1929331526', '1945982597', '1963570066', '1964498284', '1966941496', '1968982945', '1974221832', '1974395509', '1974693384', '1976124599', '1976564561', '1982877839', '1983363140', '1984734418', '1986769241', '1988781663', '1994739964', '1997012944', '1997659823', '1998120218', '1999021990', '1999988714', '2000262867', '2002169586', '2005392205', '2009868947', '2010693663', '2011607031', '2013711562', '2024763507', '2026126066', '2026473462', '2027689907', '2029248954', '2029353064', '2031306733', '2033338055', '2039055640', '2040710789', '2041733204', '2043364632', '2045923079', '2046405224', '2047482723', '2050503367', '2053298891', '2055478513', '2059995677', '2060232182', '2062396676', '2063861457', '2067624260', '2068227252', '2068416669', '2069112744', '2069118826', '2072573901', '2073913389', '2075957123', '2076046792', '2078622678', '2081738556', '2081912576', '2082439502', '2084167441', '2089494279', '2092296498', '2092354204', '2093074210', '2094738591', '2097235937', '2098355686', '2098821742', '2099150311', '2099552132', '2099606834', '2099749374', '2100526724', '2101053275', '2101347091', '2101805981', '2102377326', '2103509521', '2105415296', '2105518985', '2106028284', '2106361251', '2108215143', '2108736209', '2109754575', '2110093903', '2111604700', '2111882579', '2114308057', '2115907213', '2120592335', '2121577350', '2123922242', '2124053768', '2127631308', '2128012883', '2128018244', '2128429523', '2129087381', '2129094455', '2130798017', '2131600948', '2132889208', '2133358423', '2133382819', '2133386318', '2133498768', '2133791320', '2136197270', '2139665129', '2141665674', '2141804661', '2141945738', '2142775019', '2146924478', '2146972730', '2150631128', '2150987624', '2151726375', '2152187996', '2152712847', '2152809841', '2156976257', '2157046046', '2158533290', '2159455821', '2161437536', '2162854067', '2163591635', '2163626290', '2164304158', '2167114667', '2167595954', '2168237023', '2168258828', '2171281408', '2183139612', '2187566052', '2189287757', '2276141223', '2302450723', '2520583710', '2542179127', '2542424719', '2542917346', '2544037454', '2544192328', '2556400795'], 'bm25_doc_ids': ['2493297061', '2133670146', '1748998702', '2140398892', '1913277147', '2148191008', '1964479141', '2546175596', '2042455579', '2057925440', '170475459', '2057928000', '2119880843', '1998912926', '1906634369', '2039649878', '2088610507', '1968112912', '2086435109', '2104482786', '2114627449', '2288632406', '2152613897', '2025217965', '2138448014', '2147030833', '2238285753', '2158213899', '2101345629', '2127182692', '2079382814', '2142088390', '1971392085', '2117527396', '2006396739', '2172432503', '1838748418', '2084481049', '2149494055', '2149544470', '2075604006', '2165824859', '2057640246', '2093242091', '2949649846', '1505701726', '2519810307', '2039076295', '1965325717', '2012576934', '2172005357', '2072065288', '1978642336', '1907941930', '1577100866', '2071702269', '1550113892', '1964857432', '91273915', '2099474347', '2021242760', '2144652355', '1596456000', '2130251609', '2124908835', '2105087447', '2124507903', '2058810853', '2099593527', '2052619454', '2125399223', '1890427773', '2123335596', '1843795321', '2083539276', '2073680184', '2013701006', '1547009501', '1963589611', '2085009525', '107491560', '2007216012', '2162813111', '2078118580', '1902324625', '1997883496', '2146577386', '2127846810', '2056451974', '2206609956', '1955901102', '2472456744', '2156138629', '2340646384', '2133295669', '2008932806', '2045506926', '2046876937', '1964414683', '2147514478', '2095990746', '2562999681', '1991785450', '2113278356', '2164416618', '2019305198', '1900273225', '2402106151', '2139312349', '2052152054', '2008379856', '2012885984', '1973258879', '2001955724', '2039375240', '2141020570', '2128136167', '182571476', '2093922090', '2091727050', '2165052637', '1590130351', '1964435748', '1591091792', '2096589080', '1542457947', '2097017378', '2085819332', '2031909142', '2564929978', '1638952162', '2148222137', '2123638790', '1965272758', '1580457994', '2159579259', '2110976476', '1956585817', '2090767496', '2078791203', '2217596628', '1585376122', '2156968904', '1984926403', '1982048725', '2100205487', '1940264722', '2038789605', '2064522660', '1982701800', '2204593980', '2058904159', '2015263715', '2020355391', '201601955', '2295331046', '2541147637', '198644033', '1786203917', '2072463920', '1550223312', '2542207172', '1865311291', '2136967535', '2148795769', '2510798689', '2106852298', '2162036363', '66412119', '2018375865', '2082020538', '1481078355', '1521514493', '1491333712', '2055492845', '1540804449', '1528857385', '2168056867', '2338375948', '2042192427', '1987814270', '2487114723', '2003719038', '2171728358', '2541839572', '1964276051', '2158760744', '2123052420', '1967373529', '2486769752', '2054350873', '1983124496', '2067175278', '1965947362', '2607679123', '2110847826', '2597378264', '1721634477', '1596088243', '2165969490', '1967911865', '2115115158', '2339155617', '1499437193', '2137383239', '2053023252', '2084658294', '2064592361', '2113995354', '2035838298', '2132716725', '2166672191', '2003867509', '2166020391', '1968281190', '2003294605', '2109081800', '2147218432', '1989832314', '2066390622', '2511484725', '2949125164', '1988521056', '2026870658', '1968178768', '2432917172', '1543834821', '2027411003', '1949778830', '2097826102', '26748423', '1997195803', '2096845734', '2153003143', '1970496932', '46222836', '2524426358', '2025555808', '2047799632', '2527100376', '2113972997', '2897013524', '2332698925', '2117531622', '2477907713', '131856420', '2111650038', '1536637669', '2092131162', '2564603115', '2047329405', '2088131256', '2526639894', '1995952467', '1967174403', '2024879024', '2094279194', '2004714383', '2159369772', '1974940081', '2963306405', '2346437620', '1974402054', '2155205708', '2515770085', '1822320005', '2122738838', '2153493559', '2536626143', '1489172252', '2115318452', '2113183276', '2136863438', '2101061041', '2111233654', '2103971514', '1547537691', '1530485679', '2194747885', '2125774111', '2038724092', '2130490046', '2153382691', '2109538124', '1588805037', '2035094608', '2103971661', '2039630661', '2242857782', '2139308262', '55105203', '1461711402', '2158634666', '2059483244', '1985225657', '2098693229', '2145491404', '2157577964', '2539171194', '2115651492', '2036391489', '2121204030', '1546603013', '2102742445', '2150316953', '2041180319', '2073609604', '2152664485', '1990485410', '1966572524', '2533997725', '1579403273', '1856599132', '2135016226', '2098048262', '1597765037', '1776363246', '2106615635', '2397908863', '2146972986', '1036701977', '2095337820', '2423215181', '69471719', '2158995360', '1980257741', '2009961107', '2067921274', '1800461853', '2086287546', '2088164725', '2083332259', '1574810029', '2036971009', '2152809841', '1998088305', '2148109568', '2292033530', '2112884778', '2138544107', '124037294', '9976222', '2951965309', '2431335693', '2204810408', '2172013424', '2130672052', '2061255502', '1560641285', '2288403115', '1964266152', '2016676076', '2103107894', '1483425744', '1987725919', '2062343332', '2025381353', '2074051524', '2138806193', '1029607174', '2198234001', '1611134254', '167506040', '1927859382', '2092777354', '2108287000', '2146183452', '2547368018', '2153963799', '1622480153', '2116217298', '2096027770', '2044291182', '2399047043', '2169639817', '2519280274', '1989720346', '2109002458', '1637193363', '1487288055', '2022581176', '2039239356', '1569448606', '2081733395', '1963734899', '2135134446', '1984495421', '2062970203', '2015696762', '1498175716', '1982617714', '2015956411', '2343784477', '2567154071', '2095372183', '1546581557', '2507663930', '2111101581', '2149342259', '2100965694', '2093862447', '78644976', '2142607361', '2147953604', '2014305381', '2090490521', '2088004396', '2136858165', '2127226551', '2115611150', '2033541862', '2091780211', '2071273112', '2067425370', '2394561649', '234392334', '2310064242', '1473192048', '1593502537', '2074584451', '2079834907', '2014593794', '2143343993', '1481122358', '2091409396', '72529729', '2107037917', '2132907922', '2111420304', '2113382191', '2038960506', '2095516154', '2147886538', '2150540318', '2139795045', '2165094896', '2011607031', '2128095836', '2403971911', '202971408', '1498097761', '2035782554', '2081477191', '1932009080', '2170960656', '2026901210', '2109875196', '2004293399', '2171180631', '2163624601', '2138451337', '1788074364', '2057532222', '2018041342', '1515833090', '2160126058', '2139596361', '2134658556', '2044582964', '2080599172', '2112724657', '1958263298', '2219281669', '2114496548', '1516576696', '2034754192', '2088633793', '2161364624', '2030209391', '1993221954', '1993912132', '2170093232', '1583891911', '2043881395', '2133924537', '2103434304', '1899098437', '2016959770', '1990460470', '2540061698', '1535804263', '985006072', '2072444993', '1486722370', '2164412576', '2013825418', '2162907386', '1515556857', '2401126384', '2121681222', '2137695006', '2103751387', '2130730282', '2130105873', '2072202054', '1999062972', '2130287471', '2150193645', '2017448488', '2004022907', '2083764891', '2086374358', '2308155209', '2138820335', '2148902248', '2024015443', '2109081673', '2090504921', '1578490856', '1992025196', '2345729520', '1964927455', '2078552404', '1977060302', '2088257831', '2030407699', '1638377656', '2106654508', '2166676140', '2165077673', '2194630763', '2030961016', '2036792771', '2123722851', '2096361630', '2030996084', '2140869483', '2542582278', '1976054037', '1984603334', '1703179648', '1966831001', '1979697177', '2165299356', '2076276754', '2518976115', '2073039305', '2102433751', '2010832783', '2005238576', '2311102828', '1982597160', '47061493', '2086182562', '2084836231', '2168572310', '2081409437', '2129261353', '170773222', '2123105151', '2557839538', '2114676360', '1961978947', '2143249616', '95610390', '1572134382', '2063669303', '2069255443', '2103362776', '2196423642', '2086239884', '2168590890', '2132043498', '2108301944', '2271697525', '1652038217', '1486667329', '1967886800', '1938554918', '2138128842', '1498065840', '2063934380', '2001565863', '1815820737', '2064422984', '2085295897', '2310177304', '2094777467', '2258844511', '2146536023', '2038809248', '163002431', '2069261721', '2025996296', '2091344335', '2089127347', '1964698420', '1964650584', '2213118230', '2030754587', '1964896542', '2085192601', '2021427222', '2069247213', '2052833528', '2180782754', '1822699426', '1970810294', '2409181299', '2084507520', '1574727792', '2085894212', '2016696463', '2166939233', '1982850300', '2101055828', '2059179319', '2080389849', '2136584940', '2130204556', '1504520402', '2133131612', '2086208164', '1539708329', '2120526922', '2016822163', '2163283460', '2120552947', '1836423051', '2462094539', '2129846893', '2153597356', '1971217006', '2157566779', '2043647554', '2134302492', '2171937595', '2024898011', '2171373992', '2007294525', '1540402706', '130050112', '2058625389', '2082149009', '1969014310', '2053955554', '2144185008', '1995294744', '2019756214', '2544874028', '2169364766', '1555292308', '2167573476', '2021326137', '2085833357', '2010846950', '2008617188', '2000762676', '2025530562', '2535873859', '2007448134', '1595275800', '2149848470', '2066930008', '1988957579', '2151028858', '2404165402', '2490067726', '2011722598', '2059333881', '2155459432', '2008963215', '2300234500', '1983187788', '1994816184', '2076069872', '2131126699', '102030050', '2139111685', '2002996957', '2031365909', '1559788806', '1865963168', '2026970761', '2131041329', '2522438482', '1595582695', '2505734102', '2037577209', '1514652474', '2134200822', '2090792197', '2169249535', '2157369330', '2141583411', '2953162394', '1989105818', '2043935304', '2116631921', '2171224798', '2058969888', '2072353685', '2156962893', '1553425718', '2043382000', '2118135289', '1580172038', '2147068373', '1979937295', '1833977268', '2059482260', '2119879986', '2249960609', '72283609', '2080436193', '2015504626', '2015233651', '2078481487', '2105424570', '2046987084', '1980133717', '1915450305', '2148064792', '2008045580', '994709553', '2099846504', '2294432969', '1966478673', '2156395823', '1605644108', '2139420873', '2170401563', '2089047518', '2247447165', '2323343643', '2124026597', '2160958403', '1537615589', '167694651', '2143852144', '2043649191', '1558778037', '2146474141', '2074916159', '2141180661', '2147467057', '1983277918', '2069383457', '2155608052', '1966993003', '2046746202', '1856788541', '2055219403', '2139293878', '2135369243', '2023301637', '1535024376', '2113830917', '2093427549', '1532830306', '2079315848', '2032884192', '1557146082', '2141201071', '2058597448', '2292319643', '1990350265', '2272013280', '2078277829', '2586061342', '2047186200', '2080218616', '1992906935', '2163174014', '2165075178', '2086299392', '2013908419', '2115043626', '2130951230', '1525044501', '2101581710', '2105370109', '2158024471', '1979758679', '1589486160', '2418633638', '2161491024', '2125753548', '2130639253', '2156648957', '2276144258', '2005286252', '2256010897', '2350871683', '2090138620', '2138808949', '1992439871', '1720122913', '2182244022', '2297862116', '1581809140', '168394768', '2164478291', '2243275610', '2066478935', '2146600109', '1966510239', '2038133863', '1990794475', '2059697919', '2144035585', '1522293472', '2129572696', '2144940448', '2129253816', '2045993101', '2039557942', '2081065343', '2147736645', '2489852806', '2143217675', '2010586245', '1969911110', '2168835473', '2327301358', '406903680', '2120007488', '2145364037', '2037231462', '2102341966', '2097698119', '2102557841', '1977688791', '2025201850', '1515596567', '2171012909', '97884251', '1980173991', '2037280630', '2023043839', '41645050', '2083003645', '1990298245', '2120949517', '2949886390', '2335787378', '2000603780', '1770609444', '2075176641', '2037010263', '2104567995', '2156836445', '1568280016', '1586081892', '1840794357', '2020880266', '1604511944', '2108120108', '1982647723', '2030899956', '2055444136', '2147221263', '1537384088', '807781321', '2139735320', '2214725774', '1543876481', '1603281075', '2135919702', '277238768', '2059697359', '2101149304', '2003653668', '2023169041', '2109620722', '2123519651', '2250023239', '2155574029', '2087524482', '1987411730', '2134482476', '2027650395', '2161052281', '1544741183', '2113728360', '1982581265', '2075178410', '2167331744', '2009415882', '2530367476', '2137204852', '1578269334', '2163895617', '2083921404', '148112865', '2010195277', '2156095484', '1582978074', '2041844365', '1975809393', '2312795231', '2523746035', '2046128998', '2097729189', '2497089306', '2160806314', '2097718339', '2141274972', '2061602416', '2108422332', '1981905335', '2952589276', '2152903211', '2162253580', '1997664997', '1534114178', '3019161170', '2533481610', '2074961308', '2084773163', '1510147702', '2075988248', '1993783004', '1997069591', '1530235006', '2015088586', '330267082', '2415029628', '2010933577', '1964358753', '2116676158', '1578171516', '1586348243', '2461188013', '1986702797', '2101202999', '2134665359', '2172104841', '2105213866', '2111034319', '1947063050', '1987747093', '2039216248', '2292056273', '2115210416', '310478735', '2020447345', '1554891627', '1989039360', '2109869680', '2521429215', '2141566383', '1589407110', '2091054750', '2143377878', '2094760589', '2116589437', '2536132736', '2120731174', '2031007444', '1026142006', '2145857382', '2264228882', '1553464687', '2028485035', '2157038204', '1499119620', '2054781681', '1991049994', '2050682169', '2136202907', '2115554950', '1842053956', '2913038813', '91744528', '2105341738', '1986708280', '2132175002', '2094908632', '2491353998', '2044427756', '1550504403', '2077826368', '2059128510', '1992227055', '1995174545', '2126870171', '1526704294', '2094566372', '2121812409', '1916279783', '1871733231', '2092140019', '2131379061', '2038468148', '2015190972', '2138388836', '1980214239', '2038563812', '1592653156', '1937491688', '2126287157', '1567019178', '2082472986', '2280923931', '2116430322', '1524550699', '2040920656', '2048766821', '2011723974'], 'bm25_doc_scores': [41.32202, 40.068714, 39.453888, 38.301216, 37.888256, 36.196415, 35.961857, 35.462837, 35.005817, 34.70511, 34.466484, 34.38711, 34.31829, 33.92667, 33.890415, 33.81285, 33.76338, 33.69256, 33.68169, 33.564754, 33.54574, 33.47927, 33.47588, 33.391323, 33.385887, 33.365364, 33.335644, 33.17005, 33.148083, 33.115288, 33.04574, 32.915688, 32.743793, 32.560635, 32.401634, 32.336033, 32.29357, 32.262558, 32.134216, 31.898088, 31.805422, 31.778687, 31.717117, 31.695942, 31.588497, 31.572222, 31.50451, 31.492933, 31.273535, 31.24412, 31.113056, 31.105644, 31.102331, 30.971346, 30.918945, 30.916164, 30.90587, 30.842295, 30.765717, 30.70971, 30.687992, 30.64298, 30.625866, 30.5955, 30.527891, 30.524303, 30.521637, 30.451124, 30.43752, 30.428936, 30.357416, 30.343674, 30.286718, 30.224363, 30.22385, 30.197182, 30.132202, 30.083477, 30.077879, 30.028166, 30.01072, 29.975266, 29.855991, 29.81167, 29.691689, 29.68589, 29.63451, 29.573425, 29.531307, 29.461529, 29.392778, 29.369701, 29.358643, 29.329622, 29.303333, 29.280853, 29.261547, 29.231886, 29.176975, 29.165176, 29.087029, 29.082476, 29.070919, 28.972813, 28.927696, 28.914059, 28.90916, 28.896214, 28.866735, 28.853094, 28.841908, 28.77964, 28.770657, 28.713655, 28.705532, 28.691109, 28.67422, 28.666636, 28.665741, 28.662388, 28.657253, 28.655453, 28.646082, 28.6434, 28.612284, 28.577133, 28.55108, 28.55108, 28.499147, 28.488012, 28.486982, 28.480946, 28.449284, 28.416218, 28.387444, 28.367172, 28.34942, 28.338501, 28.333336, 28.32072, 28.308363, 28.296387, 28.272831, 28.252428, 28.203674, 28.158478, 28.143686, 28.139551, 28.105953, 28.080278, 28.069492, 28.058043, 28.049803, 28.037052, 27.984585, 27.979118, 27.963406, 27.909643, 27.88735, 27.798824, 27.794346, 27.783533, 27.775166, 27.771503, 27.757893, 27.756031, 27.728518, 27.72025, 27.71628, 27.699158, 27.680832, 27.671432, 27.600845, 27.5878, 27.580997, 27.564512, 27.553177, 27.546093, 27.524849, 27.521109, 27.516382, 27.499636, 27.478489, 27.472637, 27.460835, 27.452375, 27.450104, 27.426283, 27.418552, 27.414587, 27.411083, 27.406282, 27.403883, 27.386665, 27.36922, 27.351118, 27.350262, 27.345028, 27.287584, 27.257084, 27.249125, 27.248966, 27.219845, 27.202627, 27.202213, 27.190678, 27.179277, 27.177418, 27.173199, 27.170542, 27.16253, 27.156921, 27.153263, 27.152107, 27.150248, 27.13842, 27.124233, 27.108624, 27.103664, 27.086235, 27.081593, 27.057384, 27.031494, 27.027834, 27.022423, 27.010483, 27.009228, 26.996387, 26.986248, 26.976261, 26.96682, 26.964258, 26.96186, 26.96049, 26.935585, 26.926458, 26.908772, 26.898844, 26.896622, 26.894709, 26.893848, 26.888517, 26.861677, 26.851599, 26.829874, 26.827515, 26.826132, 26.825935, 26.801403, 26.788406, 26.78511, 26.782051, 26.774433, 26.76794, 26.764513, 26.74259, 26.741379, 26.719366, 26.710272, 26.708908, 26.70314, 26.70314, 26.69268, 26.684517, 26.67569, 26.656143, 26.65486, 26.652477, 26.643667, 26.639906, 26.634928, 26.632915, 26.619102, 26.616116, 26.608683, 26.591976, 26.579826, 26.573097, 26.572918, 26.569887, 26.541183, 26.538136, 26.533651, 26.5306, 26.52724, 26.526485, 26.522844, 26.511322, 26.509773, 26.507841, 26.492937, 26.471994, 26.471994, 26.471352, 26.45567, 26.432835, 26.432577, 26.427723, 26.42261, 26.413542, 26.41102, 26.408804, 26.407402, 26.397638, 26.39345, 26.385113, 26.379852, 26.377003, 26.367025, 26.36693, 26.360826, 26.360092, 26.357397, 26.35469, 26.35027, 26.344942, 26.329695, 26.31947, 26.304462, 26.30017, 26.297045, 26.289225, 26.288765, 26.280022, 26.271435, 26.248, 26.241957, 26.234056, 26.225025, 26.223186, 26.214947, 26.190033, 26.186476, 26.185305, 26.159506, 26.158932, 26.156904, 26.147402, 26.13989, 26.133883, 26.128069, 26.11958, 26.11958, 26.11958, 26.116716, 26.101917, 26.101864, 26.086418, 26.084593, 26.082298, 26.079964, 26.0762, 26.074896, 26.072361, 26.072123, 26.053408, 26.049118, 26.033432, 26.028778, 26.022081, 25.99998, 25.996056, 25.979614, 25.946568, 25.932976, 25.930996, 25.93086, 25.925335, 25.923065, 25.917585, 25.911507, 25.905674, 25.90402, 25.89951, 25.895317, 25.891577, 25.891, 25.879545, 25.87232, 25.869314, 25.869196, 25.860847, 25.834558, 25.83108, 25.821732, 25.818495, 25.781025, 25.779463, 25.771202, 25.761642, 25.75866, 25.750622, 25.74611, 25.738424, 25.737213, 25.729265, 25.727768, 25.727402, 25.723854, 25.72242, 25.721241, 25.71774, 25.714645, 25.713814, 25.713223, 25.711784, 25.711424, 25.702778, 25.69869, 25.697372, 25.696678, 25.682144, 25.678581, 25.675274, 25.66561, 25.662178, 25.662178, 25.661522, 25.659086, 25.654232, 25.653957, 25.648714, 25.646345, 25.643763, 25.64289, 25.64034, 25.637222, 25.63165, 25.628342, 25.6211, 25.61694, 25.610268, 25.602362, 25.6011, 25.598783, 25.59804, 25.597795, 25.597795, 25.596142, 25.593575, 25.56993, 25.560745, 25.547392, 25.545969, 25.54445, 25.539295, 25.527033, 25.519482, 25.519304, 25.50793, 25.496056, 25.49547, 25.494305, 25.485332, 25.474592, 25.470737, 25.468887, 25.467205, 25.460625, 25.45993, 25.45523, 25.455078, 25.446953, 25.446117, 25.437366, 25.437353, 25.432867, 25.417618, 25.405529, 25.39976, 25.393658, 25.392628, 25.386358, 25.37827, 25.374691, 25.374647, 25.371807, 25.371767, 25.365551, 25.355528, 25.352886, 25.350296, 25.342745, 25.34097, 25.3334, 25.331675, 25.328108, 25.324566, 25.318762, 25.315758, 25.311945, 25.311438, 25.309584, 25.308355, 25.302166, 25.296738, 25.287848, 25.286068, 25.281212, 25.276398, 25.274473, 25.25452, 25.240705, 25.228933, 25.22631, 25.224821, 25.223253, 25.21468, 25.208216, 25.195862, 25.189533, 25.186201, 25.184961, 25.184021, 25.172094, 25.170307, 25.163671, 25.161804, 25.159348, 25.154655, 25.14961, 25.14817, 25.139418, 25.130512, 25.130001, 25.128881, 25.121023, 25.11058, 25.105623, 25.105116, 25.100054, 25.09794, 25.096884, 25.096884, 25.094437, 25.093044, 25.074314, 25.058952, 25.05584, 25.054285, 25.047335, 25.041471, 25.034145, 25.029816, 25.022589, 25.021812, 25.020943, 25.020187, 25.013823, 25.0132, 25.007504, 25.005215, 25.001097, 24.99366, 24.988781, 24.971182, 24.970695, 24.963558, 24.953787, 24.946009, 24.940878, 24.937841, 24.9357, 24.933432, 24.925938, 24.924866, 24.924866, 24.91128, 24.907625, 24.907148, 24.891941, 24.890867, 24.89004, 24.88913, 24.886557, 24.885841, 24.880001, 24.87861, 24.87808, 24.868773, 24.86608, 24.863037, 24.860594, 24.859644, 24.853504, 24.853209, 24.852488, 24.851833, 24.848246, 24.847702, 24.846956, 24.838217, 24.838217, 24.83755, 24.831059, 24.815403, 24.810133, 24.785954, 24.782423, 24.76971, 24.767605, 24.762178, 24.75701, 24.748734, 24.746561, 24.745018, 24.744658, 24.743044, 24.73835, 24.73646, 24.72479, 24.718517, 24.716192, 24.712784, 24.70455, 24.702208, 24.700907, 24.698568, 24.696644, 24.690937, 24.68604, 24.684937, 24.671032, 24.659946, 24.659357, 24.65845, 24.637413, 24.63419, 24.631168, 24.617813, 24.617462, 24.612072, 24.606598, 24.605553, 24.60007, 24.597801, 24.592022, 24.59092, 24.586155, 24.574516, 24.573595, 24.57238, 24.567059, 24.559454, 24.558699, 24.557325, 24.554995, 24.541769, 24.53605, 24.532751, 24.530527, 24.52929, 24.52758, 24.525885, 24.523287, 24.52216, 24.518185, 24.518002, 24.510452, 24.507927, 24.50747, 24.500023, 24.499523, 24.499329, 24.491096, 24.48963, 24.48861, 24.482834, 24.47671, 24.464666, 24.462645, 24.46171, 24.457304, 24.456846, 24.455421, 24.452608, 24.446362, 24.444448, 24.443518, 24.442509, 24.431234, 24.429918, 24.425865, 24.424694, 24.421072, 24.416813, 24.410109, 24.407463, 24.40605, 24.405626, 24.402885, 24.392212, 24.391882, 24.38703, 24.385288, 24.380241, 24.380083, 24.372711, 24.372618, 24.369892, 24.367855, 24.365194, 24.364956, 24.36361, 24.359426, 24.35822, 24.349783, 24.348995, 24.347973, 24.347115, 24.34292, 24.341932, 24.34012, 24.340038, 24.339987, 24.338839, 24.338299, 24.331291, 24.330807, 24.327673, 24.316727, 24.308113, 24.299313, 24.29862, 24.296806, 24.295572, 24.292818, 24.29224, 24.284653, 24.278984, 24.268192, 24.267412, 24.267399, 24.266453, 24.26629, 24.266016, 24.263542, 24.260998, 24.260155, 24.257282, 24.256721, 24.253878, 24.246807, 24.24509, 24.234573, 24.232227, 24.230812, 24.2302, 24.228807, 24.228643, 24.22783, 24.22713, 24.220657, 24.21799, 24.21271, 24.209076, 24.208853, 24.20754, 24.207497, 24.206127, 24.198303, 24.194723, 24.193428, 24.190655, 24.18271, 24.176815, 24.174118, 24.169024, 24.169024, 24.166487, 24.164953, 24.161852, 24.159363, 24.159267, 24.15808, 24.153969, 24.149784, 24.140722, 24.140211, 24.134253, 24.13412, 24.13412, 24.132248, 24.132034, 24.131937, 24.131872, 24.128216, 24.127504, 24.126823, 24.125408, 24.124678, 24.122154, 24.114777, 24.113977, 24.112276, 24.112083, 24.110775, 24.108192, 24.104073, 24.102125, 24.097551, 24.091564, 24.089552, 24.089066, 24.088158, 24.084694, 24.07787, 24.075788, 24.072807, 24.071453, 24.071346, 24.069708, 24.069002, 24.067818, 24.06605, 24.061665, 24.060574, 24.059948, 24.059776, 24.057356, 24.056446, 24.053661, 24.049282, 24.032372, 24.02395, 24.019629, 24.018064, 24.01318, 24.008617, 24.003117, 23.99648, 23.991734, 23.991707, 23.987144, 23.986143, 23.986143, 23.983543, 23.982464, 23.98182, 23.981712, 23.980179, 23.976633, 23.975328, 23.97166, 23.97121, 23.97121, 23.97002, 23.969404, 23.968441, 23.966225, 23.958607, 23.958115, 23.957453, 23.949345, 23.94821, 23.946215, 23.944715, 23.94449, 23.943008, 23.942556, 23.941507, 23.933313, 23.929783, 23.929522, 23.928955, 23.926168, 23.923376, 23.922705, 23.919361, 23.914911, 23.91367, 23.911812, 23.911533, 23.906849, 23.901445, 23.898254, 23.897259, 23.889637, 23.88667, 23.880405, 23.875282, 23.869606, 23.866463, 23.865208, 23.861814, 23.855343, 23.845152, 23.842434, 23.83563, 23.832293, 23.823992, 23.823854, 23.82064, 23.814968, 23.812757, 23.807041, 23.806612, 23.804323, 23.799536, 23.799536, 23.799536, 23.798204, 23.793175, 23.791477, 23.78939, 23.785604, 23.784878, 23.777132, 23.76982, 23.76921, 23.765804, 23.765005, 23.764532, 23.763603, 23.754108, 23.753197, 23.748444, 23.748444, 23.746864, 23.745108, 23.744574, 23.739212, 23.73888, 23.733799, 23.729816, 23.727652, 23.723003, 23.720634, 23.720634, 23.714848, 23.709877, 23.705795, 23.704754, 23.704483, 23.703867, 23.690964, 23.685738, 23.684828, 23.683258, 23.681973, 23.678188, 23.67802, 23.676935, 23.67629, 23.673971, 23.671099, 23.667322, 23.665724, 23.65968, 23.658663, 23.653025, 23.647165, 23.64598, 23.638786, 23.631447, 23.62884, 23.62259, 23.621868, 23.621582, 23.618534, 23.616392, 23.616392, 23.614056, 23.612534, 23.60932, 23.606533, 23.603977, 23.60381, 23.601341, 23.600262, 23.593845, 23.59343, 23.589554, 23.588947, 23.586582, 23.583982, 23.580885, 23.574877, 23.572336, 23.57184, 23.571838, 23.561562, 23.555532, 23.545412, 23.545412, 23.544636, 23.537756, 23.536188, 23.534466, 23.533688, 23.533285, 23.533203, 23.532787, 23.527275, 23.523005, 23.52182, 23.521599, 23.515547, 23.51388, 23.513426]}, {'id': '2556211367', 'text': 'service dominant logic 2025', 'timestamp': 1488322800, 'rel_doc_ids': ['1175888821', '1973477318', '1989075208', '2002704355', '2009070185', '2032534748', '2037021691', '2040247369', '2054877429', '2086144435', '2126840412', '2158122897', '2188476365', '2254046164', '2306185662', '2482186344', '271526086', '640744729'], 'user_id': '2043899690', 'user_doc_ids': ['63755588', '152963839', '640744729', '1538326180', '1924429363', '1965600442', '1979056716', '1989075208', '1994609497', '2002704355', '2002757881', '2019354144', '2028727503', '2028799738', '2062420996', '2066106849', '2078854905', '2084935660', '2106099010', '2123350241', '2124513302', '2126840412', '2143615203', '2154224591', '2158122897', '2160740147', '2170986239', '2172104004', '2179359578', '2188476365', '2298730933', '2482186344', '2511845513'], 'bm25_doc_ids': ['2133328633', '2078854905', '2179359578', '1979056716', '1972963133', '2040071332', '1973477318', '2170986239', '2124513302', '2306185662', '2254046164', '2092501001', '2126840412', '2339133800', '640305383', '2082324170', '2028727503', '1596688523', '2083491180', '2091392779', '2084665667', '1965600442', '2061638198', '1993136301', '2103603030', '1614492977', '2055675048', '1991497413', '1989075208', '2140476057', '2084935660', '2534757302', '2252891999', '2118401512', '2092638121', '65617363', '1559598616', '2727132558', '2048986755', '2069953116', '2104672220', '1898859358', '2028799738', '90032436', '1123882910', '2022783783', '1539915773', '568599025', '2057217588', '152963839', '2324338241', '1779636320', '2106032704', '2089154467', '63755588', '123796236', '164417687', '2167861716', '2341626334', '2009070185', '2042916127', '2018823468', '1461833002', '1965887933', '2018647543', '1973803766', '2212657762', '271526086', '2394883584', '2112584744', '1521164690', '859030279', '640744729', '2269969426', '2092310800', '2134877170', '2133935461', '2070504166', '2008445642', '2063414677', '2918834805', '1992851256', '1989096157', '2103416116', '2105800853', '2129620873', '2047429527', '2041796919', '1965084688', '2124606522', '2033007590', '2054877429', '2122789584', '2041478611', '2469547856', '2032534748', '1481703006', '2158111592', '2301180339', '2172104004', '2063446019', '2093138545', '2167141711', '1991677699', '1975845483', '2086144435', '1943634648', '2145182284', '2189330886', '1748943063', '2016408572', '2006174560', '2131487644', '2066240089', '1974245918', '2026111517', '2116715455', '2109833393', '1481633656', '2120673262', '2092785387', '957193569', '2132470665', '2060031833', '2170269299', '2503917564', '2511845513', '610657276', '2108475562', '2103930660', '1524463461', '658882672', '2170885852', '1959166443', '2002704355', '1981565035', '2005404867', '2050045262', '2048452859', '1594341144', '2082343906', '1603775261', '1509575625', '1964181393', '1506337957', '2153594661', '1824683756', '2067757062', '2169803829', '2036668453', '2105477360', '1486039903', '2144714908', '1996214267', '1577796488', '2508003632', '2009410174', '185943961', '2018222436', '1605127507', '2202277747', '2120913174', '2006684927', '2171584613', '2065582972', '2128275616', '2158122897', '2062885944', '2104366971', '1596704901', '2080959267', '2148884970', '264857170', '1686247645', '2161923745', '2139779100', '1027103034', '2482186344', '2085987034', '2087924076', '2482049572', '1497846298', '2251110209', '150466060', '2024290643', '2112603895', '1564769652', '1884873684', '329689523', '2121447816', '2778703290', '2053608011', '2077356795', '2050494945', '1559744082', '2209577583', '2096116903', '2113167730', '2021165559', '2116606191', '2154224591', '2114496768', '567069161', '2170153942', '1994609497', '2097096727', '2172155104', '1702186425', '2019354144', '2112342133', '2126354371', '2103102501', '2102107784', '2159079631', '199027712', '1996865319', '1967595794', '1903080880', '1996452104', '1992851806', '2557221792', '2344877473', '2090216332', '2162349949', '103428947', '2032530235', '1680086327', '2039421732', '1778726339', '2153371694', '2084495293', '2010268490', '2513031390', '2015661615', '2215239734', '2142528360', '2099582604', '2039112452', '2078068039', '2040123795', '2159431720', '2087133636', '2188476365', '786607333', '204312987', '2078392363', '2068241680', '2539262410', '1972623330', '2153544819', '2070221480', '2169975018', '164193721', '2073329773', '2170733827', '2500878916', '2070906209', '2004113806', '2107609659', '2102526440', '1969550218', '2080117042', '2103598333', '2273859502', '2022731018', '2036032395', '2026299106', '2017513346', '2050845448', '2018361521', '2061624990', '2023900150', '1992535818', '2155921588', '2161128251', '1818142121', '2049859134', '2296552381', '2127967895', '2339168244', '1989446666', '210391069', '2077230782', '2259237737', '2110952241', '2174160989', '1996758084', '2332558102', '37439044', '2110340447', '1765212522', '2096426987', '2041506756', '1175888821', '2148968825', '2076708753', '2236455474', '2249675775', '1835478118', '1035199527', '2159799595', '207234881', '1997390900', '2285319826', '2051312161', '2112912319', '2178587318', '1574928270', '2023098295', '2170869628', '2015032584', '2051660784', '2141973803', '2114817283', '1482047020', '2128247627', '1484657937', '2089849443', '1964540759', '245203381', '2060444253', '2217354374', '2085230736', '2417954864', '2399286941', '1999436445', '2267268533', '2270511345', '2291078321', '1967854250', '1042283040', '1797135812', '1517911348', '2300185643', '1905150188', '1901150973', '1512284745', '1947561378', '2114580895', '2042714121', '2136195569', '142975991', '2255564049', '976301360', '2431181726', '44008470', '2293747895', '2071064785', '2095208580', '1992026348', '1969537712', '2482105913', '2115896676', '2068902616', '1484904834', '2026629052', '2157099947', '2283199640', '1908042258', '2261545619', '2255788808', '2398354905', '2122778706', '2542018869', '2071115126', '2112767336', '1591996218', '1498025716', '1596275251', '2872419265', '1798780498', '2118749740', '2275770336', '2119748374', '2149433282', '2117033209', '2024450887', '1505918769', '2011036166', '2984786680', '2400518564', '1632376246', '1982875408', '2108552295', '2098395989', '2109914188', '2398716132', '2062285250', '130920809', '1531852253', '2096040244', '2146127897', '286321754', '1687555669', '1567532180', '2155815459', '62457539', '2098714979', '1925988013', '2264122922', '2053770828', '2117557053', '2243430298', '2225007129', '2086002470', '1686397080', '2162193780', '2222693654', '2080398754', '2121175164', '1569896925', '2131894072', '1979064927', '2111660953', '2159421932', '2007852888', '2225631687', '2215497059', '1997604133', '1966640998', '2245058779', '2133420253', '2049647018', '2025614238', '2203243829', '1946649013', '1552736961', '2298458561', '1539276293', '2284490836', '1805395244', '1965494178', '1822902091', '2306047700', '2141323075', '1581519696', '2181786416', '2199443753', '1905801923', '2206727654', '2094635587', '1913955095', '1558219273', '1551898999', '2257006213', '2027426114', '2118067045', '1667894684', '2116959366', '1603746939', '1522723118', '2408265908', '2231216368', '2104812422', '2164520015', '58190436', '2100515331', '1593383061', '2103777085', '2147466296', '1645898153', '1803548689', '1712766829', '1979806961', '1914026091', '1599928432', '2209141744', '83593792', '2276788525', '1987571194', '1932780997', '1938964931', '2026694033', '2168173160', '2275643395', '2399748090', '2071903407', '2243493137', '1570150742', '268303684', '2077486688', '2086996424', '2399938136', '2154697147', '2090089096', '2095191933', '2220674870', '2052570768', '2275840573', '792234019', '1887908299', '2248599493', '2000972984', '1482708121', '2013309158', '2134338152', '298394317', '1590996240', '2094722405', '2081375161', '2028580099', '2136374467', '1494142510', '2213843402', '2113540078', '1985946447', '2117236085', '2030249107', '1566528295', '2095665983', '2020092346', '2237782130', '2164471319', '1983393516', '1528932758', '2046870826', '2053611722', '1768868230', '1636943114', '2267585853', '2418014742', '2519931634', '1983527755', '2151864952', '284171883', '1589547395', '2053569992', '2150773189', '2409049818', '1519608067', '1881022830', '1589514528', '1608840694', '2277532042', '2270780972', '2581396345', '1579629038', '2248638131', '2093093586', '2217376264', '2165123301', '2096620925', '2125882712', '2282891363', '1605487488', '2057716535', '2163525143', '2122824894', '1520722979', '2156658434', '2064669720', '2007447978', '2043347770', '1807431604', '1598166047', '1530850479', '2152484575', '2104750565', '2149865032', '1594195506', '2261257632', '2013470680', '1969572390', '2227733487', '2001088823', '1592677455', '2123191773', '2588106804', '2205097966', '2107471972', '1529891149', '1503964366', '87455989', '1668732859', '1987223749', '2094264732', '2521534638', '2093665514', '2012785847', '2167149134', '1969220053', '2051934751', '2062860793', '2113293829', '2021189936', '2105718221', '2287113383', '2224173000', '2067908221', '2103983743', '2275591100', '1967312763', '2048638420', '1546522669', '1573799125', '2163562754', '2587617223', '959850748', '2104846587', '2200878837', '1525408871', '229246894', '1568998984', '2161451708', '2464499006', '2093770735', '286204772', '2490276698', '1558169796', '1515090614', '2082042540', '1997259208', '1571651106', '1970339501', '996761320', '2064097199', '1549619626', '2130363885', '128272741', '2054411146', '2226624994', '2057993584', '2143637616', '2059584710', '1794618617', '2196578207', '1524066393', '2039085532', '1983928547', '2103791134', '2086816469', '2088428301', '2032558960', '1485666009', '2091160251', '162897072', '2547029309', '1575519589', '2111813090', '2200668610', '2105203192', '2527492569', '1485853096', '2001440509', '2127625687', '2242097694', '1985819648', '1562379421', '1516907863', '2155059737', '1985552127', '1550031370', '2038408649', '2228399030', '2482612281', '2021963137', '1018060126', '1988230540', '1552744980', '2139897147', '2227211083', '948312248', '953279829', '2221038857', '1498016056', '1570710905', '2981979474', '1970464914', '2104564874', '1565275587', '2137663057', '1551078623', '2022078754', '1598280950', '2004350589', '2213876301', '2123423186', '2035134444', '2115514874', '2160741635', '2168108355', '2001835026', '1578917571', '2037470529', '2112710730', '1549434744', '1587939284', '2171869249', '1601010099', '2107996056', '2066702272', '2745883085', '1484291411', '1993436735', '2018765318', '1557320602', '134178287', '1544400897', '2046194595', '2036522361', '2397003196', '1548405964', '2005077735', '2070944660', '2201082558', '2249055575', '2150900220', '2008343180', '1983547272', '2158066851', '1868739052', '1663508306', '2302861407', '2115959430', '2295768140', '1038747453', '2241108129', '324447844', '1539296130', '2280424502', '2141393302', '2018431495', '2110570415', '2115270335', '2438233480', '1554049351', '1981243171', '1987689559', '2310385797', '2104443629', '2219833294', '256645461', '985037370', '2018773943', '2037021691', '2298772260', '2207196637', '2044100157', '2006345462', '1485104364', '2149931522', '2143242709', '2049210264', '2072620702', '2105565978', '2407195821', '2054355619', '1970355818', '2015394059', '2166157139', '2101981289', '1966891030', '2094324142', '1781739817', '1988405420', '2025408282', '1761038719', '2112224173', '1954216178', '2117817802', '2138354505', '1583893771', '2117232518', '2163906760', '2413478849', '1798905086', '231721914', '2100072979', '1007197191', '2170198522', '2544298400', '2763637085', '1991585680', '1875902980', '1489533044', '2540449061', '2029916063', '1948039166', '2117500865', '2282511576', '2172194307', '1847775492', '2052805857', '47189862', '2275934786', '2204297369', '1500872859', '2145737542', '1563077933', '1538931949', '2400373974', '1569230947', '2141384578', '1572037402', '989839772', '2025822853', '2070228946', '1993457287', '2085447527', '2121373789', '2134578874', '2038629585', '1840200007', '2404680258', '936224431', '2213289055', '2088770508', '1483883043', '1815620898', '2325113826', '1823330456', '1523641596', '1483001850', '1601855661', '2038123863', '2002378456', '2073305441', '1981230993', '2164536504', '2144840700', '2010168980', '2143152055', '1821136935', '2168672270', '1573877699', '2028786085', '2168135793', '1976346346', '1601963427', '2856594161', '2205833928', '2098806794', '2273551892', '2285208496', '1598232138', '2261359378', '1967973800', '1985947699', '2109932036', '2088156449', '1555334771', '2403521323', '1944338833', '2130523398', '2101822867', '1949876277', '2080725390', '1776927546', '1496127970', '2403804594', '2102696840', '1882437937', '2257768013', '2035347930', '1978436753', '1486477601', '2081360174', '2148370032', '2154521588', '2014539911', '2048006820', '1965477836', '1544676372', '2950291051', '2279645966', '2130770832', '2115321500', '1869936965', '1544583828', '1487373936', '2224406289', '1878864002', '2119440195', '1972456335', '2104039913', '2148335156', '2090719839', '2036038071', '2582330555', '2276924837', '1545051844', '1992432802', '2187590730', '2115323533', '2038036916', '1967278718', '1566386780', '1817007106', '1553008248', '1491680937', '1564285052', '2006890607', '2123475087', '2002899783', '2081156723', '2081937715', '1976331066', '2225257741', '1915940795', '1539765239', '3150259', '1969461396', '2109587872', '1593069994', '1885475104', '1541371752', '2279684720', '2128131819', '2060703011', '2535204391', '2138928460', '2060398042', '2170447695', '2066165917', '1989275821', '2040247369', '2150219282', '2094888025', '2408230820', '2084467994', '2136278061', '2271913836', '2137187816', '1999018339', '2095743746', '2404210484', '2120536150', '2241120426', '1531252517', '2162737732', '2126821874', '1621069200', '2125476586', '2018092048', '1531557669', '1526107878', '2168541354', '1518420864', '1492295207', '1806041125', '1988656101', '2402451497', '2117718957', '2150901305', '2254078559', '1569470442', '2110948142', '1891557895', '1969259706', '2056654692', '2031753789', '1528402156', '2402427270', '2169531970', '1588811213', '1557693498', '2414133748', '2293179067', '2046373367', '2171493366', '1608302930', '2148608738', '2164376763', '1752196242', '1863104222', '2515935820', '2034908742', '2479967538', '2293289946', '2152961935', '1013806578', '2038480238', '2006874685', '2306669182', '2014754047', '2155052368', '184336709', '2141061821', '3022349616', '2110042050', '2032314490', '2061396766', '2170343124', '2024547718', '2122836287', '1587727380', '2127447525', '2206045563', '1508039094', '333834217', '2005998857', '2460908422', '2140954119', '2593894556', '2409649324', '1521685124', '2122212387', '2148036485', '2142410637', '2302712905'], 'bm25_doc_scores': [21.749474, 21.143246, 20.443527, 20.345419, 20.225952, 20.182823, 20.166742, 20.127745, 19.979439, 19.1728, 19.0854, 19.042084, 19.02029, 19.015095, 19.000353, 18.991783, 18.98394, 18.929173, 18.920671, 18.830622, 18.800987, 18.76138, 18.711452, 18.668098, 18.60525, 18.575428, 18.512878, 18.444958, 18.438328, 18.289097, 18.192183, 18.149715, 18.139023, 18.122606, 18.115934, 18.101803, 18.01326, 17.954998, 17.930449, 17.84651, 17.817635, 17.801064, 17.792135, 17.7717, 17.767483, 17.76656, 17.757898, 17.753231, 17.747686, 17.737534, 17.67136, 17.63578, 17.567852, 17.549068, 17.4717, 17.41977, 17.388947, 17.365383, 17.115858, 17.090275, 17.081814, 17.033539, 17.024273, 16.972445, 16.949383, 16.931252, 16.88248, 16.854946, 16.74805, 16.719566, 16.704199, 16.645077, 16.600254, 16.545273, 16.511524, 16.496372, 16.41357, 16.408234, 16.404476, 16.339745, 16.295841, 16.281607, 16.255684, 16.093931, 16.084782, 16.043358, 16.012985, 15.943199, 15.939891, 15.934189, 15.862745, 15.845363, 15.76587, 15.734263, 15.700474, 15.6878395, 15.685768, 15.615662, 15.5323925, 15.48491, 15.434572, 15.424168, 15.366411, 15.365327, 15.313781, 15.2923355, 15.283606, 15.194309, 15.159811, 15.154545, 15.128532, 15.125183, 15.100047, 15.079884, 15.064988, 15.017231, 14.985995, 14.9689045, 14.965926, 14.876734, 14.824175, 14.756438, 14.756147, 14.711532, 14.709759, 14.626351, 14.625319, 14.549847, 14.527729, 14.510794, 14.495583, 14.468154, 14.452815, 14.451611, 14.449011, 14.405565, 14.381838, 14.3790245, 14.362661, 14.351328, 14.331415, 14.313551, 14.313551, 14.274603, 14.264793, 14.245672, 14.210388, 14.200326, 14.168051, 14.149201, 14.149201, 14.1354885, 14.116591, 14.094917, 14.067078, 14.047575, 14.01747, 14.009349, 14.004137, 13.957989, 13.928286, 13.924936, 13.920489, 13.840292, 13.83692, 13.821945, 13.821945, 13.815424, 13.812105, 13.792498, 13.788034, 13.731801, 13.731801, 13.724911, 13.711931, 13.697075, 13.694983, 13.679476, 13.64483, 13.633501, 13.629837, 13.622181, 13.622181, 13.604937, 13.590714, 13.590714, 13.587732, 13.587732, 13.587732, 13.561483, 13.509531, 13.509178, 13.502102, 13.469718, 13.463572, 13.463572, 13.457926, 13.425266, 13.394592, 13.393842, 13.389483, 13.387154, 13.3767185, 13.370291, 13.362534, 13.346251, 13.31302, 13.31302, 13.2976055, 13.210266, 13.207449, 13.206613, 13.186409, 13.167149, 13.164912, 13.145804, 13.126767, 13.122143, 13.121283, 13.114704, 13.114704, 13.074211, 13.045088, 13.029915, 13.013098, 12.998439, 12.982527, 12.955519, 12.947285, 12.937048, 12.936663, 12.936663, 12.868921, 12.852298, 12.843515, 12.839372, 12.778969, 12.756352, 12.756352, 12.756352, 12.756352, 12.756352, 12.750598, 12.750141, 12.750141, 12.677304, 12.677304, 12.677304, 12.640296, 12.632379, 12.603454, 12.597643, 12.597643, 12.597643, 12.523987, 12.5200615, 12.480266, 12.47114, 12.470453, 12.462354, 12.410402, 12.410402, 12.410402, 12.373495, 12.353798, 12.342181, 12.309318, 12.297666, 12.283892, 12.275692, 12.2443695, 12.2443695, 12.2443695, 12.2443695, 12.2443695, 12.173043, 12.173043, 12.169074, 12.169074, 12.169074, 12.167336, 12.16493, 12.149166, 12.138832, 12.137936, 12.129963, 12.11944, 12.110456, 12.103344, 12.102571, 12.0924, 12.062786, 12.062786, 12.039597, 12.025507, 12.011581, 11.978142, 11.972627, 11.97047, 11.97047, 11.96467, 11.96388, 11.955073, 11.949659, 11.946017, 11.946017, 11.926525, 11.9252825, 11.9252825, 11.9069805, 11.899321, 11.898139, 11.897958, 11.897958, 11.897958, 11.897958, 11.897958, 11.894016, 11.894016, 11.890617, 11.881537, 11.881537, 11.869676, 11.86965, 11.862539, 11.849083, 11.848663, 11.831474, 11.810667, 11.808608, 11.807774, 11.776466, 11.762751, 11.748358, 11.744903, 11.712, 11.703932, 11.695826, 11.690655, 11.690655, 11.671858, 11.653751, 11.647551, 11.64219, 11.6386595, 11.6386595, 11.6386595, 11.6386595, 11.6386595, 11.6386595, 11.622407, 11.622407, 11.610775, 11.610775, 11.595185, 11.595185, 11.595185, 11.589495, 11.577942, 11.56555, 11.564323, 11.562608, 11.526828, 11.525389, 11.525389, 11.524948, 11.51973, 11.518628, 11.51838, 11.51838, 11.51162, 11.504194, 11.504075, 11.49986, 11.499322, 11.496359, 11.493732, 11.49117, 11.473242, 11.473242, 11.470932, 11.467966, 11.46572, 11.454332, 11.451854, 11.451854, 11.451269, 11.448425, 11.442583, 11.435516, 11.431628, 11.422454, 11.420949, 11.411791, 11.405636, 11.405636, 11.4056, 11.401776, 11.401776, 11.39271, 11.390423, 11.390423, 11.380739, 11.380739, 11.379395, 11.379395, 11.377528, 11.361448, 11.350029, 11.345008, 11.33827, 11.335344, 11.335344, 11.335344, 11.334533, 11.331194, 11.319061, 11.312466, 11.31044, 11.308587, 11.307871, 11.3007, 11.298285, 11.298285, 11.274141, 11.2707405, 11.270677, 11.267513, 11.241173, 11.231014, 11.223816, 11.2208805, 11.217512, 11.212481, 11.19824, 11.195025, 11.19262, 11.19262, 11.188692, 11.170052, 11.169625, 11.169625, 11.169329, 11.161095, 11.153288, 11.152553, 11.146835, 11.139412, 11.130806, 11.126792, 11.126792, 11.125547, 11.12306, 11.122079, 11.11791, 11.11791, 11.115592, 11.113001, 11.112434, 11.106218, 11.105114, 11.103685, 11.103685, 11.093254, 11.086323, 11.086323, 11.079061, 11.071842, 11.071434, 11.071343, 11.069363, 11.064255, 11.057396, 11.057396, 11.04274, 11.040594, 11.040216, 11.0332775, 11.02913, 11.021424, 11.009515, 11.006031, 11.005936, 10.999311, 10.998442, 10.995073, 10.993366, 10.976204, 10.973379, 10.967546, 10.960644, 10.960644, 10.95966, 10.959225, 10.95664, 10.95636, 10.950485, 10.949238, 10.94766, 10.947258, 10.944931, 10.941459, 10.939648, 10.939648, 10.938549, 10.925521, 10.925521, 10.9254, 10.9254, 10.9254, 10.921071, 10.911978, 10.909448, 10.904629, 10.904177, 10.894615, 10.891125, 10.891125, 10.886796, 10.876514, 10.876514, 10.87621, 10.875293, 10.875293, 10.870716, 10.863213, 10.858423, 10.858423, 10.858423, 10.857899, 10.849501, 10.849501, 10.844723, 10.844635, 10.844635, 10.844635, 10.829643, 10.826897, 10.823572, 10.823572, 10.822256, 10.816465, 10.811462, 10.807439, 10.799548, 10.799548, 10.796517, 10.796517, 10.796517, 10.791899, 10.785711, 10.785711, 10.785711, 10.785646, 10.782873, 10.782873, 10.767908, 10.767395, 10.763171, 10.763171, 10.763171, 10.763171, 10.751207, 10.751207, 10.751207, 10.749683, 10.744789, 10.744427, 10.744427, 10.742831, 10.742146, 10.742146, 10.736464, 10.735492, 10.735492, 10.731871, 10.715408, 10.7151575, 10.714201, 10.714201, 10.712326, 10.705425, 10.704905, 10.702893, 10.701996, 10.699512, 10.6976795, 10.69529, 10.69502, 10.69502, 10.69449, 10.69399, 10.689979, 10.687745, 10.67124, 10.67064, 10.67064, 10.67064, 10.67064, 10.665951, 10.663223, 10.662989, 10.662989, 10.662803, 10.6616125, 10.6610775, 10.654119, 10.654119, 10.652465, 10.651312, 10.645338, 10.643789, 10.642157, 10.642157, 10.642157, 10.642157, 10.639485, 10.639485, 10.637786, 10.632111, 10.622595, 10.615759, 10.615759, 10.61073, 10.61073, 10.610645, 10.6095085, 10.604437, 10.603634, 10.603634, 10.603634, 10.601985, 10.600992, 10.599591, 10.598498, 10.598498, 10.598498, 10.597649, 10.597649, 10.597649, 10.597649, 10.597595, 10.594126, 10.593212, 10.593212, 10.591364, 10.589946, 10.588682, 10.58732, 10.579332, 10.578335, 10.57578, 10.573748, 10.570213, 10.567329, 10.560088, 10.547665, 10.547665, 10.547665, 10.547665, 10.545872, 10.533728, 10.533728, 10.528393, 10.520132, 10.520132, 10.515222, 10.509493, 10.509493, 10.508152, 10.508152, 10.508152, 10.50526, 10.503763, 10.503763, 10.502154, 10.502154, 10.500036, 10.4997425, 10.499308, 10.497841, 10.490341, 10.486547, 10.4826355, 10.481632, 10.481552, 10.480748, 10.480463, 10.480246, 10.479746, 10.478538, 10.478264, 10.475329, 10.475329, 10.473372, 10.473372, 10.472427, 10.468852, 10.468037, 10.458695, 10.457632, 10.457632, 10.457632, 10.457632, 10.457632, 10.450894, 10.44858, 10.442883, 10.442883, 10.442883, 10.442883, 10.442883, 10.442104, 10.4416895, 10.4416895, 10.4416895, 10.4416895, 10.441606, 10.436481, 10.4313135, 10.431301, 10.430775, 10.428638, 10.4276495, 10.427492, 10.427492, 10.427492, 10.427492, 10.4233885, 10.4233885, 10.4233885, 10.423011, 10.42218, 10.42218, 10.420756, 10.420637, 10.420637, 10.420327, 10.418324, 10.408836, 10.408478, 10.40663, 10.398325, 10.397671, 10.397671, 10.395569, 10.390489, 10.389201, 10.389201, 10.388394, 10.385096, 10.384399, 10.384399, 10.383295, 10.381399, 10.381071, 10.381071, 10.381071, 10.378473, 10.371117, 10.370073, 10.362994, 10.361044, 10.358273, 10.358086, 10.358086, 10.358086, 10.358086, 10.356486, 10.352949, 10.352949, 10.350616, 10.349657, 10.34513, 10.3431835, 10.3431835, 10.340155, 10.340031, 10.340031, 10.339535, 10.339535, 10.337952, 10.337952, 10.334949, 10.334311, 10.333895, 10.331461, 10.330471, 10.329009, 10.329009, 10.329009, 10.329009, 10.329009, 10.326878, 10.326351, 10.323898, 10.316568, 10.31279, 10.310026, 10.310026, 10.310026, 10.310026, 10.309307, 10.308939, 10.308939, 10.308939, 10.308254, 10.297083, 10.297083, 10.293373, 10.292768, 10.291307, 10.290382, 10.290216, 10.290216, 10.290216, 10.28812, 10.28812, 10.285902, 10.285902, 10.285831, 10.283018, 10.274385, 10.27143, 10.271331, 10.270954, 10.270954, 10.270954, 10.270407, 10.270407, 10.270407, 10.269122, 10.268883, 10.265919, 10.264086, 10.264086, 10.263231, 10.263231, 10.263124, 10.263124, 10.263124, 10.262196, 10.262196, 10.262196, 10.262196, 10.260914, 10.260483, 10.260483, 10.259796, 10.259132, 10.258928, 10.258928, 10.258928, 10.257925, 10.255209, 10.253966, 10.253966, 10.251203, 10.251203, 10.251203, 10.251203, 10.251203, 10.251203, 10.251186, 10.251083, 10.249094, 10.241032, 10.237358, 10.235268, 10.23089, 10.225871, 10.224683, 10.224683, 10.214329, 10.21257, 10.211418, 10.211418, 10.211418, 10.209108, 10.208903, 10.206898, 10.20332, 10.20332, 10.202734, 10.202734, 10.202604, 10.200565, 10.200565, 10.199336, 10.195177, 10.195177, 10.195177, 10.195177, 10.195177, 10.195177, 10.195177, 10.195177, 10.191925, 10.191159, 10.190952, 10.189993, 10.183009, 10.180674, 10.175599, 10.173987, 10.172792, 10.172792, 10.172792, 10.172792, 10.16571, 10.16571, 10.16571, 10.164768, 10.162527, 10.162006, 10.157507, 10.157507, 10.157507, 10.156422, 10.155483, 10.151098, 10.151098, 10.145401, 10.145196, 10.145138, 10.144503, 10.144503, 10.144503, 10.143337, 10.143337, 10.143337, 10.139042, 10.139042, 10.139042, 10.137839, 10.13697, 10.136835, 10.134531, 10.133352, 10.129691, 10.121552, 10.121552, 10.121552, 10.117443, 10.116669, 10.116669, 10.114888, 10.111031, 10.111031, 10.103598, 10.099129, 10.099129, 10.099129, 10.099129, 10.098917, 10.098917, 10.098343, 10.098143, 10.098143, 10.098143, 10.093148, 10.093148, 10.093148, 10.093148, 10.088853, 10.088853, 10.088646, 10.088646, 10.088549, 10.086097, 10.084131, 10.084131, 10.083689, 10.083256, 10.0766325, 10.075101, 10.075101, 10.075083, 10.075083, 10.070885, 10.07041, 10.067881, 10.066677, 10.066677, 10.066677, 10.058464, 10.054955, 10.054955, 10.054955, 10.05325, 10.05325, 10.05325, 10.052971, 10.050339, 10.050339, 10.050339, 10.049998, 10.049998, 10.049998, 10.046992]}, {'id': '2562771828', 'text': 'augment atlas base liver segmentation radiotherapy treatment plan incorporate image features proximal atlas contour', 'timestamp': 1483743600, 'rel_doc_ids': ['1963830612', '1996668601', '2049503784', '2060903331', '2066511532', '2082681385', '2103858720', '2132210108', '2143781397', '2153431772', '2460693642'], 'user_id': '2072204366', 'user_doc_ids': ['83124024', '114997016', '206050697', '1548753203', '1681354664', '1685360256', '1752126726', '1955200552', '1964457818', '1965143167', '1966725574', '1967033548', '1968727469', '1972119299', '1972730986', '1973485394', '1975821819', '1975890705', '1977060865', '1977459187', '1978230327', '1981027826', '1981232896', '1982447105', '1982975891', '1983098233', '1986323284', '1987688926', '1988548917', '1989459070', '1994511515', '1995083730', '1996992165', '1998496242', '1999263719', '1999440262', '1999818569', '1999978014', '2000631958', '2001122017', '2002409241', '2003933915', '2007449041', '2009530434', '2010173017', '2011198955', '2011465695', '2011690713', '2011772916', '2014547454', '2016113444', '2016232374', '2018253946', '2023531597', '2024912216', '2026068357', '2029562792', '2030071299', '2030427337', '2031864052', '2033336457', '2033492551', '2034654776', '2035534251', '2038091743', '2039710897', '2040189727', '2040710670', '2041104233', '2041259639', '2042653706', '2044257029', '2045056851', '2045294297', '2046753879', '2046824363', '2046889579', '2047673107', '2048727728', '2052175255', '2052681370', '2054536233', '2055761056', '2055844022', '2055955846', '2056839129', '2057309390', '2057356964', '2059049227', '2061033126', '2061244628', '2062523421', '2064316361', '2064548742', '2065444952', '2066713743', '2067918107', '2068096908', '2069171686', '2069325382', '2070435510', '2070654952', '2070939367', '2070979808', '2071030732', '2071504423', '2071792593', '2071927909', '2072021306', '2076961008', '2078020354', '2078121754', '2078145091', '2079124778', '2080327493', '2082129374', '2083361336', '2084073942', '2085800312', '2088953052', '2090475106', '2091111258', '2091242904', '2092862204', '2094489827', '2094793860', '2094811672', '2095087646', '2103148202', '2108504758', '2111725465', '2122090544', '2124580275', '2141232465', '2145362479', '2152370995', '2155592420', '2159123450', '2161458209', '2168093252', '2237473774', '2292572649', '2317204702', '2326087240', '2401648896', '2412035146', '2460693642', '2479599611', '2519742592'], 'bm25_doc_ids': ['2164818735', '1509957145', '86219213', '2145029612', '2038819887', '2100481199', '1844951343', '2066511532', '2156926884', '2069637751', '2225995310', '2079487963', '2143781397', '1977571367', '2025912829', '2513595145', '2106467008', '2137905895', '2036368826', '2137961449', '2345418216', '2072042721', '1920618489', '2009752869', '2018362413', '2170187799', '2038231398', '2046890045', '2088830730', '2021177063', '2528171022', '1977484942', '2079148755', '2026780889', '1547035658', '2022936669', '1850470120', '2088171042', '2477265066', '2073698185', '1529873969', '1525170478', '251803591', '2095700415', '2396907202', '1497937796', '1273353623', '1988499416', '2010439326', '2075304274', '2503788014', '2151577965', '2045784023', '2070825176', '1974579949', '2105796115', '2074374161', '2143005321', '2334943959', '2049503784', '1969493827', '1939744189', '2140886677', '2137013440', '2156677306', '1942576506', '2034181072', '2034648871', '1846543079', '2050542229', '2460693642', '2185444432', '1965749640', '2139972113', '1885456836', '2063657341', '2257626117', '3023513797', '2240596887', '2243436439', '1519048764', '2000585255', '1540569788', '2311671475', '1872853167', '2073728234', '2122258826', '2109487857', '2002616317', '1123242753', '1512101581', '2332506823', '2090892013', '771130797', '1525020064', '2116501211', '1600530571', '1969739559', '2080858163', '1549709432', '1586870336', '2153700064', '1860949288', '2046604054', '8258973', '2087822452', '2081582681', '1544547617', '1950974976', '2107574526', '2082425609', '2085064388', '1977259935', '2081141489', '2012902854', '1680164276', '2014921603', '2146982290', '1997076326', '2099670145', '2060903331', '1885193033', '2549512150', '2124904604', '1966208695', '200110071', '2025587397', '2006755983', '2132210108', '1952176550', '1578110577', '2077622428', '2035193596', '1979296326', '2156421455', '2069746112', '2024945300', '2198454978', '2060578202', '2062557036', '2059961245', '2108174317', '2121101167', '2304189599', '2764668828', '1566542507', '2141738799', '2064032134', '976428832', '1599816163', '2049247209', '2524965620', '1494315810', '1998070036', '1609798627', '1497105615', '2327128709', '2162385163', '2328170428', '2022316095', '2032377318', '42289437', '2294706126', '2108569878', '2106570674', '2136148445', '2056535069', '2161640927', '2411939282', '2493418695', '1988832131', '1587643749', '2000716207', '1969750491', '1583338723', '2029055226', '2169846542', '2046240297', '1980314885', '2526387258', '2231259152', '2011551852', '102444427', '2162742995', '1979760685', '2067065405', '2163693436', '1966556005', '1503511673', '2066401872', '1862996023', '1489736269', '2136056439', '2094377727', '1984327987', '2162140684', '2047896044', '2091581004', '1976295728', '2461360468', '2103524006', '2135535334', '2123339593', '1597605992', '2072026181', '1999888995', '2046758959', '53653168', '2112226928', '1967866963', '1584300343', '2040729395', '185353810', '2143973889', '954948471', '2009761203', '2142456468', '2111677918', '2053336787', '2153441116', '1964491208', '2055190078', '1882751776', '1552121654', '2029578549', '2037316890', '2054553044', '2160153825', '2130223630', '2064358322', '2143529102', '1992856588', '2013995453', '1954335072', '2050687114', '2048505507', '2300288570', '2035933013', '2089458919', '2079763059', '2064114382', '2171811623', '2169725827', '2114856020', '2541327173', '2205236229', '2070644840', '1737321630', '1579425092', '1514618654', '2015474943', '1596637622', '173643456', '2180031803', '2102285421', '2113799644', '2346145504', '2104914621', '2148157540', '38089536', '2181833534', '2004004650', '1773005811', '1990823682', '2106765247', '2142361163', '2170500377', '1753384015', '2008207141', '2057332541', '2295994535', '2066424153', '2070209883', '2280367348', '17779984', '2167170465', '1992496678', '1491669110', '1974446376', '2145605942', '1140063081', '293187046', '2173852729', '1979404996', '1975258254', '1484405646', '2111381594', '2267055111', '2195702343', '2069901410', '2163108348', '2169813116', '91438110', '1990058241', '1494221348', '2103115409', '2047056289', '2039861336', '118753260', '1982020952', '2022560974', '2096316513', '2402884626', '1980585525', '2077115170', '2085091083', '2012519212', '359174816', '1254779325', '2036188537', '66531091', '2041958560', '2525029552', '2102196538', '1980053479', '2036369046', '2196038229', '2019212261', '2152180867', '2011658829', '2082771892', '2042519496', '1852200068', '62526948', '2081431829', '2105776406', '1650968761', '1878962871', '2103683686', '2563421677', '1980864508', '2033554041', '2169769956', '2172055941', '2038009303', '2310726407', '2083919746', '191561589', '2470175974', '1993127575', '283803082', '2294424171', '2088108746', '2131266058', '2163240914', '2000439270', '1973892000', '2091341334', '2052971935', '2122418160', '1872492457', '1549847950', '2091926892', '2125504089', '2138658148', '2089975325', '1986948114', '2338665208', '2233255354', '2103858720', '2155187682', '2087013692', '1982310577', '2074688019', '2059340634', '2299051733', '2167917783', '1894830265', '2039900864', '1656156931', '2018427609', '2229355251', '1971667989', '1827806256', '2214073996', '2122632052', '1985504945', '115434072', '2070361692', '1983147688', '2029042876', '1593620893', '1992533954', '2165311547', '350653933', '2040154627', '2323970349', '2056218412', '1522965957', '2163595993', '2129183224', '2318502366', '1967425671', '2168031320', '2132486003', '107042206', '2128806031', '2007638243', '2144771422', '1989277294', '1970326953', '1470416955', '1891773028', '2294233313', '2094056586', '2299211235', '2105627766', '2305311999', '1977413080', '2093723730', '2291593693', '1993015702', '2087551927', '1893483670', '2111934016', '288721753', '1968355632', '2482138772', '2129055863', '235843031', '2008768241', '2532435692', '2055493157', '2009688094', '1595726223', '2150654863', '1972350517', '2038899360', '52210563', '2092608958', '2081443357', '1584867260', '2092189193', '1507175590', '2097580797', '2336753183', '2071449464', '2143143237', '2107128450', '2490229530', '2167584786', '2423526167', '2152761085', '2156564892', '2031272895', '1983623271', '1969257438', '1974544978', '2302994076', '2002102087', '1592003876', '2045256826', '1501048741', '1814859124', '2527587234', '1987153816', '2121014637', '2039439604', '2038583697', '2146917638', '2033732385', '2048515486', '1980244218', '2109945767', '2026920314', '2000801929', '2081634262', '2153081691', '2000376553', '1964135012', '1525537728', '1993759386', '2037267618', '62412913', '2168746203', '2143750594', '1005636916', '2147167225', '2025240023', '2166118053', '2011020139', '2170776495', '100297134', '1573051247', '1514531449', '2059911466', '2101989785', '2323200062', '2288365139', '2116514219', '2129948946', '2045015220', '1997689362', '1998467743', '2272252788', '1006518382', '1994146150', '1968490449', '590653799', '2084831515', '113453631', '2012959634', '2019407777', '2052617496', '2171403509', '2109258371', '2068061977', '2293474489', '1544308832', '1539774717', '1970545387', '2129711715', '2108057069', '1970416607', '1979799410', '2013432059', '2000060187', '2433387316', '2057241125', '2053046216', '135357456', '1982441646', '1677741797', '2154702468', '2142366297', '2395128269', '2157083145', '2156823923', '2442017823', '2418683079', '1994139859', '2029718973', '2025032336', '2086683689', '2151229683', '2161540539', '1550721541', '1557690671', '2049546272', '2127827887', '2296068926', '2332496147', '2006698358', '2511998877', '2002489794', '1988003469', '2012283616', '2038689732', '2498758160', '2079516038', '1985045888', '2074660466', '21976314', '2068863154', '2149582159', '1964811235', '2129052734', '2280661219', '2026371916', '2082681385', '218648366', '2003816057', '2068333777', '2166383689', '1977454198', '2147677622', '2013799520', '2026614276', '2005129295', '2154991548', '2047440597', '4044073', '172700687', '1978359266', '2007129566', '345272982', '2091339516', '1993569594', '2073128615', '60600131', '2023398872', '1563134596', '2149122136', '2527760005', '1894025792', '2014173354', '2087666149', '2014105136', '2124080110', '2074919265', '47565262', '1491305755', '2050590295', '1976110908', '2000311672', '2113608909', '2328113634', '2441809468', '2071297770', '2017192300', '1545004291', '4574224', '2103749046', '2554282444', '2310733299', '2064439299', '2067934990', '2098684027', '2219322011', '2077232553', '2035188186', '2171429518', '2102129768', '2084443965', '2028224075', '2083546575', '2080830831', '2162403881', '1963830612', '1980974175', '2159173151', '2012759362', '2113358261', '2077079080', '2082568327', '1870126640', '2199073307', '2079628090', '2153431772', '2137007015', '2140664286', '2295831392', '2056358033', '1999488524', '2121450351', '2022051893', '2158893323', '2124589507', '2026620973', '1984728305', '2085873825', '1538471738', '2035228076', '2249689471', '2051939371', '2750196609', '2132059402', '2132457274', '2003969069', '2069528898', '2096004644', '2172177376', '1503882286', '2102431335', '2054200614', '2126496326', '1992537691', '1693606250', '2137550341', '1577540072', '2527671709', '1934867474', '2134037853', '2069331971', '17853421', '1480636030', '2138615061', '2066839705', '2071421993', '1522407983', '2415101225', '2117914607', '2043581439', '2081650149', '1892035143', '1995021884', '1997050918', '1999100909', '2156509565', '128538854', '2053835505', '2314209725', '2145694984', '2146677475', '2231058294', '1608623199', '30563380', '125636640', '2034372240', '2137610654', '2524496318', '2008430350', '2048413388', '2148101809', '2135669762', '93633279', '1517839093', '2007387522', '1969118200', '1966746991', '2011102171', '2020688250', '2163917230', '2065654212', '1970966420', '1567424305', '1994765766', '1604605276', '2149229648', '2189317234', '100280358', '2092087780', '2243214635', '2023452885', '2051094927', '2046716077', '1895265432', '2057936889', '1985748929', '1511202040', '2038611739', '2064062818', '1515345627', '2058169920', '2418786089', '2089764936', '2020230579', '2092964147', '1594048431', '1986863662', '2006244857', '2126064438', '1987167328', '1987674826', '2064884560', '2076846692', '2027121965', '2101837251', '2020044049', '2469512196', '2501195497', '2151139811', '2118573410', '1957380691', '2391545620', '2106033751', '2043188771', '1975984759', '2019929699', '2100415402', '2155310036', '2025336657', '2036566245', '2123610710', '1527745642', '2117565842', '2161277778', '1899274600', '2067694054', '67864388', '1957565308', '2132800535', '2271152362', '1983239326', '1972993674', '2296172131', '2090426589', '2064500274', '2051702839', '2523147830', '1958214287', '2116671604', '2297172423', '2106501631', '2168992427', '2021113155', '2045546537', '1993423281', '2530171160', '2058670100', '2514435989', '2327949701', '2076761842', '2141304179', '1973169445', '1979425572', '125718686', '1985180563', '1982668309', '1999565015', '49823667', '1996668601', '1507819999', '2084412859', '2151244857', '2149985996', '2089940549', '2406668603', '2129804985', '1550222568', '144315047', '2026453394', '2128249074', '2199368783', '2002236639', '2001937826', '49955865', '2012826629', '49105012', '122300302', '2117737264', '1863651598', '2107496348', '2058183920', '33787415', '2141463947', '2133794652', '1693743017', '2099469999', '2001696757', '2039198210', '1569313149', '2061282509', '2014261312', '1905565989', '1809503925', '2107556781', '2316200131', '170666187', '2030729529', '2344528587', '1534113297', '2109610430', '1589459581', '2021540355', '2115703357', '1974565082', '37179887', '2112173604', '2043177116', '1999240723', '2148716566', '2042084395', '1039451517', '2095811183', '2081599679', '1994687768', '2312323507', '2044218167', '2017980367', '173658555', '2054502676', '2036766011', '2089954740', '1845857038', '1532506532', '2039902345', '2041378224', '646974269', '1806276855', '2135066173', '2021027913', '2015749957', '1875492742', '2003419127', '2135534478', '1546217776', '2100651907', '2064582380', '2115278023', '2100895754', '2079180608', '2021114013', '45843140', '210692298', '1578557524', '2101876181', '2161255262', '1968637703', '2045125201', '1998310911', '2165023209', '2625998997', '2055700678', '1517725903', '1534081457', '1967805867', '2054670274', '2224019690', '994408881', '2273201135', '1489942343', '2295621127', '2139561288', '804074387', '2100044506', '2034143862', '1990213251', '2132358780', '2038806095', '2083540743', '2345194886', '1869262883', '2153582254', '1593222916', '1997474067', '1980297089', '2226698714', '1667869507', '1988260160', '2128649708', '2149930668', '2067195910', '1507641037', '2079422739', '2233100538', '2065511825', '1992648060', '2016628573', '1598980475', '2158701819', '2028325042', '2138173706', '1995124264', '1971158684', '2124540951', '2034830958', '2118924626', '2137350540', '1594516666', '2048011346', '2137257218', '2071517159', '81224298', '1156767201', '2140301274', '1762460071', '2067918107', '1965923891', '1726196700', '2115429240', '2041935031', '2102542391', '1910136425', '2293130453', '1019509856', '2134253957', '1991570690', '2034553100', '2160109773', '1988626696', '2145888803', '2127668516', '2015481227', '2078000552', '1836507030', '1976407450', '1996702997', '1981713348', '2093775285', '1775103940', '2072535127', '2020945569', '1947468747', '2082381940', '2060302445', '315387429', '1998213992', '2175951243', '2027215042', '2029381583', '2048700485', '2133482182', '2308352102', '2053421183', '1605311420', '1985161468', '2301930590', '1989171659', '2145671010', '2914749043', '2080840757', '390146591', '2170264680', '2111510584', '2460314221', '2084979303', '2071463325', '1983782842', '55150864', '1542557450', '1528535269', '81099017', '1982132176', '2017456026', '2115427126', '2158004453', '2021894287', '1987565821', '2106235379'], 'bm25_doc_scores': [54.887108, 54.871124, 54.037968, 53.155262, 52.8118, 52.447777, 51.85626, 51.081963, 50.69254, 50.588024, 50.35134, 50.132057, 49.576546, 49.4254, 49.19435, 48.956367, 48.545544, 48.487984, 48.44037, 48.31627, 48.222412, 47.89611, 47.787403, 47.771957, 47.692837, 47.541893, 47.514412, 47.198414, 47.082245, 46.941235, 46.857056, 46.760918, 46.587593, 46.298115, 46.263153, 46.065987, 45.36272, 45.303753, 45.233444, 45.20282, 45.19985, 45.197422, 45.002304, 44.964054, 44.883198, 44.85768, 44.690834, 44.68381, 44.633194, 44.625996, 44.621403, 44.585205, 44.574963, 44.397102, 44.214558, 44.184143, 44.16917, 44.121235, 44.073475, 43.95491, 43.73105, 43.708767, 43.619755, 43.591488, 43.278893, 43.262825, 43.05237, 42.902294, 42.81341, 42.790245, 42.776283, 42.753693, 42.70134, 42.679985, 42.60148, 42.501667, 42.430935, 42.425, 42.425, 42.410324, 42.31349, 42.301945, 42.262844, 42.172764, 42.122147, 42.112476, 42.089985, 42.072777, 42.036816, 41.94306, 41.861343, 41.855686, 41.82356, 41.820396, 41.804558, 41.735916, 41.65226, 41.59366, 41.558376, 41.543922, 41.530865, 41.487347, 41.24045, 41.21756, 41.217236, 41.182568, 41.14978, 41.133728, 41.125587, 41.014675, 40.995937, 40.917877, 40.914833, 40.8889, 40.836636, 40.778984, 40.709206, 40.62023, 40.6038, 40.555073, 40.539074, 40.52274, 40.50665, 40.468147, 40.431416, 40.40512, 40.365906, 40.260574, 40.25336, 40.21374, 40.072105, 40.066334, 40.030636, 39.916363, 39.871925, 39.77759, 39.746307, 39.737747, 39.711933, 39.70921, 39.70362, 39.632, 39.619514, 39.56787, 39.533287, 39.408783, 39.39957, 39.362488, 39.360516, 39.23, 39.211716, 39.187775, 39.18739, 39.185196, 39.17501, 39.153168, 39.1248, 39.080048, 39.068, 39.043213, 39.035336, 39.00648, 38.99147, 38.963478, 38.916702, 38.90676, 38.88093, 38.871597, 38.844425, 38.74363, 38.739174, 38.73835, 38.69005, 38.663982, 38.633156, 38.626865, 38.600716, 38.584232, 38.518295, 38.510094, 38.496986, 38.49557, 38.466995, 38.444862, 38.39953, 38.39028, 38.385113, 38.35719, 38.353, 38.344402, 38.34033, 38.334763, 38.330574, 38.321472, 38.3125, 38.298473, 38.25125, 38.243107, 38.231133, 38.208416, 38.18811, 38.16093, 38.15722, 38.153667, 38.1325, 38.129726, 38.12194, 38.116432, 38.08331, 38.07402, 38.015934, 37.91614, 37.87251, 37.852615, 37.795826, 37.775684, 37.757065, 37.756744, 37.73851, 37.710712, 37.68835, 37.56691, 37.565308, 37.552654, 37.547455, 37.52867, 37.518906, 37.4827, 37.47473, 37.470543, 37.467567, 37.455727, 37.447365, 37.430798, 37.41305, 37.406048, 37.39376, 37.389534, 37.379826, 37.370686, 37.33454, 37.331844, 37.29645, 37.296116, 37.290016, 37.28676, 37.2562, 37.24738, 37.243176, 37.242043, 37.236286, 37.218967, 37.15484, 37.12, 37.03581, 37.017483, 36.983017, 36.97355, 36.959026, 36.9302, 36.899895, 36.880318, 36.841446, 36.84027, 36.83244, 36.826786, 36.812767, 36.803894, 36.8028, 36.79626, 36.78903, 36.781677, 36.77006, 36.758907, 36.749985, 36.74162, 36.739548, 36.73876, 36.712475, 36.70909, 36.705948, 36.699368, 36.66288, 36.65225, 36.648396, 36.643764, 36.636566, 36.625298, 36.61575, 36.592766, 36.581036, 36.578415, 36.561356, 36.546745, 36.52442, 36.515408, 36.511387, 36.500706, 36.497883, 36.480133, 36.468807, 36.441597, 36.41476, 36.383263, 36.334328, 36.333214, 36.332317, 36.317352, 36.290108, 36.289486, 36.274982, 36.273746, 36.249573, 36.23754, 36.207195, 36.198467, 36.198265, 36.196316, 36.18332, 36.173664, 36.168106, 36.162766, 36.152977, 36.151066, 36.148018, 36.146416, 36.124912, 36.08094, 36.06593, 36.028954, 36.013405, 36.011, 36.010517, 35.994415, 35.937416, 35.92946, 35.92499, 35.911907, 35.908623, 35.897976, 35.87047, 35.854305, 35.83425, 35.81371, 35.802658, 35.80077, 35.7871, 35.78432, 35.77283, 35.689705, 35.687984, 35.685005, 35.640705, 35.612885, 35.61245, 35.58723, 35.583733, 35.58092, 35.577354, 35.574593, 35.56907, 35.56651, 35.562424, 35.559395, 35.541378, 35.539307, 35.514965, 35.507893, 35.48478, 35.47844, 35.477455, 35.468414, 35.464554, 35.4614, 35.456387, 35.453575, 35.3761, 35.373543, 35.372818, 35.365177, 35.36242, 35.34231, 35.335888, 35.332676, 35.33233, 35.331814, 35.297653, 35.295017, 35.281322, 35.24532, 35.23783, 35.23077, 35.224987, 35.20455, 35.20432, 35.19076, 35.182625, 35.164196, 35.162, 35.14673, 35.137314, 35.11517, 35.059887, 35.056705, 35.03881, 35.014874, 35.01483, 34.956306, 34.93721, 34.933537, 34.921417, 34.868793, 34.848564, 34.832115, 34.80544, 34.79635, 34.795826, 34.77513, 34.773224, 34.735832, 34.732735, 34.69435, 34.68988, 34.68137, 34.648952, 34.63392, 34.630253, 34.620884, 34.60303, 34.60267, 34.594208, 34.587193, 34.577816, 34.57187, 34.517864, 34.51377, 34.500996, 34.489788, 34.47939, 34.46962, 34.44805, 34.42252, 34.416687, 34.382927, 34.379017, 34.37105, 34.362225, 34.36171, 34.356564, 34.321575, 34.31676, 34.315193, 34.312008, 34.305763, 34.303978, 34.28561, 34.26686, 34.254402, 34.250057, 34.23724, 34.232872, 34.21222, 34.20777, 34.198376, 34.196236, 34.19479, 34.19275, 34.185616, 34.178944, 34.165154, 34.156506, 34.14882, 34.14741, 34.146275, 34.12648, 34.115788, 34.115566, 34.101337, 34.094036, 34.0835, 34.056255, 34.05279, 34.050117, 34.043137, 34.035492, 34.03179, 34.02789, 34.018257, 33.97899, 33.952362, 33.94262, 33.938324, 33.92264, 33.907967, 33.90425, 33.898567, 33.89313, 33.891026, 33.8879, 33.873653, 33.834316, 33.831505, 33.796814, 33.795547, 33.790916, 33.785713, 33.784096, 33.78283, 33.781105, 33.76501, 33.764404, 33.760197, 33.760162, 33.75165, 33.749535, 33.74281, 33.735794, 33.733047, 33.716362, 33.714607, 33.710663, 33.70916, 33.7046, 33.703255, 33.702816, 33.702324, 33.670475, 33.665264, 33.64093, 33.63705, 33.635544, 33.63221, 33.63194, 33.617565, 33.616844, 33.587852, 33.58678, 33.582783, 33.560947, 33.55555, 33.536217, 33.525333, 33.52026, 33.501358, 33.47941, 33.46321, 33.446957, 33.438713, 33.423244, 33.416695, 33.411457, 33.363068, 33.35275, 33.33902, 33.33593, 33.32809, 33.325844, 33.321247, 33.317337, 33.30745, 33.303364, 33.29299, 33.28629, 33.284004, 33.275578, 33.27263, 33.27053, 33.265514, 33.25825, 33.251514, 33.243385, 33.23574, 33.215244, 33.209484, 33.2, 33.195103, 33.192036, 33.178013, 33.16828, 33.1679, 33.158398, 33.15652, 33.156315, 33.14615, 33.135517, 33.135, 33.13214, 33.12821, 33.124603, 33.120575, 33.117855, 33.117794, 33.113827, 33.109684, 33.0953, 33.0915, 33.090435, 33.08508, 33.083412, 33.075634, 33.07331, 33.05565, 33.051205, 33.040462, 33.030396, 33.024654, 33.009438, 33.00819, 33.00052, 32.99843, 32.98061, 32.965557, 32.93699, 32.933796, 32.933434, 32.903908, 32.902283, 32.890076, 32.88577, 32.885536, 32.880974, 32.87034, 32.867867, 32.857357, 32.85566, 32.849937, 32.83292, 32.83117, 32.8195, 32.81113, 32.777416, 32.774506, 32.764805, 32.71823, 32.714653, 32.712036, 32.68731, 32.682327, 32.681698, 32.661552, 32.65486, 32.652676, 32.64635, 32.62905, 32.6271, 32.62519, 32.61269, 32.58733, 32.584362, 32.578537, 32.571407, 32.56513, 32.557083, 32.538162, 32.53343, 32.53329, 32.531643, 32.520058, 32.512978, 32.51117, 32.503616, 32.483814, 32.479164, 32.475075, 32.467236, 32.423534, 32.41832, 32.383198, 32.36924, 32.36849, 32.339523, 32.335667, 32.328236, 32.319763, 32.301098, 32.29981, 32.291946, 32.289845, 32.273636, 32.243366, 32.227894, 32.226833, 32.22603, 32.213253, 32.20664, 32.12524, 32.11573, 32.11282, 32.0807, 32.06997, 32.062828, 32.054222, 32.049706, 32.029377, 32.009525, 31.992579, 31.986105, 31.979074, 31.978926, 31.971237, 31.957182, 31.951508, 31.945805, 31.930847, 31.930693, 31.92156, 31.887384, 31.882097, 31.880543, 31.878378, 31.87697, 31.875175, 31.851862, 31.848576, 31.845686, 31.841705, 31.840889, 31.82608, 31.82488, 31.819508, 31.797062, 31.788656, 31.771065, 31.751738, 31.747736, 31.745337, 31.740154, 31.735426, 31.730265, 31.722326, 31.719812, 31.719418, 31.719301, 31.717978, 31.707623, 31.69962, 31.698696, 31.695751, 31.678915, 31.666588, 31.640678, 31.639341, 31.616154, 31.592268, 31.58303, 31.569204, 31.560879, 31.551647, 31.541994, 31.531813, 31.524193, 31.52084, 31.514194, 31.513992, 31.508545, 31.504719, 31.496061, 31.494865, 31.494192, 31.47937, 31.470306, 31.467339, 31.456234, 31.441029, 31.436155, 31.412594, 31.40715, 31.39799, 31.38614, 31.383404, 31.382397, 31.378134, 31.374557, 31.367529, 31.357044, 31.339455, 31.33342, 31.32549, 31.322636, 31.31697, 31.313854, 31.311514, 31.290178, 31.284199, 31.282726, 31.27404, 31.272581, 31.272057, 31.264765, 31.254833, 31.254217, 31.247286, 31.246948, 31.236933, 31.2339, 31.233315, 31.227383, 31.21206, 31.206112, 31.19612, 31.191978, 31.19041, 31.187452, 31.184109, 31.171797, 31.171272, 31.160734, 31.157352, 31.154064, 31.1378, 31.127254, 31.109776, 31.107828, 31.079575, 31.071724, 31.064442, 31.062365, 31.041965, 31.041216, 31.038082, 31.027258, 31.020168, 31.012978, 31.009895, 31.009, 31.008314, 31.000715, 31.000288, 30.992052, 30.988945, 30.9814, 30.975443, 30.96939, 30.963364, 30.93416, 30.92492, 30.922632, 30.91886, 30.917032, 30.910267, 30.907154, 30.90663, 30.89244, 30.886282, 30.880066, 30.876774, 30.866724, 30.844992, 30.837069, 30.829779, 30.82697, 30.81998, 30.819523, 30.815228, 30.812695, 30.81254, 30.80943, 30.806093, 30.80132, 30.778387, 30.776186, 30.773478, 30.773201, 30.76523, 30.76515, 30.759544, 30.758467, 30.753593, 30.741594, 30.737122, 30.734327, 30.73021, 30.714462, 30.68718, 30.67195, 30.671595, 30.670162, 30.666073, 30.658333, 30.654718, 30.621122, 30.60965, 30.60504, 30.58816, 30.584478, 30.58239, 30.57909, 30.57864, 30.57741, 30.571848, 30.571804, 30.56538, 30.552011, 30.54731, 30.538208, 30.5236, 30.519854, 30.50963, 30.504215, 30.497013, 30.489037, 30.466507, 30.464052, 30.461931, 30.458757, 30.457672, 30.456171, 30.447874, 30.443214, 30.437967, 30.436356, 30.433672, 30.42696, 30.413305, 30.41197, 30.406065, 30.405432, 30.403797, 30.402744, 30.39087, 30.390184, 30.380564, 30.372616, 30.365135, 30.364635, 30.355484, 30.3414, 30.331823, 30.32804, 30.326931, 30.304665, 30.302801, 30.286854, 30.262379, 30.259764, 30.247143, 30.232126, 30.231133, 30.22959, 30.205893, 30.202314, 30.194944, 30.18866, 30.168068, 30.154114, 30.140501, 30.135963, 30.125143, 30.122349, 30.117569, 30.110638, 30.110039, 30.087538, 30.086311, 30.07646, 30.073454, 30.073448, 30.068872, 30.049814, 30.049435, 30.041142, 30.036573, 30.033478, 30.031181, 30.026608, 30.020529, 30.016048, 30.013596, 30.00933, 30.007412, 30.003147, 29.992565, 29.985975, 29.985306, 29.979153, 29.971783, 29.969147, 29.95844, 29.953045, 29.944345, 29.942316, 29.933445, 29.929655, 29.926174, 29.923664, 29.92246, 29.91591, 29.91523, 29.912735, 29.912508, 29.905361, 29.90494, 29.902708, 29.900738, 29.898151, 29.895012, 29.887405, 29.88442, 29.881502, 29.868107]}]\n"
          ]
        }
      ],
      "source": [
        "#print(test_queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96bPrXx0EnaU"
      },
      "source": [
        "# Personalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bp3kRfLTy5el",
        "outputId": "bd2de218-f53a-49cf-a24e-c0f30022f5f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to split a string into chunks of 32 tokens\n",
        "def chunk_user_profile(profile, chunk_size=16):\n",
        "    tokens = word_tokenize(profile)  # Tokenize the string\n",
        "    chunks = [\" \".join(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
        "    #print(chunks)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFlEDpSJy1Eq"
      },
      "source": [
        "Computing Cosine Similarity between Query and Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "ba831eb36c674873a9dba49c37b87516",
            "c9f1a6f96f814b269a40fe1454d8e505",
            "8e461d3366dc40a5a80933ae72b4b683",
            "08b2ac974b154a8b94c76e7e1fb10c7b",
            "e7f1986eea8244c6a88251a388841d24",
            "0073228917f74302baf43efe5bd742c1",
            "3fc3fd306cc14102876836b222439e22",
            "796743bff4c145e8bc643e7fabe6c4b4",
            "c67841f8750e4e6eb2ba03a7e232977a",
            "628784e6fc6040088c5c0e96db410a9a",
            "34f5ce3a01d04ebaadadd877322b2ba5",
            "d354a6958da8429d8d6d40e83b4f99e9",
            "2c3d77ed74f74088a851450565228434",
            "d2c26d7e72c24de6ac217de2393b29af",
            "fcc0e679160c48c791e1b154de7e5a4a",
            "7e1f3c2a7c3e450e8131067c209f736c",
            "4f6d9c1e6e5e427d82165a295ef958ea",
            "fd3fbee31bc542eb813618e6b309b672",
            "95e220dd003d45609b641c8258d1d834",
            "0fed2e60c5294198825a7ac44ed5f805",
            "4709de63700f41bdb9103cd01f802a35",
            "93b45e4bebfe48978fb1aed1a37031c5",
            "4802459c87224ca684a32a9c812a5271",
            "561ddd8f8809442b8d6d618880762478",
            "480fc22a3da549eaa702615177988883",
            "f5c387a6e98740a885112a364a4c7324",
            "8f56473d2dcf4903aed92520cb4cb0f0",
            "a113adf82ab042c3b96ecb019833d73a",
            "70ffe74aad3e4a66b22d92e2811cc210",
            "2825d3220c2244f9a0fba8ad0e605b05",
            "e5551100c0c946f09ee413f30f0128ee",
            "ef0b491aec8f46dcae24062f7299bea3",
            "31e5ed96cffa400980bf37ed1e07e03d",
            "6294e09067304806aad4d62d5167c0db",
            "949075a702f14156a18b82b7a934880f",
            "f278812d0fe0402a949fa37e76e461e1",
            "855792a81703413d8bc9a20ed30fb694",
            "fa218d35e8d34ede888d21450d806c1d",
            "90e9008381e949368f443950016a3e7f",
            "6a026e87706e4e6cada4b1cfeac71ef0",
            "d8cfaa787cf64b2abf9b0a1a9a15d202",
            "05ddf94a80d040d2b52d8dc76f97e6fb",
            "bfbbc09512474f3d9052fd5d5a109f5b",
            "49b17870790e417bb36c51f1b0956b06",
            "02ca9f7e82de41b0996dee89301f58ca",
            "083fe28644874503af36fb580146efd8",
            "9d9fcfd961be427594c73e4ec7a0131d",
            "b4ccef1f28f04d05994020abf812ed93",
            "adc6f70a8990493bae7f8c4ce0427f59",
            "a4fd6f36d2af4b55a6169e2b268f885a",
            "88e400d29f6f4ca0a45986dcd068fc56",
            "7bfeeaf69caf44e687335acbe7c44b93",
            "43fbf16f96d243519f2bc5db11c3b3d0",
            "3cb447f0d7f84c8eb434940aeb204b17",
            "d32f3f43bb2e485aa1878edfc7b65451",
            "f3021e45ac2f4eb9be77f80a391d67b4",
            "e7d95c36c7d843a7b9665188d7611344",
            "e1e48e191c6647a08dfb993731bb1270",
            "c61addd7f8474fb8afdaea011fbc1a7e",
            "fe86cdf94ac34a90a20ea967f2364055",
            "a4532e355cef4cd1915eeacb00840526",
            "9224edc8453642e7b98c1f7ba4e749bb",
            "cfae027f81eb4a8db01e3c79e1c89b1d",
            "e890215a1d24408a8a4fd7a90eec24df",
            "2f125b9434e0469ca78b613ecb808f41",
            "01fa5f5e50fa419382e7a416af7cad5c",
            "25a0d656c40544c18b88cd8c27d8e8bb",
            "8d1cbdbd04ed4b9f9b5fceb7cf18f05e",
            "e1f60a3318f948908f854b332090a59f",
            "32103c2998cf4f83bd60befa5c32b453",
            "7df5f630de714d8a8b0774ba3546b73c",
            "ed42476645b043afa761caf4086e4d4e",
            "3b7c81f445bb4b40ae648e6c8e1be187",
            "72dc1a59af99455a91f14eba83576f2e",
            "fbbfc9f09407493c981d948cc787aba4",
            "b9fac7ee5f2648db866b34f72b2283e2",
            "d6c044900011400ca60573099a655ad9",
            "00d02863e63f448991794c7263eba5b5",
            "eecc82cf4b02416c9411c4baeea42e2a",
            "239c8b4809144937884ad86ab485c055",
            "ab366659bdb246819743d3240dcdacdc",
            "d8e083a8a22649e8bf63996040ada5b5",
            "5f354f5642754f18b0e1d44a40ad68f7",
            "e551e5adc1fc4a40ab56e4cff122eec5",
            "46cf0ebbfd9c4018ab89d8563f3c0c11",
            "f29646c4d9fe404db85721964c91a471",
            "cbf97e55f28a495ba04b66857e043bf8",
            "5ca48d66f3d54712b7b2525fccd5cd90",
            "522c7a19c30245bbac4b5e6579c51de9",
            "0995c46b27d0452ab6bd71bb1d5b3031",
            "1c0cbc48cd6e46cea569202269a5afd3",
            "1700cb48b2c54da9bf3867053255e3e7",
            "3c9001b697714a5a85da634627de8499",
            "a7b54ce2928b4f8d8cb76c0b5d5eb92e",
            "6bbf621b439242408ee996389aced477",
            "992453be32c1484882717143783bc019",
            "b9da7e941226461da105d20cac16eb4d",
            "76e4aeb581be416384a4b0a16283c966",
            "76025d88a2434dab8115197003405944",
            "feb0c1b61b32496c885c22be3dedf597",
            "aa610e02da954588b66a543bfef72e36",
            "68ed3ef5ce054972bd857f08295f5ed7",
            "f8d0a1d43e554dddb387182a1c53ca44",
            "fc49d739553d4597862d996196955b0a",
            "86830aea2e994ff6964dd9e0c9b6ec01",
            "6fcdaa7d4a97428d9c6e7c99e3e74741",
            "b7c28855692e4bbe9e44ee801ed599c3",
            "d8ee38eacf624790bae1978bf70723d9",
            "d46cadc1c41a421695c874f5fc7b718b",
            "e62423b401564489b162093e5311bd9c",
            "081487afb0ab497691e9c717fab47834",
            "1f0710849bdd4e53bcbfe5cb58a8e248",
            "28de1250114845079835585c6c90096a",
            "159278146dcd43aa97b5e14cb60b8604",
            "1d818ed00c224ec78dcd66e621628400",
            "70182f690898491492baff709f7d594b",
            "578427bb3ddc4ee5be6f37f50492a7be",
            "a13c66cb02ac45af9279c14a04cd9997",
            "58c5e065fac74d28ad8e7de41c4be977",
            "c8931190f18b45c8b6d1bbd4a698c33e",
            "a10b45553f8e411c80dbd8c897976ac1"
          ]
        },
        "id": "9gvpyCe9yz8e",
        "outputId": "476ed339-87f7-4bad-a1ae-c81387727d24"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba831eb36c674873a9dba49c37b87516"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d354a6958da8429d8d6d40e83b4f99e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4802459c87224ca684a32a9c812a5271"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6294e09067304806aad4d62d5167c0db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02ca9f7e82de41b0996dee89301f58ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3021e45ac2f4eb9be77f80a391d67b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25a0d656c40544c18b88cd8c27d8e8bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00d02863e63f448991794c7263eba5b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "522c7a19c30245bbac4b5e6579c51de9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "feb0c1b61b32496c885c22be3dedf597"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "081487afb0ab497691e9c717fab47834"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load pre-trained BERT-based model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def cos_similarity(chunk, query):\n",
        "  # Generate embeddings\n",
        "  embedding1 = model.encode(chunk).reshape(1, -1)\n",
        "  embedding2 = model.encode(query).reshape(1, -1)\n",
        "\n",
        "  # Compute cosine similarity\n",
        "  similarity = cosine_similarity(embedding1, embedding2)\n",
        "\n",
        "  #print(f\"Cosine Similarity: {similarity[0][0]:.4f}\")\n",
        "  return similarity\n",
        "# Sample texts\n",
        "text1 = \"Rumor verification is important for fake news detection.\"\n",
        "text2 = \"Detecting fake news requires strong rumor verification techniques.\"\n",
        "cos_sim = cos_similarity(text1,text2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNC7HCoWsujW"
      },
      "source": [
        "### Testing Queries - Entire Profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "id": "vOQk9fMnFec5"
      },
      "outputs": [],
      "source": [
        "# Main function to process queries and rerank candidate lists using RAG search\n",
        "#TOP 1000 CANIDATE DOCS RERANKED ONLY\n",
        "import numpy as np\n",
        "def process_queries1(queries, user_profiles):\n",
        "    query_results = {}\n",
        "\n",
        "    sim_score_means = list()\n",
        "    total_sim_scores = 0\n",
        "    total_chunks = 0\n",
        "\n",
        "    all_chunks_count = list()\n",
        "    all_relevant_chunks_count_globalMean = list()\n",
        "    all_relevant_chunks_count_localMean = list()\n",
        "\n",
        "    for query in queries:\n",
        "        user_id = query[\"user_id\"]\n",
        "        if user_id not in user_profiles:\n",
        "            continue\n",
        "\n",
        "        '''print(\"Query :\", query['text'])'''\n",
        "\n",
        "        q_chunks_sim_scores = list()\n",
        "\n",
        "        user_profile = user_profiles[user_id]\n",
        "        profile_chunks = chunk_user_profile(user_profile)\n",
        "        '''print(f\"Profile chunks: {profile_chunks}\")'''\n",
        "        candidate_docIDs = query[\"bm25_doc_ids\"][:1000]\n",
        "\n",
        "        # Initialize a list to store results for all chunks\n",
        "        chunk_results = []\n",
        "\n",
        "        # Search for each chunk\n",
        "        for i, chunk in enumerate (profile_chunks):\n",
        "            sim_score = cos_similarity(chunk, query['text'])\n",
        "            q_chunks_sim_scores.append(sim_score)\n",
        "            #chunk_results.extend(RAG.search(chunk, doc_ids=candidate_docIDs))\n",
        "\n",
        "        for i, chunk in enumerate (profile_chunks):\n",
        "          #if q_chunks_sim_scores[i] >= np.mean(q_chunks_sim_scores):\n",
        "          if q_chunks_sim_scores[i] > 0.3:\n",
        "            chunk_results.extend(RAG.search(chunk, doc_ids=candidate_docIDs))\n",
        "            #print(\"Chunk result:\", RAG.search(chunk, doc_ids=candidate_docIDs))\n",
        "            #print(f\"Chunk {i} : {chunk}\")\n",
        "\n",
        "        '''\n",
        "        # Search for each chunk\n",
        "        for chunk in enumerate (profile_chunks):\n",
        "            sim_score = cos_similarity(chunk, query['text'])\n",
        "            if sim_score > 0.1727:\n",
        "              chunk_results.extend(RAG.search(chunk, doc_ids=candidate_docIDs))\n",
        "            #chunk_results.extend(RAG.search(chunk, doc_ids=candidate_docIDs))\n",
        "        '''\n",
        "        '''\n",
        "        print(f\"Cosine similarity scores: {q_chunks_sim_scores}\")\n",
        "        print(f\"Mean similarity:{np.mean(q_chunks_sim_scores)}, Max similarity: {np.max(q_chunks_sim_scores)}, min:{np.min(q_chunks_sim_scores)} Count: {len(q_chunks_sim_scores)}\")\n",
        "        '''\n",
        "\n",
        "        sim_score_means.append(np.mean(q_chunks_sim_scores))\n",
        "        total_sim_scores += np.sum(q_chunks_sim_scores)\n",
        "        total_chunks += len(q_chunks_sim_scores)\n",
        "\n",
        "        relevant_chunks_count1 = sum(1 for score in q_chunks_sim_scores if score > 0.1727)\n",
        "        relevant_chunks_count2 = sum(1 for score in q_chunks_sim_scores if score > np.mean(q_chunks_sim_scores))\n",
        "\n",
        "        all_relevant_chunks_count_globalMean.append(relevant_chunks_count1)\n",
        "        all_relevant_chunks_count_localMean.append(relevant_chunks_count2)\n",
        "\n",
        "        all_chunks_count.append(len(q_chunks_sim_scores))\n",
        "\n",
        "        # Aggregate results by taking the maximum score per document_id\n",
        "        max_score_dict = {}\n",
        "        for result in chunk_results:\n",
        "            #print(result)\n",
        "            doc_id = result[\"document_id\"]\n",
        "            score = result[\"score\"]\n",
        "            if doc_id not in max_score_dict or score > max_score_dict[doc_id][\"score\"]:\n",
        "                max_score_dict[doc_id] = result\n",
        "\n",
        "        # Convert dict back to a list and sort by score descending\n",
        "        aggregated_results = sorted(max_score_dict.values(), key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "        # Store final list\n",
        "        query_results[query[\"id\"]] = {\n",
        "            \"sorted_CandiList\": [{\"docID\": doc[\"document_id\"], \"rank\": rank+1, \"score\": doc[\"score\"]}\n",
        "                                   for rank, doc in enumerate(aggregated_results)],\n",
        "            \"original_rel_doc_ids\": query[\"rel_doc_ids\"]\n",
        "        }\n",
        "        #print(query[\"rel_doc_ids\"])\n",
        "        #break\n",
        "    '''\n",
        "    print(f\"Queries average cosine scores: {sim_score_means}, mean of means: {np.mean(sim_score_means)}\")\n",
        "    print(f\"All Queries Macro cosine scores mean: {(total_sim_scores/total_chunks):.4f}\")\n",
        "    print(f\"All relevant chunks count GlobalMean: {all_relevant_chunks_count_globalMean}\")\n",
        "    print(f\"All relevant chunks count LocalMean: {all_relevant_chunks_count_localMean}\")\n",
        "    print(f\"All chunks count before filter: {all_chunks_count}\")\n",
        "    '''\n",
        "    return query_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "L7QbHkOM8g7T"
      },
      "outputs": [],
      "source": [
        "# Main function to process queries and rerank candidate lists using RAG search\n",
        "#TOP 1000 CANIDATE DOCS RERANKED ONLY\n",
        "def process_queries(queries, user_profiles):\n",
        "    query_results = {}\n",
        "    for query in queries:\n",
        "        user_id = query[\"user_id\"]\n",
        "        if user_id not in user_profiles:\n",
        "            continue\n",
        "\n",
        "        user_profile = user_profiles[user_id]\n",
        "        profile_chunks = chunk_user_profile(user_profile)\n",
        "        candidate_docIDs = query[\"bm25_doc_ids\"][:1000]\n",
        "\n",
        "        # Initialize a list to store results for all chunks\n",
        "        chunk_results = []\n",
        "\n",
        "        # Search for each chunk\n",
        "        for chunk in profile_chunks:\n",
        "            chunk_results.extend(RAG.search(chunk, doc_ids=candidate_docIDs))\n",
        "            #print(\"Chunk result:\", RAG.search(chunk, doc_ids=candidate_docIDs))\n",
        "\n",
        "        # Aggregate results by taking the maximum score per document_id\n",
        "        max_score_dict = {}\n",
        "        for result in chunk_results:\n",
        "            #print(result)\n",
        "            doc_id = result[\"document_id\"]\n",
        "            score = result[\"score\"]\n",
        "            if doc_id not in max_score_dict or score > max_score_dict[doc_id][\"score\"]:\n",
        "                max_score_dict[doc_id] = result\n",
        "\n",
        "        # Convert dict back to a list and sort by score descending\n",
        "        aggregated_results = sorted(max_score_dict.values(), key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "        # Store final list\n",
        "        query_results[query[\"id\"]] = {\n",
        "            \"sorted_CandiList\": [{\"docID\": doc[\"document_id\"], \"rank\": rank+1, \"score\": doc[\"score\"]}\n",
        "                                   for rank, doc in enumerate(aggregated_results)],\n",
        "            \"original_rel_doc_ids\": query[\"rel_doc_ids\"]\n",
        "        }\n",
        "        #print(query[\"rel_doc_ids\"])\n",
        "        #break\n",
        "    return query_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ySKNvOo15H",
        "outputId": "02f6f3f5-4095-4432-bb2c-92005e3e4946",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query : construct gazetteer volunteer big geo data base hadoop\n",
            "Profile chunks: ['Trajectory data have been used in a variety of studies , including human behavior analysis ,', 'transportation management , and wildlife tracking . While each study area introduces a different perspective ,', 'they share the need to integrate positioning data with domain-specific information . Semantic annotations are necessary', 'to improve discovery , reuse , and integration of trajectory data from different sources . Consequently', ', it would be beneficial if the common structure encountered in trajectory data could be annotated', 'based on a shared vocabulary , abstracting from domain-specific aspects . Ontology design patterns are an', 'increasingly popular approach to define such flexible and self-contained building blocks of annotations . They appear', 'more suitable for the annotation of interdisciplinary , multi-thematic , and multi-perspective data than the use', 'of foundational and domain ontologies alone . In this paper , we introduce such an ontology', 'design pattern for semantic trajectories . It was developed as a community effort across multiple disciplines', 'and in a data-driven fashion . We discuss the formalization of the pattern using the Web', 'Ontology Language ( OWL ) and apply the pattern to two different scenarios , personal travel', 'and wildlife monitoring . Metadata for scientific publications contain various explicit and implicit spatio-temporal references .', 'Data on conference locations as well as author and editor affiliations – both changing over time', '– enable insights into the geographic distribution of scientific fields and particular specializations . At the', 'same time , these byproducts of scientific bibliographies offer a great opportunity to integrate data across', 'different bibliographies to get a more complete picture of a domain . In this paper ,', 'we demonstrate how the Linked Data paradigm can assist in enriching and integrating such collections .', 'Starting from the bibliographies of the GIScience , COSIT , ACM GIS , and AGILE conference', 'series , we show how to convert the data to Linked Data and integrate the previously', 'separate datasets . We focus on the spatio-temporal aspects and discuss how they help in matching', 'and disambiguating entities such as authors or universities . We introduce a novel user interface to', 'explore the integrated dataset , demonstrating the potential of Linked Data for innovative applications using spatio-temporal', 'information , and discuss how more complex queries can be addressed . While we focus on', 'bibliographies , the presented work is part of the broader vision of a Linked Science infrastructure', 'for e-Science . In this document , we give a high-level overview of selected Semantic (', 'Web ) technologies , methods , and other important considerations , that are relevant for the', 'success of EarthCube . The goal of this initial document is to provide entry points and', 'references for discussions between the Semantic Technologies experts and the domain experts within EarthCube . The', 'selected topics are intended to ground the EarthCube roadmap in the state of the art in', 'semantics research and ontology engineering . The Semantic Web journal by IOS Press follows a unique', 'open and transparent process during which each submitted manuscript is available online together with the full', 'history of its successive decision statuses , assigned editors , solicited and voluntary reviewers , their', \"full text reviews , and in many cases also the authors ' response letters . Combined\", 'with a highly-customized , Drupal-based journal management system , this provides the journal with semantically rich', 'manuscript time lines and networked data about authors , reviewers , and editors . These data', 'are now exposed using a SPARQL endpoint , an extended Bibo ontology , and a modular', 'Linked Data portal that provides interactive scientometrics based on established and new analysis methods . The', 'portal can be customized for other journals as well . Computing user similarity is key for', 'personalized location-based recommender systems and geographic information retrieval . So far , most existing work has', 'focused on structured or semi-structured data to establish such measures . In this work , we', 'propose topic modeling to exploit sparse , unstructured data , e.g. , tips and reviews ,', 'as an additional feature to compute user similarity . Our model employs diagnosticity weighting based on', 'the entropy of topics in order to assess the role of commonalities and variabilities between similar', 'users . Finally , we offer a validation technique and results using data from the location-based', 'social network Foursquare . Semantic technologies and ontologies play an increasing role in scientiﬁc workﬂow systems', 'and knowledge infrastructures . While ontologies are mostly used for the semantic annotation of metadata ,', 'semantic technologies enable searching metadata catalogs beyond simple keywords , with some early evidence of semantics', 'used for data translation . However , the next generation of distributed and interdisciplinary knowledge infrastructures', 'will require capabilities beyond simple subsumption reasoning over subclass relations . In this work , we', 'report from the EarthCube Semantics Community by highlighting which role semantics and ontologies should play in', 'the EarthCube knowledge infrastructure . We target the interested domain scientist and , thus , introduce', 'the value proposition of semantic technologies in a non-technical language . Finally , we commit ourselves', 'to some guiding principles for the successful implementation and application of semantic technologies and ontologies within', 'EarthCube . The Linked Data paradigm has made significant inroads into research and practice around spatial', 'information and it is time to reflect on what this means for GIScience . Technically ,', 'Linked Data is just data in the simplest possible data model ( that of triples )', ', allowing for linking records or data sets anywhere across the web using controlled semantics .', 'Conceptually , Linked Data offers radically new ways of thinking about , structuring , publishing ,', 'discovering , accessing , and integrating data . It is of particular novelty and value to', 'the producers and users of geographic data , as these are commonly thought to require more', 'complex data models . The paper explains the main innovations brought about by Linked Data and', 'demonstrates them with examples . It concludes that many longstanding problems in GIScience have become approachable', 'in novel ways , while new and more specific research challenges emerge . The Sensor Web', \"provides access to observations and measurements through standardized interfaces defined by the Open Geospa-tial Consortium 's\", 'Sensor Web Enablement ( SWE ) initiative . While clients compliant to these standards have access', 'to the generated sensor data , it remains partially hidden from other knowledge infrastructures building on', 'higher-level W3C standards . To overcome this problem , it has been proposed to make sensor', 'data accessible using Linked Data principles and RESTful services . This position paper discusses the embedding', 'of such data into the Linked Data cloud with a focus on the outgoing links that', 'hook them up with other data sources . We outline how such links can be generated', 'in a semi-automatic way , and argue why curation of the links is required . Finally', ', we point to the query potential of such an additional interface to observation data ,', 'and outline the requirements for SPARQL endpoints . Abstract While geocoding returns coordinates for a full', 'or partial address , the converse process of reverse geocoding maps coordinates to a set of', 'candidate place identifiers such as addresses or toponyms . For example , numerous Web APIs map', 'geographic point coordinates , e.g. , from a user ’ s smartphone , to an ordered', 'set of nearby Places Of Interest ( POI ) . Typically , these services return the', 'k nearest POI within a certain radius and measure distance to order the results . Reverse', 'geocoding is a crucial task for many applications and research questions as it translates between spatial', 'and platial views on geographic location . What makes this process difficult is the uncertainty of', 'the queried location and of the point features used to represent places . Even if both', 'could be determined with a high level of accuracy , it would still be unclear how', 'to map a smartphone ’ s GPS fix to one of many possible places in a', 'multi-story building or a shopping mall . In this work , we break up the dependency', 'on space alone by introducing time as a second variable for reverse geocoding . We mine', 'the geosocial behavior of users of online location-based social networks to extract temporal semantic signatures .', 'In analogy to the notion of scale distortion in cartography , we present a model that', 'uses these signatures to distort the location of POI relative to the query location and time', ', thereby reordering the set of potentially matching places . We demonstrate the strengths of our', 'method by evaluating it against a purely spatial baseline by determining the Mean Reciprocal Rank and', 'the normalized Discounted Cumulative Gain . Our method performs substantially better than said baseline . Sensor', 'observations are usually offered in relation to a specific purpose , e.g. , for reporting fine', 'dust emissions , following strict procedures , and spatio-temporal scales . Consequently , the huge amount', \"of data gathered by today 's public and private sensor networks is most often not reused\", 'outside of its initial creation context . Fostering the reusability of observations and derived applications calls', 'for ( i ) spatial , temporal , and thematic aggregation of measured values , and', '( ii ) easy integration mechanisms with external data sources . In this paper , we', 'investigate how work on sensor observation aggregation can be incorporated into a Linked Data framework focusing', 'on external linkage as well as provenance information . We show that Linked Data adds new', 'aspects to the aggregation problem , e.g. , whether external links from one of the original', 'observations can be preserved for the aggregate . The Stimulus-Sensor-Observation ( SSO ) ontology design pattern', 'is extended by classes and relations necessary to model the aggregation of sensor observations . ArcGIS', 'Online is a unified Web portal designed by Environment System Research Institute ( ESRI ) .', 'It contains a rich collection of Web maps , layers , and services contributed by GIS', 'users throughout the world . The metadata about these GIS resources reside in data silos that', 'can be accessed via a Web API . While this is sufficient for simple syntax-based searches', ', it does not support more advanced queries , e.g. , finding maps based on the', 'semantics of the search terms , or performing customized queries that are not pre-designed in the', 'API . In metadata , titles and descriptions are commonly available attributes which provide important information', 'about the content of the GIS resources . However , such data can not be easily', 'used since they are in the form of unstructured natural language . To address these difficulties', ', we combine data-driven techniques with theory-driven approaches to enable semantic search and knowledge discovery for', 'ArcGIS Online . We develop an ontology for ArcGIS Online data , convert the metadata into', 'Linked Data , and enrich the metadata by extracting thematic concepts and geographic entities from titles', 'and descriptions . Based on a human participant experiment , we calibrate a linear regression model', 'for semantic search , and demonstrate the flexible queries for knowledge discovery that are not possible', 'in the existing Web API . While this research is based on the ArcGIS Online data', ', the presented methods can also be applied to other GIS cloud services and data infrastructures', '. EarthCube is a major effort of the National Science Foundation to establish a next-generation knowledge', 'architecture for the broader geosciences . Data storage , retrieval , access , and reuse are', 'central parts of this new effort . Currently , EarthCube is organized around several building blocks', 'and research coordination networks . OceanLink is a semanticsenabled building block that aims at improving data', 'retrieval and reuse via ontologies , Semantic Web technologies , and Linked Data for the ocean', 'sciences . Cruises , in the sense of research expeditions , are central events for ocean', 'scientists . Consequently , information about these cruises and the involved vessels has to be shared', 'and made retrievable . For example , the ability to find cruises in the vicinity of', 'physiographic features of interest , e.g. , a hydrothermal vent field or a fracture zone ,', 'is of primary interest for oceanographers . In this paper , we use a design pattern-centric', 'strategy to engineer ontologies for OceanLink . We provide a formal axiomatization of the introduced patterns', 'and ontologies using the Web Ontology Language , explain design choices , discuss the re-usability of', 'our models , and provide lessons learned for the future geo-ontologies . The Semantic Web journal', 'relies on a non-standard review process which in particular features open and transparent reviews and encourages', 'reviewers to disclose their identities [ 1,2 ] . While the continued success of the Semantic', 'Web journal shows that this setup works in practice and is suitable for a high-quality journal', ', the overhead caused by the non-standard process has been significant , in particular for the', 'editors and the editors-in-chief . The main reason for this overhead was the unavailability of suitable', 'manuscript review systems , as existing systems support only standard processes and are merely customizable within', 'rigid and conventional bounds . For more than two years we thus had a rather cumbersome', 'setup using , on the one hand , a commercial journal review management system , and', 'on the other hand a public Website [ 2 ] . As a result , we', 'had to constantly copy information from the review management system to the Website , and in', 'fact we have done so manually . It was clear from the beginning that this was', 'not going to be a solution which is sustainable over the long run , and so', 'it was necessary to set up our own manuscript review system , tailored to the specific', 'needs of the journal . In particular , we needed a tight integration of the review', 'system with the journal ’ s website , so that it would no longer be necessary', 'to handle two independent systems . The new review system went online in October 2012 ,', 'and it has since served us very well . It is realized in Drupal , the', 'popular Web Content Management System which underlies the Semantic Web journal website . Visitors of the', 'website may in fact hardly notice a difference . However , authors who want to submit', 'papers do so directly via the journal ’ s website , rather than through a separate', 'submission site . Likewise , reviewers and editors interact directly with the website . While we', 'are very pleased with the improvements resulting from the custom system , we will continue to', 'develop it , and are happy about any feedback or comments we may receive . For', '2013 we plan to embed Semantic Web technologies and thus support new types of queries over', 'the journal ’ s dataset . In this issue of the journal , we present three', 'papers . The first two are very substantial surveys of current research topics , and they', 'continue our series of highly selective and well-received high-quality surveys published in the journal . The', 'third article is a vision paper which argues for a systematic investigation of order ( in', 'the mathematical sense ) for reasoning in the Big Data age . While we usually do', 'not publish vision papers , we are happy to include this article of exceptional quality .', 'It was fully reviewed to the standards of the journal . Shared reference is an essential', 'aspect of meaning . It is also indispensable for the semantic web , since it enables', 'to weave the global graph , i.e. , it allows different users to contribute to an', 'identical referent . For example , an essential kind of referent is a geographic place ,', 'to which users may contribute observations . We argue for a human-centric , operational approach towards', 'reference , based on respective human competences . These competences encompass perceptual , cognitive as well', 'as technical ones , and together they allow humans to inter-subjectively refer to a phenomenon in', 'their environment . The technology stack of the semantic web should be extended by such operations', '. This would allow establishing new kinds of observation-based reference systems that help constrain and integrate', 'the semantic web bottom-up . Ontologies are a common approach to improve semantic interoperability by explicitly', 'specifying the vocabulary used by a particular information community . Complex expressions are defined in terms', 'of primitive ones . This shifts the problem of semantic interoperability to the problem of how', 'to ground primitive symbols . One approach are semantic datums , which determine reproducible mappings (', 'measurement scales ) from observable structures to symbols . Measurement theory offers a formal basis for', 'such mappings . From an ontological point of view , this leaves two important questions unanswered', '. Which qualities provide semantic datums ? How are these qualities related to the primitive entities', 'in our ontology ? Based on a scenario from hydrology , we first argue that human', 'or technical sensors implement semantic datums , and secondly that primitive symbols are definable from the', 'meaningful environment , a formalized quality space established through such sensors . The Geosciences and Geography', 'are not just yet another application area for semantic technologies . The vast heterogeneity of the', 'involved disciplines ranging from the natural sciences to the social sciences introduces new challenges in terms', 'of interoperability . Moreover , the inherent spatial and temporal information components also require distinct semantic', 'approaches . For these reasons , geospatial semantics , geo-ontologies , and semantic interoperability have been', 'active research areas over the last 20 years . The geospatial semantics community has been among', 'the early adopters of the Semantic Web , contributing methods , ontologies , use cases ,', 'and datasets . Today , geographic information is a crucial part of many central hubs on', 'the Linked Data Web . In this editorial , we outline the research field of geospatial', 'semantics , highlight major research directions and trends , and glance at future challenges . We', 'hope that this text will be valuable for geoscientists interested in semantics research as well as', 'knowledge engineers interested in spatiotemporal data . In the last years , several methodologies for ontology', 'engineering have been proposed . Most of these methodologies guide the engineer from a first paper', 'draft to an implemented -- mostly description logics-based -- ontology . A quality assessment of how', 'accurately the resulting ontology fits the initial conceptualization and intended application has not been proposed so', 'far . In this paper , we investigate the role of semantic similarity as a quality', 'indicator . Based on similarity rankings , our approach allows for a qualitative estimation whether the', \"domain experts ' initial conceptualization is reflected by the developed ontology and whether it fits the\", \"users ' application area . Our approach does not propose yet another ontology engineering methodology but\", 'can be integrated into existing ones . A plug-in to the Protege ontology editor implementing our', 'approach is introduced and applied to a scenario from hydrology . The benefits and restrictions of', 'similarity as a quality indicator are pointed out . The SIM-DL theory has been developed to', 'enable similarity measurement between concept specifications using description logics . It thus closes the gap between', 'similarity theories from psychology and formal representation languages from the AI community , such as the', 'Web Ontology Language ( OWL ) . In this paper , we present the results of', 'a human participants test which investigates the cognitive plausibility of SIM-DL , that is , how', 'well the rankings computed by the similarity theory match human similarity judgments . For this purpose', ', a questionnaire on the similarity between geographic feature types from the hydrographic domain was handed', 'out to a group of participants . We discuss the set up and the results of', 'this test , as well as the development of the according hydrographic feature type ontology and', 'user interface . Finally , we give an outlook on the future development of SIM-DL and', 'further potential application areas . Similarity measurement is currently being established as a method to explore', 'content on the Semantic Web . Semantically annotated content requires formal concept specifications . Such concepts', 'are dynamic and their semantics can change depending on the current context . The influence of', 'context on similarity measurement is beyond dispute and reflected in recent similarity theories . However ,', 'the systematics of this influence has not been investigated so far . Intuitively , the results', 'of similarity measurements should change depending on the impact of the current context . Particularly ,', 'such change should converge to 0 with a decreasing impact of the respective contexts . To', 'hold up to this assertion , a quantification of the impact of context on similarity measurements', 'is required . In this paper , we use a combination of the SIM-DL theory ,', 'which measures similarity between concepts represented using description logic , and a context model distinguishing between', 'internal and external context to quantify this impact . The behavior of similarity measurements within an', 'ontology specifying geospatial feature types is observed under varying contexts . The results are discussed with', 'respect to the corresponding impact values . Space and time have not received much attention on', 'the Semantic Web so far . While their importance has been recognized recently , existing work', 'reduces them to simple latitude-longitude pairs and time stamps . In contrast , we argue that', 'space and time are fundamental ordering relations for knowledge organization , representation , and reasoning .', 'While most research on Semantic Web reasoning has focused on thematic aspects , this paper argues', 'for a unified view combining a spatial , temporal , and thematic component . Besides their', 'impact on the representation of and reasoning about individuals and classes , we outline the role', 'of space and time for ontology modularization , evolution , and the handling of vague and', 'contradictory knowledge . Instead of proposing yet another specific methodology , the presented work illustrates the', 'relevance of space and time using various examples from the geo-sciences . When interacting with the', 'environment subjects tend to classify entities with respect to the functionalities they offer for solving specific', 'tasks . The theory of affordances accounts for this agent-environment interaction , while similarity allows for', 'measuring resemblances among entities and entity types . Most similarity measures separate the similarity estimations from', 'the context -- the agents , their tasks and environment -- and focus on structural and', 'static descriptions of the compared entities and types . This paper argues that an affordance-based representation', 'of the context in which similarity is measured , makes the estimations situation-aware and therefore improves', 'their quality . It also leads to a better understanding of how unfamiliar entities are grouped', 'together to ad-hoc categories , which has not been explained in terms of similarity yet .', 'We propose that types of entities are the more similar the more common functionalities their instances', 'afford an agent . This paper presents a framework for representing affordances , which allows determining', 'similarity between them . The approach is demonstrated through a planning task . The understanding of', 'complex environmental phenomena , such as deforestation and epidemics , requires observations at multiple scales .', \"This scale dependency is not handled well by today 's rather technical sensor definitions . Geosensor\", 'networks are normally defined as distributed ad-hoc wireless networks of computing platforms serving to monitor phenomena', 'in geographic space . Such definitions also do not admit animals as sensors . Consequently ,', 'they exclude human sensors , which are the key to volunteered geographic information , and they', 'fail to support connections between phenomena observed at multiple scales . We propose definitions of sensors', 'as information sources at multiple aggregation levels , relating physical stimuli to observations . An algebraic', 'formalization shows their behavior as well as their aggregations and generalizations . It is intended as', 'a basis for defining consistent application programming interfaces to sense the environment at multiple scales of', 'observations and with different types of sensors . Semantic similarity measurement is a key methodology in', 'various domains ranging from cognitive science to geographic information retrieval on the Web . Meaningful notions', 'of similarity , however , can not be determined without taking additional contextual information into account', '. One way to make similarity measures context-aware is by introducing weights for specific characteristics .', 'Existing approaches to automatically determine such weights are rather limited or require application specific adjustments .', 'In the past , the possibility to tweak similarity theories until they fit a specific use', 'case has been one of the major criticisms for their evaluation . In this work ,', \"we propose a novel approach to semi-automatically adapt similarity theories to the user 's needs and\", 'hence make them context-aware . Our methodology is inspired by the process of georeferencing images in', 'which known control points between the image and geographic space are used to compute a suitable', 'transformation . We propose to semi-automatically calibrate weights to compute inter-instance and inter-concept similarities by allowing', 'the user to adjust pre-computed similarity rankings . These known control similarities are then used to', 'reference other similarity values . Acquisition and semantic annotation of data are fundamental tasks within the', 'domain of cultural heritage . With the increasing amount of available data and ad hoc cross', 'linking between their providers and users ( e.g . through web services ) , data integration', 'and knowledge refinement becomes even more important . To integrate information from several sources it has', 'to be guaranteed that objects of discourse ( which may be artifacts , events , persons', ', places or periods ) refer to the same real world phenomena within all involved data', 'sources . Local ( database ) identifiers however only disambiguate internal data , but fail in', 'establishing connections to/between external data , while global identifiers can only partially solve this problem .', 'Software assistants should support users in establishing such connections by delivering identity assumptions , i.e .', 'by estimating whether examined data actually concerns the same real word phenomenon . This paper points', 'out how similarity measures can act as groundwork for such assistants by introducing a similarity-based identity', 'assumption assistant for historical places to support scholars in establishing links between distributed historical knowledge .', 'While semantic similarity plays a crucial role for human categorization and reasoning , computational similarity measures', 'have also been applied to fields such as semantics-based information retrieval or ontology engineering . Several', 'measures have been developed to compare concepts specified in various description logics . In most cases', ', these measures are either structural or require a populated ontology . Structural measures fail with', 'an increasing expressivity of the used description logic , while several ontologies , e.g. , geographic', 'feature type ontologies , are not populated at all . In this paper , we present', 'an approach to reduce inter-concept to inter-instance similarity and thereby avoid the canonization problem of structural', 'measures . The novel approach , called SIM-DLA , reuses existing similarity functions such as co-occurrence', 'or network measures from our previous SIM-DL measure . The required instances for comparison are derived', 'from the completion tree of a slightly modified DL-tableau algorithm as used for satisfiability checking .', 'Instead of trying to find one ( clash-free ) model , the new algorithm generates a', 'set of proxy individuals used for comparison . The paper presents the algorithm , alignment matrix', ', and similarity functions as well as a detailed example . Around 2006 , the inception', 'of Linked Data [ 2 ] has led to a realignment of the Semantic Web vision', 'and the realization that data is not merely a way to evaluate our theoretical considerations ,', 'but a key research enabler in its own right that inspires novel theoretical and foundational research', 'questions . Since then , Linked Data is growing rapidly and is altering research , governments', ', and industry . Simply put , Linked Data takes the World Wide Web ’ s', 'ideas of global identifiers and links and applies them to ( raw ) data , not', 'just documents . Moreover , and regularly highlighted by Tim Berners-Lee , Anybody can say Anything', 'about Any topic ( AAA ) 1 [ 1 ] , which leads to a multi-thematic', ', multi-perspective , and multi-medial global data graph . More recently , Big Data has made', 'its appearance in the shared mindset of researchers , practitioners , and funding agencies , driven', 'by the awareness that concerted efforts are needed to address 21st century data collection , analysis', ', management , ownership , and privacy issues . While there is no generally agreed understanding', 'of what exactly is ( or more importantly , what is not ) Big Data ,', 'an increasing number of V ’ s has been used to characterize different dimensions and challenges', 'of Big Data : volume , velocity , variety , value , and veracity . Interestingly', ', different ( scientific ) disciplines highlight certain dimensions and neglect others . For instance ,', 'super computing seems to be mostly interested in the volume dimension while researchers working on sensor', 'webs and the internet of things seem to push on the velocity front . The social', 'sciences and humanities , in contrast , are more interested in value and veracity . As', 'argued before [ 13,17 ] , the variety dimensions seems to be the most intriguing one', 'for the Semantic Web and the one where we can contribute- Numerous systems and tools have', 'been developed for spatial decision support ( SDS ) , but they generally suffer from a', 'lack of re-usability , inconsistent terminology , and weak conceptualization . We introduce a collaborative effort', 'by the SDS Consortium to build a SDS knowledge portal . We present the formal representation', 'of knowledge about SDS , the various ontologies captured and made accessible by the portal ,', 'and the processes used to create them . We describe the portal in action , and', 'the ways in which users can search , browse , and make use of its content', '. Finally , we discuss the lessons learned from this effort , and future development directions', '. Our work demonstrates how ontologies and semantic technologies can support the documentation and retrieval of', 'dynamic knowledge in GIScience by offering flexible schemata instead of fixed data structures . The volume', ', velocity , and variety of data that are now becoming available allow us to study', 'urban environments based on human behaviour with a spatial , temporal , and thematic granularity that', 'was not achievable until now . Such data-driven approaches open up additional , complementary perspectives on', 'how urban systems function , especially if they are based on user-generated content ( UGC )', '. While the data sources , such as social media , introduce specific biases , they', 'also open up new possibilities for scientists and the broader public . For instance , they', 'provide answers to questions that previously could only be addressed by complex simulations or extensive human-participant', 'surveys . Unfortunately , many of the required data sets are locked in data silos that', 'are accessible only via restricted APIs . Even if these data could be fully accessed ,', 'their naϊve processing and visualization would surpass the abilities of modern computer architectures . Finally ,', 'the established place schemata used to study urban spaces differ substantially from UGC-based point-of-interest ( POI', ') schemata . In this work , we present a multi-granular , data-driven , and theory-informed', 'approach that addresses the key issues outlined above by introducing a theoretical and technical framework to', 'interactively explore the pulse of a city based on social media . RESUME : Le volume', ', la rapidite et la variete des donnees maintenant disponibles permettent d ’ etudier les milieux', 'urbains en fonction du comportement humain a un niveau spatial , temporel et granulaire thematique sans', 'precedent . De telles methodes axees sur les donnees , et surtout celles fondees sur le', 'contenu genere par l ’ utilisateur ( CGU ) , proposent d ’ autres perspectives complementaires', 'sur le fonctionnement des systemes urbains . Les sources de donnees ( p . ex. ,', 'medias sociaux ) introduisent des biais precis , mais elles offrent de nouvelles possibilites aux scientifiques', 'et au grand public ( p . ex. , en repondant a des questions qui ,', 'auparavant , necessitaient des simulations complexes ou de vastes enquetes aupres de participants humains ) .', 'Malheureusement , beaucoup d ’ ensembles de donnees necessaires sont enfermes dans des reserves de donnees', 'accessibles seulement par l ’ entremise d ’ interfaces de programmation d ’ applications ( API', ') restreintes . Meme si l ’ on pouvait avoir pleinement accesa ces donnees , leur', 'traitement et leur visualisation naϊfs depasseraient les capacites des architectures informatiques modernes . Enfin , le', 'schema des lieux etabli que l ’ on utilise pour etudier les espaces urbains est considerablement', 'different du schema des points d ’ interet ( PI ) fondes sur le CGU .', 'L ’ article presente une methode multigranulaire fondee sur la theorie et les donnees qui abordent', 'les principales difficultes soulevees precedemment en introduisant un cadre theorique et technique pour explorer de facon', 'interactive le pouls d ’ une ville en fonction des medias sociaux . Geospatial semantics as', 'a research field studies how to publish , retrieve , reuse , and integrate geo-data ,', 'how to describe geo-data by conceptual models , and how to develop formal specifications on top', 'of data structures to reduce the risk of incompatibilities . Geo-data is highly heterogeneous and ranges', 'from qualitative interviews and thematic maps to satellite imagery and complex simulations . This makes ontologies', ', semantic annotations , and reasoning support essential ingredients towards a Geospatial Semantic Web . In', 'this paper , we present an overview of major research questions , recent findings , and', 'core literature . Abstract Urban areas of interest ( AOI ) refer to the regions within', \"an urban environment that attract people 's attention . Such areas often have high exposure to\", 'the general public , and receive a large number of visits . As a result ,', 'urban AOI can reveal useful information for city planners , transportation analysts , and location-based service', 'providers to plan new business , extend existing infrastructure , and so forth . Urban AOI', \"exist in people 's perception and are defined by behaviors . However , such perception was\", 'rarely captured until the Social Web information technology revolution . Social media data record the interactions', 'between users and their surrounding environment , and thus have the potential to uncover interesting urban', 'areas and their underlying spatiotemporal dynamics . This paper presents a coherent framework for extracting and', 'understanding urban AOI based on geotagged photos . Six different cities from six different countries have', 'been selected for this study , and Flickr photo data covering these cities in the past', 'ten years ( 2004–2014 ) have been retrieved . We identify AOI using DBSCAN clustering algorithm', ', understand AOI by extracting distinctive textual tags and preferable photos , and discuss the spatiotemporal', 'dynamics as well as some insights derived from the AOI . An interactive prototype has also', 'been implemented as a proof-of-concept . While Flickr data have been used in this study ,', 'the presented framework can also be applied to other geotagged photos . Semantic similarity measurement gained', 'attention as a methodology for ontology-based information retrieval within GIScience over the last years . Several', 'theories explain how to determine the similarity between entities , concepts or spatial scenes , while', 'concrete implementations and applications are still missing . In addition , most existing similarity theories use', 'their own representation language while the majority of geo-ontologies is annotated using the Web Ontology Language', '( OWL ) . This paper presents a context and blocking aware semantic similarity theory for', 'the description logic ALCHQ as well as its prototypical implementation within the open source SIM-DL similarity', 'server . An application scenario is introduced showing how the Alexandria Digital Library Gazetteer can benefit', 'from similarity in terms of improved search and annotation capabilities . Directions for further work are', 'discussed . While similarity has gained in importance in research about information retrieval on the (', 'geospatial ) semantic Web , information retrieval paradigms and their integration into existing spatial data infrastructures', 'have not been examined in detail so far . In this paper , intensional and extensional', 'paradigms for similarity-based information retrieval are introduced . The differences between these paradigms with respect to', 'the query and results are pointed out . Web user interfaces implementing two of these paradigms', 'are presented , and steps towards the integration of the SIM-DL similarity theory into a spatial', 'data infrastructure are discussed . Remaining difficulties are highlighted and directions of further work are given', '. In 2010 Tim Berners-Lee introduced a 5 star rating to his Linked Data design issues', 'page to encourage data publishers along the road to good Linked Data . What makes the', 'star rating so effective is its simplicity , clarity , and a pinch of psychology --', 'is your data 5 star ? While there is an abundance of 5 star Linked Data', 'available today , finding , querying , and integrating/interlinking these data is , to say the', 'least , difficult . While the literature has largely focused on describing datasets , e.g. ,', 'by adding provenance information , or interlinking them , e.g. , by co-reference resolution tools ,', \"we would like to take Berners-Lee 's original proposal to the next level by introducing a\", '5 star rating for Linked Data vocabulary use . Life Cycle Assessment ( LCA ) is', 'the study of the environmental impact of products taking into account their entire life-span and production', 'chain . This requires gathering data from a variety of heterogeneous sources into a Life Cycle', 'Inventory ( LCI ) . LCI preparation involves the integration of observations and engineering models with', 'reference data and literature results from around the world , from different domains , and at', 'varying levels of granularity . Existing LCA data formats only address syntactic interoperability , thereby often', 'ignoring semantics . This leads to inefficiencies in information collection and management and thus a variety', 'of challenges , e.g. , difficulties in reproducing assessments published in the literature . In this', 'work , we present an ontology pattern that specifies key aspects of LCA/LCI data models ,', 'i.e. , the notions of flows , activities , agents , and products , as well', 'as their properties . Abstract The temporal characteristics of human behavior with respect to points of', 'interest ( POI ) differ significantly with place type . Intuitively , we are more likely', 'to visit a restaurant during typical lunch and dinner times than at midnight . Aggregating geosocial', 'check-ins of millions of users to the place type level leads to powerful temporal bands and', 'signatures . In previous work these signatures have been used to estimate the place being visited', 'based purely on the check-in time , to label uncategorized places based on their individual signature', \"'s similarity to a type signature , and to mine POI categories and their hierarchical structure\", 'from the bottom up . However , not all hours of the day and days of', 'the week are equally indicative of the place type , i.e. , the information gain between', 'temporal bands that jointly form a place type signature differs . To give a concrete example', ', places can be more easily categorized into weekend and weekday places than into Monday and', 'Tuesday places . Nonetheless , research on the regional variability of temporal signatures is lacking .', 'Intuitively , one would assume that certain types of places are more prone to regional differences', 'with respect to the temporal check-in behavior than others . This variability will impact the predictive', 'power of the signatures and reduce the number of POI types that can be distinguished .', 'In this work , we address the regional variability hypothesis by trying to prove that all', 'place types are created equal with respect to their temporal signatures , i.e. , temporal check-in', 'behavior does not change across space . We reject this hypothesis by comparing the inter-signature similarity', 'of 321 place types in three major cities in the USA ( Los Angeles , New', 'York , and Chicago ) . Next , we identify a common core of least varying', 'place types and compare it against signatures extracted from the city of Shanghai , China for', 'cross-culture comparison . Finally , we discuss the impact of our findings on POI categorization and', 'the reliability of temporal signatures for check-in behavior in general . The categorization of our environment', 'into feature types is an essential prerequisite for cartography , geographic information retrieval , routing applications', ', spatial decision support systems , and data sharing in general . However , there is', 'no a priori conceptualization of the world and the creation of features and types is an', 'act of cognition . Humans conceptualize their environment based on multiple criteria such as their cultural', 'background , knowledge , motivation , and particularly by space and time . Sharing and making', 'these conceptualizations explicit in a formal , unambiguous way is at the core of semantic interoperability', '. One way to cope with semantic heterogeneities is by standardization , i.e. , by agreeing', 'on a shared conceptualization . This bears the danger of losing local diversity . In contrast', ', this work proposes the use of microtheories for Spatial Data Infrastructures , such as INSPIRE', ', to account for the diversity of local conceptualizations while maintaining their semantic interoperability at a', 'global level . We introduce a novel methodology to structure ontologies by spatial and temporal aspects', ', in our case administrative boundaries , which reflect variations in feature conceptualization . A local', ', bottom-up approach , based on non-standard inference , is used to compute global feature definitions', 'which are neither too broad nor too specific . Using different conceptualizations of rivers and other', 'geographic feature types , we demonstrate how the present approach can improve the INSPIRE data model', 'and ease its adoption by European member states . With the increasing success and commercial integration', 'of Volunteered Geographic Information ( VGI ) , the focus shifts away from coverage to data', 'quality and homogeneity . Within the last years , several studies have been published analyzing the', 'positional accuracy of features , completeness of specific attributes , or the topological consistency of line', 'and polygon features . However , most of these studies do not take geographic feature types', 'into account . This is for two reasons . First , and in contrast to street', 'networks , choosing a reference set is difficult . Second , we lack the measures to', 'quantify the degree of feature type miscategorization . In this work , we present a methodology', 'to analyze the spatial-semantic interaction of point features in Volunteered Geographic Information . Feature types in', 'VGI can be considered special in both , the way they are formed and the way', 'they are applied . Given that they reflect community agreement more accurately than top-down approaches ,', 'we argue that they should be used as the primary basis for assessing spatial-semantic interaction .', 'We present a case study on a spatial and semantic subset of OpenStreetMap , and introduce', 'a novel semantic similarity measure based on the change history of OpenStreetMap elements . Our results', 'set the stage for systems that assist VGI contributors in suggesting the types of new features', ', cleaning up existing data , and integrating data from different sources . The Digital Earth', '[ 13 ] aims at developing a digital representation of the planet . It is motivated', 'by the need for integrating and interlinking vast geo-referenced , multi-thematic , and multi-perspective knowledge archives', 'that cut through domain boundaries . Complex scientific questions can not be answered from within one', 'domain alone but span over multiple scientific disciplines . For instance , studying disease dynamics for', 'prediction and policy making requires data and models from a diverse body of science ranging from', 'medical science and epidemiology over geography and economics to mining the social Web . The naive', 'assumption that such problems can simply be addressed by more data with a higher spatial ,', 'temporal , and thematic resolution fails as long as this more on data is not supported', 'by more knowledge on how to combine and interpret the data . This makes semantic interoperability', 'a core research topic of data-intensive science . While the Digital Earth vision includes processing services', ', it is , at its very core , a data archive and infrastructure . We', 'propose to redefine the Digital Earth as a knowledge engine and discuss what the Semantic Web', 'has to offer in this context and to Big Data in general . Big Data ,', 'Linked Data , Smart Dust , Digital Earth , and e-Science are just some of the', 'names for research trends that surfaced over the last years . While all of them address', 'different visions and needs , they share a common theme : How do we manage massive', 'amounts of heterogeneous data , derive knowledge out of them instead of drowning in information ,', 'and how do we make our findings reproducible and reusable by others ? In a network', 'of knowledge , topics span across scientific disciplines and the idea of domain ontologies as common', 'agreements seems like an illusion . In this work , we argue that these trends require', 'a radical paradigm shift in ontology engineering away from a small number of authoritative , global', 'ontologies developed top-down , to a high number of local ontologies that are driven by application', 'needs and developed bottom-up out of observation data . Similarly as the early Web was replaced', 'by a social Web in which volunteers produce data instead of purely consuming it , the', 'next generation of knowledge infrastructures has to enable users to become knowledge engineers themselves . Surprisingly', ', existing ontology engineering frameworks are not well suited for this new perspective . Hence ,', 'we propose an observation-driven ontology engineering framework , show how its layers can be realized using', 'specific methodologies , and relate the framework to existing work on geo-ontologies . There has been', 'significant progress transforming semi-structured data about places into knowledge graphs that can be used in a', 'wide variety of geographic information systems such as digital gazetteers or geographic information retrieval systems .', 'For instance , in addition to information about events , actors , and objects , DBpedia', 'contains data about hundreds of thousands of places from Wikipedia and publishes it as Linked Data', '. Repositories that store data about places are among the most interlinked hubs on the Linked', 'Data cloud . However , most content about places resides in unstructured natural language text ,', 'and therefore it is not captured in these knowledge graphs . Instead , place representations are', 'limited to facts such as their population counts , geographic locations , and relations to other', 'entities , for example , headquarters of companies or historical figures . In this paper ,', 'we present a novel method to enrich the information stored about places in knowledge graphs using', 'thematic signatures that are derived from unstructured text through the process of topic modeling . As', 'proof of concept , we demonstrate that this enables the automatic categorization of articles into place', 'types defined in the DBpedia ontology e.g. , mountain and also provides a mechanism to infer', 'relationships between place types that are not captured in existing ontologies . This method can also', 'be used to uncover miscategorized places , which is a common problem arising from the automatic', 'lifting of unstructured and semi-structured data . To a large degree , the attraction of Big', 'Data lies in the variety of its heterogeneous multi-thematic and multi-dimensional data sources and not merely', 'its volume . To fully exploit this variety , however , requires conflation . This is', 'a two-step process . First , one has to establish identity relations between information entities across', 'different data sources ; and second , attribute values have to be merged according to certain', 'procedures that avoid logical contradictions . The first step , also called matching , can be', 'thought of as a weighted combination of common attributes according to some similarity measures . In', 'this work , we propose such a matching based on multiple attributes of Points of Interest', '( POI ) from the Location-based Social Network Foursquare and the local directory service Yelp .', 'While both contain overlapping attributes that can be used for matching , they have specific strengths', 'and weaknesses that make their conflation desirable . For instance , Foursquare offers information about user', 'check-in ... Abstract The vision of a Digital Earth calls for more dynamic information systems ,', 'new sources of information , and stronger capabilities for their integration . Sensor networks have been', 'identified as a major information source for the Digital Earth , while Semantic Web technologies have', 'been proposed to facilitate integration . So far , sensor data are stored and published using', 'the Observations & Measurements standard of the Open Geospatial Consortium ( OGC ) as data model', '. With the advent of Volunteered Geographic Information and the Semantic Sensor Web , work on', 'an ontological model gained importance within Sensor Web Enablement ( SWE ) . In contrast to', 'data models , an ontological approach abstracts from implementation details by focusing on modeling the physical', 'world from the perspective of a particular domain . Ontologies restrict the interpretation of vocabularies toward', 'their intended meaning . The ongoing paradigm shift to Linked Sensor Data complements this attempt .', 'Two questions have to be addressed : ( 1 ) ... Ontology design patterns ease the', 'engineering of ontologies , improve their quality , foster reusability , and support the alignment of', 'ontologies by acting as common building blocks or strategies for reoccurring modeling problems . This makes', 'ontology design patterns key enablers of semantic interoperability and , hence , a crucial technology for', 'representing the body of knowledge of such heterogeneous domains as the geosciences . While different types', 'of patterns can be distinguished , existing work on geo-ontology design patterns has solely focused on', 'content patterns , i.e. , design solutions for domain classes and relationships . In this work', ', we propose a logical pattern that addresses a frequent modeling problem that has hampered the', 'development of sophisticated geo-ontologies in the past , namely how to model the quantification over types', '. We argue for the need for such a pattern , explain why it is difficult', 'to model , demonstrate how to implement it using the Web Ontology Language OWL , and', 'finally show how it can be applied to modeling concepts such as biodiversity . To a', 'large degree , the attraction of Big Data lies in the variety of its heterogeneous multi-thematic', 'and multi-dimensional data sources and not merely its volume . To fully exploit this variety ,', 'however , requires conflation . This is a two step process . First , one has', 'to establish identity relations between information entities across the different data sources ; and second ,', 'attribute values have to be merged according to certain procedures which avoid logical contradictions . The', 'first step , also called matching , can be thought of as a weighted combination of', 'common attributes according to some similarity measures . In this work , we propose such a', 'matching based on multiple attributes of Points of Interests ( POI ) from the Location-based Social', 'Network Foursquare and the Yelp local directory service . While both contain overlapping attributes that can', 'be use for matching , they have specific strengths and weaknesses which makes their conflation desirable', '. We present a weighted multi-attribute matching strategy and evaluate its performance . Our strategy can', 'automatically match 97 % of randomly selected Yelp POI to their corresponding Foursquare entities . Gazetteers', 'are key components of georeferenced information systems , including applications such as Web-based mapping services .', 'Existing gazetteers lack the capabilities to fully integrate user-contributed and vernacular geographic information , as well', 'as to support complex queries . To address these issues , a next generation gazetteer should', 'leverage formal semantics , harvesting of implicit geographic information -- such as geotagged photos -- as', 'well as models of trust for contributors . In this paper , we discuss these requirements', 'in detail . We elucidate how existing standards can be integrated to realize a gazetteer infrastructure', 'allowing for bottom-up contribution as well as information exchange between different gazetteers . We show how', 'to ensure the quality of user-contributed information and demonstrate how to improve querying and navigation using', 'semantics-based information retrieval . Building on abstract reference models , the Open Geospatial Consortium ( OGC', ') has established standards for storing , discovering , and processing geographical information . These standards', 'act as a basis for the implementation of specific services and Spatial Data Infrastructures ( SDI', ') . Research on geo-semantics plays an increasing role to support complex queries and retrieval across', 'heterogeneous information sources , as well as for service orchestration , semantic translation , and on-the-fly', 'integration . So far , this research targets individual solutions or focuses on the Semantic Web', ', leaving the integration into SDI aside . What is missing is a shared and transparent', 'Semantic Enablement Layer for SDI which also integrates reasoning services known from the Semantic Web .', 'Instead of developing new semantically enabled services from scratch , we propose to create profiles of', 'existing services that implement a transparent mapping between the OGC and the Semantic Web world .', 'Finally , we point out how to combine SDI with linked data . Semantic similarity measurement', 'has been an active research area in GIScience and the Semantic Web for many years .', 'However , implementations of these measures were largely missing , not publicly available , or tailored', 'to specific application needs . To foster the application of similarity reasoning in information retrieval ,', 'ontology engineering , and spatial decision support , we implemented the SIM-DL semantic similarity server as', 'well as a plug-in for the popular Protege ontology editor . While SIM-DL has been successfully', 'applied to several application areas , the implemented similarity theory was largely structural , could not', 'handle concept and instance similarity within the same framework , and was based on a Protege', 'version and DIG interface that have been re-engineered over the last years . This paper introduces', 'a new version , called SIM-DLA , engineered from scratch to addresses these shortcomings . It', 'is based on our new similarity theory , can handle inter-instance and inter-concept similarity using the', 'same functions and alignments , and is available for the new Protege version 4.1 . Gazetteers', 'are more than basic place name directories containing names and locations for named geographic places .', 'Most of them contain additional information , including a categorization of gazetteer entries using a typing', 'scheme . This paper focuses on the nature of these categorization schemes . We argue that', 'gazetteers can benefit from an ontological approach to typing schemes , providing a formalization that will', 'better support gazetteer applications , maintenance , interoperability , and semi-automatic feature annotation . We discuss', 'the process of developing such an ontology as a modification of an existing feature type thesaurus', '; the difficulties in mapping from thesauri to ontologies are described in detail . To demonstrate', 'the benefits of a categorization based on ontologies , a new gazetteer Web ( and programming', ') interface is introduced and the impact on gazetteer interoperability is discussed . Place name disambiguation', 'is an important task for improving the accuracy of geographic information retrieval . This task becomes', 'more challenging when the input texts are short . Wikipedia provides information about places and has', 'often been employed for named entity recognition . However , the natural language representation of Wikipedia', 'articles limits more effective use of this rich knowledge base . DBpedia is the Semantic Web', 'version of Wikipedia , which provides structured and machine-understandable knowledge mined from Wikipedia articles . This', 'paper presents an approach for combining Wikipedia and DBpedia to disambiguate place names in short texts', '. We discuss the pros and cons of the two knowledge bases , and argue that', 'a combination of both performs better than each of them alone . We evaluate our proposed', 'method by conducting experiments against baselines of three established methods . The result indicates that our', 'method has a generally higher precision and recall . While our study employs DBpedia , the', 'proposed method is generic and can be extended to other structured Linked Datasets such as Freebase', 'or Wikidata . The landscape of social media applications is littered with novel approaches to using', 'location information . The latest platform to emerge in this geosocial media realm is Yik Yak', ', an application that allows users to share geo-tagged , ( currently ) text-based , and', 'most importantly , anonymous content . The fast adoption of this platform by college students as', 'well as the recent availability of data offers a unique research opportunity . This work takes', 'a first step in exploring this novel type of data through a range of textual ,', 'topical , and spatial data exploration methods . We are particularly interested in the question of', 'whether Yik Yak differs from other geosocial data sources such as Twitter . Is it just', 'another location-based social network or does it differ from existing social networks , establishing itself as', 'a valuable resource for feature extraction ? While open access is established in the world of', 'academic publishing , open reviews are rare . The Semantic Web journal goes further than just', 'open review by implementing an open and transparent review process in which reviews are publicly available', ', the assigned editors and reviewers are known by name , and are published together with', 'accepted manuscripts . In this article we introduce the steps to realize such a process from', 'the conceptual design , over the implementation , a overview of the results so far ,', 'and up to lessons learned . In this paper , we develop a semantic annotation technique', 'for location-based social networks to automatically annotate all places with category tags which are a crucial', 'prerequisite for location search , recommendation services , or data cleaning . Our annotation algorithm learns', 'a binary support vector machine ( SVM ) classifier for each tag in the tag space', 'to support multi-label classification . Based on the check-in behavior of users , we extract features', 'of places from i ) explicit patterns ( EP ) of individual places and ii )', 'implicit relatedness ( IR ) among similar places . The features extracted from EP are summarized', 'from all check-ins at a specific place . The features from IR are derived by building', 'a novel network of related places ( NRP ) where similar places are linked by virtual', 'edges . Upon NRP , we determine the probability of a category tag for each place', 'by exploring the relatedness of places . Finally , we conduct a comprehensive experimental study based', 'on a real dataset collected from a location-based social network , Whrrl . The results demonstrate', 'the suitability of our approach and show the strength of taking both EP and IR into', 'account in feature extraction . This paper presents a collaborative 3D GIS to support public participation', '. Realizing that public-involved decision-making is often a multistage process , the proposed system is designed', 'to provide coherent support for collaborations in the different stages . We differentiate ubiquitous participation and', 'intensive participation and identify their suitable application stages . The proposed system , then , supports', 'both of the two types of participation by providing synchronous and asynchronous collaboration functionalities . Applying', 'the concept of Digital Earth , the proposed system also features a virtual globe-based user interface', '. Such an interface integrates a variety of data , functions , and services into a', 'unified virtual environment which is delivered to both experts and public participants through the Internet .', 'The system has been designed as a general software framework and can be tailored for specific', 'projects . In this study , we demonstrate it using a scene modeling case and provide', 'a preliminary evalua ... The term Volunteered Geographic Information ( VGI ) describes various layperson-based ,', 'geo-collaboration projects to collect , maintain , and visualize information . VGI has been successfully utilized', 'in scenarios such as emergency response and is also increasingly integrated into commercial products . Based', 'on an analysis of existing projects and research , we propose to extend the idea of', 'VGI by introducing Volunteered Geographic Services ( VGS ) . Instead of contributing information , volunteers', 'can request or offer microservices to their local community . We provide a flexible server framework', 'that handles service requests and offers . We also implement a smartphone application developed using Google', \"'s Android platform . The server and mobile client are realized following the Linked Data paradigm\", 'and using Semantic Web technologies . In this paper , we discuss the idea behind VGS', ', motivate it using two scenarios , and explain the technical realization . Feature types play', 'a crucial role in understanding and analyzing geographic information . Usually , these types are defined', ', standardized , and controlled by domain experts and cover geographic features on the mesoscale level', ', e.g. , populated places , forests , or lakes . While feature types also underlie', 'most Location-Based Services ( LBS ) , assigning a consistent typing schema for Points Of Interest', '( POI ) across different data sets is challenging . In case of Volunteered Geographic Information', '( VGI ) , types are assigned as tags by a heterogeneous community with different backgrounds', 'and applications in mind . Consequently , VGI research is shifting away from data completeness and', 'positional accuracy as quality measures towards attribute accuracy . As tags can be assigned by everybody', 'and have no formal or stable definition , we propose to study category tags via indirect', 'observations . We extract user check-ins from massive real-world data crawled from Location-based Social Networks to', 'understand the temporal dimension of Points Of Interest . While users may assign different category tags', 'to places , we argue that their temporal characteristics , e.g. , opening times , will', 'show distinguishable patterns . This work1 enables an automated integration of sensors and services on the', 'Sensor Web . The Sensor Web is defined as an infrastructure which enables the interoperable usage', 'of sensor resources by providing services for ( 1 ) discovery , ( 2 ) access', ', ( 3 ) tasking , as well as ( 4 ) eventing & alerting [', '2 ] . The notion of the Sensor Web has been largely influenced by the developments', \"of OGC 's Sensor Web Enablement ( SWE ) initiative [ 3 ] , however ,\", 'there are also other implementations complying to the Sensor Web idea , such as Sensorpedia [', '4 ] , SensorMap with its underlying SenseWeb infrastructure [ 5 ] , SensorBase [ 6', '] , or Cosm2 ( formerly known as Pachube ) . Linked Data ( LD )', 'has been an active research area for more than 6 years and many aspects about publishing', ', retrieving , linking , and cleaning Linked Data have been investigated . There seems to', 'be a broad and general agreement that in principle LD datasets can be very useful for', 'solving a wide variety of problems ranging from practical industrial analytics to highly specific research problems', '. Having these notions in mind , we started exploring the use of notable LD datasets', 'such as DBpedia , Freebase , Geonames and others for a commercial application . However ,', 'it turns out that using these datasets in realistic settings is not always easy . Surprisingly', ', in many cases the underlying issues are not tech- nical but legal barriers erected by', 'the LD data publishers . In this paper we argue that these barriers are often not', 'justified , detrimental to both data publishers and users , and are often built without much', 'consideration of their consequences . The W3C Semantic Sensor Network Incubator group ( the SSN-XG )', 'produced an OWL 2 ontology to describe sensors and observations - the SSN ontology , available', 'at http : //purl.oclc.org/NET/ssnx/ssn . The SSN ontology can describe sensors in terms of capabilities ,', 'measurement processes , observations and deployments . This article describes the SSN ontology . It further', 'gives an example and describes the use of the ontology in recent research projects . Environmental', 'sensors have continuously improved by becoming smaller , cheaper , and more intelligent over the past', 'years . As consequence of these technological advancements , sensors are increasingly deployed to monitor our', 'environment . The large variety of available sensor types with often incompatible protocols complicates the integration', 'of sensors into observing systems . The standardized Web service interfaces and data encodings defined within', 'OGC ’ s Sensor Web Enablement ( SWE ) framework make sensors available over the Web', 'and hide the heterogeneous sensor protocols from applications . So far , the SWE framework does', 'not describe how to integrate sensors on-the-fly with minimal human intervention . The driver software which', 'enables access to sensors has to be implemented and the measured sensor data has to be', 'manually mapped to the SWE models . In this article we introduce a Sensor Plug &', 'Play infrastructure for the Sensor Web by combining ( 1 ) semantic matchmaking functionality , (', '2 ) a publish/subscribe mechanism underlying the SensorWeb , as well as ( 3 ) a', 'model for the declarative description of sensor interfaces which serves as a generic driver mechanism .', 'We implement and evaluate our approach by applying it to an oil spill scenario . The', 'matchmaking is realized using existing ontologies and reasoning engines and provides a strong case for the', 'semantic integration capabilities provided by Semantic Web research . Ontology-based information publishing , retrieval , reuse', ', and integration have become popular research topics to address the challenges involved in exchanging data', 'between heterogeneous sources . However , in most cases ontologies are still developed in a centralized', 'top-down manner by a few knowledge engineers . Consequently , the role that developers play in', 'conceptualizing a domain such as the geosciences is disproportional compared with the role of domain experts', 'and especially potential end-users . These and other drawbacks have stimulated the creation of new methodologies', 'focusing around collaboration . Based on a review of existing approaches , this article presents a', 'two-step methodology and implementation to foster collaborative ontology engineering in the geosciences . Our approach consists', 'of the development of a minimalistic core ontology acting as a catalyst and the creation of', 'a virtual collaborative development cycle . Both methodology and prototypical implementation have been tested in the', 'context of the EU-funded ForeStClim project which addresses environmental protection with respect to forests and climate', 'change . The goal of the Sensor Web Enablement ( SWE ) initiative of the Open', 'Geospatial Consortium ( OGC ) is the definition of web service interfaces and data encodings to', 'make sensors discoverable , taskable and accessible on the World Wide Web . The SWE specifications', 'enable a standardized communication and interaction with arbitrary types of sensors and sensor systems . The', \"central concepts within OGC 's Sensor Web architecture are sensors , observations and features of interest\", '. Sensors and their observations can be registered and stored through the Sensor Observation Service (', 'SOS ) to make them accessible for clients . So far , mechanisms are missing which', 'support a semantic matching between features of interest stored in a database and referred to by', 'an observation . The same applies for the matching between observations as sensor outputs and the', 'properties of the features of interest . By taking a use case from disaster management ,', 'we outline the challenges and demonstrate how semantically annotated SWE data models and service interfaces support', 'semantic matching . The result is a roadmap towards a semantically enabled sensor plug & play', 'within the Sensor Web . Semantic similarity measurement plays a significant role in semantic interoperability and', 'in information retrieval within the geo domain as it supports the detection of conceptually close but', 'not identical entities . In feature-based models , the similarity measurement is done by comparing common', 'and different features such as parts , attributes and functions . This paper suggests adding thematic', 'roles as an additional type of features to be compared , and shows why and how', 'the usage of thematic roles may prevent wrong function matches . What prevents the Geospatial Semantic', 'Web from taking off is not a missing architecture and protocol stack but , beside other', 'aspects , the question of how Web services can be semi-automatically discovered and whether and to', 'what degree they satisfy user requirements . Two approaches turned out to be useful for semanticenabled', 'geospatial information retrieval : subsumption reasoning and similarity measurement . However , while the former one', 'can be applied to query service ontologies described in OWL-S or WSMO/WSML , most existing similarity', 'theories are not able to cope with logic-based service descriptions . This chapter presents initial results', 'on developing a directed and context-aware similarity measure that compares WSML concept descriptions for overlap and', 'therefore supports retrieval within the upcoming Geospatial Semantic The relation between data , annotations , and', 'schemata seems straightforward at first : Data are annotated with additional meta information according to some', 'schemata in order to expose additional non-intrinsic characteristics relevant to the meaningful interpretation of said data', '. However , on closer examination , things are not as simple . Focusing on geo-information', 'retrieval , we will try to disentangle the aforementioned relations . We will report from our', 'own experience and from observations gathered by editing papers about ontologies and Linked Data for the', 'Semantic Web journal . Semantic desktops are a novel approach to improve user interfaces by recording', \", semantically annotating , and learning from the user 's activities to create a personalized user\", 'experience and improve search . Such activities , however , are restricted to the information universe', ', i.e. , they only cover events on the local desktop . A next step towards', \"smart mobile devices is the integration of those desktop events with the user 's activities in\", 'the physical world . Establishing such mappings enables the device to draw conclusions from the recorded', 'desktop events to those that the user is likely performing in the physical world . A', 'Personal Information Management ( PIM ) system can then better assist the user in task planning', 'and routing . In this work , we propose activity ontologies as blueprints to model the', \"user 's activities in the physical world , and use these ontologies to link the Semantic\", 'Desktop and the information available on the Web of Linked Data . We discuss the principles', 'of designing the activity ontologies and how to employ them to associate local files and applications', 'with complementary information from the Web . We design a specific activity ontology for a conference', 'use case and present a user interface that extends the Zeitgeist Semantic Desktop to evaluate our', 'approach . The concepts of scale is at the core of cartographic abstraction and mapping .', 'It defines which geographic phenomena should be displayed , which type of geometry and map symbol', 'to use , which measures can be taken , as well as the degree to which', 'features need to be exaggerated or spatially displaced . In this work , we present an', 'ontology design pattern for map scaling using the Web Ontology Language ( OWL ) within a', 'particular extension of the OWL RL profile . We explain how it can be used to', 'describe scaling applications , to reason over scale levels , and geometric representations . We propose', 'an axiomatization that allows us to impose meaningful constraints on the pattern , and , thus', ', to go beyond simple surface semantics . Interestingly , this includes several functional constraints currently', 'not expressible in any of the OWL profiles . We show that for this specific scenario', ', the addition of such constraints does not increase the reasoning complexity which remains tractable .', 'Geoportals provide integrated access to geospatial resources , and enable both authorities and the general public', 'to contribute and share data and services . An essential goal of geoportals is to facilitate', 'the discovery of the available resources . Such process heavily relies on the quality of metadata', '. While multiple metadata standards have been established , data contributers may adopt different standards when', 'sharing their data via the same geoportal . This is especially the case for user-generated content', 'where various terms and topics can be introduced to describe similar datasets . While this heterogeneity', 'provides a wealth of perspectives , it also complicates resource discovery . With the fast development', 'of the Semantic Web technologies , there is a rise of Linked-Data-driven portals . Although these', 'novel portals open up new ways to organizing metadata and retrieving resources , they lack effective', 'semantic search methods . This paper addresses the two challenges discussed above , namely the topic', 'heterogeneity brought by multiple metadata standards as well as the lack of established semantic search in', 'Linked-Data-driven geoportals . To harmonize the metadata topics , we employ a natural language processing method', ', namely Labeled Latent Dirichlet Allocation ( LLDA ) , and train it using standardized metadata', 'from Data.gov . With respect to semantic search , we construct thematic and geographic matching features', 'from the textual metadata descriptions , and train a regression model via a human participants experiment', '. We evaluate our methods by examining their performances in addressing the two issues . Finally', ', we implement a semantics-enabled and Linked-Data-driven prototypical geoportal using a sample dataset from Esri ’', 's ArcGIS Online . While catchphrases such as big data , smart data , dataintensive science', ', or smart dust highlight different aspects , they share a common theme : Namely ,', 'a shift towards a data-centric perspective in which the synthesis and analysis of data at an', 'ever-increasing spatial , temporal , and thematic resolution promises new insights , while , at the', 'same time , reducing the need for strong domain theories as starting points . In terms', 'of the envisioned methodologies , those catchphrases tend to emphasize the role of predictive analytics ,', 'i.e. , statistical techniques including data mining and machine learning , as well as supercomputing .', 'Interestingly , however , while this perspective takes the availability of data as a given ,', 'it does not answer the question how one would discover the required data in today ’', 's chaotic information universe , how one would understand which datasets can be meaningfully integrated ,', 'and how to communicate the results to humans and machines alike . The Semantic Web addresses', 'these questions . In the following , we argue why the data train needs semantic rails', '. We point out that making sense of data and gaining new insights works best if', 'inductive and deductive techniques go hand-in-hand instead of competing over the prerogative of interpretation . Similarity', 'measures have a long tradition in fields such as information retrieval , artificial intelligence , and', 'cognitive science . Within the last years , these measures have been extended and reused to', 'measure semantic similarity ; i.e. , for comparing meanings rather than syntactic differences . Various measures', 'for spatial applications have been de- veloped , but a solid foundation for answering what they', 'measure ; how they are best ap- plied in information retrieval ; which role contextual information', 'plays ; and how similarity values or rankings should be interpreted is still missing . It', 'is therefore difficult to decide which measure should be used for a particular application or to', 'compare results from dif- ferent similarity theories . Based on a review of existing similarity measures', ', we introduce a framework to specify the semantics of similarity . We discuss similarity-based information', 'retrieval paradigms as well as their implementation in web-based user interfaces for geo- graphic information retrieval', 'to demonstrate the applicability of the framework . Finally , we formulate open challenges for similarity', 'research . Similarity measurement theories play an increasing role in GIScience and especially in information retrieval', 'and integration Existing feature and geometric models have proven useful in detecting close but not identical', 'concepts and entities However , until now none of these theories are able to handle the', 'expressivity of description logics for various reasons and therefore are not applicable to the kind of', 'ontologies usually developed for geographic information systems or the upcoming geospatial semantic web To close the', 'resulting gap between available similarity theories on the one side and existing ontologies on the other', ', this paper presents ongoing work to develop a context-aware similarity theory for concepts specified in', 'expressive description logics such as $ mathcal ALCNR $ . 2012 brought a major change to', 'the semantics research community . Discussions on the use and benefits of semantic technologies are shifting', 'away from the why to the how . Surprisingly this more in stakeholder interest is not', 'accompanied by a more detailed understanding of what semantics research is about . Instead of blaming', 'others for their ( wrong ) expectations , we need to learn how to emphasize the', 'paradigm shift proposed by semantics research while abstracting from technical details and advocate the added value', 'in a way that relates to the immediate needs of individual stakeholders without overselling . This', 'paper highlights some of the major ingredients to prepare your next Semantics Elevator Talk . Building', 'on abstract reference models , the Open Geospatial Consortium ( OGC ) has established standards for', 'storing , discovering , and processing geographical information . These standards act as basis for the', 'implementation of specic services and Spatial Data Infrastructures ( SDI ) . Research on geo-semantics plays', 'an increasing role to support complex queries and retrieval across heterogeneous information sources , as well', 'as for service orchestration , semantic translation , and on-the-y integration . So far , this', 'research targets individual solutions or focuses on the Semantic Web , leaving the integration into SDI', 'aside . What is missing is a shared and transparent semantic enablement layer for Spatial Data', 'Infrastructures which also integrates reasoning services known from the Semantic Web . Focusing on Sensor Web', 'Enablement ( SWE ) , we outline how Spatial Data Infrastructures in general can benet from', 'such a semantic enablement layer . Instead of developing new semantically enabled services from scratch ,', 'we propose to create proles of existing services that implement a transparent mapping between the OGC', 'and the Semantic Web world . Semantic similarity measurement gained attention over the last years as', 'a non-standard inference service for various kinds of knowledge representations including description logics . Most existing', 'similarity measures compute an undirected overall similarity , i.e. , they do not take the context', 'of the similarity query into account . If they do , the notion of context is', 'usually reduced to the selection of particular concepts for comparison ( instead of comparing all concepts', 'within an examined ontology ) . The importance of context in deriving meaningful similarity judgments is', 'beyond question and has been examined within recent research . This paper argues that there are', 'several kinds of contexts . Each of them has its own impact on the resulting similarity', 'values , but also on their interpretation . To support this view , the paper introduces', 'definitions for the examined contexts and illustrates their influence by example . The research field of', 'scientometrics is concerned with measuring and analyzing science . In practice , this is often done', 'by restricting the impact of publications , journals , and researchers to a mere frequency .', 'However , scientific activities ( co-publication , citation , labor mobility ) display clear spatiotemporal patterns', ', and such patterns have rarely been considered in traditional scientometrics . In this work we', 'focus on the study of citations and present a spatiotemporal scientometrics framework to measure the citation', 'impact of research output by taking physical space , place , and time into account .', 'Specifically , we use the statistics of categorical places ( institutions , cities , and countries', ') , spatiotemporal kernel density estimations , cartograms , distance distribution curves , and point-pattern analysis', 'to identify spatiotemporal citation patterns . Moreover , we propose a series of s-indices , such', \"as S_institution-index , S_city-index , and S_country-index to evaluate a scientist 's impact as a complement\", 'to non-spatial citation indicators , e.g. , h-index and g-index . In addition , we have', 'developed an interactive web application which allows users to visually explore research topics , authors ,', 'publications , as well as the spread of citations through space and time . Our work', 'offers insights on the role of location in scientific knowledge diffusion . The semantic integration of', 'heterogeneous , spatiotemporal information is a major challenge for achieving the vision of a multi-thematic and', 'multi-perspective Digital Earth . The Semantic Web technology stack has been proposed to address the integration', 'problem by knowledge representation languages and reasoning . However approaches such as the Web Ontology Languages', '( OWL ) were developed with decidability in mind . They do not integrate well with', 'established modeling paradigms in the geosciences that are dominated by numerical and geometric methods . Additionally', ', work on the Semantic Web is mostly feature-centric and a field-based view is difficult to', 'integrate . A layer specifying the transition from observation data to classes and relations is missing', '. In this work we combine OWL with geometric and topological language constructs based on similarity', 'spaces . Our approach provides three main benefits . First , class constructors can be built', 'from a larger palette of mathematical operations based on vector algebra . Second , it affords', 'the representation of prototype-based classes . Third , it facilitates the representation of classes derived from', 'machine learning classifiers that utilize a multi-dimensional feature space . Instead of following a one-size-fits-all approach', ', our work allows one to derive contextualized OWL ontologies by reification of observation data .', 'With the increasing amount of geographic information available on the Internet , searching , browsing ,', 'and organizing such information has become a major challenge within the field of Geographic Information Science', '( GIScience ) . As all information is ultimately for and from human beings , the', 'methodologies applied to retrieve and organize this information should correlate with human similarity judgments . Semantic', 'similarity measurement , which originated in psychology , is a methodology fulfilling this requirement and supporting', 'geographic information retrieval . The following special issue presents work on semantic similarity measurement from different', 'perspectives , including cognitive science , information retrieval , and ontology engineering , with a focus', 'on applications in GIScience . It originated in the Workshop on Semantic Similarity Measurement and Geospatial', 'Applications held in conjunction with COSIT 2007 , the International Conference on Spatial Information Theory (', 'http : //www.cosit.info/ ) . A substantial part of the workshop contributions addressed the need for', 'similarity measurement in geographic information retrieval , including applications in web service discovery , knowledge management', ', and emergency scenarios . The call for papers to this issue was based on these', 'workshop contributions , discussions , and results ( http : //musil.uni-muenster.de ) , but open to', 'any submissions on the role of semantic similarity in GIScience . Eleven papers were submitted and', 'then Geographic location is a key component for information retrieval on the Web , recommendation systems', 'in mobile computing and social networks , and place-based integration on the Linked Data cloud .', 'Previous work has addressed how to estimate locations by named entity recognition , from images ,', 'and via structured data . In this paper , we estimate geographic regions from unstructured ,', 'non geo-referenced text by computing a probability distribution over the Earth ’ s surface . Our', 'methodology combines natural language processing , geostatistics , and a data-driven bottom-up semantics . We illustrate', 'its potential for mapping geographic regions from non geo-referenced text . Reference to places is a', 'central but largely underexposed problem of information science . Place has been a major object of', 'research in many domains including Geography , Cognitive Science and Geographic Information Science . However ,', 'Geographic Information Systems ( GIS ) have been built solely on space reference systems creating a', 'gap between human conceptualization and machine representation . While reference to space only partially captures reference', 'to place , most existing definitions of place either reduce the latter to the former or', 'lack a formal characterization of how places are constructed . In a spatial coordinate system ,', 'locations are referenced by angles and distances to other referents . In this paper , we', 'suggest that place reference systems can be built based on localizing things ( locatums ) involved', 'in simulated activities relative to other involved referents ( locators ) . We propose a formal', 'theory about relevant types of activities and their involved participants , and show how place referents', 'can be identified and localized by choosing locators and locatum among the participants . We formally', 'derive an ontology of places , publish a corresponding OWL version , and demonstrate how to', 'compute a market place and a vantage place in a GIS . Recent years have witnessed', 'a large increase in the amount of information available from the Web and many other sources', '. Such an information deluge presents a challenge for individuals who have to identify useful information', 'items to complete particular tasks in hand . Information value theory IVT from economics and artificial', 'intelligence has provided some guidance on this issue . However , existing IVT studies often focus', 'on monetary values , while ignoring the spatiotemporal properties which can play important roles in everyday', 'tasks . In this paper , we propose a theoretical framework for task-oriented information value measurement', '. This framework integrates IVT with the space-time prism from time geography and measures the value', 'of information based on its impact on an individual ’ s space-time prisms and its capability', 'of improving task planning . We develop and formalize this framework by extending the utility function', 'from space-time accessibility studies and elaborate it using a simplified example from time geography . We', 'conduct a simulation on a real-world transportation network using the proposed framework . Our research could', 'be applied to improving information display on small-screen mobile devices e.g. , smartwatches by assigning priorities', 'to different information items . GeoLink is one of the building block projects within EarthCube ,', 'a major effort of the National Science Foundation to establish a next-generation knowledge infrastructure for geosciences', '. As part of this effort , GeoLink aims to improve data retrieval , reuse ,', 'and integration of seven geoscience data repositories through the use of ontologies . In this paper', ', we report on the GeoLink modular ontology , which consists of an interlinked collection of', 'ontology design patterns engineered as the result of a collaborative modeling effort . We explain our', 'design choices , present selected modeling details , and discuss how data integration can be achieved', 'using the patterns while respecting the existing heterogeneity within the participating repositories . A major focus', 'in the design of Semantic Web ontology languages used to be on finding a suitable balance', 'between the expressivity of the language and the tractability of reasoning services defined over this language', '. This focus mirrors the original vision of a Web composed of machine readable and understandable', 'data . Similarly to the classical Web a few years ago , the attention is recently', 'shifting towards a user-centric vision of the Semantic Web . Essentially , the information stored on', 'the Web is from and for humans . This new focus is not only reflected in', 'the fast growing Linked Data Web but also in the increasing influence of research from cognitive', 'science , human computer interaction , and machine-learning . Cognitive aspects emerge as an essential ingredient', 'for future work on knowledge acquisition , representation , reasoning , and interactions on the Semantic', 'Web . Visual interfaces have to support semantic-based retrieval and at the same time hide the', 'complexity of the underlying reasoning machinery from the user . Analogical and similarity-based reasoning should assist', 'users in browsing and navigating through the rapidly increasing amount of information . Instead of pre-defined', 'conceptualizations of the world , the selection and conceptualization of relevant information has to be tailored', \"to the user 's context on-the-fly . This involves work on ontology modularization and context-awareness ,\", 'but also approaches from ecological psychology such as affordance theory which also plays an increasing role', 'in robotics and AI . During the Dagstuhl Seminar 12221 we discussed the most promising ways', 'to move forward on the vision of bringing findings from cognitive science to the Semantic Web', ', and to create synergies between the different areas of research . While the seminar focused', 'on the use of cognitive engineering for a user-centric Semantic Web , it also discussed the', 'reverse direction , i.e. , how can the Semantic Web work on knowledge representation and reasoning', 'feed back to the cognitive science community . The Worskhop on Ontology and Semantic Web Patterns', '( WOP2015 , 6th edition ) was held on October 11 , 2015 in conjunction with', 'the 14th International Semantic Web Conference in Bethlehem , PA , USA . At the workshop', ', the organizers conducted a discussion with the participants regarding the promises and obstacles of ontology', 'design patterns ( ODPs for short ) . This editorial reports on those discussions . We', 'begin with a brief introduction of ODPs for the unfamiliar reader . In this work we', 'discuss an ontology design pattern for material transformations . It models the relation between products ,', 'resources , and catalysts in the transformation process . Our axiomatization goes beyond a mere surface', 'semantics . While we focus on the construction domain , the pattern can also be applied', 'to chemistry and other domains . This paper presents an overview of ongoing work to develop', 'a generic ontology design pattern for observation-based data on the Semantic Web . The core classes', 'and relationships forming the pattern are discussed in detail and are aligned to the DOLCE foundational', 'ontology to improve semantic interoperability and clarify the underlying ontological commitments . The pattern also forms', 'the top-level of the the Semantic Sensor Network ontology developed by the W3C Semantic Sensor Network', 'Incubator Group . The integration of both ontologies is discussed and directions of further work are', 'pointed out . Information plays an important role in disaster response . In the past ,', 'there has been a lack of up-to-date information following major disasters due to the limited means', 'of communication . This situation has changed substantially in recent years . With the ubiquity of', 'mobile devices , people experiencing emergency events may still be able to share information via social', 'media and peer-to-peer networks . Meanwhile , volunteers throughout the world are remotely convened by humanitarian', 'organizations to digitize satellite images for the impacted area . These processes produce rich information which', 'presents a new challenge for decision makers who have to interpret large amount of heterogeneous information', 'within limited time . This short paper discusses this problem and outlines a potential solution to', 'prioritizing information in emergency situations . Specifically , we focus on information about road network connectivity', ', i.e. , whether a road segment is still accessible after a disaster . We propose', 'to integrate information value theory with graph theory , and prioritize information items based on their', 'contributions to the successes of potential rescue tasks and to the more accurate estimation of road', 'network connectivity . Finally , we point out directions for future work . GIScience 2016 Short', 'Paper Proceedings Understanding the Mapping Sequence of Online Volunteers in Disaster Response Yingjie Hu and Krzysztof', 'Janowicz { yingjiehu , jano } @ geog.ucsb.edu STKO Lab , Department of Geography , University', 'of California , Santa Barbara , USA Abstract In recent years , online volunteers have been', 'actively participating in disaster response , thanks to the advancement of information technologies and the support', 'from humanitarian organizations . One important way in which online volunteers contribute to disaster response is', 'by mapping the a↵ected area based on remote sensing imagery . Such online mapping generates up-to-date', 'geographic informa- tion which can provide valuable support for the decision making of emergency responders .', 'Typically , the area a↵ected by an disaster is divided into a number of cells using', 'a grid-based tessellation and each volunteer can select one cell to start the mapping . While', 'this approach coordinates the e↵orts from many online volunteers , it is unclear in which sequence', 'these grid cells have been mapped . This sequence is important because it determines when the', 'geographic information within a particular cell will become available to emergency responders , which in turn', 'can directly influence the efficiency of rescue tasks and other relief e↵orts . In this work', ', we study three online mapping projects which were deployed and utilized in 2015 Nepal ,', '2016 Ecuador , and 2016 Japan earthquakes to gain insights into the mapping sequences performed by', 'online volunteers . Keywords : Disaster response , crisis mapping , volunteered geographic information Introduction In', 'recent years , online volunteers have been actively involved in disaster response . On the one', 'side , infor- mation and communication technologies allow volunteers to contribute to disaster relief without having', 'to be physically present at the a↵ected areas . On the other side , humanitarian communities', ', such as Standby Task Force ( Meier , 2012a ) and Crisis Mapper ( Shanley', 'et al. , 2013 ) , play an important role in bringing together online volunteers and', 'coordinating their e↵orts . With the support from technologies and hu- manitarian organizations , volunteers have', 'made important contributions to 2010 Haiti earthquake ( Zook et al. , 2010 ) , 2012', 'Hurricane Sandy in the U.S. ( Meier , 2012b ) , 2013 Typhoon Haiyan in the', 'Philippines ( Humanitarian OpenStreetMap Team , 2013 ) , and the 2015 Nepal Earthquake ( Hu', 'and Janowicz , 2015 ) . One important way in which volunteers contribute to disaster response', 'is by mapping the a↵ected areas based on remote sensing images . During the online mapping', 'process , volunteers digitize geographic features which may be missing from the previous maps , as', 'well as update the existing geographic data to reflect the current status ( e.g. , a', 'road may be blocked after an earthquake ) . This process generates up- to-date geographic information', 'which can provide valuable support for the decision making of emergency responders . While these volunteer-contributed', 'data may not be of highest quality , they generally satisfy the needs of disaster response', '( Goodchild and Glennon , 2010 ) . Since many volunteers may be participating in online', 'mapping at the same time , humanitarian orga- nizations often divide the a↵ected area into cells', 'using a grid-based tessellation . Each online volunteer can then select a grid cell to start', 'the mapping task . While this approach helps avoid editing conflicts and duplications , there is', 'a lack of understanding on the sequence in which online volunteers map the grid cells .', 'Such a sequence is important because it directly determines the time when the geographic information within', 'a particular cell will become available . From our observation , the mapping time di↵er- ence', 'between two neighboring grid cells can be 4 days and sometimes even longer . In disaster', 'response , the first 72 hours after a disaster have been widely considered as the critical', 'period for rescue tasks . After this period , the survival rate drops dramatically ( Fiedrich', 'et al. , 2000 ; Comfort et al. , 2004 ; Ochoa and Santos , 2015', ') . Thus , if the grid cells that contain the critical information for disaster response', 'are mapped first , more people can be potentially saved . Intuitively , selecting cells at', 'random is not an ideal solution , since population , transportation infrastructure , and potential rescue', 'routes should be taken into account . Sensor data is stored and published using OGC ’', 's Observation & Measurement specifications as underlying data model . With the advent of volunteered geographic', 'information and the Semantic Sensor Web , work on an ontological , i.e . conceptual ,', 'model gains importance within the Sensor Web Enablement community . In contrast to a data model', ', an ontological approach abstracts from implementation details by focusing on modeling the real world from', 'the perspective of a particular domain or application and , hence , restricts the interpretation of', 'the used terminology towards their intended meaning . The shift to linked sensor data , however', ', requires yet another perspective . Two challenges have to be addressed , ( i )', 'how to refer to changing and frequently updated data sets such as stored in Sensor Observation', 'Services using Uniform Resource Identifiers , and ( ii ) how to establish meaningful links between', 'those data sets , i.e. , observations , sensors , features of interest , observed properties', ', and further participants in the measurement process . In this short paper we focus on', 'the problem of assigning meaningful URIs . Traditional gazetteers are built and maintained by authoritative mapping', 'agencies . In the age of Big Data , it is possible to construct gazetteers in', 'a data-driven approach by mining rich volunteered geographic information ( VGI ) from the Web .', 'In this research , we build a scalable distributed platform and a high-performance geoprocessing workflow based', 'on the Hadoop ecosystem to harvest crowd-sourced gazetteer entries . Using experiments based on geotagged datasets', 'in Flickr , we find that the MapReduce-based workflow running on the spatially enabled Hadoop cluster', 'can reduce the processing time compared with traditional desktop-based operations by an order of magnitude .', 'We demonstrate how to use such a novel spatial-computing infrastructure to facilitate gazetteer research . In', 'addition , we introduce a provenance-based trust model for quality assurance . This work offers new', 'insights on enriching future gazetteers with the use of Hadoop clusters , and makes contributions in', 'connecting GIS to the cloud computing environment for the next frontier of Big Geo-Data analytics .']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity scores: [array([[0.04133633]], dtype=float32), array([[0.10168575]], dtype=float32), array([[0.2740295]], dtype=float32), array([[0.20839812]], dtype=float32), array([[0.1718004]], dtype=float32), array([[0.11827105]], dtype=float32), array([[0.13275981]], dtype=float32), array([[0.1943388]], dtype=float32), array([[0.17921267]], dtype=float32), array([[0.18198656]], dtype=float32), array([[0.20303169]], dtype=float32), array([[0.18992338]], dtype=float32), array([[0.21303508]], dtype=float32), array([[0.21277577]], dtype=float32), array([[0.19749165]], dtype=float32), array([[0.23808303]], dtype=float32), array([[0.16581991]], dtype=float32), array([[0.35094875]], dtype=float32), array([[0.11380272]], dtype=float32), array([[0.2605111]], dtype=float32), array([[0.22461057]], dtype=float32), array([[0.1463552]], dtype=float32), array([[0.38664907]], dtype=float32), array([[0.15024084]], dtype=float32), array([[0.27342382]], dtype=float32), array([[0.06834944]], dtype=float32), array([[0.10262391]], dtype=float32), array([[0.24779914]], dtype=float32), array([[0.27117372]], dtype=float32), array([[0.26472998]], dtype=float32), array([[0.1783003]], dtype=float32), array([[0.14836979]], dtype=float32), array([[0.10656789]], dtype=float32), array([[0.02331317]], dtype=float32), array([[0.2732209]], dtype=float32), array([[0.19107363]], dtype=float32), array([[0.2272911]], dtype=float32), array([[0.15297902]], dtype=float32), array([[0.16155326]], dtype=float32), array([[0.30904076]], dtype=float32), array([[0.16093789]], dtype=float32), array([[0.08526479]], dtype=float32), array([[0.08601565]], dtype=float32), array([[-0.00500865]], dtype=float32), array([[0.2396878]], dtype=float32), array([[0.31350046]], dtype=float32), array([[0.2097481]], dtype=float32), array([[0.19201577]], dtype=float32), array([[0.35934958]], dtype=float32), array([[0.14315695]], dtype=float32), array([[0.30776697]], dtype=float32), array([[0.35580587]], dtype=float32), array([[0.15229127]], dtype=float32), array([[0.22209048]], dtype=float32), array([[0.41411433]], dtype=float32), array([[0.13723373]], dtype=float32), array([[0.2604927]], dtype=float32), array([[0.29911402]], dtype=float32), array([[0.31191283]], dtype=float32), array([[0.2015392]], dtype=float32), array([[0.28589362]], dtype=float32), array([[0.30056953]], dtype=float32), array([[0.11372986]], dtype=float32), array([[0.17301033]], dtype=float32), array([[0.26146227]], dtype=float32), array([[0.10970306]], dtype=float32), array([[0.2354351]], dtype=float32), array([[0.06505375]], dtype=float32), array([[0.2757541]], dtype=float32), array([[0.28282553]], dtype=float32), array([[0.2351014]], dtype=float32), array([[0.05818249]], dtype=float32), array([[0.25869167]], dtype=float32), array([[0.3316713]], dtype=float32), array([[0.17430738]], dtype=float32), array([[0.30717885]], dtype=float32), array([[0.11752373]], dtype=float32), array([[0.1368978]], dtype=float32), array([[0.03803418]], dtype=float32), array([[0.35932454]], dtype=float32), array([[0.16976689]], dtype=float32), array([[0.14385563]], dtype=float32), array([[-0.01174741]], dtype=float32), array([[0.1936493]], dtype=float32), array([[0.14568886]], dtype=float32), array([[0.30994397]], dtype=float32), array([[0.25758755]], dtype=float32), array([[0.04656474]], dtype=float32), array([[0.10286076]], dtype=float32), array([[0.18379022]], dtype=float32), array([[0.01446084]], dtype=float32), array([[0.03353214]], dtype=float32), array([[0.10818607]], dtype=float32), array([[0.13596977]], dtype=float32), array([[0.19647215]], dtype=float32), array([[0.1589333]], dtype=float32), array([[0.18788362]], dtype=float32), array([[0.26347923]], dtype=float32), array([[0.3357665]], dtype=float32), array([[0.25290427]], dtype=float32), array([[0.13011831]], dtype=float32), array([[0.24361274]], dtype=float32), array([[0.1745073]], dtype=float32), array([[0.126792]], dtype=float32), array([[0.33877814]], dtype=float32), array([[0.3373201]], dtype=float32), array([[0.08734491]], dtype=float32), array([[0.23418105]], dtype=float32), array([[0.07478943]], dtype=float32), array([[0.1182897]], dtype=float32), array([[0.2510471]], dtype=float32), array([[0.07799783]], dtype=float32), array([[0.24698165]], dtype=float32), array([[0.31971496]], dtype=float32), array([[0.3762905]], dtype=float32), array([[-0.06212611]], dtype=float32), array([[0.1359229]], dtype=float32), array([[0.19243835]], dtype=float32), array([[0.34190187]], dtype=float32), array([[0.2532313]], dtype=float32), array([[0.45793658]], dtype=float32), array([[0.2349908]], dtype=float32), array([[0.2608045]], dtype=float32), array([[0.3173101]], dtype=float32), array([[0.03980331]], dtype=float32), array([[0.01093741]], dtype=float32), array([[0.09695848]], dtype=float32), array([[-0.01077809]], dtype=float32), array([[0.10007164]], dtype=float32), array([[0.22461486]], dtype=float32), array([[0.15223864]], dtype=float32), array([[0.42735994]], dtype=float32), array([[-0.02115121]], dtype=float32), array([[-0.00947059]], dtype=float32), array([[0.26056173]], dtype=float32), array([[0.00152366]], dtype=float32), array([[0.1427589]], dtype=float32), array([[0.14124414]], dtype=float32), array([[0.0746953]], dtype=float32), array([[0.21330386]], dtype=float32), array([[0.09072918]], dtype=float32), array([[0.07161291]], dtype=float32), array([[0.0702677]], dtype=float32), array([[0.11676686]], dtype=float32), array([[0.18132347]], dtype=float32), array([[0.06986174]], dtype=float32), array([[0.16073902]], dtype=float32), array([[0.11771392]], dtype=float32), array([[0.19122379]], dtype=float32), array([[0.27010944]], dtype=float32), array([[0.07622749]], dtype=float32), array([[0.02657591]], dtype=float32), array([[0.09635357]], dtype=float32), array([[0.13093275]], dtype=float32), array([[0.13945507]], dtype=float32), array([[0.26325047]], dtype=float32), array([[0.18229108]], dtype=float32), array([[0.08909527]], dtype=float32), array([[0.10270163]], dtype=float32), array([[0.02814461]], dtype=float32), array([[0.26502275]], dtype=float32), array([[0.04870018]], dtype=float32), array([[0.06221019]], dtype=float32), array([[0.10783118]], dtype=float32), array([[0.15273285]], dtype=float32), array([[0.12208533]], dtype=float32), array([[0.199564]], dtype=float32), array([[-0.06770624]], dtype=float32), array([[-0.02177195]], dtype=float32), array([[0.30386433]], dtype=float32), array([[0.2211959]], dtype=float32), array([[0.22931898]], dtype=float32), array([[0.06462178]], dtype=float32), array([[0.10562267]], dtype=float32), array([[0.1286157]], dtype=float32), array([[-0.01134822]], dtype=float32), array([[0.13900183]], dtype=float32), array([[0.1268489]], dtype=float32), array([[0.17557767]], dtype=float32), array([[0.07896981]], dtype=float32), array([[0.24962053]], dtype=float32), array([[0.21067393]], dtype=float32), array([[0.08264368]], dtype=float32), array([[0.180249]], dtype=float32), array([[0.33871025]], dtype=float32), array([[0.33275303]], dtype=float32), array([[0.26880604]], dtype=float32), array([[0.38471812]], dtype=float32), array([[0.43452466]], dtype=float32), array([[0.12144546]], dtype=float32), array([[0.3008058]], dtype=float32), array([[0.36666363]], dtype=float32), array([[0.00742115]], dtype=float32), array([[0.1850897]], dtype=float32), array([[0.20562142]], dtype=float32), array([[0.07910085]], dtype=float32), array([[0.04579213]], dtype=float32), array([[0.19056867]], dtype=float32), array([[0.21290419]], dtype=float32), array([[0.234864]], dtype=float32), array([[0.12000775]], dtype=float32), array([[0.06567149]], dtype=float32), array([[0.06074525]], dtype=float32), array([[-0.03427095]], dtype=float32), array([[0.22455555]], dtype=float32), array([[-0.03606088]], dtype=float32), array([[0.04721713]], dtype=float32), array([[0.09849668]], dtype=float32), array([[0.13603593]], dtype=float32), array([[0.14974718]], dtype=float32), array([[0.15110123]], dtype=float32), array([[0.06975481]], dtype=float32), array([[0.14937931]], dtype=float32), array([[-0.01902898]], dtype=float32), array([[0.02918434]], dtype=float32), array([[-0.06423699]], dtype=float32), array([[0.0705739]], dtype=float32), array([[0.00299897]], dtype=float32), array([[0.09769067]], dtype=float32), array([[0.07423806]], dtype=float32), array([[0.03883588]], dtype=float32), array([[0.04694194]], dtype=float32), array([[0.28087673]], dtype=float32), array([[0.06724767]], dtype=float32), array([[0.2838376]], dtype=float32), array([[0.2160551]], dtype=float32), array([[0.15775624]], dtype=float32), array([[0.17544803]], dtype=float32), array([[0.14936751]], dtype=float32), array([[0.04007821]], dtype=float32), array([[0.19773921]], dtype=float32), array([[0.02001278]], dtype=float32), array([[0.19173773]], dtype=float32), array([[-0.00523958]], dtype=float32), array([[0.00341182]], dtype=float32), array([[0.14372681]], dtype=float32), array([[0.00630173]], dtype=float32), array([[0.1169707]], dtype=float32), array([[0.09030619]], dtype=float32), array([[0.10446377]], dtype=float32), array([[0.0593368]], dtype=float32), array([[0.13376911]], dtype=float32), array([[0.07937647]], dtype=float32), array([[0.04550282]], dtype=float32), array([[0.15763392]], dtype=float32), array([[0.15323988]], dtype=float32), array([[0.10432994]], dtype=float32), array([[-0.00299262]], dtype=float32), array([[0.27959347]], dtype=float32), array([[0.00802352]], dtype=float32), array([[-0.00486987]], dtype=float32), array([[0.09436922]], dtype=float32), array([[0.08147159]], dtype=float32), array([[0.12757482]], dtype=float32), array([[0.23859553]], dtype=float32), array([[0.01478443]], dtype=float32), array([[0.10015232]], dtype=float32), array([[0.06725822]], dtype=float32), array([[0.05352192]], dtype=float32), array([[0.04885126]], dtype=float32), array([[0.07365108]], dtype=float32), array([[0.29675713]], dtype=float32), array([[-0.01963204]], dtype=float32), array([[0.08486754]], dtype=float32), array([[0.04408344]], dtype=float32), array([[0.11570272]], dtype=float32), array([[0.25496054]], dtype=float32), array([[0.21073109]], dtype=float32), array([[0.17125854]], dtype=float32), array([[0.08606161]], dtype=float32), array([[0.21802872]], dtype=float32), array([[0.18744293]], dtype=float32), array([[0.18099636]], dtype=float32), array([[0.06265494]], dtype=float32), array([[0.04921668]], dtype=float32), array([[0.12341763]], dtype=float32), array([[0.29183728]], dtype=float32), array([[0.03998421]], dtype=float32), array([[0.17990568]], dtype=float32), array([[0.04141684]], dtype=float32), array([[0.1911725]], dtype=float32), array([[0.23615092]], dtype=float32), array([[0.21045257]], dtype=float32), array([[0.14452466]], dtype=float32), array([[0.08026308]], dtype=float32), array([[0.12221336]], dtype=float32), array([[0.1751111]], dtype=float32), array([[0.07623432]], dtype=float32), array([[0.1435254]], dtype=float32), array([[0.02875493]], dtype=float32), array([[0.218816]], dtype=float32), array([[0.11442401]], dtype=float32), array([[0.075073]], dtype=float32), array([[0.32987815]], dtype=float32), array([[0.30156595]], dtype=float32), array([[0.14281875]], dtype=float32), array([[0.10220532]], dtype=float32), array([[0.05378012]], dtype=float32), array([[0.35003895]], dtype=float32), array([[0.0860023]], dtype=float32), array([[0.304568]], dtype=float32), array([[0.09316966]], dtype=float32), array([[0.3099712]], dtype=float32), array([[0.07086542]], dtype=float32), array([[0.30720925]], dtype=float32), array([[-0.02286717]], dtype=float32), array([[0.17969787]], dtype=float32), array([[0.15659377]], dtype=float32), array([[0.04626664]], dtype=float32), array([[0.00705462]], dtype=float32), array([[0.31716526]], dtype=float32), array([[0.19548792]], dtype=float32), array([[0.13830626]], dtype=float32), array([[0.23931685]], dtype=float32), array([[0.22752571]], dtype=float32), array([[0.09864604]], dtype=float32), array([[0.13549002]], dtype=float32), array([[0.15261288]], dtype=float32), array([[0.19535944]], dtype=float32), array([[0.27692124]], dtype=float32), array([[0.1573763]], dtype=float32), array([[0.19270623]], dtype=float32), array([[0.3236766]], dtype=float32), array([[0.1921894]], dtype=float32), array([[0.13491766]], dtype=float32), array([[0.16777152]], dtype=float32), array([[0.0302957]], dtype=float32), array([[0.34478563]], dtype=float32), array([[0.07296757]], dtype=float32), array([[0.10234397]], dtype=float32), array([[0.20510028]], dtype=float32), array([[0.28933224]], dtype=float32), array([[0.07351589]], dtype=float32), array([[0.30475783]], dtype=float32), array([[0.0355114]], dtype=float32), array([[0.16668795]], dtype=float32), array([[0.04018566]], dtype=float32), array([[-0.04043003]], dtype=float32), array([[0.06986833]], dtype=float32), array([[0.02421188]], dtype=float32), array([[0.03776225]], dtype=float32), array([[0.04462365]], dtype=float32), array([[0.1344135]], dtype=float32), array([[0.03573719]], dtype=float32), array([[0.03409394]], dtype=float32), array([[0.16006094]], dtype=float32), array([[0.04185556]], dtype=float32), array([[0.04868601]], dtype=float32), array([[0.02971584]], dtype=float32), array([[0.02561467]], dtype=float32), array([[0.19201861]], dtype=float32), array([[0.46372217]], dtype=float32), array([[0.36963475]], dtype=float32), array([[0.41463995]], dtype=float32), array([[0.2579251]], dtype=float32), array([[0.35301593]], dtype=float32), array([[0.07740983]], dtype=float32), array([[0.18659315]], dtype=float32), array([[0.13124137]], dtype=float32), array([[0.18783776]], dtype=float32), array([[0.22665727]], dtype=float32), array([[0.19193935]], dtype=float32), array([[-0.01867246]], dtype=float32), array([[0.18674473]], dtype=float32), array([[0.23310572]], dtype=float32), array([[0.11579014]], dtype=float32), array([[0.271829]], dtype=float32), array([[0.27512255]], dtype=float32), array([[0.15752281]], dtype=float32), array([[0.18600711]], dtype=float32), array([[-0.00992868]], dtype=float32), array([[0.25182092]], dtype=float32), array([[0.22789508]], dtype=float32), array([[0.24153823]], dtype=float32), array([[0.01688409]], dtype=float32), array([[0.09826481]], dtype=float32), array([[0.33964473]], dtype=float32), array([[0.1283014]], dtype=float32), array([[0.16588506]], dtype=float32), array([[0.48871502]], dtype=float32), array([[0.20408043]], dtype=float32), array([[0.08243524]], dtype=float32), array([[0.4317782]], dtype=float32), array([[0.02387233]], dtype=float32), array([[0.10840506]], dtype=float32), array([[0.10510973]], dtype=float32), array([[0.1446186]], dtype=float32), array([[0.44180745]], dtype=float32), array([[0.16414794]], dtype=float32), array([[0.3114254]], dtype=float32), array([[0.01365455]], dtype=float32), array([[0.3262539]], dtype=float32), array([[0.32538962]], dtype=float32), array([[0.17197977]], dtype=float32), array([[0.22374472]], dtype=float32), array([[0.05588946]], dtype=float32), array([[0.1751435]], dtype=float32), array([[-0.0415534]], dtype=float32), array([[0.16621044]], dtype=float32), array([[0.02520299]], dtype=float32), array([[0.22944413]], dtype=float32), array([[0.24532737]], dtype=float32), array([[0.16602813]], dtype=float32), array([[0.06595545]], dtype=float32), array([[0.19886672]], dtype=float32), array([[-0.08543485]], dtype=float32), array([[-0.05947503]], dtype=float32), array([[0.06423549]], dtype=float32), array([[0.24256897]], dtype=float32), array([[0.19758797]], dtype=float32), array([[0.13337317]], dtype=float32), array([[0.17417404]], dtype=float32), array([[0.13045867]], dtype=float32), array([[0.0141049]], dtype=float32), array([[0.14174192]], dtype=float32), array([[0.08549345]], dtype=float32), array([[0.14543472]], dtype=float32), array([[0.20292051]], dtype=float32), array([[0.08608314]], dtype=float32), array([[-0.08091033]], dtype=float32), array([[0.07384297]], dtype=float32), array([[0.15472372]], dtype=float32), array([[0.1462886]], dtype=float32), array([[0.02524577]], dtype=float32), array([[0.23264152]], dtype=float32), array([[0.13888024]], dtype=float32), array([[0.16846392]], dtype=float32), array([[0.01456254]], dtype=float32), array([[-0.03959613]], dtype=float32), array([[0.2210247]], dtype=float32), array([[0.26055786]], dtype=float32), array([[0.0972074]], dtype=float32), array([[-0.02378456]], dtype=float32), array([[0.15768361]], dtype=float32), array([[0.04117303]], dtype=float32), array([[0.11038235]], dtype=float32), array([[0.12712038]], dtype=float32), array([[0.39255852]], dtype=float32), array([[0.16459897]], dtype=float32), array([[0.2983346]], dtype=float32), array([[0.08707131]], dtype=float32), array([[-0.00782336]], dtype=float32), array([[0.10300614]], dtype=float32), array([[0.32860172]], dtype=float32), array([[0.12697582]], dtype=float32), array([[0.43847558]], dtype=float32), array([[0.04763874]], dtype=float32), array([[-0.02019164]], dtype=float32), array([[0.13801752]], dtype=float32), array([[0.01596424]], dtype=float32), array([[0.12606105]], dtype=float32), array([[0.02897143]], dtype=float32), array([[0.35611084]], dtype=float32), array([[-0.02938756]], dtype=float32), array([[0.07536031]], dtype=float32), array([[0.09160601]], dtype=float32), array([[0.3016113]], dtype=float32), array([[0.20462275]], dtype=float32), array([[0.11312263]], dtype=float32), array([[0.44114634]], dtype=float32), array([[0.23847014]], dtype=float32), array([[0.35921854]], dtype=float32), array([[0.06445025]], dtype=float32), array([[0.05438806]], dtype=float32), array([[0.1629616]], dtype=float32), array([[0.15628316]], dtype=float32), array([[0.23513901]], dtype=float32), array([[0.25322586]], dtype=float32), array([[0.25128388]], dtype=float32), array([[0.28952822]], dtype=float32), array([[0.3691746]], dtype=float32), array([[0.3744057]], dtype=float32), array([[0.3007084]], dtype=float32), array([[0.3327393]], dtype=float32), array([[0.10689234]], dtype=float32), array([[0.22081922]], dtype=float32), array([[0.22875953]], dtype=float32), array([[0.1348475]], dtype=float32), array([[0.1469744]], dtype=float32), array([[-0.00256571]], dtype=float32), array([[0.21450199]], dtype=float32), array([[0.2671082]], dtype=float32), array([[0.34186244]], dtype=float32), array([[0.30919117]], dtype=float32), array([[0.13955465]], dtype=float32), array([[0.23672256]], dtype=float32), array([[0.2785095]], dtype=float32), array([[0.40800273]], dtype=float32), array([[0.33809823]], dtype=float32), array([[0.5491055]], dtype=float32), array([[0.21124953]], dtype=float32), array([[0.3907198]], dtype=float32), array([[0.3920084]], dtype=float32), array([[0.37718752]], dtype=float32), array([[0.21206844]], dtype=float32), array([[0.21740589]], dtype=float32), array([[0.24219137]], dtype=float32), array([[0.30228657]], dtype=float32), array([[0.1179424]], dtype=float32), array([[0.11762173]], dtype=float32), array([[0.3009593]], dtype=float32), array([[0.2639991]], dtype=float32), array([[0.11759809]], dtype=float32), array([[0.34859276]], dtype=float32), array([[0.25399217]], dtype=float32), array([[0.02943568]], dtype=float32), array([[0.12274586]], dtype=float32), array([[0.13745373]], dtype=float32), array([[0.02281516]], dtype=float32), array([[0.03595397]], dtype=float32), array([[0.12640646]], dtype=float32), array([[0.2809683]], dtype=float32), array([[-0.01454143]], dtype=float32), array([[0.18135719]], dtype=float32), array([[0.30967253]], dtype=float32), array([[0.22708033]], dtype=float32), array([[0.3467686]], dtype=float32), array([[0.32778504]], dtype=float32), array([[0.3876828]], dtype=float32), array([[0.5023447]], dtype=float32), array([[0.14697957]], dtype=float32), array([[0.18380558]], dtype=float32), array([[0.20474114]], dtype=float32), array([[0.23035958]], dtype=float32), array([[0.2026856]], dtype=float32), array([[0.23319514]], dtype=float32), array([[0.1611418]], dtype=float32), array([[0.1751632]], dtype=float32), array([[0.2706005]], dtype=float32), array([[0.2623285]], dtype=float32), array([[0.17768718]], dtype=float32), array([[0.08353023]], dtype=float32), array([[0.3663355]], dtype=float32), array([[0.0327191]], dtype=float32), array([[0.26219124]], dtype=float32), array([[0.10679267]], dtype=float32), array([[0.32409155]], dtype=float32), array([[0.26619905]], dtype=float32), array([[0.05238034]], dtype=float32), array([[0.17995888]], dtype=float32), array([[0.00570397]], dtype=float32), array([[-0.00521651]], dtype=float32), array([[0.03363218]], dtype=float32), array([[0.11499185]], dtype=float32), array([[0.18602192]], dtype=float32), array([[0.0151479]], dtype=float32), array([[0.12812738]], dtype=float32), array([[0.27396116]], dtype=float32), array([[0.3226202]], dtype=float32), array([[0.5092569]], dtype=float32), array([[0.5169395]], dtype=float32), array([[0.3334829]], dtype=float32), array([[0.0600232]], dtype=float32), array([[0.51853245]], dtype=float32), array([[0.40404934]], dtype=float32), array([[0.20508182]], dtype=float32), array([[0.35578424]], dtype=float32), array([[0.32753307]], dtype=float32), array([[0.2996388]], dtype=float32), array([[0.3398716]], dtype=float32), array([[0.25413835]], dtype=float32), array([[0.2485122]], dtype=float32), array([[0.05666316]], dtype=float32), array([[0.15026097]], dtype=float32), array([[0.22915912]], dtype=float32), array([[0.33688107]], dtype=float32), array([[0.22863525]], dtype=float32), array([[0.31231153]], dtype=float32), array([[0.07639134]], dtype=float32), array([[0.12508945]], dtype=float32), array([[0.30251992]], dtype=float32), array([[0.25083733]], dtype=float32), array([[0.05015529]], dtype=float32), array([[0.12689875]], dtype=float32), array([[0.2660174]], dtype=float32), array([[0.10474158]], dtype=float32), array([[0.08628735]], dtype=float32), array([[0.23264474]], dtype=float32), array([[0.25971067]], dtype=float32), array([[0.38077423]], dtype=float32), array([[0.01118941]], dtype=float32), array([[0.40495718]], dtype=float32), array([[0.43345892]], dtype=float32), array([[0.21990296]], dtype=float32), array([[0.23311096]], dtype=float32), array([[0.448013]], dtype=float32), array([[0.49701905]], dtype=float32), array([[0.36770034]], dtype=float32), array([[0.22371146]], dtype=float32), array([[0.19558409]], dtype=float32), array([[0.27236456]], dtype=float32), array([[0.14002563]], dtype=float32), array([[0.2551473]], dtype=float32), array([[0.12176093]], dtype=float32), array([[0.08243778]], dtype=float32), array([[-0.0580786]], dtype=float32), array([[0.17112824]], dtype=float32), array([[0.37681255]], dtype=float32), array([[0.2853081]], dtype=float32), array([[0.34310812]], dtype=float32), array([[0.34005713]], dtype=float32), array([[0.1142263]], dtype=float32), array([[0.37404555]], dtype=float32), array([[0.31219366]], dtype=float32), array([[0.20881408]], dtype=float32), array([[0.22498329]], dtype=float32), array([[0.19022208]], dtype=float32), array([[0.15904117]], dtype=float32), array([[0.16912895]], dtype=float32), array([[0.06518123]], dtype=float32), array([[0.09600665]], dtype=float32), array([[0.17529231]], dtype=float32), array([[0.19120008]], dtype=float32), array([[0.1554943]], dtype=float32), array([[0.23896137]], dtype=float32), array([[0.25237656]], dtype=float32), array([[0.1081522]], dtype=float32), array([[0.06816895]], dtype=float32), array([[0.12659147]], dtype=float32), array([[0.04556708]], dtype=float32), array([[0.14833032]], dtype=float32), array([[0.21356353]], dtype=float32), array([[0.04135469]], dtype=float32), array([[0.15741435]], dtype=float32), array([[0.2779039]], dtype=float32), array([[0.03478964]], dtype=float32), array([[0.20860268]], dtype=float32), array([[0.16414335]], dtype=float32), array([[0.1676766]], dtype=float32), array([[0.23708308]], dtype=float32), array([[0.13165276]], dtype=float32), array([[0.2863899]], dtype=float32), array([[0.16546081]], dtype=float32), array([[0.16532679]], dtype=float32), array([[0.14886929]], dtype=float32), array([[0.10346933]], dtype=float32), array([[0.352203]], dtype=float32), array([[0.3276086]], dtype=float32), array([[0.07519653]], dtype=float32), array([[0.09330392]], dtype=float32), array([[0.37316018]], dtype=float32), array([[0.21344829]], dtype=float32), array([[0.20306042]], dtype=float32), array([[0.24775447]], dtype=float32), array([[0.23141196]], dtype=float32), array([[0.05692059]], dtype=float32), array([[0.22031397]], dtype=float32), array([[0.21424831]], dtype=float32), array([[0.1602152]], dtype=float32), array([[0.37208635]], dtype=float32), array([[0.4762761]], dtype=float32), array([[0.11200823]], dtype=float32), array([[0.18243384]], dtype=float32), array([[0.06470323]], dtype=float32), array([[-0.01842446]], dtype=float32), array([[0.2605579]], dtype=float32), array([[0.07092803]], dtype=float32), array([[0.09757717]], dtype=float32), array([[0.15485685]], dtype=float32), array([[0.171535]], dtype=float32), array([[0.24648479]], dtype=float32), array([[0.07129652]], dtype=float32), array([[0.1415666]], dtype=float32), array([[0.16476783]], dtype=float32), array([[0.17423753]], dtype=float32), array([[0.25885665]], dtype=float32), array([[0.22535852]], dtype=float32), array([[0.21863642]], dtype=float32), array([[0.24341413]], dtype=float32), array([[0.24180764]], dtype=float32), array([[0.05443109]], dtype=float32), array([[0.25574067]], dtype=float32), array([[0.39554456]], dtype=float32), array([[0.10457705]], dtype=float32), array([[-0.03573335]], dtype=float32), array([[0.2815001]], dtype=float32), array([[0.23321822]], dtype=float32), array([[0.28148964]], dtype=float32), array([[0.2719434]], dtype=float32), array([[0.10616025]], dtype=float32), array([[0.20598319]], dtype=float32), array([[0.24489075]], dtype=float32), array([[0.07136586]], dtype=float32), array([[0.09864637]], dtype=float32), array([[0.14482339]], dtype=float32), array([[0.22104082]], dtype=float32), array([[0.16279276]], dtype=float32), array([[0.11550885]], dtype=float32), array([[0.04717585]], dtype=float32), array([[0.1782108]], dtype=float32), array([[0.1091761]], dtype=float32), array([[0.27949694]], dtype=float32), array([[0.1775147]], dtype=float32), array([[-0.01803]], dtype=float32), array([[0.08254175]], dtype=float32), array([[0.24356031]], dtype=float32), array([[0.25074837]], dtype=float32), array([[0.22230065]], dtype=float32), array([[0.2603438]], dtype=float32), array([[0.11078724]], dtype=float32), array([[0.20892996]], dtype=float32), array([[0.11388636]], dtype=float32), array([[0.11034898]], dtype=float32), array([[0.37350157]], dtype=float32), array([[0.19967946]], dtype=float32), array([[0.11872506]], dtype=float32), array([[0.14191014]], dtype=float32), array([[0.16312169]], dtype=float32), array([[0.33257014]], dtype=float32), array([[0.15080546]], dtype=float32), array([[0.11777289]], dtype=float32), array([[0.22437188]], dtype=float32), array([[0.25884584]], dtype=float32), array([[0.07407303]], dtype=float32), array([[0.17967607]], dtype=float32), array([[0.04283855]], dtype=float32), array([[0.14825475]], dtype=float32), array([[0.23369057]], dtype=float32), array([[0.14850342]], dtype=float32), array([[0.16212472]], dtype=float32), array([[0.25588167]], dtype=float32), array([[0.04301075]], dtype=float32), array([[0.06107525]], dtype=float32), array([[0.021925]], dtype=float32), array([[0.0961723]], dtype=float32), array([[0.0523785]], dtype=float32), array([[0.13124561]], dtype=float32), array([[0.18259317]], dtype=float32), array([[0.29266858]], dtype=float32), array([[0.23719376]], dtype=float32), array([[0.1355836]], dtype=float32), array([[0.06260931]], dtype=float32), array([[0.36756158]], dtype=float32), array([[0.23114258]], dtype=float32), array([[0.12218803]], dtype=float32), array([[0.26002502]], dtype=float32), array([[0.13951051]], dtype=float32), array([[0.2828098]], dtype=float32), array([[0.23321858]], dtype=float32), array([[0.15070498]], dtype=float32), array([[0.19145843]], dtype=float32), array([[0.07318283]], dtype=float32), array([[0.09641147]], dtype=float32), array([[0.11072377]], dtype=float32), array([[0.06341849]], dtype=float32), array([[0.15990403]], dtype=float32), array([[0.20958838]], dtype=float32), array([[0.1947709]], dtype=float32), array([[0.35026833]], dtype=float32), array([[0.21847023]], dtype=float32), array([[0.16567335]], dtype=float32), array([[0.12911361]], dtype=float32), array([[0.12064603]], dtype=float32), array([[0.14413723]], dtype=float32), array([[0.00850099]], dtype=float32), array([[0.03631061]], dtype=float32), array([[0.33304417]], dtype=float32), array([[0.12752002]], dtype=float32), array([[0.11274913]], dtype=float32), array([[-0.02531834]], dtype=float32), array([[0.06250431]], dtype=float32), array([[0.09379493]], dtype=float32), array([[0.0186709]], dtype=float32), array([[0.23528837]], dtype=float32), array([[0.35471782]], dtype=float32), array([[0.24456678]], dtype=float32), array([[0.23140824]], dtype=float32), array([[0.2743477]], dtype=float32), array([[0.12910533]], dtype=float32), array([[0.25048888]], dtype=float32), array([[0.35032806]], dtype=float32), array([[0.26669228]], dtype=float32), array([[0.17596786]], dtype=float32), array([[0.2353544]], dtype=float32), array([[0.38433078]], dtype=float32), array([[0.21945572]], dtype=float32), array([[0.34866977]], dtype=float32), array([[0.13937278]], dtype=float32), array([[0.05475262]], dtype=float32), array([[0.27394894]], dtype=float32), array([[0.28991434]], dtype=float32), array([[0.03899546]], dtype=float32), array([[0.18766242]], dtype=float32), array([[0.19181836]], dtype=float32), array([[0.08821648]], dtype=float32), array([[0.09184946]], dtype=float32), array([[0.05141712]], dtype=float32), array([[0.2576943]], dtype=float32), array([[0.20617451]], dtype=float32), array([[0.25533235]], dtype=float32), array([[0.2518394]], dtype=float32), array([[0.16407305]], dtype=float32), array([[0.19060934]], dtype=float32), array([[-0.01683854]], dtype=float32), array([[0.08428527]], dtype=float32), array([[-0.05349233]], dtype=float32), array([[0.0363764]], dtype=float32), array([[0.17153892]], dtype=float32), array([[0.12213001]], dtype=float32), array([[-0.01226773]], dtype=float32), array([[0.01962886]], dtype=float32), array([[0.026292]], dtype=float32), array([[0.07794173]], dtype=float32), array([[0.19143325]], dtype=float32), array([[0.13179643]], dtype=float32), array([[0.11828706]], dtype=float32), array([[0.09897376]], dtype=float32), array([[0.04702638]], dtype=float32), array([[0.01568821]], dtype=float32), array([[0.41451856]], dtype=float32), array([[0.12900405]], dtype=float32), array([[0.10024573]], dtype=float32), array([[0.03778165]], dtype=float32), array([[0.1412958]], dtype=float32), array([[0.11441775]], dtype=float32), array([[0.06611134]], dtype=float32), array([[-0.0961149]], dtype=float32), array([[0.08344527]], dtype=float32), array([[0.04359251]], dtype=float32), array([[0.13119827]], dtype=float32), array([[0.24866791]], dtype=float32), array([[0.29948562]], dtype=float32), array([[0.2946594]], dtype=float32), array([[0.23225468]], dtype=float32), array([[0.1878271]], dtype=float32), array([[0.18217127]], dtype=float32), array([[0.2576171]], dtype=float32), array([[0.27539742]], dtype=float32), array([[0.34649134]], dtype=float32), array([[0.21182182]], dtype=float32), array([[0.25861722]], dtype=float32), array([[0.18596768]], dtype=float32), array([[0.15780844]], dtype=float32), array([[0.03019626]], dtype=float32), array([[0.08029462]], dtype=float32), array([[0.00868545]], dtype=float32), array([[0.04566012]], dtype=float32), array([[-0.02550997]], dtype=float32), array([[0.03959796]], dtype=float32), array([[0.00112948]], dtype=float32), array([[0.00847619]], dtype=float32), array([[0.0368529]], dtype=float32), array([[0.09764456]], dtype=float32), array([[0.16845587]], dtype=float32), array([[0.02115297]], dtype=float32), array([[0.13022579]], dtype=float32), array([[0.13441703]], dtype=float32), array([[0.15028828]], dtype=float32), array([[0.09276922]], dtype=float32), array([[0.17376797]], dtype=float32), array([[0.1585243]], dtype=float32), array([[0.17640644]], dtype=float32), array([[0.09355669]], dtype=float32), array([[0.2491426]], dtype=float32), array([[0.18942825]], dtype=float32), array([[0.25041863]], dtype=float32), array([[0.40324563]], dtype=float32), array([[0.18533678]], dtype=float32), array([[0.16141525]], dtype=float32), array([[0.16201614]], dtype=float32), array([[0.22022219]], dtype=float32), array([[0.10463305]], dtype=float32), array([[0.29286242]], dtype=float32), array([[0.15756121]], dtype=float32), array([[0.01073092]], dtype=float32), array([[-0.00384126]], dtype=float32), array([[0.10753272]], dtype=float32), array([[0.2900871]], dtype=float32), array([[0.389005]], dtype=float32), array([[0.39439282]], dtype=float32), array([[0.08362512]], dtype=float32), array([[0.07117213]], dtype=float32), array([[0.00951871]], dtype=float32), array([[0.27112377]], dtype=float32), array([[0.12919419]], dtype=float32), array([[0.22376975]], dtype=float32), array([[0.18306054]], dtype=float32), array([[0.16382621]], dtype=float32), array([[0.31317264]], dtype=float32), array([[0.08836545]], dtype=float32), array([[0.12869868]], dtype=float32), array([[0.11930028]], dtype=float32), array([[0.26345098]], dtype=float32), array([[0.33728692]], dtype=float32), array([[0.25471616]], dtype=float32), array([[0.23741232]], dtype=float32), array([[0.22068393]], dtype=float32), array([[0.3364555]], dtype=float32), array([[0.27003187]], dtype=float32), array([[0.2270424]], dtype=float32), array([[0.1860567]], dtype=float32), array([[0.30195698]], dtype=float32), array([[0.04589152]], dtype=float32), array([[0.09262988]], dtype=float32), array([[0.12572382]], dtype=float32), array([[0.08090026]], dtype=float32), array([[0.31216034]], dtype=float32), array([[0.06917112]], dtype=float32), array([[0.04926757]], dtype=float32), array([[0.14082934]], dtype=float32), array([[0.3361331]], dtype=float32), array([[0.24396724]], dtype=float32), array([[0.21590495]], dtype=float32), array([[0.24922547]], dtype=float32), array([[0.03334378]], dtype=float32), array([[0.02485418]], dtype=float32), array([[0.01553468]], dtype=float32), array([[0.08001649]], dtype=float32), array([[0.23072733]], dtype=float32), array([[0.18712227]], dtype=float32), array([[0.0288072]], dtype=float32), array([[0.18128012]], dtype=float32), array([[0.10401475]], dtype=float32), array([[0.10847809]], dtype=float32), array([[0.2996598]], dtype=float32), array([[0.36793143]], dtype=float32), array([[0.37274468]], dtype=float32), array([[0.44491202]], dtype=float32), array([[0.39253813]], dtype=float32), array([[0.17613609]], dtype=float32), array([[0.33154103]], dtype=float32), array([[0.18832569]], dtype=float32), array([[0.24496451]], dtype=float32), array([[0.07983908]], dtype=float32), array([[0.05866341]], dtype=float32), array([[0.23987973]], dtype=float32), array([[0.23541392]], dtype=float32), array([[0.09491203]], dtype=float32), array([[0.27588433]], dtype=float32), array([[-0.03803354]], dtype=float32), array([[0.1136308]], dtype=float32), array([[0.06113736]], dtype=float32), array([[0.08730055]], dtype=float32), array([[0.08949715]], dtype=float32), array([[0.15068482]], dtype=float32), array([[0.19003424]], dtype=float32), array([[-0.00452834]], dtype=float32), array([[0.01668729]], dtype=float32), array([[0.16983607]], dtype=float32), array([[0.06929737]], dtype=float32), array([[0.13336034]], dtype=float32), array([[0.12059717]], dtype=float32), array([[0.18616682]], dtype=float32), array([[0.1070742]], dtype=float32), array([[0.2929072]], dtype=float32), array([[0.12710336]], dtype=float32), array([[0.08896501]], dtype=float32), array([[0.11004198]], dtype=float32), array([[0.09499971]], dtype=float32), array([[0.01383568]], dtype=float32), array([[0.08650891]], dtype=float32), array([[-0.01517805]], dtype=float32), array([[0.36149988]], dtype=float32), array([[0.01599129]], dtype=float32), array([[0.13565102]], dtype=float32), array([[0.23587872]], dtype=float32), array([[0.30550712]], dtype=float32), array([[0.1575397]], dtype=float32), array([[0.22645038]], dtype=float32), array([[0.09904808]], dtype=float32), array([[0.18612494]], dtype=float32), array([[0.24165043]], dtype=float32), array([[0.3183846]], dtype=float32), array([[0.15722144]], dtype=float32), array([[0.11765486]], dtype=float32), array([[0.14342782]], dtype=float32), array([[0.1263253]], dtype=float32), array([[0.10875002]], dtype=float32), array([[0.09069589]], dtype=float32), array([[0.18487132]], dtype=float32), array([[0.30501273]], dtype=float32), array([[0.25625736]], dtype=float32), array([[0.33320954]], dtype=float32), array([[0.27436203]], dtype=float32), array([[0.24049446]], dtype=float32), array([[0.2647335]], dtype=float32), array([[0.3086782]], dtype=float32), array([[0.10433339]], dtype=float32), array([[0.24918886]], dtype=float32), array([[0.15351546]], dtype=float32), array([[0.04737448]], dtype=float32), array([[0.17340748]], dtype=float32), array([[0.12087297]], dtype=float32), array([[0.31617036]], dtype=float32), array([[0.20687294]], dtype=float32), array([[0.37348926]], dtype=float32), array([[0.27360895]], dtype=float32), array([[0.22598793]], dtype=float32), array([[0.11444215]], dtype=float32), array([[0.23537111]], dtype=float32), array([[0.25085565]], dtype=float32), array([[0.24727966]], dtype=float32), array([[0.175396]], dtype=float32), array([[0.13766623]], dtype=float32), array([[0.18377951]], dtype=float32), array([[0.22205007]], dtype=float32), array([[0.23523708]], dtype=float32), array([[0.33810288]], dtype=float32), array([[0.33438474]], dtype=float32), array([[0.21050182]], dtype=float32), array([[0.18181665]], dtype=float32), array([[0.30455709]], dtype=float32), array([[0.2896093]], dtype=float32), array([[0.15917736]], dtype=float32), array([[0.30402017]], dtype=float32), array([[0.24018247]], dtype=float32), array([[0.1467464]], dtype=float32), array([[0.22678676]], dtype=float32), array([[0.05526647]], dtype=float32), array([[0.11693724]], dtype=float32), array([[0.10686549]], dtype=float32), array([[0.10229753]], dtype=float32), array([[0.02613616]], dtype=float32), array([[0.12816438]], dtype=float32), array([[0.12866955]], dtype=float32), array([[0.1864543]], dtype=float32), array([[0.201078]], dtype=float32), array([[0.39028293]], dtype=float32), array([[0.18366848]], dtype=float32), array([[0.19863597]], dtype=float32), array([[0.17824082]], dtype=float32), array([[-0.05898114]], dtype=float32), array([[0.14476475]], dtype=float32), array([[0.06384315]], dtype=float32), array([[0.21386111]], dtype=float32), array([[0.12219734]], dtype=float32), array([[0.15822877]], dtype=float32), array([[0.00893616]], dtype=float32), array([[0.42546922]], dtype=float32), array([[0.69510096]], dtype=float32), array([[0.4948956]], dtype=float32), array([[0.4199226]], dtype=float32), array([[0.73914695]], dtype=float32), array([[0.5397396]], dtype=float32), array([[0.17794678]], dtype=float32), array([[0.63025707]], dtype=float32), array([[0.1076322]], dtype=float32), array([[0.7724323]], dtype=float32), array([[0.46085423]], dtype=float32)]\n",
            "Mean similarity:0.17600269615650177, Max similarity: 0.7724323272705078, min:-0.09611489623785019 Count: 1039\n",
            "Queries average cosine similarities: [0.1760027], mean of means: 0.17600269615650177\n",
            "Macro mean: 0.1760\n"
          ]
        }
      ],
      "source": [
        "final_results = process_queries(test_queries, user_profiles)\n",
        "#print(final_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2WMhBMCDbNa",
        "outputId": "552da24e-302b-46c3-9efc-8dee14e4aeaf",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final query results saved to final_keywords_results.json\n"
          ]
        }
      ],
      "source": [
        "# Process queries\n",
        "#final_results = process_queries1(test_queries, user_profiles)\n",
        "final_results = process_queries(test_queries, user_profiles)\n",
        "\n",
        "# Save final results to JSON\n",
        "with open(\"final_keywords_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(final_results, f, indent=4)\n",
        "\n",
        "print(\"Final query results saved to final_keywords_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7q9B10R_WGN",
        "outputId": "70c08f9d-0a0e-4b5f-be1f-f0c7f8aa5b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision (MAP): 0.0544\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compute Mean Average Precision (MAP) for top 100 documents\n",
        "def mean_average_precision(final_query_results):\n",
        "    average_precisions = []\n",
        "\n",
        "    for query_id, result in final_query_results.items():\n",
        "        retrieved_docs = [doc[\"docID\"] for doc in result[\"sorted_CandiList\"]][:100]\n",
        "        relevant_docs = set(result[\"original_rel_doc_ids\"])\n",
        "\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "\n",
        "        precision_sum = 0\n",
        "        relevant_found = 0\n",
        "\n",
        "        for i, doc in enumerate(retrieved_docs):\n",
        "            if doc in relevant_docs:\n",
        "                relevant_found += 1\n",
        "                precision_at_i = relevant_found / (i + 1)\n",
        "                precision_sum += precision_at_i\n",
        "\n",
        "        ap = precision_sum / len(relevant_docs) if relevant_docs else 0\n",
        "        average_precisions.append(ap)\n",
        "\n",
        "    return np.mean(average_precisions) if average_precisions else 0\n",
        "\n",
        "# Compute MAP\n",
        "map_score = mean_average_precision(final_results)\n",
        "\n",
        "# Print MAP result\n",
        "print(f\"Mean Average Precision (MAP): {map_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBlvwcUs9IIp"
      },
      "source": [
        "# Results and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-Ng2fI0Ck7k",
        "outputId": "6c13914a-7ba9-48dc-ec68-b2f9dd755c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@20 (average): 0.0632\n",
            "Mean Reciprocal Rank (MRR): 0.3202\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load final query results\n",
        "with open(\"final_keywords_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    final_query_results = json.load(f)\n",
        "\n",
        "def precision_at_k(retrieved_docs, relevant_docs, k=20):\n",
        "    top_k_retrieved = retrieved_docs[:k]\n",
        "    relevant_count = sum(1 for doc in top_k_retrieved if doc in relevant_docs)\n",
        "    return relevant_count / k\n",
        "\n",
        "# Compute Reciprocal Rank (RR)\n",
        "def reciprocal_rank(retrieved_docs, relevant_docs):\n",
        "    for i, doc in enumerate(retrieved_docs[:10]):  # RR considers top-10\n",
        "        if doc in relevant_docs:\n",
        "            return 1 / (i + 1)\n",
        "    return 0  # No relevant document found in top 10\n",
        "\n",
        "# Evaluate Precision@20 and MRR across all queries\n",
        "precisions = []\n",
        "reciprocal_ranks = []\n",
        "\n",
        "for query_id, result in final_query_results.items():\n",
        "    retrieved_docs = [doc[\"docID\"] for doc in result[\"sorted_CandiList\"]]\n",
        "    relevant_docs = set(result[\"original_rel_doc_ids\"])\n",
        "\n",
        "    # Compute metrics\n",
        "    p20 = precision_at_k(retrieved_docs, relevant_docs, k=20)\n",
        "    rr = reciprocal_rank(retrieved_docs, relevant_docs)\n",
        "\n",
        "    precisions.append(p20)\n",
        "    reciprocal_ranks.append(rr)\n",
        "\n",
        "# Compute overall statistics\n",
        "mean_precision_20 = np.mean(precisions)\n",
        "mean_mrr = np.mean(reciprocal_ranks)\n",
        "\n",
        "# Print results\n",
        "print(f\"Precision@20 (average): {mean_precision_20:.4f}\")\n",
        "print(f\"Mean Reciprocal Rank (MRR): {mean_mrr:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e501fa4e575439398fc060c912649b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1916431cac94c2c9159e5db76d76cb8",
              "IPY_MODEL_98ad3f183267462aa28a2b0d5bfa2b9e",
              "IPY_MODEL_e65e402ca7a844898950944fa93a03b8"
            ],
            "layout": "IPY_MODEL_4bd2c4cf05a841ba9d67548250d0176b"
          }
        },
        "b1916431cac94c2c9159e5db76d76cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6abb15809ee49a0bdebe6c521c87e58",
            "placeholder": "​",
            "style": "IPY_MODEL_a03c6b9f60d64bb392b15cd161392bf2",
            "value": "artifact.metadata: 100%"
          }
        },
        "98ad3f183267462aa28a2b0d5bfa2b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f3e6b6e7c844fa38532b53f0675b5d2",
            "max": 1633,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90bd898753a947ecafaff8ca9300781e",
            "value": 1633
          }
        },
        "e65e402ca7a844898950944fa93a03b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c225aea8f194c1fb81128e7e98a4530",
            "placeholder": "​",
            "style": "IPY_MODEL_2cb6e6c0db874823b6013ba8bd07ce66",
            "value": " 1.63k/1.63k [00:00&lt;00:00, 48.5kB/s]"
          }
        },
        "4bd2c4cf05a841ba9d67548250d0176b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6abb15809ee49a0bdebe6c521c87e58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a03c6b9f60d64bb392b15cd161392bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f3e6b6e7c844fa38532b53f0675b5d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90bd898753a947ecafaff8ca9300781e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c225aea8f194c1fb81128e7e98a4530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cb6e6c0db874823b6013ba8bd07ce66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b324cead5d914b6e836e1ce1bed5bfff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36c8a996c9e14ae197776548a5fd31df",
              "IPY_MODEL_d3af08638a454ab6829746a901494578",
              "IPY_MODEL_6a1ebfc60ef248f59612bf9631baee4d"
            ],
            "layout": "IPY_MODEL_3afd6b15fb4c4970b445ee2a7e6130b7"
          }
        },
        "36c8a996c9e14ae197776548a5fd31df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfc43905d92f4b878f1524906295231b",
            "placeholder": "​",
            "style": "IPY_MODEL_9bf0e035ba184a5fafcd57aacab2a851",
            "value": "config.json: 100%"
          }
        },
        "d3af08638a454ab6829746a901494578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ca68e0e2dce48c3900a6770a4e524f0",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9eb5e71513e4400ebb7b071dba8bd880",
            "value": 743
          }
        },
        "6a1ebfc60ef248f59612bf9631baee4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bab35a255de4638b95bd373ddef2d34",
            "placeholder": "​",
            "style": "IPY_MODEL_df413cb0a2104891847c41360173e057",
            "value": " 743/743 [00:00&lt;00:00, 55.4kB/s]"
          }
        },
        "3afd6b15fb4c4970b445ee2a7e6130b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfc43905d92f4b878f1524906295231b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bf0e035ba184a5fafcd57aacab2a851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ca68e0e2dce48c3900a6770a4e524f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eb5e71513e4400ebb7b071dba8bd880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bab35a255de4638b95bd373ddef2d34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df413cb0a2104891847c41360173e057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afd20f988f454e598941f54f1c55c1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92a5afd444a94513b6b95d21bf79a8e1",
              "IPY_MODEL_04c95094b6c14bc6811b0a52e3d0ee0d",
              "IPY_MODEL_4a2e5004a3094c5d8db6afbcc35e2cfc"
            ],
            "layout": "IPY_MODEL_ae470a3f348f4342a9df26a3e29167ab"
          }
        },
        "92a5afd444a94513b6b95d21bf79a8e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f86b6e2fb924472b1ed88380df96b18",
            "placeholder": "​",
            "style": "IPY_MODEL_a4cb158700f5462e8d16a92d4aa73a4f",
            "value": "model.safetensors: 100%"
          }
        },
        "04c95094b6c14bc6811b0a52e3d0ee0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a44b8e82126c47a998385cc1a083e433",
            "max": 438349816,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b34f7a856380418fad6b31b2ef296f60",
            "value": 438349816
          }
        },
        "4a2e5004a3094c5d8db6afbcc35e2cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac28c379700047b78efa4bf7aa1b7abe",
            "placeholder": "​",
            "style": "IPY_MODEL_4704667bd5d54e8aa4234c7e456d9a99",
            "value": " 438M/438M [00:03&lt;00:00, 129MB/s]"
          }
        },
        "ae470a3f348f4342a9df26a3e29167ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f86b6e2fb924472b1ed88380df96b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4cb158700f5462e8d16a92d4aa73a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a44b8e82126c47a998385cc1a083e433": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b34f7a856380418fad6b31b2ef296f60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac28c379700047b78efa4bf7aa1b7abe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4704667bd5d54e8aa4234c7e456d9a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d6ca87f55c2483c8f0e6c96905fb227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_840d25354d1549e58bcf9f0d94078ac8",
              "IPY_MODEL_6d01e4dfc63644dab3b8d9ff316e0afa",
              "IPY_MODEL_5638f9c221d54983aed87b6f158d4e6e"
            ],
            "layout": "IPY_MODEL_5c67c00d6bc14f269e72e8840d2779e6"
          }
        },
        "840d25354d1549e58bcf9f0d94078ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7b2578dcd794d248626cfbd880c50dc",
            "placeholder": "​",
            "style": "IPY_MODEL_d02b28af06ff4008ab2b5e7929bd4137",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6d01e4dfc63644dab3b8d9ff316e0afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6aa18fb275f44b569a4e96743b15a5cb",
            "max": 405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e16cae277e5743b18bf215ae8dfb6314",
            "value": 405
          }
        },
        "5638f9c221d54983aed87b6f158d4e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80513a6c1777440ab331d3b5a6f5834f",
            "placeholder": "​",
            "style": "IPY_MODEL_05f1916e2c1a472fbca9a907f7a2cfe2",
            "value": " 405/405 [00:00&lt;00:00, 30.8kB/s]"
          }
        },
        "5c67c00d6bc14f269e72e8840d2779e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7b2578dcd794d248626cfbd880c50dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d02b28af06ff4008ab2b5e7929bd4137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6aa18fb275f44b569a4e96743b15a5cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e16cae277e5743b18bf215ae8dfb6314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80513a6c1777440ab331d3b5a6f5834f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05f1916e2c1a472fbca9a907f7a2cfe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba9e773f3a864f438dcfe3bf5296e521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_604582f442554eef861a8ca95f22cf29",
              "IPY_MODEL_78a0a05e57a34ed3b7d71369307e9617",
              "IPY_MODEL_e1998397075c4da193b4f1293c1231f2"
            ],
            "layout": "IPY_MODEL_a2f4229d5c3c43c29992994602b402f2"
          }
        },
        "604582f442554eef861a8ca95f22cf29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49b62e2727684d7baa930e9d24e43052",
            "placeholder": "​",
            "style": "IPY_MODEL_60929da74ba04fbf909d80db44e43714",
            "value": "vocab.txt: 100%"
          }
        },
        "78a0a05e57a34ed3b7d71369307e9617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3099ff8ac3bc496188b5d97eadcaba67",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ae694d7601949a787313f7157e664a2",
            "value": 231508
          }
        },
        "e1998397075c4da193b4f1293c1231f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d29f22283eb748a5ac24d08af3ebb68c",
            "placeholder": "​",
            "style": "IPY_MODEL_152bd15c232249f2800e016f264b9fc6",
            "value": " 232k/232k [00:00&lt;00:00, 1.41MB/s]"
          }
        },
        "a2f4229d5c3c43c29992994602b402f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49b62e2727684d7baa930e9d24e43052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60929da74ba04fbf909d80db44e43714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3099ff8ac3bc496188b5d97eadcaba67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ae694d7601949a787313f7157e664a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d29f22283eb748a5ac24d08af3ebb68c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152bd15c232249f2800e016f264b9fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "948c68f8e17449fea1bcfaf56cfbac78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7852479fa56f41b0a337e2753c0ef167",
              "IPY_MODEL_8adbfb374b6e41b599a81188e5dede67",
              "IPY_MODEL_d3bd4b6b399e47da97931ca624e66de8"
            ],
            "layout": "IPY_MODEL_510959e4f2f9478d8f1455107f7bf790"
          }
        },
        "7852479fa56f41b0a337e2753c0ef167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_639cbb2c753b472db88536c19a6d2beb",
            "placeholder": "​",
            "style": "IPY_MODEL_d11a4d12be1949b38af153535e891b6e",
            "value": "tokenizer.json: 100%"
          }
        },
        "8adbfb374b6e41b599a81188e5dede67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef7ad9a97f9e4f909a5a2888c9b6ef93",
            "max": 466081,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_367918d1de85452389541f507c73b42c",
            "value": 466081
          }
        },
        "d3bd4b6b399e47da97931ca624e66de8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e71e43d786f743058b34e7515b072dbe",
            "placeholder": "​",
            "style": "IPY_MODEL_d0266b33255e47e29a1c8bb1306d9a03",
            "value": " 466k/466k [00:00&lt;00:00, 5.55MB/s]"
          }
        },
        "510959e4f2f9478d8f1455107f7bf790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "639cbb2c753b472db88536c19a6d2beb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d11a4d12be1949b38af153535e891b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef7ad9a97f9e4f909a5a2888c9b6ef93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "367918d1de85452389541f507c73b42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e71e43d786f743058b34e7515b072dbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0266b33255e47e29a1c8bb1306d9a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44b1bf96d5334d0ba17d59633c1145bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c3cf946e3dd461fa472adf5a90b482d",
              "IPY_MODEL_13d782ef3d3b4d0d9521af0124d81a58",
              "IPY_MODEL_98b7f292364742f2907b3ecabdfb8b1d"
            ],
            "layout": "IPY_MODEL_600388b80d2c4090ac5d8ee573aa8715"
          }
        },
        "2c3cf946e3dd461fa472adf5a90b482d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8e3aac63feb4a49b47b6b064c95948b",
            "placeholder": "​",
            "style": "IPY_MODEL_58bc23b25ed145a28c8469fa2324ecc5",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "13d782ef3d3b4d0d9521af0124d81a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed42a47db7384a168814c878ebf21eab",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29718f8b5b93491ba4dcbc6d942a183c",
            "value": 112
          }
        },
        "98b7f292364742f2907b3ecabdfb8b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_921dffef62f34e64949a3caee34f1254",
            "placeholder": "​",
            "style": "IPY_MODEL_a616e2943d7c4db9bfc02bdc8e98e2b7",
            "value": " 112/112 [00:00&lt;00:00, 10.2kB/s]"
          }
        },
        "600388b80d2c4090ac5d8ee573aa8715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8e3aac63feb4a49b47b6b064c95948b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58bc23b25ed145a28c8469fa2324ecc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed42a47db7384a168814c878ebf21eab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29718f8b5b93491ba4dcbc6d942a183c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "921dffef62f34e64949a3caee34f1254": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a616e2943d7c4db9bfc02bdc8e98e2b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba831eb36c674873a9dba49c37b87516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9f1a6f96f814b269a40fe1454d8e505",
              "IPY_MODEL_8e461d3366dc40a5a80933ae72b4b683",
              "IPY_MODEL_08b2ac974b154a8b94c76e7e1fb10c7b"
            ],
            "layout": "IPY_MODEL_e7f1986eea8244c6a88251a388841d24"
          }
        },
        "c9f1a6f96f814b269a40fe1454d8e505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0073228917f74302baf43efe5bd742c1",
            "placeholder": "​",
            "style": "IPY_MODEL_3fc3fd306cc14102876836b222439e22",
            "value": "modules.json: 100%"
          }
        },
        "8e461d3366dc40a5a80933ae72b4b683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_796743bff4c145e8bc643e7fabe6c4b4",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c67841f8750e4e6eb2ba03a7e232977a",
            "value": 349
          }
        },
        "08b2ac974b154a8b94c76e7e1fb10c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_628784e6fc6040088c5c0e96db410a9a",
            "placeholder": "​",
            "style": "IPY_MODEL_34f5ce3a01d04ebaadadd877322b2ba5",
            "value": " 349/349 [00:00&lt;00:00, 24.2kB/s]"
          }
        },
        "e7f1986eea8244c6a88251a388841d24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0073228917f74302baf43efe5bd742c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fc3fd306cc14102876836b222439e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "796743bff4c145e8bc643e7fabe6c4b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c67841f8750e4e6eb2ba03a7e232977a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "628784e6fc6040088c5c0e96db410a9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34f5ce3a01d04ebaadadd877322b2ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d354a6958da8429d8d6d40e83b4f99e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c3d77ed74f74088a851450565228434",
              "IPY_MODEL_d2c26d7e72c24de6ac217de2393b29af",
              "IPY_MODEL_fcc0e679160c48c791e1b154de7e5a4a"
            ],
            "layout": "IPY_MODEL_7e1f3c2a7c3e450e8131067c209f736c"
          }
        },
        "2c3d77ed74f74088a851450565228434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f6d9c1e6e5e427d82165a295ef958ea",
            "placeholder": "​",
            "style": "IPY_MODEL_fd3fbee31bc542eb813618e6b309b672",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "d2c26d7e72c24de6ac217de2393b29af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95e220dd003d45609b641c8258d1d834",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fed2e60c5294198825a7ac44ed5f805",
            "value": 116
          }
        },
        "fcc0e679160c48c791e1b154de7e5a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4709de63700f41bdb9103cd01f802a35",
            "placeholder": "​",
            "style": "IPY_MODEL_93b45e4bebfe48978fb1aed1a37031c5",
            "value": " 116/116 [00:00&lt;00:00, 4.98kB/s]"
          }
        },
        "7e1f3c2a7c3e450e8131067c209f736c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f6d9c1e6e5e427d82165a295ef958ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd3fbee31bc542eb813618e6b309b672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95e220dd003d45609b641c8258d1d834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fed2e60c5294198825a7ac44ed5f805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4709de63700f41bdb9103cd01f802a35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93b45e4bebfe48978fb1aed1a37031c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4802459c87224ca684a32a9c812a5271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_561ddd8f8809442b8d6d618880762478",
              "IPY_MODEL_480fc22a3da549eaa702615177988883",
              "IPY_MODEL_f5c387a6e98740a885112a364a4c7324"
            ],
            "layout": "IPY_MODEL_8f56473d2dcf4903aed92520cb4cb0f0"
          }
        },
        "561ddd8f8809442b8d6d618880762478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a113adf82ab042c3b96ecb019833d73a",
            "placeholder": "​",
            "style": "IPY_MODEL_70ffe74aad3e4a66b22d92e2811cc210",
            "value": "README.md: 100%"
          }
        },
        "480fc22a3da549eaa702615177988883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2825d3220c2244f9a0fba8ad0e605b05",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5551100c0c946f09ee413f30f0128ee",
            "value": 10659
          }
        },
        "f5c387a6e98740a885112a364a4c7324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef0b491aec8f46dcae24062f7299bea3",
            "placeholder": "​",
            "style": "IPY_MODEL_31e5ed96cffa400980bf37ed1e07e03d",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 948kB/s]"
          }
        },
        "8f56473d2dcf4903aed92520cb4cb0f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a113adf82ab042c3b96ecb019833d73a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70ffe74aad3e4a66b22d92e2811cc210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2825d3220c2244f9a0fba8ad0e605b05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5551100c0c946f09ee413f30f0128ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef0b491aec8f46dcae24062f7299bea3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31e5ed96cffa400980bf37ed1e07e03d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6294e09067304806aad4d62d5167c0db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_949075a702f14156a18b82b7a934880f",
              "IPY_MODEL_f278812d0fe0402a949fa37e76e461e1",
              "IPY_MODEL_855792a81703413d8bc9a20ed30fb694"
            ],
            "layout": "IPY_MODEL_fa218d35e8d34ede888d21450d806c1d"
          }
        },
        "949075a702f14156a18b82b7a934880f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90e9008381e949368f443950016a3e7f",
            "placeholder": "​",
            "style": "IPY_MODEL_6a026e87706e4e6cada4b1cfeac71ef0",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "f278812d0fe0402a949fa37e76e461e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8cfaa787cf64b2abf9b0a1a9a15d202",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05ddf94a80d040d2b52d8dc76f97e6fb",
            "value": 53
          }
        },
        "855792a81703413d8bc9a20ed30fb694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfbbc09512474f3d9052fd5d5a109f5b",
            "placeholder": "​",
            "style": "IPY_MODEL_49b17870790e417bb36c51f1b0956b06",
            "value": " 53.0/53.0 [00:00&lt;00:00, 101B/s]"
          }
        },
        "fa218d35e8d34ede888d21450d806c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90e9008381e949368f443950016a3e7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a026e87706e4e6cada4b1cfeac71ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8cfaa787cf64b2abf9b0a1a9a15d202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05ddf94a80d040d2b52d8dc76f97e6fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bfbbc09512474f3d9052fd5d5a109f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49b17870790e417bb36c51f1b0956b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02ca9f7e82de41b0996dee89301f58ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_083fe28644874503af36fb580146efd8",
              "IPY_MODEL_9d9fcfd961be427594c73e4ec7a0131d",
              "IPY_MODEL_b4ccef1f28f04d05994020abf812ed93"
            ],
            "layout": "IPY_MODEL_adc6f70a8990493bae7f8c4ce0427f59"
          }
        },
        "083fe28644874503af36fb580146efd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4fd6f36d2af4b55a6169e2b268f885a",
            "placeholder": "​",
            "style": "IPY_MODEL_88e400d29f6f4ca0a45986dcd068fc56",
            "value": "config.json: 100%"
          }
        },
        "9d9fcfd961be427594c73e4ec7a0131d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bfeeaf69caf44e687335acbe7c44b93",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43fbf16f96d243519f2bc5db11c3b3d0",
            "value": 612
          }
        },
        "b4ccef1f28f04d05994020abf812ed93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cb447f0d7f84c8eb434940aeb204b17",
            "placeholder": "​",
            "style": "IPY_MODEL_d32f3f43bb2e485aa1878edfc7b65451",
            "value": " 612/612 [00:00&lt;00:00, 50.8kB/s]"
          }
        },
        "adc6f70a8990493bae7f8c4ce0427f59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4fd6f36d2af4b55a6169e2b268f885a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88e400d29f6f4ca0a45986dcd068fc56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bfeeaf69caf44e687335acbe7c44b93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43fbf16f96d243519f2bc5db11c3b3d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3cb447f0d7f84c8eb434940aeb204b17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d32f3f43bb2e485aa1878edfc7b65451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3021e45ac2f4eb9be77f80a391d67b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7d95c36c7d843a7b9665188d7611344",
              "IPY_MODEL_e1e48e191c6647a08dfb993731bb1270",
              "IPY_MODEL_c61addd7f8474fb8afdaea011fbc1a7e"
            ],
            "layout": "IPY_MODEL_fe86cdf94ac34a90a20ea967f2364055"
          }
        },
        "e7d95c36c7d843a7b9665188d7611344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4532e355cef4cd1915eeacb00840526",
            "placeholder": "​",
            "style": "IPY_MODEL_9224edc8453642e7b98c1f7ba4e749bb",
            "value": "model.safetensors: 100%"
          }
        },
        "e1e48e191c6647a08dfb993731bb1270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfae027f81eb4a8db01e3c79e1c89b1d",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e890215a1d24408a8a4fd7a90eec24df",
            "value": 90868376
          }
        },
        "c61addd7f8474fb8afdaea011fbc1a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f125b9434e0469ca78b613ecb808f41",
            "placeholder": "​",
            "style": "IPY_MODEL_01fa5f5e50fa419382e7a416af7cad5c",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 206MB/s]"
          }
        },
        "fe86cdf94ac34a90a20ea967f2364055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4532e355cef4cd1915eeacb00840526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9224edc8453642e7b98c1f7ba4e749bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfae027f81eb4a8db01e3c79e1c89b1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e890215a1d24408a8a4fd7a90eec24df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f125b9434e0469ca78b613ecb808f41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01fa5f5e50fa419382e7a416af7cad5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25a0d656c40544c18b88cd8c27d8e8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d1cbdbd04ed4b9f9b5fceb7cf18f05e",
              "IPY_MODEL_e1f60a3318f948908f854b332090a59f",
              "IPY_MODEL_32103c2998cf4f83bd60befa5c32b453"
            ],
            "layout": "IPY_MODEL_7df5f630de714d8a8b0774ba3546b73c"
          }
        },
        "8d1cbdbd04ed4b9f9b5fceb7cf18f05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed42476645b043afa761caf4086e4d4e",
            "placeholder": "​",
            "style": "IPY_MODEL_3b7c81f445bb4b40ae648e6c8e1be187",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e1f60a3318f948908f854b332090a59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72dc1a59af99455a91f14eba83576f2e",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbbfc9f09407493c981d948cc787aba4",
            "value": 350
          }
        },
        "32103c2998cf4f83bd60befa5c32b453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9fac7ee5f2648db866b34f72b2283e2",
            "placeholder": "​",
            "style": "IPY_MODEL_d6c044900011400ca60573099a655ad9",
            "value": " 350/350 [00:00&lt;00:00, 22.8kB/s]"
          }
        },
        "7df5f630de714d8a8b0774ba3546b73c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed42476645b043afa761caf4086e4d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b7c81f445bb4b40ae648e6c8e1be187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72dc1a59af99455a91f14eba83576f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbbfc9f09407493c981d948cc787aba4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9fac7ee5f2648db866b34f72b2283e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6c044900011400ca60573099a655ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00d02863e63f448991794c7263eba5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eecc82cf4b02416c9411c4baeea42e2a",
              "IPY_MODEL_239c8b4809144937884ad86ab485c055",
              "IPY_MODEL_ab366659bdb246819743d3240dcdacdc"
            ],
            "layout": "IPY_MODEL_d8e083a8a22649e8bf63996040ada5b5"
          }
        },
        "eecc82cf4b02416c9411c4baeea42e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f354f5642754f18b0e1d44a40ad68f7",
            "placeholder": "​",
            "style": "IPY_MODEL_e551e5adc1fc4a40ab56e4cff122eec5",
            "value": "vocab.txt: 100%"
          }
        },
        "239c8b4809144937884ad86ab485c055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46cf0ebbfd9c4018ab89d8563f3c0c11",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f29646c4d9fe404db85721964c91a471",
            "value": 231508
          }
        },
        "ab366659bdb246819743d3240dcdacdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbf97e55f28a495ba04b66857e043bf8",
            "placeholder": "​",
            "style": "IPY_MODEL_5ca48d66f3d54712b7b2525fccd5cd90",
            "value": " 232k/232k [00:00&lt;00:00, 1.42MB/s]"
          }
        },
        "d8e083a8a22649e8bf63996040ada5b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f354f5642754f18b0e1d44a40ad68f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e551e5adc1fc4a40ab56e4cff122eec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46cf0ebbfd9c4018ab89d8563f3c0c11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f29646c4d9fe404db85721964c91a471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbf97e55f28a495ba04b66857e043bf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ca48d66f3d54712b7b2525fccd5cd90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "522c7a19c30245bbac4b5e6579c51de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0995c46b27d0452ab6bd71bb1d5b3031",
              "IPY_MODEL_1c0cbc48cd6e46cea569202269a5afd3",
              "IPY_MODEL_1700cb48b2c54da9bf3867053255e3e7"
            ],
            "layout": "IPY_MODEL_3c9001b697714a5a85da634627de8499"
          }
        },
        "0995c46b27d0452ab6bd71bb1d5b3031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7b54ce2928b4f8d8cb76c0b5d5eb92e",
            "placeholder": "​",
            "style": "IPY_MODEL_6bbf621b439242408ee996389aced477",
            "value": "tokenizer.json: 100%"
          }
        },
        "1c0cbc48cd6e46cea569202269a5afd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_992453be32c1484882717143783bc019",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9da7e941226461da105d20cac16eb4d",
            "value": 466247
          }
        },
        "1700cb48b2c54da9bf3867053255e3e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76e4aeb581be416384a4b0a16283c966",
            "placeholder": "​",
            "style": "IPY_MODEL_76025d88a2434dab8115197003405944",
            "value": " 466k/466k [00:00&lt;00:00, 5.55MB/s]"
          }
        },
        "3c9001b697714a5a85da634627de8499": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7b54ce2928b4f8d8cb76c0b5d5eb92e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bbf621b439242408ee996389aced477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "992453be32c1484882717143783bc019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9da7e941226461da105d20cac16eb4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76e4aeb581be416384a4b0a16283c966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76025d88a2434dab8115197003405944": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "feb0c1b61b32496c885c22be3dedf597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa610e02da954588b66a543bfef72e36",
              "IPY_MODEL_68ed3ef5ce054972bd857f08295f5ed7",
              "IPY_MODEL_f8d0a1d43e554dddb387182a1c53ca44"
            ],
            "layout": "IPY_MODEL_fc49d739553d4597862d996196955b0a"
          }
        },
        "aa610e02da954588b66a543bfef72e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86830aea2e994ff6964dd9e0c9b6ec01",
            "placeholder": "​",
            "style": "IPY_MODEL_6fcdaa7d4a97428d9c6e7c99e3e74741",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "68ed3ef5ce054972bd857f08295f5ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7c28855692e4bbe9e44ee801ed599c3",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8ee38eacf624790bae1978bf70723d9",
            "value": 112
          }
        },
        "f8d0a1d43e554dddb387182a1c53ca44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d46cadc1c41a421695c874f5fc7b718b",
            "placeholder": "​",
            "style": "IPY_MODEL_e62423b401564489b162093e5311bd9c",
            "value": " 112/112 [00:00&lt;00:00, 7.69kB/s]"
          }
        },
        "fc49d739553d4597862d996196955b0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86830aea2e994ff6964dd9e0c9b6ec01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fcdaa7d4a97428d9c6e7c99e3e74741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7c28855692e4bbe9e44ee801ed599c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ee38eacf624790bae1978bf70723d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d46cadc1c41a421695c874f5fc7b718b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e62423b401564489b162093e5311bd9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "081487afb0ab497691e9c717fab47834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f0710849bdd4e53bcbfe5cb58a8e248",
              "IPY_MODEL_28de1250114845079835585c6c90096a",
              "IPY_MODEL_159278146dcd43aa97b5e14cb60b8604"
            ],
            "layout": "IPY_MODEL_1d818ed00c224ec78dcd66e621628400"
          }
        },
        "1f0710849bdd4e53bcbfe5cb58a8e248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70182f690898491492baff709f7d594b",
            "placeholder": "​",
            "style": "IPY_MODEL_578427bb3ddc4ee5be6f37f50492a7be",
            "value": "1_Pooling%2Fconfig.json: 100%"
          }
        },
        "28de1250114845079835585c6c90096a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a13c66cb02ac45af9279c14a04cd9997",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58c5e065fac74d28ad8e7de41c4be977",
            "value": 190
          }
        },
        "159278146dcd43aa97b5e14cb60b8604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8931190f18b45c8b6d1bbd4a698c33e",
            "placeholder": "​",
            "style": "IPY_MODEL_a10b45553f8e411c80dbd8c897976ac1",
            "value": " 190/190 [00:00&lt;00:00, 4.56kB/s]"
          }
        },
        "1d818ed00c224ec78dcd66e621628400": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70182f690898491492baff709f7d594b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "578427bb3ddc4ee5be6f37f50492a7be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a13c66cb02ac45af9279c14a04cd9997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58c5e065fac74d28ad8e7de41c4be977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8931190f18b45c8b6d1bbd4a698c33e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a10b45553f8e411c80dbd8c897976ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}